{
  "paper": {
    "id": "2302.01318v1",
    "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
    "abstract": "We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.",
    "authors": [
      "Charlie Chen",
      "Sebastian Borgeaud",
      "Geoffrey Irving",
      "Jean-Baptiste Lespiau",
      "Laurent Sifre",
      "John Jumper"
    ],
    "published": "2023-02-02T18:44:11.000Z",
    "updated": "2023-02-02T18:44:11.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2302.01318v1",
    "absUrl": "https://arxiv.org/abs/2302.01318v1"
  },
  "chunks": [
    {
      "id": "2302.01318v1-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "chunkIndex": 0,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-1",
      "content": "Charlie Chen 1 , Sebastian Borgeaud 1 , Geoffrey Irving 1 , Jean-Baptiste Lespiau 1 , Laurent Sifre 1 and John Jumper 1\n\n1 All authors from DeepMind\n\nWe present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2 -2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "chunkIndex": 1,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-2",
      "content": "Scaling transformer models to 500B+ parameters has led to large performance improvements on many natural language, computer vision and reinforcement learning tasks (Arnab et al., 2021; Brown et al., 2020; Chowdhery et al., 2022; Dosovitskiy et al., 2020; Hoffmann et al., 2022; Rae et al., 2021). However, transformer decoding remains a highly costly and inefficient process in this regime.\n\nTransformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of hardware, the time to generate a single token in transformer models is proportional to a first order approximation to the size of parameters and the size of the transformer memory. The size of language models also necessitates serving with model parallelism - adding communication overheads (Pope et al., 2022) and multiplying resource requirements. Since each new token depends on the past, many such transformer calls are required to sample a new sequence.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Introduction",
        "chunkIndex": 2,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-3",
      "content": "- adding communication overheads (Pope et al., 2022) and multiplying resource requirements. Since each new token depends on the past, many such transformer calls are required to sample a new sequence.\n\nWe present an algorithm to accelerate transformer sampling for latency critical applications, which we call speculative sampling (SpS). This is achieved by:\n\n1. Generating a short draft of length ğ¾ . This can be attained with either a parallel model (Stern et al., 2018) or by calling a faster, auto-regressive model ğ¾ times. We shall refer to this model as the draft model , and focus on the case where it is auto-regressive.\n3. Using a modified rejection sampling scheme, accept a subset of the ğ¾ draft tokens from left to right, recovering the distribution of the target model in the process.\n2. Scoring the draft using the larger, more powerful model from we wish to sample from. We shall refer to this model as the target model .",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Introduction",
        "chunkIndex": 3,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-4",
      "content": "covering the distribution of the target model in the process.\n2. Scoring the draft using the larger, more powerful model from we wish to sample from. We shall refer to this model as the target model .\n\nIntuitively , there are often sequences where the next token might be 'obvious'. Therefore, if there is strong agreement between the draft and target model's distributions on a given token or sub-sequence of tokens, this setup permits the generation of multiple tokens each time the target model is called .\n\nWe show that the expected acceptance rate of draft tokens is sufficient to offset the overhead of the\n\ndrafting process for large language models, resulting in an effective and practical method for reducing sampling latency without the need for modifying the target model or biasing the sample distribution. Depending on the evaluation domain, SpS leads to a 2-2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; speedup when sampling from Chinchilla (Hoffmann et al., 2022).",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Introduction",
        "chunkIndex": 4,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-5",
      "content": "target model or biasing the sample distribution. Depending on the evaluation domain, SpS leads to a 2-2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; speedup when sampling from Chinchilla (Hoffmann et al., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised ceiling on auto-regressive sampling speed imposed by the memory bandwidth.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Introduction",
        "chunkIndex": 5,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-6",
      "content": "There has been a substantial body of work focused on improving sampling latency of large transformers and other auto-regressive models.\n\nSince sampling performance is heavily coupled with the model size in memory, quantisation to int8 or even int4 (Dettmers et al., 2022; Yao et al., 2022) and distillation (Jiao et al., 2020; Sanh et al., 2019) of transformers are effective techniques for reducing sampling latency with little to no performance penalty. The observation that model size contributes less to the final performance than expected (Hoffmann et al., 2022) has also encouraged smaller language models in general.\n\nDuring sampling, a cache of the keys and values is maintained for every attention layer, and could become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Related Work",
        "chunkIndex": 6,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-7",
      "content": "n layer, and could become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However these techniques are most effective at maximising throughout (at larger batch sizes) instead of latency, especially for larger models where the majority of the memory bandwidth budget is consumed by the parameters.\n\nUsing a combination of the above techniques, in addition to a number of low-level optimisations to TPUs, Pope et al. (2022) have greatly improved the serving latency and efficiency of PaLM 540B.\n\nThere is an existing body of similar work exploiting the efficiency of transformers and sequence models operating in parallel. This includes block parallel sampling (Stern et al., 2018), aggressive decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020).",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Related Work",
        "chunkIndex": 7,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-8",
      "content": "pling (Stern et al., 2018), aggressive decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be adapted to typical language model use-cases since they either only work with greedy sampling, bias the results or are focused on other modalities. Further, to our knowledge none of these techniques have been scaled to distributed setups, which is necessary for the most expensive decoders with the tens or hundreds of billions of parameters.\n\nCoincidentally, the work in this manuscript was undertaken concurrently and independently of the work on speculative decoding from Leviathan et al. (2022). We focus more heavily the distributed serving setting for large models and offer some incremental optimisations, but otherwise the core underlying idea is the same.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Related Work",
        "chunkIndex": 8,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-9",
      "content": "Whilst transformers can be trained efficiently and in parallel on TPUs and GPUs, samples are typically drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is highly memory bandwidth bound and thus cannot make effective use of modern accelerator hardware (Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the batch, hence generating multiple tokens introduces a large amount of latency in any system which makes use of it.\n\nThis is especially problematic as the number of parameters in the model increases. Since all the model parameters need to pass through at least one accelerator chip, the model size divided by the total memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling speed. Larger models also require serving on multiple accelerators, introducing a further source of latency due to inter-device communication overheads.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Auto-regressive Sampling",
        "chunkIndex": 9,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-10",
      "content": "```\nGiven auto-regressive target model ğ‘ ' GLYPH<147> j GLYPH<147> ' and initial prompt sequence ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘¡ and target sequence length ğ‘‡ . Initialise ğ‘› ğ‘¡ . while ğ‘› GLYPH<157> ğ‘‡ do Sample ğ‘¥ ğ‘› , 1 GLYPH<24> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› ' ğ‘› ğ‘› , 1 end while\n```",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Algorithm 1 Auto-regressive (ArS) with Auto-Regressive Models",
        "chunkIndex": 10,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-11",
      "content": "```\nGiven lookahead ğ¾ and minimum target sequence length ğ‘‡ . Given auto-regressive target model ğ‘ ' GLYPH<147> j GLYPH<147> ' , and auto-regressive draft model ğ‘ ' GLYPH<147> j GLYPH<147> ' , initial prompt sequence ğ‘¥ 0 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘¡ . Initialise ğ‘› ğ‘¡ . while ğ‘› GLYPH<157> ğ‘‡ do for ğ‘¡ = 1 : ğ¾ do Sample draft auto-regressively Ëœ ğ‘¥ ğ‘¡ GLYPH<24> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ‘¡ GLYPH<0> 1 ' end for In parallel, compute ğ¾ , 1 sets of logits from drafts Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ¾ : ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› ' GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 ' GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148>",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models",
        "chunkIndex": 11,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-12",
      "content": "ğ‘› ' GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 ' GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ¾ ' for ğ‘¡ = 1 : ğ¾ do Sample ğ‘Ÿ GLYPH<24> ğ‘ˆ Â» 0 GLYPH<148> 1 â€¦ from a uniform distribution. if ğ‘Ÿ GLYPH<157> min GLYPH<16> 1 GLYPH<148> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148>ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148>ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' GLYPH<17> , then Set ğ‘¥ ğ‘› , ğ‘¡ Ëœ ğ‘¥ ğ‘¡ and ğ‘› ğ‘› , 1. else sample ğ‘¥ ğ‘› , ğ‘¡ GLYPH<24> ' ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' GLYPH<0> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 '', and exit for loop.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models",
        "chunkIndex": 12,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-13",
      "content": "YPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' GLYPH<0> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 '', and exit for loop. end if end for If all tokens ğ‘¥ ğ‘› , 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ¾ are accepted, sample extra token ğ‘¥ ğ‘› , ğ¾ , 1 GLYPH<24> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> ğ‘¥ ğ‘› , ğ¾ ' and set ğ‘› ğ‘› , 1. end while\n```",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models",
        "chunkIndex": 13,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-14",
      "content": "For speculative sampling (See algorithm 2), we first make the observation that computing the logits of a short continuation of ğ¾ tokens in parallel has a very similar latency to that of sampling a single\n\ntoken. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et al., 2019). For these models the majority of sampling time can be attributed to three components:\n\n1. Linear Layers: For small batch sizes, each linear layer only processes a small number of embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys, values computations and the final attention projection to become memory bound. For small ğ¾ , this will continue to be memory bound and therefore take a similar amount of time.\n3. All-reduces: As models grow in size, its parameters need to be divided across multiple accelerators, leading to communication overheads.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Conditional Scoring",
        "chunkIndex": 14,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-15",
      "content": "be memory bound and therefore take a similar amount of time.\n3. All-reduces: As models grow in size, its parameters need to be divided across multiple accelerators, leading to communication overheads. With Megatron, this manifests itself as an all-reduce after every feed-forward and attention layer. Since only the activations for a small number of tokens are transmitted, this operation is typically latency bound instead of throughput bound for both sampling and scoring (for small ğ¾ ). Again, this results in a similar amount of time spent in the two cases.\n2. The Attention Mechanism: The attention mechanism is also memory bound. During sampling, we maintain a cache of all the keys and values of the previous tokens in the sequence to avoid re-computation. These KV-caches are large, and accounts for the majority of the memory bandwidth utilisation for the attention mechanism.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Conditional Scoring",
        "chunkIndex": 15,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-16",
      "content": "eys and values of the previous tokens in the sequence to avoid re-computation. These KV-caches are large, and accounts for the majority of the memory bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not change as we increase ğ¾ , there is little to no delta in this component.\n\nOther sources of overhead may exist, depending on the exact transformer implementation. Therefore it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring and sampling. However, if the conditions are met such that the above components dominate then scoring should not be significantly slower for small ğ¾ .",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Conditional Scoring",
        "chunkIndex": 16,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-17",
      "content": "We require a method to recover the distribution of the target model from samples from the draft model, and logits of said tokens from both models.\n\nTo achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a sequence of tokens ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› , and ğ¾ draft tokens Ëœ ğ‘¥ ğ‘› , 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; Ëœ ğ‘¥ ğ‘› , ğ¾ generated from ğ‘ ' GLYPH&lt;147&gt; j GLYPH&lt;147&gt; ' , we accept Ëœ ğ‘¥ ğ‘› , 1 with probability:\n\nWhere ğ‘ ' Ëœ ğ‘¥ ğ‘› , 1 j ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' and ğ‘ ' Ëœ ğ‘¥ ğ‘› , 1 j ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' are the probability of Ëœ ğ‘¥ ğ‘› , 1 according to the target and draft models respectively, conditioned on the context so far.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Modified Rejection Sampling",
        "chunkIndex": 17,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-18",
      "content": "gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' are the probability of Ëœ ğ‘¥ ğ‘› , 1 according to the target and draft models respectively, conditioned on the context so far.\n\n<!-- formula-not-decoded -->\n\nIf the token is accepted, we set ğ‘¥ ğ‘› , 1 Ëœ ğ‘¥ ğ‘› , 1 and repeat the process for Ëœ ğ‘¥ ğ‘› , 2 until either a token is rejected or all tokens have been accepted.\n\nIf Ëœ ğ‘¥ ğ‘› , 1 is rejected, we resample ğ‘¥ ğ‘› , 1 from the following distribution:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhere ' GLYPH&lt;147&gt; ', denotes:\n\nBy applying this sequentially, we recover the distribution of the target model for the accepted tokens (see proof in Theorem 1) within hardware numerics. Note that:\n\n- At least one token will always be generated from a draft-accept loop - if the first token is rejected, a valid token is resampled.\n- Since the final token of the draft gives us the logits for the next token, if every drafted token is accepted, we can sample from it normally.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Modified Rejection Sampling",
        "chunkIndex": 18,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-19",
      "content": "he first token is rejected, a valid token is resampled.\n- Since the final token of the draft gives us the logits for the next token, if every drafted token is accepted, we can sample from it normally. This gives us a maximum of ğ¾ , 1 tokens per loop, over the naive implementation which would only return ğ¾ tokens.\n\nWith standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we can modify the probabilities accordingly before applying this rejection sampling scheme. We have observed that the overall acceptance rate is robust to the exact parameters used.\n\nBecause we do not interact with the body of the transformer itself, this method can be used in conjunction many other techniques for accelerating or optimising the memory use of sampling, such as quantisation and multi-query attention.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Modified Rejection Sampling",
        "chunkIndex": 19,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-20",
      "content": "Since the acceptance criterion guarantees the distribution of the target model in our samples, we are free to choose the method for drafting a continuation as long as it exposes logits, and there is a high enough acceptance rate and/or low enough latency to break-even. There exist several approaches here:\n\n- Incorporating draft generation into the target model, and train the model from the start. This is the strategy used by Stern et al. (2018), which adds multiple heads into the transformer to generate multiple tokens.\n- Set a portion of the activations of the target model as an input to the draft model, and train the draft model with this input.\n- Using sequence level distillation (Kim and Rush, 2016) to generate a second model which predicts ğ¾ tokens in parallel. This strategy was employed by Ge et al. (2022).",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Choice of Draft Models",
        "chunkIndex": 20,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-21",
      "content": "e draft model with this input.\n- Using sequence level distillation (Kim and Rush, 2016) to generate a second model which predicts ğ¾ tokens in parallel. This strategy was employed by Ge et al. (2022).\n\nAlthough these methods will likely yield powerful drafts, they require a large number of data generated from the target model or changes to the target model. Sequence level distillation in particular would require a large compute budget. This makes them less practical for large scale applications.\n\nWhilst large language models produce better samples, intuitively there are \"easier\" tokens to predict for which smaller models may be sufficient. Therefore we may simply use a smaller version of the target language model as the draft and obtain high acceptance rates. This would also be convenient from an engineering and workflow perspective, since robust tooling for such models should already exist to train the target model in the first place.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Choice of Draft Models",
        "chunkIndex": 21,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-22",
      "content": "from an engineering and workflow perspective, since robust tooling for such models should already exist to train the target model in the first place.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Choice of Draft Models",
        "chunkIndex": 22,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-23",
      "content": "We train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s - the same hardware that is typically used to serve Chinchilla for research purposes. This model was trained with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8 layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token compared to 14.1ms/token for Chinchilla. For details, please refer to the hyperparameters in Table 2.\n\nFor distributed setups it is insufficient to naively choose a small model as the draft, since different models have different optimal inference setups. For example, it is typical to serve Chinchilla 70B\n\nTable 1 j Chinchilla performance and speed on XSum and HumanEval with naive and speculative sampling at batch size 1 and ğ¾ = 4. XSum was executed with nucleus parameter ğ‘ = 0 GLYPH&lt;147&gt; 8, and HumanEval with ğ‘ = 0 GLYPH&lt;147&gt; 95 and temperature 0 GLYPH&lt;147&gt; 8.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Results",
        "chunkIndex": 23,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-24",
      "content": "speculative sampling at batch size 1 and ğ¾ = 4. XSum was executed with nucleus parameter ğ‘ = 0 GLYPH&lt;147&gt; 8, and HumanEval with ğ‘ = 0 GLYPH&lt;147&gt; 95 and temperature 0 GLYPH&lt;147&gt; 8.\n\n| Sampling Method             | Benchmark            | Result      | Mean Token Time           | Speed Up                            |\n|-----------------------------|----------------------|-------------|---------------------------|-------------------------------------|\n| ArS (Nucleus) SpS (Nucleus) | XSum (ROUGE-2)       | 0.112 0.114 | 14.1ms/Token 7.52ms/Token | 1 GLYPH<2> 1 GLYPH<147> 92 GLYPH<2> |\n| ArS (Greedy) SpS (Greedy)   | XSum (ROUGE-2)       | 0.157 0.156 | 14.1ms/Token 7.00ms/Token | 1 GLYPH<2> 2 GLYPH<147> 01 GLYPH<2> |\n| ArS (Nucleus) SpS (Nucleus) | HumanEval (100 Shot) | 45.1% 47.0% | 14.1ms/Token 5.73ms/Token | 1 GLYPH<2> 2 GLYPH<147> 46 GLYPH<2> |",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Results",
        "chunkIndex": 24,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-25",
      "content": "14.1ms/Token 7.00ms/Token | 1 GLYPH<2> 2 GLYPH<147> 01 GLYPH<2> |\n| ArS (Nucleus) SpS (Nucleus) | HumanEval (100 Shot) | 45.1% 47.0% | 14.1ms/Token 5.73ms/Token | 1 GLYPH<2> 2 GLYPH<147> 46 GLYPH<2> |\n\non 16 TPU v4s (where it achieves the aforementioned 14.1ms/token ), whereas a chinchilla-optimal 7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token ). For smaller models, the additional memory bandwidth and flops are insufficient to offset the additional communication overhead between more devices - serving a 7B on 16 TPUs actually increases the latency. This means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and we will not make full utilisation of the hardware during drafting.\n\nWe can sidestep this issue by training a wider model with a relatively few number of layers in order to minimise communication overhead.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Results",
        "chunkIndex": 25,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-26",
      "content": "ill not make full utilisation of the hardware during drafting.\n\nWe can sidestep this issue by training a wider model with a relatively few number of layers in order to minimise communication overhead. It has been observed that the performance of language models is relatively robust to changes in model aspect ratio (Levine et al., 2020), so this allows us to serve a powerful draft model which can be sampled rapidly on the same hardware as the target model.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Results",
        "chunkIndex": 26,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-27",
      "content": "We evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:\n\n- The XSum (Narayan et al., 2018) benchmark. This is a natural language summarisation task using a 1-shot prompt where we sample a total of 11,305 sequences with a maximum sequence length 128.\n- The 100-shot HumanEval task (Chen et al., 2021). This is a code generation task involves the generation of 16,400 samples with a maximum sequence length of 512.\n\nEven with greedy sampling, a single token deviating due to numerics could result in two sequences diverging wildly. Since pseudo-random seeds are processed differently between ArS and SpS, and because the different computation graphs lead to different numerics, we cannot not expect identical outputs. However, we expect the samples to come from the same distribution within numerics and we empirically verify this by evaluating these benchmarks.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Evaluation on XSum and HumanEval",
        "chunkIndex": 27,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-28",
      "content": "erent numerics, we cannot not expect identical outputs. However, we expect the samples to come from the same distribution within numerics and we empirically verify this by evaluating these benchmarks.\n\nWe run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low variance, and we can measure it directly from TPU profiles. To obtain the average speedup, standard deviations and other metrics, we log the amount of tokens generated for each speculative loop. In Table 1 we show the performance on the XSum and HumanEval benchmarks for naive and speculative sampling with Chinchilla.\n\nWe obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; . Yet, we have parity in the benchmark metrics - the underlying samples distribution is provably the same up to numerics, and this verifies that the draft model is not biasing the results empirically.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Evaluation on XSum and HumanEval",
        "chunkIndex": 28,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-29",
      "content": ". Yet, we have parity in the benchmark metrics - the underlying samples distribution is provably the same up to numerics, and this verifies that the draft model is not biasing the results empirically. In the case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Evaluation on XSum and HumanEval",
        "chunkIndex": 29,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-30",
      "content": "It is apparent that the acceptance rate is dependent on the application and the decoding method. HumanEval achieves a significantly larger speedup - We hypothesize that this is due to a combination of code containing a lot of common sub-sequences (e.g. for i in range(len(arr)): would be relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens and the temperature value sharpening both the draft and target logits.\n\nFigure 1 j Left: The average time to generate 128 tokens, with standard deviation. Note that as ğ¾ increases, the overall speedup plateaus or even regresses, with XSum being optimal at ğ¾ = 3. The variance consistently increases with ğ¾ . Middle: The average number of tokens accepted divided by ğ¾ , 1 - this serves as a measure of the overall efficiency of the modified rejection scheme, which decreases with the lookahead.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Acceptance rate changes per domain",
        "chunkIndex": 30,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-31",
      "content": "ses with ğ¾ . Middle: The average number of tokens accepted divided by ğ¾ , 1 - this serves as a measure of the overall efficiency of the modified rejection scheme, which decreases with the lookahead. Right: Average time per loop increases approximately linearly with ğ¾ due to the increased number of model calls. Note that the gradient is slightly higher than the sampling speed of the draft model, due to additional overheads in nucleus decoding.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Acceptance rate changes per domain",
        "chunkIndex": 31,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-32",
      "content": "We visualise the trade-off of increasing ğ¾ , the number of tokens sampled by the draft model in Figure 1. As ğ¾ increases, we need fewer scoring calls from the large models to generate the same sequence length, potentially giving us a larger speedup. However, the total loop time increases approximately linearly with the larger number of draft model calls and small increases in the scoring time. The overall efficiency of the proportion of accepted tokens decreases as ğ¾ increases, since later tokens depend on the acceptance of previous tokens. This results in the average speedup plateauing or even degrading with a larger ğ¾ (for example, XSum with nucleus's latency is minimised at ğ¾ = 3), depending on the domain.\n\nFurther, even though larger values of ğ¾ may yield marginally greater mean speedups in certain circumstances, it also increases variance of the time to generate a full sequence. This could be problematic for settings where the P90, P99 latencies of concern.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Trade off between longer drafts and more frequent scoring",
        "chunkIndex": 32,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-33",
      "content": "In this work, we demonstrate a new algorithm and workflow for accelerating the decoding of language models. Speculative sampling does not require making any modifications to the target language model's parameters or architecture, is provably lossless within numerics, scales well with the appropriate draft model and complements many existing techniques for reducing latency in the small batch size setting.\n\nWe optimise and scale the technique to Chinchilla 70B using a draft model which was easy to train with existing infrastructure, demonstrating that it yields a large speedup across benchmark tasks and common decoding methods in the process. We verify that it is indeed lossless empirically in its downstream tasks.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Conclusion",
        "chunkIndex": 33,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-34",
      "content": "- A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid. Vivit: A video vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 6816-6826. IEEE Computer Society, 2021.\n- T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\n- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 34,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-35",
      "content": "is, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\n- A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.\n- A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly , et al.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 35,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-36",
      "content": "ansformers at scale. arXiv preprint arXiv:2208.07339 , 2022.\n- A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly , et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n- T. Ge, H. Xia, X. Sun, S. Chen, and F. Wei. Lossless acceleration for seq2seq generation with aggressive decoding. ArXiv , abs/2205.10350, 2022.\n\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 4163-4174, Online, Nov. 2020. Association for Computational Linguistics.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 36,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-37",
      "content": "lling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 4163-4174, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL https://aclanthology.org/2020. findings-emnlp.372 .\n- Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR , abs/1606.07947, 2016. URL http://arxiv.org/abs/1606.07947 .\n- Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. ArXiv , abs/2211.17192, 2022.\n- Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention. arXiv preprint arXiv:2006.12467 , 2020.\n- S. Narayan, S. B. Cohen, and M. Lapata. Don't give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797-1807, Brussels, Belgium, Oct.-Nov.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 37,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-38",
      "content": "aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797-1807, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206 .\n- R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022.\n- J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.\n- V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n- N. Shazeer. Fast transformer decoding: One write-head is all you need.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 38,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-39",
      "content": "T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n- N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR , abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150 .\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 , 2019.\n- Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 9791-9800. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html .\n- M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR , abs/1811.03115, 2018.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 39,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-40",
      "content": "18-24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html .\n- M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR , abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115 .\n- A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 10260-10269. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/wiggers20a.html .\n\n- Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "References",
        "chunkIndex": 40,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-41",
      "content": "- Initial proposal: Charlie Chen, John Jumper and Geoffrey Irving\n- Modified Rejection Sampling Scheme: John Jumper\n- Initial Implementation, Optimisation and Scaling: Charlie Chen\n- Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen\n- Draft of Manuscript: Charlie Chen and Sebastian Borgeaud\n- Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre\n- Manuscript Feedback: Laurent Sifre, Geoffrey Irving and John Jumper",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Author Contributions",
        "chunkIndex": 41,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-42",
      "content": "We'd like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. We'd also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for your support in navigating the publishing process. Finally, we'd like to thank Blake Hechtman, Berkin Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks that can be applied to the transformer architecture.",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 42,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-43",
      "content": "Table 2 j Hyperparameters for the draft model\n\n| Model               |   ğ‘‘ model |   Heads |   Layers | Params   |\n|---------------------|-----------|---------|----------|----------|\n| Target (Chinchilla) |      8192 |      64 |       80 | 70B      |\n| Draft               |      6144 |      48 |        8 | 4B       |",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Hyperparams",
        "chunkIndex": 43,
        "totalChunks": 45
      }
    },
    {
      "id": "2302.01318v1-chunk-44",
      "content": "Theorem 1 (Modified Rejection Sampling recovers the target distribution) . Given discrete distributions ğ‘ , ğ‘ and a single draft sample Ëœ ğ‘¥ GLYPH&lt;24&gt; ğ‘ , let ğ‘‹ be the final resulting sample. For ğ‘‹ = ğ‘¥ to be true, we must either sample Ëœ ğ‘¥ = ğ‘¥ and then accept it, or resample it after Ëœ ğ‘¥ ( of any value) is rejected. Hence:\n\n<!-- formula-not-decoded -->\n\nFor the first term, we apply the acceptance rule:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhere ' GLYPH&lt;147&gt; ', denotes:\n\n<!-- formula-not-decoded -->\n\nFor the second conditional term, we apply the resampling rule:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFinally, we calculate the probability of rejection:\n\n<!-- formula-not-decoded -->\n\nThis is equal to the denominator of ' ğ‘ ' ğ‘¥ ' GLYPH&lt;0&gt; ğ‘ ' ğ‘¥ '', , so:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nand we have recovered the desired target.\n\nHence:",
      "metadata": {
        "source": "arxiv:2302.01318v1",
        "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
        "authors": [
          "Charlie Chen",
          "Sebastian Borgeaud",
          "Geoffrey Irving",
          "Jean-Baptiste Lespiau",
          "Laurent Sifre",
          "John Jumper"
        ],
        "section": "Proofs",
        "chunkIndex": 44,
        "totalChunks": 45
      }
    }
  ],
  "fullText": "<!-- image -->\n\n## Accelerating Large Language Model Decoding with Speculative Sampling\n\nCharlie Chen 1 , Sebastian Borgeaud 1 , Geoffrey Irving 1 , Jean-Baptiste Lespiau 1 , Laurent Sifre 1 and John Jumper 1\n\n1 All authors from DeepMind\n\nWe present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2 -2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.\n\n## Introduction\n\nScaling transformer models to 500B+ parameters has led to large performance improvements on many natural language, computer vision and reinforcement learning tasks (Arnab et al., 2021; Brown et al., 2020; Chowdhery et al., 2022; Dosovitskiy et al., 2020; Hoffmann et al., 2022; Rae et al., 2021). However, transformer decoding remains a highly costly and inefficient process in this regime.\n\nTransformer sampling is typically memory bandwidth bound (Shazeer, 2019), so for a given set of hardware, the time to generate a single token in transformer models is proportional to a first order approximation to the size of parameters and the size of the transformer memory. The size of language models also necessitates serving with model parallelism - adding communication overheads (Pope et al., 2022) and multiplying resource requirements. Since each new token depends on the past, many such transformer calls are required to sample a new sequence.\n\nWe present an algorithm to accelerate transformer sampling for latency critical applications, which we call speculative sampling (SpS). This is achieved by:\n\n1. Generating a short draft of length ğ¾ . This can be attained with either a parallel model (Stern et al., 2018) or by calling a faster, auto-regressive model ğ¾ times. We shall refer to this model as the draft model , and focus on the case where it is auto-regressive.\n3. Using a modified rejection sampling scheme, accept a subset of the ğ¾ draft tokens from left to right, recovering the distribution of the target model in the process.\n2. Scoring the draft using the larger, more powerful model from we wish to sample from. We shall refer to this model as the target model .\n\nIntuitively , there are often sequences where the next token might be 'obvious'. Therefore, if there is strong agreement between the draft and target model's distributions on a given token or sub-sequence of tokens, this setup permits the generation of multiple tokens each time the target model is called .\n\nWe show that the expected acceptance rate of draft tokens is sufficient to offset the overhead of the\n\ndrafting process for large language models, resulting in an effective and practical method for reducing sampling latency without the need for modifying the target model or biasing the sample distribution. Depending on the evaluation domain, SpS leads to a 2-2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; speedup when sampling from Chinchilla (Hoffmann et al., 2022). Notably, the mean tokens per second with SpS often exceeds the idealised ceiling on auto-regressive sampling speed imposed by the memory bandwidth.\n\n## Related Work\n\nThere has been a substantial body of work focused on improving sampling latency of large transformers and other auto-regressive models.\n\nSince sampling performance is heavily coupled with the model size in memory, quantisation to int8 or even int4 (Dettmers et al., 2022; Yao et al., 2022) and distillation (Jiao et al., 2020; Sanh et al., 2019) of transformers are effective techniques for reducing sampling latency with little to no performance penalty. The observation that model size contributes less to the final performance than expected (Hoffmann et al., 2022) has also encouraged smaller language models in general.\n\nDuring sampling, a cache of the keys and values is maintained for every attention layer, and could become a memory bandwidth bottleneck as the batch size increases. Methods such as multi-query attention (Shazeer, 2019) aims to improve sampling performance by shrinking this cache. However these techniques are most effective at maximising throughout (at larger batch sizes) instead of latency, especially for larger models where the majority of the memory bandwidth budget is consumed by the parameters.\n\nUsing a combination of the above techniques, in addition to a number of low-level optimisations to TPUs, Pope et al. (2022) have greatly improved the serving latency and efficiency of PaLM 540B.\n\nThere is an existing body of similar work exploiting the efficiency of transformers and sequence models operating in parallel. This includes block parallel sampling (Stern et al., 2018), aggressive decoding (Ge et al., 2022), in addition to some work in parallelizing autoregressive models in the image domain (Song et al., 2021; Wiggers and Hoogeboom, 2020). These methods have yet to be adapted to typical language model use-cases since they either only work with greedy sampling, bias the results or are focused on other modalities. Further, to our knowledge none of these techniques have been scaled to distributed setups, which is necessary for the most expensive decoders with the tens or hundreds of billions of parameters.\n\nCoincidentally, the work in this manuscript was undertaken concurrently and independently of the work on speculative decoding from Leviathan et al. (2022). We focus more heavily the distributed serving setting for large models and offer some incremental optimisations, but otherwise the core underlying idea is the same.\n\n## Auto-regressive Sampling\n\nWhilst transformers can be trained efficiently and in parallel on TPUs and GPUs, samples are typically drawn auto-regressively (See algorithm 1). For most applications, auto-regressive sampling (ArS) is highly memory bandwidth bound and thus cannot make effective use of modern accelerator hardware (Shazeer, 2019). A memory bound model call only generates a single token for every sequence in the batch, hence generating multiple tokens introduces a large amount of latency in any system which makes use of it.\n\nThis is especially problematic as the number of parameters in the model increases. Since all the model parameters need to pass through at least one accelerator chip, the model size divided by the total memory bandwidth across all chips gives us a hard ceiling on the maximum auto-regressive sampling speed. Larger models also require serving on multiple accelerators, introducing a further source of latency due to inter-device communication overheads.\n\n## Algorithm 1 Auto-regressive (ArS) with Auto-Regressive Models\n\n```\nGiven auto-regressive target model ğ‘ ' GLYPH<147> j GLYPH<147> ' and initial prompt sequence ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘¡ and target sequence length ğ‘‡ . Initialise ğ‘› ğ‘¡ . while ğ‘› GLYPH<157> ğ‘‡ do Sample ğ‘¥ ğ‘› , 1 GLYPH<24> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› ' ğ‘› ğ‘› , 1 end while\n```\n\n## Algorithm 2 Speculative Sampling (SpS) with Auto-Regressive Target and Draft Models\n\n```\nGiven lookahead ğ¾ and minimum target sequence length ğ‘‡ . Given auto-regressive target model ğ‘ ' GLYPH<147> j GLYPH<147> ' , and auto-regressive draft model ğ‘ ' GLYPH<147> j GLYPH<147> ' , initial prompt sequence ğ‘¥ 0 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘¡ . Initialise ğ‘› ğ‘¡ . while ğ‘› GLYPH<157> ğ‘‡ do for ğ‘¡ = 1 : ğ¾ do Sample draft auto-regressively Ëœ ğ‘¥ ğ‘¡ GLYPH<24> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ‘¡ GLYPH<0> 1 ' end for In parallel, compute ğ¾ , 1 sets of logits from drafts Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ¾ : ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› ' GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 ' GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> Ëœ ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> Ëœ ğ‘¥ ğ¾ ' for ğ‘¡ = 1 : ğ¾ do Sample ğ‘Ÿ GLYPH<24> ğ‘ˆ Â» 0 GLYPH<148> 1 â€¦ from a uniform distribution. if ğ‘Ÿ GLYPH<157> min GLYPH<16> 1 GLYPH<148> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148>ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148>ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' GLYPH<17> , then Set ğ‘¥ ğ‘› , ğ‘¡ Ëœ ğ‘¥ ğ‘¡ and ğ‘› ğ‘› , 1. else sample ğ‘¥ ğ‘› , ğ‘¡ GLYPH<24> ' ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 ' GLYPH<0> ğ‘ ' ğ‘¥ j ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ‘¡ GLYPH<0> 1 '', and exit for loop. end if end for If all tokens ğ‘¥ ğ‘› , 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› , ğ¾ are accepted, sample extra token ğ‘¥ ğ‘› , ğ¾ , 1 GLYPH<24> ğ‘ ' ğ‘¥ j GLYPH<148> ğ‘¥ 1 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> ğ‘¥ ğ‘› GLYPH<148> ğ‘¥ ğ‘› , ğ¾ ' and set ğ‘› ğ‘› , 1. end while\n```\n\n## Speculative Sampling\n\n## Conditional Scoring\n\nFor speculative sampling (See algorithm 2), we first make the observation that computing the logits of a short continuation of ğ¾ tokens in parallel has a very similar latency to that of sampling a single\n\ntoken. We focus our attention on large transformers, sharded in the Megatron style (Shoeybi et al., 2019). For these models the majority of sampling time can be attributed to three components:\n\n1. Linear Layers: For small batch sizes, each linear layer only processes a small number of embeddings. This causes the dense matrix multiplies in the feed-forward layers, queries, keys, values computations and the final attention projection to become memory bound. For small ğ¾ , this will continue to be memory bound and therefore take a similar amount of time.\n3. All-reduces: As models grow in size, its parameters need to be divided across multiple accelerators, leading to communication overheads. With Megatron, this manifests itself as an all-reduce after every feed-forward and attention layer. Since only the activations for a small number of tokens are transmitted, this operation is typically latency bound instead of throughput bound for both sampling and scoring (for small ğ¾ ). Again, this results in a similar amount of time spent in the two cases.\n2. The Attention Mechanism: The attention mechanism is also memory bound. During sampling, we maintain a cache of all the keys and values of the previous tokens in the sequence to avoid re-computation. These KV-caches are large, and accounts for the majority of the memory bandwidth utilisation for the attention mechanism. However, since the KV-cache size does not change as we increase ğ¾ , there is little to no delta in this component.\n\nOther sources of overhead may exist, depending on the exact transformer implementation. Therefore it is still possible that the choice of positioning encoding, decoding method (e.g. a sort might be required for nucleus sampling), hardware limitations etc. can introduce some deltas between scoring and sampling. However, if the conditions are met such that the above components dominate then scoring should not be significantly slower for small ğ¾ .\n\n## Modified Rejection Sampling\n\nWe require a method to recover the distribution of the target model from samples from the draft model, and logits of said tokens from both models.\n\nTo achieve this, we introduce the following rejection sampling scheme of the drafted tokens. Given a sequence of tokens ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› , and ğ¾ draft tokens Ëœ ğ‘¥ ğ‘› , 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; Ëœ ğ‘¥ ğ‘› , ğ¾ generated from ğ‘ ' GLYPH&lt;147&gt; j GLYPH&lt;147&gt; ' , we accept Ëœ ğ‘¥ ğ‘› , 1 with probability:\n\nWhere ğ‘ ' Ëœ ğ‘¥ ğ‘› , 1 j ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' and ğ‘ ' Ëœ ğ‘¥ ğ‘› , 1 j ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' are the probability of Ëœ ğ‘¥ ğ‘› , 1 according to the target and draft models respectively, conditioned on the context so far.\n\n<!-- formula-not-decoded -->\n\nIf the token is accepted, we set ğ‘¥ ğ‘› , 1 Ëœ ğ‘¥ ğ‘› , 1 and repeat the process for Ëœ ğ‘¥ ğ‘› , 2 until either a token is rejected or all tokens have been accepted.\n\nIf Ëœ ğ‘¥ ğ‘› , 1 is rejected, we resample ğ‘¥ ğ‘› , 1 from the following distribution:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhere ' GLYPH&lt;147&gt; ', denotes:\n\nBy applying this sequentially, we recover the distribution of the target model for the accepted tokens (see proof in Theorem 1) within hardware numerics. Note that:\n\n- At least one token will always be generated from a draft-accept loop - if the first token is rejected, a valid token is resampled.\n- Since the final token of the draft gives us the logits for the next token, if every drafted token is accepted, we can sample from it normally. This gives us a maximum of ğ¾ , 1 tokens per loop, over the naive implementation which would only return ğ¾ tokens.\n\nWith standard sampling methods such as nucleus, top-k sampling and adjusting temperature, we can modify the probabilities accordingly before applying this rejection sampling scheme. We have observed that the overall acceptance rate is robust to the exact parameters used.\n\nBecause we do not interact with the body of the transformer itself, this method can be used in conjunction many other techniques for accelerating or optimising the memory use of sampling, such as quantisation and multi-query attention.\n\n## Choice of Draft Models\n\nSince the acceptance criterion guarantees the distribution of the target model in our samples, we are free to choose the method for drafting a continuation as long as it exposes logits, and there is a high enough acceptance rate and/or low enough latency to break-even. There exist several approaches here:\n\n- Incorporating draft generation into the target model, and train the model from the start. This is the strategy used by Stern et al. (2018), which adds multiple heads into the transformer to generate multiple tokens.\n- Set a portion of the activations of the target model as an input to the draft model, and train the draft model with this input.\n- Using sequence level distillation (Kim and Rush, 2016) to generate a second model which predicts ğ¾ tokens in parallel. This strategy was employed by Ge et al. (2022).\n\nAlthough these methods will likely yield powerful drafts, they require a large number of data generated from the target model or changes to the target model. Sequence level distillation in particular would require a large compute budget. This makes them less practical for large scale applications.\n\nWhilst large language models produce better samples, intuitively there are \"easier\" tokens to predict for which smaller models may be sufficient. Therefore we may simply use a smaller version of the target language model as the draft and obtain high acceptance rates. This would also be convenient from an engineering and workflow perspective, since robust tooling for such models should already exist to train the target model in the first place.\n\n## Results\n\nWe train a 4 billion parameter draft model optimised for sampling latency on 16 TPU v4s - the same hardware that is typically used to serve Chinchilla for research purposes. This model was trained with the same tokeniser and dataset as Chinchilla, with a slightly smaller width and with only 8 layers. The relatively few number of layers allows it to achieve a sampling speed of 1.8ms/token compared to 14.1ms/token for Chinchilla. For details, please refer to the hyperparameters in Table 2.\n\nFor distributed setups it is insufficient to naively choose a small model as the draft, since different models have different optimal inference setups. For example, it is typical to serve Chinchilla 70B\n\nTable 1 j Chinchilla performance and speed on XSum and HumanEval with naive and speculative sampling at batch size 1 and ğ¾ = 4. XSum was executed with nucleus parameter ğ‘ = 0 GLYPH&lt;147&gt; 8, and HumanEval with ğ‘ = 0 GLYPH&lt;147&gt; 95 and temperature 0 GLYPH&lt;147&gt; 8.\n\n| Sampling Method             | Benchmark            | Result      | Mean Token Time           | Speed Up                            |\n|-----------------------------|----------------------|-------------|---------------------------|-------------------------------------|\n| ArS (Nucleus) SpS (Nucleus) | XSum (ROUGE-2)       | 0.112 0.114 | 14.1ms/Token 7.52ms/Token | 1 GLYPH<2> 1 GLYPH<147> 92 GLYPH<2> |\n| ArS (Greedy) SpS (Greedy)   | XSum (ROUGE-2)       | 0.157 0.156 | 14.1ms/Token 7.00ms/Token | 1 GLYPH<2> 2 GLYPH<147> 01 GLYPH<2> |\n| ArS (Nucleus) SpS (Nucleus) | HumanEval (100 Shot) | 45.1% 47.0% | 14.1ms/Token 5.73ms/Token | 1 GLYPH<2> 2 GLYPH<147> 46 GLYPH<2> |\n\non 16 TPU v4s (where it achieves the aforementioned 14.1ms/token ), whereas a chinchilla-optimal 7B achieves its lowest sampling latency on 4 TPU v4s (where it achieves 5ms/token ). For smaller models, the additional memory bandwidth and flops are insufficient to offset the additional communication overhead between more devices - serving a 7B on 16 TPUs actually increases the latency. This means the 7B would provide only a modest speedup if used as a draft with its optimal topology, and we will not make full utilisation of the hardware during drafting.\n\nWe can sidestep this issue by training a wider model with a relatively few number of layers in order to minimise communication overhead. It has been observed that the performance of language models is relatively robust to changes in model aspect ratio (Levine et al., 2020), so this allows us to serve a powerful draft model which can be sampled rapidly on the same hardware as the target model.\n\n## Evaluation on XSum and HumanEval\n\nWe evaluate speculative sampling with Chinchilla on two tasks and summarize the results in Table 1:\n\n- The XSum (Narayan et al., 2018) benchmark. This is a natural language summarisation task using a 1-shot prompt where we sample a total of 11,305 sequences with a maximum sequence length 128.\n- The 100-shot HumanEval task (Chen et al., 2021). This is a code generation task involves the generation of 16,400 samples with a maximum sequence length of 512.\n\nEven with greedy sampling, a single token deviating due to numerics could result in two sequences diverging wildly. Since pseudo-random seeds are processed differently between ArS and SpS, and because the different computation graphs lead to different numerics, we cannot not expect identical outputs. However, we expect the samples to come from the same distribution within numerics and we empirically verify this by evaluating these benchmarks.\n\nWe run the tasks at batch size 1 with SpS and ArS. The time taken per SpS/ArS loop has low variance, and we can measure it directly from TPU profiles. To obtain the average speedup, standard deviations and other metrics, we log the amount of tokens generated for each speculative loop. In Table 1 we show the performance on the XSum and HumanEval benchmarks for naive and speculative sampling with Chinchilla.\n\nWe obtain a substantial speedup in both tasks, with HumanEval reaching speedups of almost 2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; . Yet, we have parity in the benchmark metrics - the underlying samples distribution is provably the same up to numerics, and this verifies that the draft model is not biasing the results empirically. In the case of HumanEval and greedy XSum, this speedup exceeded the theoretical memory bandwidth limit of the hardware for autoregressive sampling (model size divided by the total memory bandwidth).\n\n## Acceptance rate changes per domain\n\nIt is apparent that the acceptance rate is dependent on the application and the decoding method. HumanEval achieves a significantly larger speedup - We hypothesize that this is due to a combination of code containing a lot of common sub-sequences (e.g. for i in range(len(arr)): would be relatively easy for a draft model to guess), is often decomposed into a smaller set of shorter tokens and the temperature value sharpening both the draft and target logits.\n\nFigure 1 j Left: The average time to generate 128 tokens, with standard deviation. Note that as ğ¾ increases, the overall speedup plateaus or even regresses, with XSum being optimal at ğ¾ = 3. The variance consistently increases with ğ¾ . Middle: The average number of tokens accepted divided by ğ¾ , 1 - this serves as a measure of the overall efficiency of the modified rejection scheme, which decreases with the lookahead. Right: Average time per loop increases approximately linearly with ğ¾ due to the increased number of model calls. Note that the gradient is slightly higher than the sampling speed of the draft model, due to additional overheads in nucleus decoding.\n\n<!-- image -->\n\n## Trade off between longer drafts and more frequent scoring\n\nWe visualise the trade-off of increasing ğ¾ , the number of tokens sampled by the draft model in Figure 1. As ğ¾ increases, we need fewer scoring calls from the large models to generate the same sequence length, potentially giving us a larger speedup. However, the total loop time increases approximately linearly with the larger number of draft model calls and small increases in the scoring time. The overall efficiency of the proportion of accepted tokens decreases as ğ¾ increases, since later tokens depend on the acceptance of previous tokens. This results in the average speedup plateauing or even degrading with a larger ğ¾ (for example, XSum with nucleus's latency is minimised at ğ¾ = 3), depending on the domain.\n\nFurther, even though larger values of ğ¾ may yield marginally greater mean speedups in certain circumstances, it also increases variance of the time to generate a full sequence. This could be problematic for settings where the P90, P99 latencies of concern.\n\n## Conclusion\n\nIn this work, we demonstrate a new algorithm and workflow for accelerating the decoding of language models. Speculative sampling does not require making any modifications to the target language model's parameters or architecture, is provably lossless within numerics, scales well with the appropriate draft model and complements many existing techniques for reducing latency in the small batch size setting.\n\nWe optimise and scale the technique to Chinchilla 70B using a draft model which was easy to train with existing infrastructure, demonstrating that it yields a large speedup across benchmark tasks and common decoding methods in the process. We verify that it is indeed lossless empirically in its downstream tasks.\n\n## References\n\n- A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid. Vivit: A video vision transformer. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 6816-6826. IEEE Computer Society, 2021.\n- T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\n- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\n- A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 , 2022.\n- A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly , et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n- T. Ge, H. Xia, X. Sun, S. Chen, and F. Wei. Lossless acceleration for seq2seq generation with aggressive decoding. ArXiv , abs/2205.10350, 2022.\n\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 4163-4174, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL https://aclanthology.org/2020. findings-emnlp.372 .\n- Y. Kim and A. M. Rush. Sequence-level knowledge distillation. CoRR , abs/1606.07947, 2016. URL http://arxiv.org/abs/1606.07947 .\n- Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. ArXiv , abs/2211.17192, 2022.\n- Y. Levine, N. Wies, O. Sharir, H. Bata, and A. Shashua. The depth-to-width interplay in self-attention. arXiv preprint arXiv:2006.12467 , 2020.\n- S. Narayan, S. B. Cohen, and M. Lapata. Don't give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 1797-1807, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206 .\n- R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022.\n- J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, et al. Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.\n- V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n- N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR , abs/1911.02150, 2019. URL http://arxiv.org/abs/1911.02150 .\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 , 2019.\n- Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 9791-9800. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/song21a.html .\n- M. Stern, N. Shazeer, and J. Uszkoreit. Blockwise parallel decoding for deep autoregressive models. CoRR , abs/1811.03115, 2018. URL http://arxiv.org/abs/1811.03115 .\n- A. Wiggers and E. Hoogeboom. Predictive sampling with forecasting autoregressive models. In H. D. III and A. Singh, editors, Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pages 10260-10269. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/wiggers20a.html .\n\n- Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.\n\n## Supplementary Materials\n\n## Author Contributions\n\n- Initial proposal: Charlie Chen, John Jumper and Geoffrey Irving\n- Modified Rejection Sampling Scheme: John Jumper\n- Initial Implementation, Optimisation and Scaling: Charlie Chen\n- Engineering Improvements: Jean-Baptiste Lespiau and Charlie Chen\n- Draft of Manuscript: Charlie Chen and Sebastian Borgeaud\n- Experiments: Charlie Chen, Sebastian Borgeaud and Laurent Sifre\n- Manuscript Feedback: Laurent Sifre, Geoffrey Irving and John Jumper\n\n## Acknowledgements\n\nWe'd like to thank Oriol Vinyals and Koray Kavukcuoglu for your kind advice and leadership. We'd also like to thank Evan Senter for your additional feedback on the manuscript and Amelia Glaese for your support in navigating the publishing process. Finally, we'd like to thank Blake Hechtman, Berkin Ilbeyi for your valuable advice on XLA and Nikolai Grigoriev for our discussions on the various tricks that can be applied to the transformer architecture.\n\n## Hyperparams\n\nTable 2 j Hyperparameters for the draft model\n\n| Model               |   ğ‘‘ model |   Heads |   Layers | Params   |\n|---------------------|-----------|---------|----------|----------|\n| Target (Chinchilla) |      8192 |      64 |       80 | 70B      |\n| Draft               |      6144 |      48 |        8 | 4B       |\n\n## Proofs\n\nTheorem 1 (Modified Rejection Sampling recovers the target distribution) . Given discrete distributions ğ‘ , ğ‘ and a single draft sample Ëœ ğ‘¥ GLYPH&lt;24&gt; ğ‘ , let ğ‘‹ be the final resulting sample. For ğ‘‹ = ğ‘¥ to be true, we must either sample Ëœ ğ‘¥ = ğ‘¥ and then accept it, or resample it after Ëœ ğ‘¥ ( of any value) is rejected. Hence:\n\n<!-- formula-not-decoded -->\n\nFor the first term, we apply the acceptance rule:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhere ' GLYPH&lt;147&gt; ', denotes:\n\n<!-- formula-not-decoded -->\n\nFor the second conditional term, we apply the resampling rule:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFinally, we calculate the probability of rejection:\n\n<!-- formula-not-decoded -->\n\nThis is equal to the denominator of ' ğ‘ ' ğ‘¥ ' GLYPH&lt;0&gt; ğ‘ ' ğ‘¥ '', , so:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nand we have recovered the desired target.\n\nHence:",
  "tables": [
    {
      "index": 0,
      "markdown": "| Sampling Method             | Benchmark            | Result      | Mean Token Time           | Speed Up                            |\n|-----------------------------|----------------------|-------------|---------------------------|-------------------------------------|\n| ArS (Nucleus) SpS (Nucleus) | XSum (ROUGE-2)       | 0.112 0.114 | 14.1ms/Token 7.52ms/Token | 1 GLYPH<2> 1 GLYPH<147> 92 GLYPH<2> |\n| ArS (Greedy) SpS (Greedy)   | XSum (ROUGE-2)       | 0.157 0.156 | 14.1ms/Token 7.00ms/Token | 1 GLYPH<2> 2 GLYPH<147> 01 GLYPH<2> |\n| ArS (Nucleus) SpS (Nucleus) | HumanEval (100 Shot) | 45.1% 47.0% | 14.1ms/Token 5.73ms/Token | 1 GLYPH<2> 2 GLYPH<147> 46 GLYPH<2> |"
    },
    {
      "index": 1,
      "markdown": "| Model               |   ğ‘‘ model |   Heads |   Layers | Params   |\n|---------------------|-----------|---------|----------|----------|\n| Target (Chinchilla) |      8192 |      64 |       80 | 70B      |\n| Draft               |      6144 |      48 |        8 | 4B       |"
    }
  ],
  "stats": {
    "pages": 11,
    "chunksCreated": 45,
    "totalCharacters": 31006,
    "totalWords": 4936,
    "numTables": 2,
    "processingTimeMs": 11209
  }
}