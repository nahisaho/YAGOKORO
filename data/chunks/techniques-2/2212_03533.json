{
  "paper": {
    "id": "2212.03533v2",
    "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
    "abstract": "This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.",
    "authors": [
      "Liang Wang",
      "Nan Yang",
      "Xiaolong Huang",
      "Binxing Jiao",
      "Linjun Yang",
      "Daxin Jiang",
      "Rangan Majumder",
      "Furu Wei"
    ],
    "published": "2022-12-07T09:25:54.000Z",
    "updated": "2024-02-22T06:21:51.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2212.03533v2",
    "absUrl": "https://arxiv.org/abs/2212.03533v2"
  },
  "chunks": [
    {
      "id": "2212.03533v2-chunk-0",
      "content": "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao Linjun Yang , Daxin Jiang , Rangan Majumder , Furu Wei\n\nMicrosoft Corporation https://github.com/microsoft/unilm",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "chunkIndex": 0,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-1",
      "content": "This paper presents E5 1 , a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40 × more parameters.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-2",
      "content": "Text embeddings are low-dimensional vector representations for arbitrary-length texts and play key roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface easily consumable by downstream applications.\n\nWhile pre-trained language models such as BERT [17] and GPT [7] can produce transferrable text representations, they are not ideal for tasks such as retrieval and text matching where a singlevector embedding of texts is more desired due to its efficiency and versatility. To obtain better text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 2,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-3",
      "content": "stive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings. For example, GTR [43] and Sentence-T5 [44] fine-tune pre-trained models with supervised datasets to learn embeddings customized for passage retrieval and semantic textual similarity, respectively. Other works learn unsupervised embeddings from automatically constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT) [9], random cropping [28] and neighboring text spans [41], etc. While such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning [40].",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-4",
      "content": "e such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning [40].\n\nIn this work, we learn a high-quality general-purpose text embedding termed E5, E mb E ddings from bidir E ctional E ncoder r E presentations. E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing\n\n1 E5: E mb E ddings from bidir E ctional E ncoder r E presentations\n\nheterogeneous training signals.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-5",
      "content": "e contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing\n\n1 E5: E mb E ddings from bidir E ctional E ncoder r E presentations\n\nheterogeneous training signals. We construct the CCPairs dataset by combining various semistructured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform aggressive filtering with a consistency-based filter [15] to improve data quality. We choose a simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model. Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of the proposed method. On the BEIR zero-shot retrieval benchmark [53], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-6",
      "content": "hot retrieval benchmark [53], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved. Results on 56 datasets from the recently introduced MTEBbenchmark [40] show that our E5base is competitive against GTRxxl and Sentence-T5xxl, which have 40 × more parameters.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-7",
      "content": "There have been long-lasting interests in transforming texts into low-dimensional dense embeddings. Early works include Latent Semantic Indexing (LSA) [16] and Latent Dirichlet Allocation (LDA) [3]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. Arora et al. show that a simple weighted average of word vectors [38] can be a strong baseline for sentence embeddings.\n\nWith the development of pre-trained language models [17, 35, 48] and large-scale labeled datasets such as SNLI [6] and MS-MARCO [8], methods like Sentence-BERT [49], SimCSE [22], SentenceT5 [44] and SGPT [39] directly fine-tune language models to output continuous embeddings. Most research focuses on short texts and thus uses the term \"sentence embeddings\". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 7,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-8",
      "content": "t research focuses on short texts and thus uses the term \"sentence embeddings\". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information. Contrastive loss popularized by SimCLR [10] turns out to be more effective than classificationbased losses [49, 14] for embeddings. LaBSE [20], LASER [2] and CLIP [47] further extend to multilingual and multi-modal scenarios using parallel sentences and image-text pairs.\n\nAnother direction is to design self-supervised pre-training tasks for text matching and retrieval. [9] proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [28] shows that random cropping with data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [41] use neighboring texts as positives and scale up the model size to 175 B.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 8,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-9",
      "content": "h data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [41] use neighboring texts as positives and scale up the model size to 175 B. Oguz et al. [45] performs domain-matched pre-training to improve in-domain results. SPAR [11] trains a dense retriever by treating BM25 as a teacher model. Although the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data tend to be of low quality. Results on the BEIR benchmark [53] show they struggle to match the performance of BM25 if not further fine-tuned on labeled datasets.\n\nEvaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the embedding quality through downstream task performances. For example, SentEval [13] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [53] focuses on zero-shot information retrieval scenarios.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 9,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-10",
      "content": "nces. For example, SentEval [13] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [53] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark [40] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.\n\nMost closely related to our work is a series of community efforts by sentence-transformers 2 to train embeddings with a collection of labeled and automatically collected datasets. In this paper, we show that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 10,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-11",
      "content": "ised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 11,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-12",
      "content": "The quality and diversity of the data is crucial for training general-purpose text embeddings. In this work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which provide diverse training signals transferring well to a wide range of tasks.\n\n2 https://github.com/UKPLab/sentence-transformers\n\nFigure 1: Overview of our data curation pipeline and model architecture.\n\n<!-- image -->\n\nHarvesting semi-structured data sources Large-scale high-quality datasets like C4 [48] and CCMatrix [51] are vital for the success of language model pre-training and machine translation. For learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI [22] and MS-MARCO [8] or adopt heuristics such as random cropping [28] to obtain large-scale but very noisy supervision signals.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3 CCPairs: A Large Collection of Text Pair Dataset",
        "chunkIndex": 12,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-13",
      "content": "sting works either utilize small-scale human-annotated data such as NLI [22] and MS-MARCO [8] or adopt heuristics such as random cropping [28] to obtain large-scale but very noisy supervision signals.\n\nInstead, we curate a text pair dataset CCPairs ( C olossal C lean text Pairs ) by harvesting heterogeneous semi-structured data sources. Let ( q , p ) denote a text pair consisting of a query q and a passage p . Here we use ' passage ' to denote word sequences of arbitrary length, which can be a short sentence, a paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3 , (question, upvoted answer) pairs from Stackexchange 4 , (entity name + section title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [36], and (title, passage) pairs from Common Crawl 5 web pages and various News sources.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3 CCPairs: A Large Collection of Text Pair Dataset",
        "chunkIndex": 13,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-14",
      "content": "ction title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [36], and (title, passage) pairs from Common Crawl 5 web pages and various News sources.\n\nWe only include data sources that can be automatically mined, and some subsets are directly reused from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common Crawl. For example, we remove Reddit comments that are either too long ( &gt; 4096 characters) or receive score less than 1 , and remove passages from web pages with high perplexity [60]. After preliminary filtering, we end up with ∼ 1 . 3 billion text pairs, most of which come from Reddit and Common Crawl. For more details and examples, please refer to Appendix A.\n\nConsistency-based filter To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1 .",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3 CCPairs: A Large Collection of Text Pair Dataset",
        "chunkIndex": 14,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-15",
      "content": "o Appendix A.\n\nConsistency-based filter To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1 . 3 B noisy text pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept only if it falls in the topk ranked lists. In other words, the model's prediction should be consistent with the training labels. Here we set k = 2 based on manual inspection of data quality. After this step, we end up with ∼ 270 Mtext pairs for contrastive pre-training.\n\nThe intuition for this technique comes from the memorization behaviors of neural networks [19]: when trained on noisy datasets, neural networks tend to memorize the clean labels first and then gradually overfit the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3 CCPairs: A Large Collection of Text Pair Dataset",
        "chunkIndex": 15,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-16",
      "content": "it the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3 CCPairs: A Large Collection of Text Pair Dataset",
        "chunkIndex": 16,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-17",
      "content": "Our embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pretraining. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to further boost the quality of the resulted embeddings. See Figure 1 for an overview.\n\n3 https://files.pushshift.io/reddit/\n\n4 https://archive.org/details/stackexchange\n\n5 https://commoncrawl.org/",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4 Method",
        "chunkIndex": 17,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-18",
      "content": "Contrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative pairs. Given a collection of text pairs { ( q i , p i ) } n i =1 , we assign a list of negative passages { p -ij } j m =1 for the i -th example. Then the InfoNCE contrastive loss [10] is as follows:\n\n<!-- formula-not-decoded -->\n\nwhere s θ ( q, p ) is a scoring function between query q and passage p parameterized by θ . Following the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings E q and E p . The score is the cosine similarity scaled by a temperature hyperparameter τ :\n\n<!-- formula-not-decoded -->\n\nWhere τ is set to 0 . 01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers 'query:' and 'passage:' to q and d respectively.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Contrastive Pre-training with Unlabeled Data",
        "chunkIndex": 18,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-19",
      "content": "e τ is set to 0 . 01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers 'query:' and 'passage:' to q and d respectively. For some data sources such as citation pairs, it is not obvious which side should be the query, we randomly choose one for simplicity. Such an asymmetric design turns out to be important for some retrieval tasks where there exist paraphrases of the query in the target corpus.\n\nAnother critical issue for contrastive training is how to select the negative samples. Here we choose to use the in-batch negatives [10], where the passages from other pairs in a batch serve as negative samples. We find that this simple strategy enables more stable training and outperforms methods such as MoCo [25] when the batch size is sufficiently large.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Contrastive Pre-training with Unlabeled Data",
        "chunkIndex": 19,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-20",
      "content": "While contrastive pre-training on the CCPairs provides a solid foundation for general-purpose embeddings, further training on labeled data can inject human knowledge into the model to boost the performance. Although these datasets are small, existing works [43, 44] have shown that supervised fine-tuning leads to consistent performance gains. In this paper, we choose to further train with a combination of 3 datasets: NLI 6 (Natural Language Inference), MS-MARCO passage ranking dataset [8], and NQ (Natural Questions) dataset [30, 32]. Empirically, tasks like STS (Semantic Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets transfer well to retrieval tasks.\n\nBuilding on the practices of training state-of-the-art dense retrievers [50, 58], we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Fine-tuning with Labeled Data",
        "chunkIndex": 20,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-21",
      "content": "e mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The loss function is a linear interpolation between contrastive loss L cont for hard labels and KL divergence D KL for distilling soft labels from the teacher model.\n\n<!-- formula-not-decoded -->\n\nWhere p ce and p stu are the probabilities from the cross-encoder teacher model and our student model. α is a hyperparameter to balance the two loss functions. L cont is the same as in Equation 1.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Fine-tuning with Labeled Data",
        "chunkIndex": 21,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-22",
      "content": "After the above two steps, we obtain high-quality text embeddings transferring well to a wide range of tasks without fine-tuning the model parameters. Combined with techniques like approximate nearest neighbor search, embeddings provide a scalable and efficient solution for applications like web search. Here we briefly illustrate several use cases of our text embeddings.\n\nZero-shot Retrieval First, the passage embeddings for the target corpus are computed and indexed offline. Then for each query, we compute its query embedding and return the topk ranked lists from the corpus based on cosine similarity.\n\nFew-shot Text Classification A linear classifier is trained on top of the frozen embeddings with a few labeled examples. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning [27].\n\n6 The version released by SimCSE.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Applications to Text Embedding Tasks",
        "chunkIndex": 22,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-23",
      "content": "es. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning [27].\n\n6 The version released by SimCSE.\n\nZero-shot Text Classification The input and label texts are converted to sentences based on manually written prompt templates. The predicted label is the one closest to the input text in the embedding space. Take the sentiment classification of movie reviews as an example, with the original input ' I enjoy watching it ', the label text is ' it is an example of terrible/great movie review ' and the input text becomes ' movie review: I enjoy watching it '.\n\nSemantic Textual Similarity Given two text embeddings, we use the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Applications to Text Embedding Tasks",
        "chunkIndex": 23,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-24",
      "content": "e the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients.\n\nText Clustering Standard clustering algorithms such as k-means can be applied straightforwardly. Texts belonging to the same category are expected to be close in the embedding space.\n\nFor tasks other than zero-shot text classification and retrieval, we use the query embeddings by default.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Applications to Text Embedding Tasks",
        "chunkIndex": 24,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-25",
      "content": "Pre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base and E5large initialized from MiniLM [59], bert-base-uncased , and bert-large-uncased-whole-wordmasking respectively. The batch size is set to a large value of 32 , 768 to increase the number of negatives. The learning rate is { 3 , 2 , 1 } × 10 -4 for the {small, base, large} models, with linear decay and the first 1 , 000 steps for warmup. We pre-train for 20 k steps in total with AdamW optimizer, which is approximately 2 . 5 epochs over the dataset. It takes { 16 , 32 , 64 } V100 GPUs and { 1 , 1 , 2 } days for the {small, base, large} models. To improve training efficiency and reduce GPU memory usage, we adopt mixed precision training and gradient checkpointing.\n\nFine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [8], NQ [32, 30], and NLI [22] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Pre-training and Fine-tuning Configurations",
        "chunkIndex": 25,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-26",
      "content": "on the concatenation of 3 datasets: MS-MARCO passage ranking [8], NQ [32, 30], and NLI [22] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8 GPUs. Learning rate is { 3 , 2 , 1 } × 10 -5 for the {small, base, large} models with 400 steps warmup. For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each example, 6 sentences are randomly sampled from the entire corpus.\n\nWe use E5-PT to denote models with contrastive pre-training only. More implementation details can be found in Appendix B.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Pre-training and Fine-tuning Configurations",
        "chunkIndex": 26,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-27",
      "content": "BEIR Benchmark [53] is a collection of 19 information retrieval datasets, ranging across ad-hoc web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate the 15 datasets that provide public downloads. The main metric is nDCG@ 10 .\n\nMTEB Benchmark [40] is recently proposed for benchmarking massive text embedding tasks. Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are still only available in English. In this paper, we evaluate the English subsets, which have 56 datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification (PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively. Please refer to the MTEB paper for details.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.2 Evaluation Datasets",
        "chunkIndex": 27,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-28",
      "content": "Results with Unsupervised Methods In Table 1, we show model results that do not use any labeled data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1 . 2 points. To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark. When scaling up to E5-PTlarge, we see further benefits from 42 . 9 to 44 . 2 .\n\nTable 1: Unsupervised methods on the BEIR benchmark (nDCG@ 10 ). For SimCSE, we report results with BERTbase. cpt300M [41] is only available through paid API and evaluation results on some datasets are missing in the original paper. The highest number for each dataset is in bold, and the second highest is underlined. † : we report the LaPraDor [62] results without ensembling with BM25. ∗ : reproduction with the released checkpoint.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 28,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-29",
      "content": "e highest number for each dataset is in bold, and the second highest is underlined. † : we report the LaPraDor [62] results without ensembling with BM25. ∗ : reproduction with the released checkpoint.\n\n|               |   BM25 |   SimCSE | LaPraDor †   |   Contriever | cpt 300M   |   E5-PT small |   E5-PT base |   E5-PT large |\n|---------------|--------|----------|--------------|--------------|------------|---------------|--------------|---------------|\n| MS MARCO      |   22.8 |      9.4 | 16.9 ∗       |         20.6 | 19.9       |          25.4 |         26   |          26.2 |\n| Trec-Covid    |   65.6 |     26.2 | 22.7         |         27.4 | 52.9       |          52   |         61   |          61.8 |\n| NFCorpus      |   32.5 |      9.9 | 31.1         |         31.7 | 32.0       |          29.3 |         35.8 |          33.7 |\n| NQ            |   32.9 |     11.7 | 18.1         |         25.4 | -          |          37.3 |         39   |          41.7 |\n| HotpotQA      |   60.3 |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 29,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-30",
      "content": "29.3 |         35.8 |          33.7 |\n| NQ            |   32.9 |     11.7 | 18.1         |         25.4 | -          |          37.3 |         39   |          41.7 |\n| HotpotQA      |   60.3 |     19.8 | 30.3         |         48.1 | 51.5       |          46   |         52.4 |          52.2 |\n| FiQA          |   23.6 |      9.8 | 20.3         |         24.5 | 34.1       |          38.3 |         40   |          43.2 |\n| ArguAna       |   31.5 |     38.3 | 45.9         |         37.9 | 38.7       |          42.5 |         42.2 |          44.4 |\n| Touche-2020   |   36.7 |      8.9 | 9.4          |         19.3 | 21.0       |          19.9 |         16.9 |          19.8 |\n| CQADupStack   |   29.9 |     13.2 | 22.0         |         28.4 | -          |          35   |         35.4 |          38.9 |\n| Quora         |   78.9 |     78   | 78.7         |         83.5 | 68.1       |          85.8 |         85.7 |          86.1 |\n| DBPedia       |   31.3 |     15   | 25.0         |         2",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 30,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-31",
      "content": "38.9 |\n| Quora         |   78.9 |     78   | 78.7         |         83.5 | 68.1       |          85.8 |         85.7 |          86.1 |\n| DBPedia       |   31.3 |     15   | 25.0         |         29.2 | 27.2       |          34.5 |         35.4 |          37.1 |\n| Scidocs       |   15.8 |      5.5 | 13.3         |         14.9 | -          |          19.9 |         21.1 |          21.8 |\n| Fever         |   75.3 |     21.1 | 36.8         |         68.2 | 57.1       |          62.5 |         63.4 |          68.6 |\n| Climate-Fever |   21.3 |     11.8 | 13.8         |         15.5 | 15.8       |          14.5 |         15.4 |          15.7 |\n| Scifact       |   66.5 |     25.7 | 55.5         |         64.9 | 65.4       |          68.5 |         73.7 |          72.3 |\n| Average       |   41.7 |     20.3 | 29.3         |         36   | -          |          40.8 |         42.9 |          44.2 |\n| Best on       |    5   |      0   | 1            |          0   | 0          |           0",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 31,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-32",
      "content": ".7 |     20.3 | 29.3         |         36   | -          |          40.8 |         42.9 |          44.2 |\n| Best on       |    5   |      0   | 1            |          0   | 0          |           0   |          2   |           7   |\n\nIn terms of pre-training tasks, Contriever adopts random cropping, while LaPraDor combines ICT and dropout-as-positive-instance from SimCSE. The methods can easily obtain large-scale training data, while our approach requires more effort in dataset curation. Such efforts pay off with better results. Recent studies [34, 60, 21] also show that improving data quality is a vital step for training large language models.\n\nTable 2: Supervised fine-tuning results on the BEIR benchmark. Results for ANCE [61], ColBERT [31] and Contriever come from Izacard et al. [28]. The best result is in bold, and the second best is underlined.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 32,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-33",
      "content": ": Supervised fine-tuning results on the BEIR benchmark. Results for ANCE [61], ColBERT [31] and Contriever come from Izacard et al. [28]. The best result is in bold, and the second best is underlined.\n\n|               |   ANCE |   GTR base |   ColBERT |   Contriever | cpt 300M   |   GTR large |   E5 small |   E5 base |   E5 large |\n|---------------|--------|------------|-----------|--------------|------------|-------------|------------|-----------|------------|\n| MS MARCO      |   38.8 |       42   |      40.1 |         40.7 | -          |        43   |       42.3 |      43.1 |       44.1 |\n| Trec-Covid    |   65.4 |       53.9 |      67.7 |         59.6 | 67.9       |        55.7 |       76.8 |      79.6 |       78.3 |\n| NFCorpus      |   23.7 |       30.8 |      30.5 |         32.8 | 33.2       |        32.9 |       33.9 |      36.6 |       36.1 |\n| NQ            |   44.6 |       49.5 |      52.4 |         49.8 | -          |        54.7 |       58.7 |      60   |       62.9 |\n| Hotp",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 33,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-34",
      "content": "2       |        32.9 |       33.9 |      36.6 |       36.1 |\n| NQ            |   44.6 |       49.5 |      52.4 |         49.8 | -          |        54.7 |       58.7 |      60   |       62.9 |\n| HotpotQA      |   45.6 |       53.5 |      59.3 |         63.8 | 59.4       |        57.9 |       56.3 |      62.2 |       63.3 |\n| FiQA          |   29.5 |       34.9 |      31.7 |         32.9 | 38.4       |        42.4 |       34.8 |      36.4 |       38.6 |\n| ArguAna       |   41.5 |       51.1 |      23.3 |         44.6 | 47.0       |        52.5 |       46.7 |      51.4 |       49.4 |\n| Touche-2020   |   24   |       20.5 |      20.2 |         23   | 28.5       |        21.9 |       26.8 |      28.3 |       27.2 |\n| CQADupStack   |   29.6 |       35.7 |      35   |         34.5 | -          |        38.4 |       36.1 |      38.9 |       39.4 |\n| Quora         |   85.2 |       88.1 |      85.4 |         86.5 | 70.6       |        89   |       87.7 |      87.9 |       88.2 |\n| DBPedia",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 34,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-35",
      "content": "|        38.4 |       36.1 |      38.9 |       39.4 |\n| Quora         |   85.2 |       88.1 |      85.4 |         86.5 | 70.6       |        89   |       87.7 |      87.9 |       88.2 |\n| DBPedia       |   28.1 |       34.7 |      39.2 |         41.3 | 36.2       |        39.1 |       38.6 |      41   |       42.4 |\n| Scidocs       |   12.2 |       14.9 |      14.5 |         16.5 | -          |        15.8 |       16.4 |      19   |       20.1 |\n| Fever         |   66.9 |       66   |      77.1 |         75.8 | 72.1       |        71.2 |       53.5 |      58.2 |       65   |\n| Climate-Fever |   19.8 |       24.1 |      18.4 |         23.7 | 18.5       |        26.2 |       15.8 |      15.4 |       22.4 |\n| Scifact       |   50.7 |       60   |      67.1 |         67.7 | 67.2       |        63.9 |       65.6 |      73.1 |       72.6 |\n| Average       |   40.5 |       44   |      44.4 |         46.6 | -          |        47   |       46   |      48.7 |       50   |\n| Best on       |    0",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 35,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-36",
      "content": "63.9 |       65.6 |      73.1 |       72.6 |\n| Average       |   40.5 |       44   |      44.4 |         46.6 | -          |        47   |       46   |      48.7 |       50   |\n| Best on       |    0   |        0   |       1   |          1   | 1          |         4   |        0   |       3   |        5   |\n\nResults with Supervised Fine-tuning In Table 2, we fine-tune our models on supervised datasets and then transfer them to the BEIR benchmark. Since our fine-tuning datasets include MS-MARCO and NQ, the corresponding numbers are in-domain results. For other datasets, these are zero-shot transfer results. Our E5base model achieves an average nDCG@ 10 of 48 . 7 , already surpassing existing methods with more parameters such as GTRlarge [43]. Most datasets benefit from supervised finetuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 36,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-37",
      "content": "s benefit from supervised finetuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets.\n\nTable 3: Results on the MTEB benchmark [40] (56 datasets in English subset). Here we only report averaged numbers on each task category for space reasons, please check out Appendix B for a detailed version. BERT-FTbase uses the same fine-tuning data as E5 but initializes from BERTbase.\n\n| # of datasets →     | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|---------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised models |             |             |                |            |            |          |           |          |\n| Glove               | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 37,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-38",
      "content": "|            |            |          |           |          |\n| Glove               | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| BERT                | 61.7        | 30.1        | 56.3           | 43.4       | 10.6       | 54.4     | 29.8      | 38.3     |\n| SimCSE-BERT-unsup   | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| E5-PT small         | 67.0        | 41.7        | 78.2           | 53.1       | 40.8       | 68.8     | 32.7      | 54.3     |\n| E5-PT base          | 67.9        | 43.4        | 79.2           | 53.5       | 42.9       | 69.5     | 31.1      | 55.6     |\n| E5-PT large         | 69.0        | 44.3        | 80.3           | 54.4       | 44.2       | 69.9     | 32.6      | 56.6     |\n| Supervised models   |             |             |                |            |            |          |           |          |\n| SimCSE-BERT-sup",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 38,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-39",
      "content": "| 44.2       | 69.9     | 32.6      | 56.6     |\n| Supervised models   |             |             |                |            |            |          |           |          |\n| SimCSE-BERT-sup     | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| BERT-FT base        | 68.7        | 33.9        | 82.6           | 50.5       | 41.5       | 79.2     | 29.0      | 55.2     |\n| Contriever          | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR large           | 67.1        | 41.6        | 85.3           | 55.4       | 47.4       | 78.2     | 29.5      | 58.3     |\n| Sentence-T5 large   | 72.3        | 41.7        | 85.0           | 54.0       | 36.7       | 81.8     | 29.6      | 57.1     |\n| E5 small            | 71.7        | 39.5        | 85.1           | 54.5       | 46.0       | 80.9     | 31.4      | 58.9     |\n| E5 base             | 72.6        | 42.1",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 39,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-40",
      "content": "| 29.6      | 57.1     |\n| E5 small            | 71.7        | 39.5        | 85.1           | 54.5       | 46.0       | 80.9     | 31.4      | 58.9     |\n| E5 base             | 72.6        | 42.1        | 85.1           | 55.7       | 48.7       | 81.0     | 31.0      | 60.4     |\n| E5 large            | 73.1        | 43.3        | 85.9           | 56.5       | 50.0       | 82.1     | 31.0      | 61.4     |\n| Larger models       |             |             |                |            |            |          |           |          |\n| GTR xxl             | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl     | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Results on BEIR benchmark",
        "chunkIndex": 40,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-41",
      "content": "In Table 3, E5 models not only substantially outperform existing ones with similar sizes, but also match the results of much larger models. The top2 models on MTEB leaderboard 7 GTRxxl and Sentence-T5xxl have 4 . 8 B parameters, while our E5large model is more than 10 × smaller with 300 M parameters. We expect that our model will benefit from continual scaling up.\n\nSince the difference between BERT-FTbase and E5base is that BERT-FTbase only has fine-tuning stage, their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset. For most task categories except Clustering, performance improves after supervised fine-tuning. Consistent with prior works [43, 44], this once again demonstrates the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.4 Results on MTEB benchmark",
        "chunkIndex": 41,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-42",
      "content": "the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.\n\nTable 4: Zero-shot text classification results. 'Majority' always predicts the majority class label. Zero-shot BERTbase uses the average pooling of the last layer as text embeddings.\n\n|            | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Full Fine-tune   | Full Fine-tune   |\n|------------|-------------|-------------|-------------|-------------|-------------|------------------|------------------|\n|            | Majority    | BERT base   | E5 small    | E5 base     | E5 large    | BERT base        | BERT large       |\n| SST-2 [52] | 50.9        | 58.9        | 79.7        | 81.3        | 85.3        | 93.5             | 94.9             |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.4 Results on MTEB benchmark",
        "chunkIndex": 42,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-43",
      "content": "small    | E5 base     | E5 large    | BERT base        | BERT large       |\n| SST-2 [52] | 50.9        | 58.9        | 79.7        | 81.3        | 85.3        | 93.5             | 94.9             |\n\nTable 4 shows the zero-shot text classification results on the dev set of the SST-2 dataset [52]. By formulating text classification as embedding matching between input and label texts, our model can be much better than the 'majority' baseline in a zero-shot setting. We use the prompt template from Section 4.3.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.4 Results on MTEB benchmark",
        "chunkIndex": 43,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-44",
      "content": "In this section, we conduct a series of analyses to examine various design choices. All the numbers in this section are from base-size models. For the BEIR benchmark, we choose 6 datasets with more stable results across different runs. Some negative results are also listed in Appendix C.\n\nImpacts of Batch Size Since we use in-batch negatives for contrastive pre-training, larger batch size will provide more negatives and therefore improve the quality of the learned text embeddings. In\n\n7 https://huggingface.co/spaces/mteb/leaderboard , as of November 22, 2022\n\nTable 5: Impacts of different batch sizes for contrastive pre-training.\n\n| batch size   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------|--------|---------|-----------|-----------|-------|\n| 32k          |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| 8k           |       33.3 | 38.5 |   37.6 |    85.7 |      34   |      71.8 |  50.2 |\n| 1k",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 44,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-45",
      "content": "-----|-------|\n| 32k          |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| 8k           |       33.3 | 38.5 |   37.6 |    85.7 |      34   |      71.8 |  50.2 |\n| 1k           |       28.2 | 33.1 |   30.4 |    84   |      30.1 |      69.1 |  45.8 |\n\nTable 5, increasing batch size from 1 K to 32 K leads to consistent gains across all 6 datasets. It is also possible to train with smaller batch sizes by adding hard negatives [50]. However, the engineering efforts of mining hard negatives for large datasets (&gt; 100 M) are non-trivial.\n\nTable 6: Fine-tuning with different combinations of labeled data.\n\n| Fine-tuned on   |   Retrieval |   STS |   Classification |   Summ. |   MTEB Avg |\n|-----------------|-------------|-------|------------------|---------|------------|\n| No fine-tuning  |        42.9 |  69.5 |             67.9 |    31.1 |       55.6 |\n| MS-MARCO + NQ   |        50.3 |  78.3 |             68.3 |    30.6 |       59   |\n| NLI             |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 45,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-46",
      "content": "---|\n| No fine-tuning  |        42.9 |  69.5 |             67.9 |    31.1 |       55.6 |\n| MS-MARCO + NQ   |        50.3 |  78.3 |             68.3 |    30.6 |       59   |\n| NLI             |        38.3 |  81.1 |             72.6 |    31.6 |       57.3 |\n| All above       |        48.7 |  81   |             73.1 |    31   |       60.4 |\n\nFine-tuning Datasets GTR models are fine-tuned with 'MS-MARCO + NQ', while Sentence-T5 models use NLI instead. In Table 6, we can see that the 'MS-MARCO + NQ' setting performs best on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings.\n\nTable 7: Data filtering. For the top 2 rows, we train with 1 Mrandom text pairs.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 46,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-47",
      "content": "scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings.\n\nTable 7: Data filtering. For the top 2 rows, we train with 1 Mrandom text pairs.\n\n| # of pairs   |            |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------------|------|--------|---------|-----------|-----------|-------|\n| 1M           | w/o filter |       23   | 15.1 |   18.5 |    83.1 |      18.2 |      51.4 |  34.9 |\n| 1M           | w/ filter  |       26.8 | 22.7 |   24.5 |    85   |      27.5 |      57.5 |  40.7 |\n| All          | w/o filter |       34.5 | 35.4 |   39.1 |    85.7 |      32.9 |      72.5 |  50   |\n| All          | w/ filter  |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 47,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-48",
      "content": "All          | w/o filter |       34.5 | 35.4 |   39.1 |    85.7 |      32.9 |      72.5 |  50   |\n| All          | w/ filter  |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n\nData Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs. In Table 7, when training with 1 Mpairs, using filtered data has a nearly 6 points advantage. When all the text pairs are used, the 'w/o filter' setting has about 4 × more data but is still behind by 1 . 6 points. Though recent studies [29, 47] show that deep learning models are quite robust to dataset noises, data filtering still has benefits in improving training efficiency and model quality.\n\nNegative Sampling We explore two alternative methods to enlarge the number of negatives: Prebatch negatives [33] reuse embeddings from previous batches as additional negatives, while MoCo [25] introduces a momentum encoder and uses a FIFO queue to store negatives.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 48,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-49",
      "content": "the number of negatives: Prebatch negatives [33] reuse embeddings from previous batches as additional negatives, while MoCo [25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches, the negative size can be easily scaled up without incurring much GPU memory overhead. The downside is that most negatives are produced by an older version of model parameters. In Table 8, in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain hyperparameters such as temperature, better results are possible with more tuning.\n\nBM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace the long-standing BM25 algorithm from now on? The answer is likely ' not yet '. BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 49,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-50",
      "content": "val models, can we replace the long-standing BM25 algorithm from now on? The answer is likely ' not yet '. BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily on exact lexical match (Fever) [54], further research efforts are still necessary to improve current dense retrievers.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.5 Analysis",
        "chunkIndex": 50,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-51",
      "content": "In this work, we train a general-purpose text embedding model E5 from weak supervision signals. We adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf\n\nTable 8: Comparison of different negative sampling strategies.\n\n|             | # negatives   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|-------------|---------------|------------|------|--------|---------|-----------|-----------|-------|\n| In batch    | 32k           |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| + pre-batch | 64k           |       29.4 | 27.2 |   29.4 |    84.6 |      25   |      64.3 |  43.3 |\n| MoCo        | 130k          |       29.7 | 36.1 |   32   |    81.6 |      29.9 |      63.6 |  45.5 |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 51,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-52",
      "content": "re-batch | 64k           |       29.4 | 27.2 |   29.4 |    84.6 |      25   |      64.3 |  43.3 |\n| MoCo        | 130k          |       29.7 | 36.1 |   32   |    81.6 |      29.9 |      63.6 |  45.5 |\n\nperformance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. When further customized for downstream tasks, E5 achieves superior fine-tuned performance compared to existing embedding models with 40 × more parameters on the large, 56-task MTEB benchmark datasets.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 52,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-53",
      "content": "- [1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL https://openreview.net/forum?id=SyK00v5xx .\n- [2] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics , 7:597-610, 2019. doi: 10.1162/tacl\\_a\\_00288. URL https://aclanthology. org/Q19-1038 .\n- [3] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages 601-608. MIT Press, 2001.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-54",
      "content": "l Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages 601-608. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/hash/ 296472c9542ad4d4788d543508116cbc-Abstract.html .\n- [4] Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. Overview of touché 2022: argument retrieval. In International Conference of the CrossLanguage Evaluation Forum for European Languages , pages 311-336. Springer, 2022.\n- [5] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval , pages 716-722. Springer, 2016.\n- [6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-55",
      "content": "for medical information retrieval. In European Conference on Information Retrieval , pages 716-722. Springer, 2016.\n- [6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632-642, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //aclanthology.org/D15-1075 .\n- [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-56",
      "content": ", Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .\n- [8] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. ArXiv , abs/1611.09268, 2016.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-57",
      "content": "en, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. ArXiv , abs/1611.09268, 2016.\n\n- [9] Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id=rkg-mA4FDr .\n- [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages 1597-1607. PMLR, 2020.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-58",
      "content": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages 1597-1607. PMLR, 2020. URL http: //proceedings.mlr.press/v119/chen20j.html .\n- [11] Xilun Chen, Kushal Lakhotia, Barlas O˘ guz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918 , 2021.\n- [12] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2270-2282, 2020.\n- [13] Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-59",
      "content": "Annual Meeting of the Association for Computational Linguistics , pages 2270-2282, 2020.\n- [13] Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1269 .\n- [14] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670-680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070 .\n- [15] Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-60",
      "content": "s. doi: 10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070 .\n- [15] Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. ArXiv , abs/2209.11755, 2022.\n- [16] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science , 41(6):391-407, 1990.\n- [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-61",
      "content": "tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .\n- [18] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint arXiv:2012.00614 , 2020.\n- [19] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-62",
      "content": ", and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html .\n- [20] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Languageagnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 878-891, 2022.\n- [21] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv , abs/2101.00027, 2021.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-63",
      "content": "pe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv , abs/2101.00027, 2021.\n\n- [22] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.552 .\n- [23] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Samy Bengio, Hanna M.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-64",
      "content": "Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 8536-8546, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ a19744e268754fb0148b017647355b7b-Abstract.html .\n- [24] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. Dbpedia-entity v2: a test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1265-1268, 2017.\n- [25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-65",
      "content": "of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1265-1268, 2017.\n- [25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages 9726-9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/ CVPR42600.2020.00975 .\n- [26] Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. Cqadupstack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian document computing symposium , pages 1-8, 2015.\n- [27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-66",
      "content": "5.\n- [27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research , pages 2790-2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html .\n- [28] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. ArXiv , abs/2112.09118, 2021.\n- [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-67",
      "content": "fei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 4904-4916. PMLR, 2021. URL http://proceedings.mlr.press/v139/jia21b.html .\n- [30] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550 .\n- [31] Omar Khattab and Matei Zaharia.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-68",
      "content": "es 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550 .\n- [31] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , pages 39-48. ACM, 2020. doi: 10.1145/3397271.3401075. URL https://doi.org/10.1145/3397271.3401075 .\n\n- [32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-69",
      "content": "Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_00276. URL https://aclanthology.org/Q19-1026 .\n- [33] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 6634-6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-70",
      "content": "n Natural Language Processing (Volume 1: Long Papers) , pages 6634-6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021. acl-long.518 .\n- [34] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In ACL , 2022.\n- [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv , abs/1907.11692, 2019.\n- [36] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4969-4983, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-71",
      "content": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4969-4983, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https: //aclanthology.org/2020.acl-main.447 .\n- [37] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www'18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018 , pages 19411942, 2018.\n- [38] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In ICLR , 2013.\n- [39] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv , abs/2202.08904, 2022.\n- [40] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-72",
      "content": "ighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv , abs/2202.08904, 2022.\n- [40] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. ArXiv , abs/2210.07316, 2022.\n- [41] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pretraining. ArXiv , abs/2201.10005, 2022.\n- [42] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with selfensembling.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-73",
      "content": "01.10005, 2022.\n- [42] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with selfensembling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview. net/forum?id=HkgsPhNYPS .\n- [43] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern'andez 'Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. ArXiv , abs/2112.07899, 2021.\n- [44] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, 2022.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-74",
      "content": "niel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, 2022.\n\n- [45] Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. Domainmatched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022 , pages 1524-1534. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.114. URL https://doi.org/10.18653/v1/2022.findings-naacl.114 .\n- [46] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-75",
      "content": "la Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In North American Chapter of the Association for Computational Linguistics , 2020.\n- [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html .\n- [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-76",
      "content": "PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html .\n- [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21:1-67, 2020.\n- [49] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410 .\n- [50] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-77",
      "content": ".\n- [50] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2825-2835, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.224. URL https://aclanthology.org/2021.emnlp-main.224 .\n- [51] Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 6490-6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-78",
      "content": "th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 6490-6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507. URL https://aclanthology.org/2021.acl-long.507 .\n- [52] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing , 2013.\n- [53] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\n- [54] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-79",
      "content": "Datasets and Benchmarks Track (Round 2) , 2021.\n- [54] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809-819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https: //aclanthology.org/N18-1074 .\n- [55] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic\n\ninformation retrieval test collection. In ACM SIGIR Forum , volume 54, pages 1-12. ACM New York, NY, USA, 2021.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-80",
      "content": "e Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic\n\ninformation retrieval test collection. In ACM SIGIR Forum , volume 54, pages 1-12. ACM New York, NY, USA, 2021.\n\n- [56] Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 241-251, 2018.\n- [57] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7534-7550, 2020.\n- [58] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-81",
      "content": "020.\n- [58] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. ArXiv , abs/2207.02578, 2022.\n- [59] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 2140-2151, 2021.\n- [60] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4003-4012, Marseille, France, 2020. European Language Resources Association. ISBN 979-10-95546-34-4.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-82",
      "content": "rom web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4003-4012, Marseille, France, 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494 .\n- [61] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview. net/forum?id=zeFrfgyZln .\n- [62] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 3557-3569, 2022.\n- [63] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 82,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-83",
      "content": "Association for Computational Linguistics: ACL 2022 , pages 3557-3569, 2022.\n- [63] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369-2380, 2018.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 83,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-84",
      "content": "For Common Crawl, we download the 2022-33 snapshot and cc\\_net 8 is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by Petroni et al. [46]. To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string match.\n\nReddit data is collected from the year 2018 to August 2022. For the S2ORC data, we use a sample weight of 0 . 3 during training to avoid over-fitting the scientific domains.\n\nFor the BEIR benchmark, we use the 15 datasets that provide public downloads: MS MARCO [8], Trec-Covid [55], NFCorpus [5], NQ [32], HotpotQA [63], FiQA [37], ArguAna [56], Touche-2020 [4], CQADupStack [26], Quora, DBPedia [24], Scidocs [12], Fever [54], Climate-F",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Dataset Details",
        "chunkIndex": 84,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-85",
      "content": "lic downloads: MS MARCO [8], Trec-Covid [55], NFCorpus [5], NQ [32], HotpotQA [63], FiQA [37], ArguAna [56], Touche-2020 [4], CQADupStack [26], Quora, DBPedia [24], Scidocs [12], Fever [54], Climate-Fever [18], and Scifact [57].\n\n8 https://github.com/facebookresearch/cc\\_net\n\nTable 9: Details for each data source after filtering. The 'Others' category includes 'SimpleWiki', 'GooAQ', 'WikiHow', 'Yahoo Answers' from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data .\n\n| data source   | type of text pairs                                                      | random example                                                                                                                                             | # of pairs   |\n|---------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Dataset Details",
        "chunkIndex": 85,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-86",
      "content": "------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| Wikipedia     | (entity+section title, passage)                                         | q : Lexden History p : The site on which Lexden now stands was crossed by the fortifications of iron age Colchester. . .                                   | 24 M         |\n| Reddit        | (post, upvoted comment)                                                 | q : What makes a client good quality to you? I'm putting together my ideal client . . . p : Respectful of schedules. And pays on time.. . .                | 60 M         |\n| Common Crawl  | (title, passage)                                                        | q : Central Intake Unit | Broome County p : Caseworkers from Central Intake assess the household and risk of placement. If eligible. . .",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Dataset Details",
        "chunkIndex": 86,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-87",
      "content": "ssage)                                                        | q : Central Intake Unit | Broome County p : Caseworkers from Central Intake assess the household and risk of placement. If eligible. . .                   | 69 M         |\n| Stackexchange | (title, answer) (title+description, answer)                             | q : Will killing Python made problems for Apache p : Python and Apache aren't related, unless your app is making use of Python. . . .                      | 19 M         |\n| S2ORC         | (title, abstract) (title, citation title) (abstract, citation abstract) | q : Constructive Dual DP for Reservoir Optimization p : Dynamic programming (DP) is a well established technique for optimization of reservoir manage...   | 90 M         |\n| News          | (title, passage) (highlight, passage)                                   | q : LG Display reports Q1 operating loss as. . .",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Dataset Details",
        "chunkIndex": 87,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-88",
      "content": "for optimization of reservoir manage...   | 90 M         |\n| News          | (title, passage) (highlight, passage)                                   | q : LG Display reports Q1 operating loss as. . . p : April 25 (Reuters) - South Korea's LG Display Co Ltd reported its first quarterly operating loss. . . | 3 M          |\n| Others        | misc.                                                                   | misc.                                                                                                                                                      | 6 M          |\n| All above     | -                                                                       | -                                                                                                                                                          | ∼ 270 M      |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Dataset Details",
        "chunkIndex": 88,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-89",
      "content": "We list the hyperparameters in Table 11. Since some evaluation datasets have long texts, we freeze the position embeddings during both pre-training and fine-tuning and set the maximum text length to 512 for evaluation.\n\nFor the Quora duplicate retrieval task in the BEIR benchmark, we add prefix ' query: ' to all the questions. For other retrieval tasks, we use ' query: ' and ' passage: ' prefixes correspondingly.\n\nThe MS-MARCO results in Table 12 use document titles provided by RocketQA [50]. This evaluation setup is consistent with most state-of-the-art dense retrievers. However, the MS-MARCO data from the BEIR benchmark does not have titles, so the results are expected to be lower.\n\nTable 10: Model configurations.\n\n|          |   # layers |   hidden size | # params   |\n|----------|------------|---------------|------------|\n| E5 small |         12 |           384 | 33 M       |\n| E5 base  |         12 |           768 | 110 M      |\n| E5 large |         24 |          1024 | 330 M",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 89,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-90",
      "content": "------|---------------|------------|\n| E5 small |         12 |           384 | 33 M       |\n| E5 base  |         12 |           768 | 110 M      |\n| E5 large |         24 |          1024 | 330 M      |\n\nTable 11: Hyperparameters for contrastive pre-training and fine-tuning.\n\n|                | pre-training   | pre-training   | pre-training   | fine-tuning   | fine-tuning   | fine-tuning   |\n|----------------|----------------|----------------|----------------|---------------|---------------|---------------|\n|                | E5-PT small    | E5-PT base     | E5-PT large    | E5 small      | E5 base       | E5 large      |\n| learning rate  | 3 × 10 - 4     | 2 × 10 - 4     | 10 - 4         | 3 × 10 - 5    | 2 × 10 - 5    | 10 - 5        |\n| GPUs           | 16             | 32             | 64             | 8             | 8             | 8             |\n| warmup steps   | 1000           | 1000           | 1000           | 400           | 400           | 400           |\n| batch size",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 90,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-91",
      "content": "64             | 8             | 8             | 8             |\n| warmup steps   | 1000           | 1000           | 1000           | 400           | 400           | 400           |\n| batch size     | 32 K           | 32 K           | 32 K           | 256           | 256           | 256           |\n| max steps      | 20 K           | 20 K           | 20 K           | n.a.          | n.a.          | n.a.          |\n| max length     | 128            | 128            | 128            | 192           | 192           | 192           |\n| epochs         | n.a.           | n.a.           | n.a.           | 3             | 3             | 3             |\n| τ              | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| α              | n.a.           | n.a.           | n.a.           | 0 . 2         | 0 . 2         | 0 . 2         |\n| weight decay   | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 .",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 91,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-92",
      "content": "| n.a.           | n.a.           | 0 . 2         | 0 . 2         | 0 . 2         |\n| weight decay   | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| hard negatives | 0              | 0              | 0              | 7             | 7             | 7             |\n\nIn-domain Evaluation We report results for in-domain datasets in Table 12. These results can help illustrate the benefits brought by contrastive pre-training when abundant in-domain labeled data are\n\navailable. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ dataset, Recall@20 and Recall@100 are the main metrics.\n\nTable 12: In-domain results. 'target pre-train' refers to intermediate pre-training on the target corpus before supervised fine-tuning. For NQ, we use the passage retrieval setting from DPR [30].",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 92,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-93",
      "content": "cs.\n\nTable 12: In-domain results. 'target pre-train' refers to intermediate pre-training on the target corpus before supervised fine-tuning. For NQ, we use the passage retrieval setting from DPR [30].\n\n|                 | target pre-train?   | MS-MARCO   | MS-MARCO   | NQ   | NQ    |\n|-----------------|---------------------|------------|------------|------|-------|\n|                 | target pre-train?   | MRR@10     | R@1k       | R@20 | R@100 |\n| ANCE [61]       | ✗                   | 33.0       | 95.9       | 81.9 | 87.5  |\n| RocketQAv2 [50] | ✗                   | 38.8       | 98.1       | 83.7 | 89.0  |\n| SimLM [58]      | ✓                   | 41.1       | 98.7       | 85.2 | 89.7  |\n| E5 small        | ✗                   | 37.5       | 98.1       | 84.6 | 89.8  |\n| E5 base         | ✗                   | 38.5       | 98.5       | 86.1 | 90.7  |\n| E5 large        | ✗                   | 39.4       | 98.7       | 86.4 | 90.5  |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 93,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-94",
      "content": "| ✗                   | 38.5       | 98.5       | 86.1 | 90.7  |\n| E5 large        | ✗                   | 39.4       | 98.7       | 86.4 | 90.5  |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Implementation Details",
        "chunkIndex": 94,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-95",
      "content": "Here are some attempts that we eventually give up on:\n\nAdding BM25 hard negatives Similar to DPR [30], we add one BM25 hard negative for each positive pair during training. When using 15 Mdata, this strategy improves the overall results by ∼ 0 . 5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250 M+ dataset is too time-consuming even with multi-node and multi-process parallelism.\n\nUsing RoBERTa instead of BERT for initialization Though RoBERTa shows consistent gains on many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on most of the BEIR benchmark datasets.\n\nAuxiliary MLM objective We add a masked language modeling loss for 25 %of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 95,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-96",
      "content": "asets.\n\nAuxiliary MLM objective We add a masked language modeling loss for 25 %of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.\n\nTable 13: Results for each dataset in the MTEB benchmark [40]. The numbers for the Retrieval category are not included here since the datasets are the same as the BEIR benchmark.\n\n|                                    | unsupervised   | unsupervised   | unsupervised   | supervised   | supervised   | supervised   |\n|------------------------------------|----------------|----------------|----------------|--------------|--------------|--------------|\n|                                    | E5-PT small    | E5-PT base     | E5-PT large    | E5 small     | E5 base      | E5 large     |\n| AmazonCounterfactualClassification | 71.7           | 73.6           | 70.4           | 76.2         | 79.7         | 77.7         |\n| AmazonPolarityClassification       | 76.1           | 77.0           | 83.2",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 96,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-97",
      "content": "erfactualClassification | 71.7           | 73.6           | 70.4           | 76.2         | 79.7         | 77.7         |\n| AmazonPolarityClassification       | 76.1           | 77.0           | 83.2           | 87.5         | 88.0         | 90.1         |\n| AmazonReviewsClassification        | 35.0           | 35.8           | 37.4           | 42.6         | 42.7         | 43.0         |\n| Banking77Classification            | 82.1           | 82.9           | 83.5           | 81.9         | 83.3         | 84.1         |\n| EmotionClassification              | 42.2           | 44.2           | 43.5           | 46.9         | 49.4         | 48.1         |\n| ImdbClassification                 | 67.9           | 67.3           | 77.7           | 75.6         | 76.0         | 82.1         |\n| MassiveIntentClassification        | 70.2           | 71.1           | 70.8           | 72.2         | 72.3         | 73.2         |\n| MassiveScenarioClassification      | 74.6           | 75.4",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 97,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-98",
      "content": "assiveIntentClassification        | 70.2           | 71.1           | 70.8           | 72.2         | 72.3         | 73.2         |\n| MassiveScenarioClassification      | 74.6           | 75.4           | 75.9           | 75.8         | 76.8         | 77.4         |\n| MTOPDomainClassification           | 91.3           | 92.3           | 93.2           | 92.1         | 93.2         | 93.9         |\n| MTOPIntentClassification           | 71.9           | 74.0           | 74.2           | 73.2         | 74.8         | 76.4         |\n| ToxicConversationsClassification   | 67.0           | 67.4           | 66.1           | 72.8         | 74.1         | 70.6         |\n| TweetSentimentExtractionClass.     | 54.4           | 53.3           | 52.5           | 63.3         | 61.4         | 61.2         |\n| ArxivClusteringP2P                 | 47.9           | 49.3           | 49.4           | 44.1         | 44.6         | 46.2         |\n| ArxivClusteringS2S                 | 39.9           | 42",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 98,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-99",
      "content": "|\n| ArxivClusteringP2P                 | 47.9           | 49.3           | 49.4           | 44.1         | 44.6         | 46.2         |\n| ArxivClusteringS2S                 | 39.9           | 42.8           | 43.6           | 37.1         | 40.5         | 41.4         |\n| BiorxivClusteringP2P               | 38.5           | 38.8           | 39.2           | 35.8         | 36.2         | 37.6         |\n| BiorxivClusteringS2S               | 35.4           | 36.5           | 36.7           | 31.9         | 32.7         | 35.1         |\n| MedrxivClusteringP2P               | 34.4           | 33.7           | 33.3           | 31.3         | 31.5         | 32.3         |\n| MedrxivClusteringS2S               | 32.0           | 32.1           | 32.2           | 28.2         | 28.3         | 29.7         |\n| RedditClustering                   | 46.9           | 49.3           | 52.4           | 42.9         | 48.2         | 50.7         |\n| RedditClusteringP2P                | 60.2",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 99,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-100",
      "content": "| 29.7         |\n| RedditClustering                   | 46.9           | 49.3           | 52.4           | 42.9         | 48.2         | 50.7         |\n| RedditClusteringP2P                | 60.2           | 64.4           | 64.6           | 56.4         | 62.2         | 61.4         |\n| StackExchangeClustering            | 57.7           | 60.2           | 63.3           | 59.1         | 63.9         | 65.0         |\n| StackExchangeClusteringP2P         | 32.0           | 34.0           | 34.7           | 30.3         | 32.6         | 33.6         |\n| TwentyNewsgroupsClustering         | 34.4           | 36.2           | 37.9           | 37.5         | 42.6         | 43.8         |\n| SprintDuplicateQuestions           | 91.6           | 90.8           | 92.0           | 95.3         | 94.9         | 95.4         |\n| TwitterSemEval2015                 | 60.0           | 62.8           | 64.7           | 74.2         | 74.4         | 76.1         |\n| TwitterURLCorpus                   |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 100,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-101",
      "content": "9         | 95.4         |\n| TwitterSemEval2015                 | 60.0           | 62.8           | 64.7           | 74.2         | 74.4         | 76.1         |\n| TwitterURLCorpus                   | 83.2           | 84.0           | 84.1           | 85.8         | 86.0         | 86.3         |\n| AskUbuntuDupQuestions              | 57.8           | 57.6           | 58.3           | 59.4         | 59.7         | 60.1         |\n| MindSmallReranking                 | 29.0           | 29.6           | 29.2           | 29.6         | 30.1         | 30.8         |\n| SciDocsRR                          | 81.1           | 82.6           | 84.3           | 79.8         | 82.9         | 83.9         |\n| StackOverflowDupQuestions          | 44.4           | 44.2           | 45.8           | 49.1         | 50.1         | 51.3         |\n| BIOSSES                            | 69.2           | 71.9           | 69.7           | 84.2         | 85.1         | 84.7         |\n| SICK-R",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 101,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-102",
      "content": "| 50.1         | 51.3         |\n| BIOSSES                            | 69.2           | 71.9           | 69.7           | 84.2         | 85.1         | 84.7         |\n| SICK-R                             | 66.6           | 68.7           | 69.7           | 78.9         | 79.7         | 80.5         |\n| STS12                              | 60.7           | 57.9           | 54.7           | 75.2         | 74.2         | 75.9         |\n| STS13                              | 71.1           | 73.5           | 74.0           | 81.8         | 83.3         | 85.2         |\n| STS14                              | 64.2           | 64.0           | 65.3           | 78.5         | 78.5         | 80.5         |\n| STS15                              | 74.3           | 75.4           | 75.8           | 87.5         | 88.4         | 88.8         |\n| STS16                              | 76.6           | 79.8           | 80.1           | 84.6         | 84.2         | 85.3         |\n| STS17",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 102,
        "totalChunks": 104
      }
    },
    {
      "id": "2212.03533v2-chunk-103",
      "content": "| 87.5         | 88.4         | 88.8         |\n| STS16                              | 76.6           | 79.8           | 80.1           | 84.6         | 84.2         | 85.3         |\n| STS17                              | 78.3           | 77.2           | 76.0           | 87.9         | 87.2         | 89.4         |\n| STS22                              | 59.2           | 56.2           | 62.8           | 63.8         | 62.9         | 63.0         |\n| STSBenchmark                       | 67.7           | 70.5           | 70.9           | 86.4         | 86.2         | 87.2         |\n| SummEval                           | 32.7           | 31.1           | 32.6           | 31.4         | 31.0         | 31.0         |",
      "metadata": {
        "source": "arxiv:2212.03533v2",
        "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Binxing Jiao",
          "Linjun Yang",
          "Daxin Jiang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Negative Results",
        "chunkIndex": 103,
        "totalChunks": 104
      }
    }
  ],
  "fullText": "## Text Embeddings by Weakly-Supervised Contrastive Pre-training\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao Linjun Yang , Daxin Jiang , Rangan Majumder , Furu Wei\n\nMicrosoft Corporation https://github.com/microsoft/unilm\n\n## Abstract\n\nThis paper presents E5 1 , a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40 × more parameters.\n\n## 1 Introduction\n\nText embeddings are low-dimensional vector representations for arbitrary-length texts and play key roles in many NLP tasks such as large-scale retrieval. Compared to the high-dimensional and sparse representations like TF-IDF, text embeddings have the potential to overcome the lexical mismatch issue and facilitate efficient retrieval and matching between texts. It also offers a versatile interface easily consumable by downstream applications.\n\nWhile pre-trained language models such as BERT [17] and GPT [7] can produce transferrable text representations, they are not ideal for tasks such as retrieval and text matching where a singlevector embedding of texts is more desired due to its efficiency and versatility. To obtain better text embeddings, contrastive learning is often the go-to framework to enhance the sequence-level representations from text pairs. Along this line of research, some works are geared towards learning task-specific embeddings. For example, GTR [43] and Sentence-T5 [44] fine-tune pre-trained models with supervised datasets to learn embeddings customized for passage retrieval and semantic textual similarity, respectively. Other works learn unsupervised embeddings from automatically constructed text pairs. Typical methods to construct text pairs include Inverse Close Task (ICT) [9], random cropping [28] and neighboring text spans [41], etc. While such synthetic data are of unlimited quantity, they are often poor in quality and the resulted embeddings fail to match the performance of the classic BM25 baseline without further fine-tuning [40].\n\nIn this work, we learn a high-quality general-purpose text embedding termed E5, E mb E ddings from bidir E ctional E ncoder r E presentations. E5 aims to provide strong off-the-shelf text embeddings suitable for any tasks requiring single-vector representations in both zero-shot or fine-tuned settings. To achieve this goal, instead of relying on limited labeled data or low-quality synthetic text pairs, we contrastively train E5 embeddings from CCPairs, a curated web-scale text pair dataset containing\n\n1 E5: E mb E ddings from bidir E ctional E ncoder r E presentations\n\nheterogeneous training signals. We construct the CCPairs dataset by combining various semistructured data sources such as CommunityQA, Common Crawl and Scientific papers, and perform aggressive filtering with a consistency-based filter [15] to improve data quality. We choose a simple contrastive learning recipe using in-batch negatives with a large batch-size to train our model. Extensive experiments on both BEIR and MTEB benchmarks demonstrate the effectiveness of the proposed method. On the BEIR zero-shot retrieval benchmark [53], E5 is the first model to outperform the strong BM25 baseline without using any labeled data. When fine-tuned on labeled datasets, the performance can be further improved. Results on 56 datasets from the recently introduced MTEBbenchmark [40] show that our E5base is competitive against GTRxxl and Sentence-T5xxl, which have 40 × more parameters.\n\n## 2 Related Work\n\nThere have been long-lasting interests in transforming texts into low-dimensional dense embeddings. Early works include Latent Semantic Indexing (LSA) [16] and Latent Dirichlet Allocation (LDA) [3]. LSA utilizes the decomposition of a word-document co-occurrence matrix to generate document embeddings, while LDA adopts probabilistic graphical models to learn topic distributions. Arora et al. show that a simple weighted average of word vectors [38] can be a strong baseline for sentence embeddings.\n\nWith the development of pre-trained language models [17, 35, 48] and large-scale labeled datasets such as SNLI [6] and MS-MARCO [8], methods like Sentence-BERT [49], SimCSE [22], SentenceT5 [44] and SGPT [39] directly fine-tune language models to output continuous embeddings. Most research focuses on short texts and thus uses the term \"sentence embeddings\". For long documents, it remains an open research question whether fixed-length embeddings can encode all the information. Contrastive loss popularized by SimCLR [10] turns out to be more effective than classificationbased losses [49, 14] for embeddings. LaBSE [20], LASER [2] and CLIP [47] further extend to multilingual and multi-modal scenarios using parallel sentences and image-text pairs.\n\nAnother direction is to design self-supervised pre-training tasks for text matching and retrieval. [9] proposes the well-known inverse cloze task (ICT), where a random sentence within a passage is chosen as a pseudo-query and the rest is treated as a positive sample. However, Contriever [28] shows that random cropping with data augmentation is more effective than ICT on a range of zero-shot information retrieval tasks. OpenAI text embeddings [41] use neighboring texts as positives and scale up the model size to 175 B. Oguz et al. [45] performs domain-matched pre-training to improve in-domain results. SPAR [11] trains a dense retriever by treating BM25 as a teacher model. Although the aforementioned approaches can easily obtain abundant supervision signals, such synthetic data tend to be of low quality. Results on the BEIR benchmark [53] show they struggle to match the performance of BM25 if not further fine-tuned on labeled datasets.\n\nEvaluation and interpretation of text embeddings are also non-trivial. Most benchmarks measure the embedding quality through downstream task performances. For example, SentEval [13] uses linear probing and a collection of semantic textual similarity (STS) datasets, while the BEIR benchmark [53] focuses on zero-shot information retrieval scenarios. The recently introduced MTEB benchmark [40] combines 56 datasets spanning across 8 tasks and 112 languages. Experiments show no model can achieve state-of-the-art results on all embedding tasks yet. In this paper, we do not use the SentEval toolkit since its linear probing setup depends on the optimization hyperparameters.\n\nMost closely related to our work is a series of community efforts by sentence-transformers 2 to train embeddings with a collection of labeled and automatically collected datasets. In this paper, we show that it is possible to train high-quality embeddings using self-supervised pre-training only. In terms of benchmark results, our model can achieve superior performance when fine-tuned on less labeled data.\n\n## 3 CCPairs: A Large Collection of Text Pair Dataset\n\nThe quality and diversity of the data is crucial for training general-purpose text embeddings. In this work, we mine and assemble CCPairs, a large high-quality text pair dataset from web sources which provide diverse training signals transferring well to a wide range of tasks.\n\n2 https://github.com/UKPLab/sentence-transformers\n\nFigure 1: Overview of our data curation pipeline and model architecture.\n\n<!-- image -->\n\nHarvesting semi-structured data sources Large-scale high-quality datasets like C4 [48] and CCMatrix [51] are vital for the success of language model pre-training and machine translation. For learning text embeddings, existing works either utilize small-scale human-annotated data such as NLI [22] and MS-MARCO [8] or adopt heuristics such as random cropping [28] to obtain large-scale but very noisy supervision signals.\n\nInstead, we curate a text pair dataset CCPairs ( C olossal C lean text Pairs ) by harvesting heterogeneous semi-structured data sources. Let ( q , p ) denote a text pair consisting of a query q and a passage p . Here we use ' passage ' to denote word sequences of arbitrary length, which can be a short sentence, a paragraph, or a long document. Our dataset includes (post, comment) pairs from Reddit 3 , (question, upvoted answer) pairs from Stackexchange 4 , (entity name + section title, passage) pairs from English Wikipedia, (title, abstract) and citation pairs from Scientific papers [36], and (title, passage) pairs from Common Crawl 5 web pages and various News sources.\n\nWe only include data sources that can be automatically mined, and some subsets are directly reused from existing datasets. Simple heuristic rules are applied to filter data from Reddit and Common Crawl. For example, we remove Reddit comments that are either too long ( &gt; 4096 characters) or receive score less than 1 , and remove passages from web pages with high perplexity [60]. After preliminary filtering, we end up with ∼ 1 . 3 billion text pairs, most of which come from Reddit and Common Crawl. For more details and examples, please refer to Appendix A.\n\nConsistency-based filter To further improve data quality and make training costs manageable, we propose a consistency-based data filtering technique: a model is first trained on the 1 . 3 B noisy text pairs, and then used to rank each pair against a pool of 1 million random passages. A text pair is kept only if it falls in the topk ranked lists. In other words, the model's prediction should be consistent with the training labels. Here we set k = 2 based on manual inspection of data quality. After this step, we end up with ∼ 270 Mtext pairs for contrastive pre-training.\n\nThe intuition for this technique comes from the memorization behaviors of neural networks [19]: when trained on noisy datasets, neural networks tend to memorize the clean labels first and then gradually overfit the noisy labels. Similar techniques [42, 15, 23] have been widely used for removing dataset noises. It is also possible to apply this filter iteratively, we will leave it for future work.\n\n## 4 Method\n\nOur embeddings can be trained with only unlabeled text pairs from CCPairs with contrastive pretraining. A second-stage fine-tuning on small, high-quality labeled datasets can be performed to further boost the quality of the resulted embeddings. See Figure 1 for an overview.\n\n3 https://files.pushshift.io/reddit/\n\n4 https://archive.org/details/stackexchange\n\n5 https://commoncrawl.org/\n\n## 4.1 Contrastive Pre-training with Unlabeled Data\n\nContrastive pre-training aims to distinguish the relevant text pairs from other irrelevant or negative pairs. Given a collection of text pairs { ( q i , p i ) } n i =1 , we assign a list of negative passages { p -ij } j m =1 for the i -th example. Then the InfoNCE contrastive loss [10] is as follows:\n\n<!-- formula-not-decoded -->\n\nwhere s θ ( q, p ) is a scoring function between query q and passage p parameterized by θ . Following the popular biencoder architecture, we use a pre-trained Transformer encoder and average pooling over the output layer to get fixed-size text embeddings E q and E p . The score is the cosine similarity scaled by a temperature hyperparameter τ :\n\n<!-- formula-not-decoded -->\n\nWhere τ is set to 0 . 01 in our experiments by default. We use a shared encoder for all input texts and break the symmetry by adding two prefix identifiers 'query:' and 'passage:' to q and d respectively. For some data sources such as citation pairs, it is not obvious which side should be the query, we randomly choose one for simplicity. Such an asymmetric design turns out to be important for some retrieval tasks where there exist paraphrases of the query in the target corpus.\n\nAnother critical issue for contrastive training is how to select the negative samples. Here we choose to use the in-batch negatives [10], where the passages from other pairs in a batch serve as negative samples. We find that this simple strategy enables more stable training and outperforms methods such as MoCo [25] when the batch size is sufficiently large.\n\n## 4.2 Fine-tuning with Labeled Data\n\nWhile contrastive pre-training on the CCPairs provides a solid foundation for general-purpose embeddings, further training on labeled data can inject human knowledge into the model to boost the performance. Although these datasets are small, existing works [43, 44] have shown that supervised fine-tuning leads to consistent performance gains. In this paper, we choose to further train with a combination of 3 datasets: NLI 6 (Natural Language Inference), MS-MARCO passage ranking dataset [8], and NQ (Natural Questions) dataset [30, 32]. Empirically, tasks like STS (Semantic Textual Similarity) and linear probing benefit from NLI data, while MS-MARCO and NQ datasets transfer well to retrieval tasks.\n\nBuilding on the practices of training state-of-the-art dense retrievers [50, 58], we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model for the MS-MARCO and NQ datasets. For the NLI dataset, contradiction sentences are regarded as hard negatives. The loss function is a linear interpolation between contrastive loss L cont for hard labels and KL divergence D KL for distilling soft labels from the teacher model.\n\n<!-- formula-not-decoded -->\n\nWhere p ce and p stu are the probabilities from the cross-encoder teacher model and our student model. α is a hyperparameter to balance the two loss functions. L cont is the same as in Equation 1.\n\n## 4.3 Applications to Text Embedding Tasks\n\nAfter the above two steps, we obtain high-quality text embeddings transferring well to a wide range of tasks without fine-tuning the model parameters. Combined with techniques like approximate nearest neighbor search, embeddings provide a scalable and efficient solution for applications like web search. Here we briefly illustrate several use cases of our text embeddings.\n\nZero-shot Retrieval First, the passage embeddings for the target corpus are computed and indexed offline. Then for each query, we compute its query embedding and return the topk ranked lists from the corpus based on cosine similarity.\n\nFew-shot Text Classification A linear classifier is trained on top of the frozen embeddings with a few labeled examples. Different tasks only need to train and save the parameters of the classification heads. It can be seen as a particular form of parameter-efficient learning [27].\n\n6 The version released by SimCSE.\n\nZero-shot Text Classification The input and label texts are converted to sentences based on manually written prompt templates. The predicted label is the one closest to the input text in the embedding space. Take the sentiment classification of movie reviews as an example, with the original input ' I enjoy watching it ', the label text is ' it is an example of terrible/great movie review ' and the input text becomes ' movie review: I enjoy watching it '.\n\nSemantic Textual Similarity Given two text embeddings, we use the cosine function to measure their semantic similarity. Since the absolute similarity scores do not enable an easy interpretation, the evaluation is usually based on rank correlation coefficients.\n\nText Clustering Standard clustering algorithms such as k-means can be applied straightforwardly. Texts belonging to the same category are expected to be close in the embedding space.\n\nFor tasks other than zero-shot text classification and retrieval, we use the query embeddings by default.\n\n## 5 Experiments\n\n## 5.1 Pre-training and Fine-tuning Configurations\n\nPre-training We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base and E5large initialized from MiniLM [59], bert-base-uncased , and bert-large-uncased-whole-wordmasking respectively. The batch size is set to a large value of 32 , 768 to increase the number of negatives. The learning rate is { 3 , 2 , 1 } × 10 -4 for the {small, base, large} models, with linear decay and the first 1 , 000 steps for warmup. We pre-train for 20 k steps in total with AdamW optimizer, which is approximately 2 . 5 epochs over the dataset. It takes { 16 , 32 , 64 } V100 GPUs and { 1 , 1 , 2 } days for the {small, base, large} models. To improve training efficiency and reduce GPU memory usage, we adopt mixed precision training and gradient checkpointing.\n\nFine-tuning is performed on the concatenation of 3 datasets: MS-MARCO passage ranking [8], NQ [32, 30], and NLI [22] datasets. We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets. Models are fine-tuned for 3 epochs with batch size 256 on 8 GPUs. Learning rate is { 3 , 2 , 1 } × 10 -5 for the {small, base, large} models with 400 steps warmup. For each example, we use 7 hard negatives. Since the NLI dataset only has 1 hard negative for each example, 6 sentences are randomly sampled from the entire corpus.\n\nWe use E5-PT to denote models with contrastive pre-training only. More implementation details can be found in Appendix B.\n\n## 5.2 Evaluation Datasets\n\nBEIR Benchmark [53] is a collection of 19 information retrieval datasets, ranging across ad-hoc web search, question answering, fact verification and duplicate question retrieval, etc. We evaluate the 15 datasets that provide public downloads. The main metric is nDCG@ 10 .\n\nMTEB Benchmark [40] is recently proposed for benchmarking massive text embedding tasks. Though MTEB is multilingual due to the inclusion of bitext mining datasets, most datasets are still only available in English. In this paper, we evaluate the English subsets, which have 56 datasets spanning across 6 categories: Classification (Class.), Clustering (Clust.), Pair Classification (PairClass.), Rerank, Retrieval (Retr.), STS, and Summarization (Summ.). The evaluation metrics are accuracy, v-measure, average precision, MAP, nDCG@10, and Spearman coefficients, respectively. Please refer to the MTEB paper for details.\n\n## 5.3 Results on BEIR benchmark\n\nResults with Unsupervised Methods In Table 1, we show model results that do not use any labeled data. When averaged over all 15 datasets, E5-PTbase outperforms the classic BM25 algorithm by 1 . 2 points. To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark. When scaling up to E5-PTlarge, we see further benefits from 42 . 9 to 44 . 2 .\n\nTable 1: Unsupervised methods on the BEIR benchmark (nDCG@ 10 ). For SimCSE, we report results with BERTbase. cpt300M [41] is only available through paid API and evaluation results on some datasets are missing in the original paper. The highest number for each dataset is in bold, and the second highest is underlined. † : we report the LaPraDor [62] results without ensembling with BM25. ∗ : reproduction with the released checkpoint.\n\n|               |   BM25 |   SimCSE | LaPraDor †   |   Contriever | cpt 300M   |   E5-PT small |   E5-PT base |   E5-PT large |\n|---------------|--------|----------|--------------|--------------|------------|---------------|--------------|---------------|\n| MS MARCO      |   22.8 |      9.4 | 16.9 ∗       |         20.6 | 19.9       |          25.4 |         26   |          26.2 |\n| Trec-Covid    |   65.6 |     26.2 | 22.7         |         27.4 | 52.9       |          52   |         61   |          61.8 |\n| NFCorpus      |   32.5 |      9.9 | 31.1         |         31.7 | 32.0       |          29.3 |         35.8 |          33.7 |\n| NQ            |   32.9 |     11.7 | 18.1         |         25.4 | -          |          37.3 |         39   |          41.7 |\n| HotpotQA      |   60.3 |     19.8 | 30.3         |         48.1 | 51.5       |          46   |         52.4 |          52.2 |\n| FiQA          |   23.6 |      9.8 | 20.3         |         24.5 | 34.1       |          38.3 |         40   |          43.2 |\n| ArguAna       |   31.5 |     38.3 | 45.9         |         37.9 | 38.7       |          42.5 |         42.2 |          44.4 |\n| Touche-2020   |   36.7 |      8.9 | 9.4          |         19.3 | 21.0       |          19.9 |         16.9 |          19.8 |\n| CQADupStack   |   29.9 |     13.2 | 22.0         |         28.4 | -          |          35   |         35.4 |          38.9 |\n| Quora         |   78.9 |     78   | 78.7         |         83.5 | 68.1       |          85.8 |         85.7 |          86.1 |\n| DBPedia       |   31.3 |     15   | 25.0         |         29.2 | 27.2       |          34.5 |         35.4 |          37.1 |\n| Scidocs       |   15.8 |      5.5 | 13.3         |         14.9 | -          |          19.9 |         21.1 |          21.8 |\n| Fever         |   75.3 |     21.1 | 36.8         |         68.2 | 57.1       |          62.5 |         63.4 |          68.6 |\n| Climate-Fever |   21.3 |     11.8 | 13.8         |         15.5 | 15.8       |          14.5 |         15.4 |          15.7 |\n| Scifact       |   66.5 |     25.7 | 55.5         |         64.9 | 65.4       |          68.5 |         73.7 |          72.3 |\n| Average       |   41.7 |     20.3 | 29.3         |         36   | -          |          40.8 |         42.9 |          44.2 |\n| Best on       |    5   |      0   | 1            |          0   | 0          |           0   |          2   |           7   |\n\nIn terms of pre-training tasks, Contriever adopts random cropping, while LaPraDor combines ICT and dropout-as-positive-instance from SimCSE. The methods can easily obtain large-scale training data, while our approach requires more effort in dataset curation. Such efforts pay off with better results. Recent studies [34, 60, 21] also show that improving data quality is a vital step for training large language models.\n\nTable 2: Supervised fine-tuning results on the BEIR benchmark. Results for ANCE [61], ColBERT [31] and Contriever come from Izacard et al. [28]. The best result is in bold, and the second best is underlined.\n\n|               |   ANCE |   GTR base |   ColBERT |   Contriever | cpt 300M   |   GTR large |   E5 small |   E5 base |   E5 large |\n|---------------|--------|------------|-----------|--------------|------------|-------------|------------|-----------|------------|\n| MS MARCO      |   38.8 |       42   |      40.1 |         40.7 | -          |        43   |       42.3 |      43.1 |       44.1 |\n| Trec-Covid    |   65.4 |       53.9 |      67.7 |         59.6 | 67.9       |        55.7 |       76.8 |      79.6 |       78.3 |\n| NFCorpus      |   23.7 |       30.8 |      30.5 |         32.8 | 33.2       |        32.9 |       33.9 |      36.6 |       36.1 |\n| NQ            |   44.6 |       49.5 |      52.4 |         49.8 | -          |        54.7 |       58.7 |      60   |       62.9 |\n| HotpotQA      |   45.6 |       53.5 |      59.3 |         63.8 | 59.4       |        57.9 |       56.3 |      62.2 |       63.3 |\n| FiQA          |   29.5 |       34.9 |      31.7 |         32.9 | 38.4       |        42.4 |       34.8 |      36.4 |       38.6 |\n| ArguAna       |   41.5 |       51.1 |      23.3 |         44.6 | 47.0       |        52.5 |       46.7 |      51.4 |       49.4 |\n| Touche-2020   |   24   |       20.5 |      20.2 |         23   | 28.5       |        21.9 |       26.8 |      28.3 |       27.2 |\n| CQADupStack   |   29.6 |       35.7 |      35   |         34.5 | -          |        38.4 |       36.1 |      38.9 |       39.4 |\n| Quora         |   85.2 |       88.1 |      85.4 |         86.5 | 70.6       |        89   |       87.7 |      87.9 |       88.2 |\n| DBPedia       |   28.1 |       34.7 |      39.2 |         41.3 | 36.2       |        39.1 |       38.6 |      41   |       42.4 |\n| Scidocs       |   12.2 |       14.9 |      14.5 |         16.5 | -          |        15.8 |       16.4 |      19   |       20.1 |\n| Fever         |   66.9 |       66   |      77.1 |         75.8 | 72.1       |        71.2 |       53.5 |      58.2 |       65   |\n| Climate-Fever |   19.8 |       24.1 |      18.4 |         23.7 | 18.5       |        26.2 |       15.8 |      15.4 |       22.4 |\n| Scifact       |   50.7 |       60   |      67.1 |         67.7 | 67.2       |        63.9 |       65.6 |      73.1 |       72.6 |\n| Average       |   40.5 |       44   |      44.4 |         46.6 | -          |        47   |       46   |      48.7 |       50   |\n| Best on       |    0   |        0   |       1   |          1   | 1          |         4   |        0   |       3   |        5   |\n\nResults with Supervised Fine-tuning In Table 2, we fine-tune our models on supervised datasets and then transfer them to the BEIR benchmark. Since our fine-tuning datasets include MS-MARCO and NQ, the corresponding numbers are in-domain results. For other datasets, these are zero-shot transfer results. Our E5base model achieves an average nDCG@ 10 of 48 . 7 , already surpassing existing methods with more parameters such as GTRlarge [43]. Most datasets benefit from supervised finetuning, but there are also a few exceptions such as FiQA, Scidocs, and Fever, etc. This is likely due to the lack of enough domain diversity for the fine-tuning datasets.\n\nTable 3: Results on the MTEB benchmark [40] (56 datasets in English subset). Here we only report averaged numbers on each task category for space reasons, please check out Appendix B for a detailed version. BERT-FTbase uses the same fine-tuning data as E5 but initializes from BERTbase.\n\n| # of datasets →     | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|---------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised models |             |             |                |            |            |          |           |          |\n| Glove               | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| BERT                | 61.7        | 30.1        | 56.3           | 43.4       | 10.6       | 54.4     | 29.8      | 38.3     |\n| SimCSE-BERT-unsup   | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| E5-PT small         | 67.0        | 41.7        | 78.2           | 53.1       | 40.8       | 68.8     | 32.7      | 54.3     |\n| E5-PT base          | 67.9        | 43.4        | 79.2           | 53.5       | 42.9       | 69.5     | 31.1      | 55.6     |\n| E5-PT large         | 69.0        | 44.3        | 80.3           | 54.4       | 44.2       | 69.9     | 32.6      | 56.6     |\n| Supervised models   |             |             |                |            |            |          |           |          |\n| SimCSE-BERT-sup     | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| BERT-FT base        | 68.7        | 33.9        | 82.6           | 50.5       | 41.5       | 79.2     | 29.0      | 55.2     |\n| Contriever          | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR large           | 67.1        | 41.6        | 85.3           | 55.4       | 47.4       | 78.2     | 29.5      | 58.3     |\n| Sentence-T5 large   | 72.3        | 41.7        | 85.0           | 54.0       | 36.7       | 81.8     | 29.6      | 57.1     |\n| E5 small            | 71.7        | 39.5        | 85.1           | 54.5       | 46.0       | 80.9     | 31.4      | 58.9     |\n| E5 base             | 72.6        | 42.1        | 85.1           | 55.7       | 48.7       | 81.0     | 31.0      | 60.4     |\n| E5 large            | 73.1        | 43.3        | 85.9           | 56.5       | 50.0       | 82.1     | 31.0      | 61.4     |\n| Larger models       |             |             |                |            |            |          |           |          |\n| GTR xxl             | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl     | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |\n\n## 5.4 Results on MTEB benchmark\n\nIn Table 3, E5 models not only substantially outperform existing ones with similar sizes, but also match the results of much larger models. The top2 models on MTEB leaderboard 7 GTRxxl and Sentence-T5xxl have 4 . 8 B parameters, while our E5large model is more than 10 × smaller with 300 M parameters. We expect that our model will benefit from continual scaling up.\n\nSince the difference between BERT-FTbase and E5base is that BERT-FTbase only has fine-tuning stage, their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset. For most task categories except Clustering, performance improves after supervised fine-tuning. Consistent with prior works [43, 44], this once again demonstrates the importance of incorporating human knowledge for learning better text embeddings. It remains an open question whether state-of-the-art embeddings can be obtained in a purely self-supervised manner.\n\nTable 4: Zero-shot text classification results. 'Majority' always predicts the majority class label. Zero-shot BERTbase uses the average pooling of the last layer as text embeddings.\n\n|            | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Full Fine-tune   | Full Fine-tune   |\n|------------|-------------|-------------|-------------|-------------|-------------|------------------|------------------|\n|            | Majority    | BERT base   | E5 small    | E5 base     | E5 large    | BERT base        | BERT large       |\n| SST-2 [52] | 50.9        | 58.9        | 79.7        | 81.3        | 85.3        | 93.5             | 94.9             |\n\nTable 4 shows the zero-shot text classification results on the dev set of the SST-2 dataset [52]. By formulating text classification as embedding matching between input and label texts, our model can be much better than the 'majority' baseline in a zero-shot setting. We use the prompt template from Section 4.3.\n\n## 5.5 Analysis\n\nIn this section, we conduct a series of analyses to examine various design choices. All the numbers in this section are from base-size models. For the BEIR benchmark, we choose 6 datasets with more stable results across different runs. Some negative results are also listed in Appendix C.\n\nImpacts of Batch Size Since we use in-batch negatives for contrastive pre-training, larger batch size will provide more negatives and therefore improve the quality of the learned text embeddings. In\n\n7 https://huggingface.co/spaces/mteb/leaderboard , as of November 22, 2022\n\nTable 5: Impacts of different batch sizes for contrastive pre-training.\n\n| batch size   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------|--------|---------|-----------|-----------|-------|\n| 32k          |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| 8k           |       33.3 | 38.5 |   37.6 |    85.7 |      34   |      71.8 |  50.2 |\n| 1k           |       28.2 | 33.1 |   30.4 |    84   |      30.1 |      69.1 |  45.8 |\n\nTable 5, increasing batch size from 1 K to 32 K leads to consistent gains across all 6 datasets. It is also possible to train with smaller batch sizes by adding hard negatives [50]. However, the engineering efforts of mining hard negatives for large datasets (&gt; 100 M) are non-trivial.\n\nTable 6: Fine-tuning with different combinations of labeled data.\n\n| Fine-tuned on   |   Retrieval |   STS |   Classification |   Summ. |   MTEB Avg |\n|-----------------|-------------|-------|------------------|---------|------------|\n| No fine-tuning  |        42.9 |  69.5 |             67.9 |    31.1 |       55.6 |\n| MS-MARCO + NQ   |        50.3 |  78.3 |             68.3 |    30.6 |       59   |\n| NLI             |        38.3 |  81.1 |             72.6 |    31.6 |       57.3 |\n| All above       |        48.7 |  81   |             73.1 |    31   |       60.4 |\n\nFine-tuning Datasets GTR models are fine-tuned with 'MS-MARCO + NQ', while Sentence-T5 models use NLI instead. In Table 6, we can see that the 'MS-MARCO + NQ' setting performs best on retrieval tasks, and the NLI data is beneficial for STS and linear probing classification. Similar observations are also made by Muennighoff et al. [40]. Combining all of them leads to the best overall scores on the MTEB benchmark. This also illustrates the importance of dataset diversity for learning text embeddings.\n\nTable 7: Data filtering. For the top 2 rows, we train with 1 Mrandom text pairs.\n\n| # of pairs   |            |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------------|------|--------|---------|-----------|-----------|-------|\n| 1M           | w/o filter |       23   | 15.1 |   18.5 |    83.1 |      18.2 |      51.4 |  34.9 |\n| 1M           | w/ filter  |       26.8 | 22.7 |   24.5 |    85   |      27.5 |      57.5 |  40.7 |\n| All          | w/o filter |       34.5 | 35.4 |   39.1 |    85.7 |      32.9 |      72.5 |  50   |\n| All          | w/ filter  |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n\nData Filtering One crucial step in our dataset curation pipeline is filtering out low-quality text pairs. In Table 7, when training with 1 Mpairs, using filtered data has a nearly 6 points advantage. When all the text pairs are used, the 'w/o filter' setting has about 4 × more data but is still behind by 1 . 6 points. Though recent studies [29, 47] show that deep learning models are quite robust to dataset noises, data filtering still has benefits in improving training efficiency and model quality.\n\nNegative Sampling We explore two alternative methods to enlarge the number of negatives: Prebatch negatives [33] reuse embeddings from previous batches as additional negatives, while MoCo [25] introduces a momentum encoder and uses a FIFO queue to store negatives. For both approaches, the negative size can be easily scaled up without incurring much GPU memory overhead. The downside is that most negatives are produced by an older version of model parameters. In Table 8, in-batch negatives still perform favorably. Empirically, we find that MoCo is more sensitive to certain hyperparameters such as temperature, better results are possible with more tuning.\n\nBM25 vs Dense Retrieval With the rapid development of dense retrieval models, can we replace the long-standing BM25 algorithm from now on? The answer is likely ' not yet '. BM25 still holds obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily on exact lexical match (Fever) [54], further research efforts are still necessary to improve current dense retrievers.\n\n## 6 Conclusion\n\nIn this work, we train a general-purpose text embedding model E5 from weak supervision signals. We adopt a simple contrastive training framework with in-batch negatives and learn from a large-scale text pair dataset we harvest from heterogeneous data sources across the web. E5 offers strong off-the-shelf\n\nTable 8: Comparison of different negative sampling strategies.\n\n|             | # negatives   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|-------------|---------------|------------|------|--------|---------|-----------|-----------|-------|\n| In batch    | 32k           |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| + pre-batch | 64k           |       29.4 | 27.2 |   29.4 |    84.6 |      25   |      64.3 |  43.3 |\n| MoCo        | 130k          |       29.7 | 36.1 |   32   |    81.6 |      29.9 |      63.6 |  45.5 |\n\nperformance for a wide range of tasks requiring single-vector text representations such as retrieval, semantic textual similarity, and text matching. When further customized for downstream tasks, E5 achieves superior fine-tuned performance compared to existing embedding models with 40 × more parameters on the large, 56-task MTEB benchmark datasets.\n\n## References\n\n- [1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net, 2017. URL https://openreview.net/forum?id=SyK00v5xx .\n- [2] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics , 7:597-610, 2019. doi: 10.1162/tacl\\_a\\_00288. URL https://aclanthology. org/Q19-1038 .\n- [3] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada] , pages 601-608. MIT Press, 2001. URL https://proceedings.neurips.cc/paper/2001/hash/ 296472c9542ad4d4788d543508116cbc-Abstract.html .\n- [4] Alexander Bondarenko, Maik Fröbe, Johannes Kiesel, Shahbaz Syed, Timon Gurcke, Meriem Beloucif, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, et al. Overview of touché 2022: argument retrieval. In International Conference of the CrossLanguage Evaluation Forum for European Languages , pages 311-336. Springer, 2022.\n- [5] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. A full-text learning to rank dataset for medical information retrieval. In European Conference on Information Retrieval , pages 716-722. Springer, 2016.\n- [6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632-642, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //aclanthology.org/D15-1075 .\n- [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .\n- [8] Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. Ms marco: A human generated machine reading comprehension dataset. ArXiv , abs/1611.09268, 2016.\n\n- [9] Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. Pre-training tasks for embedding-based large-scale retrieval. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id=rkg-mA4FDr .\n- [10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , volume 119 of Proceedings of Machine Learning Research , pages 1597-1607. PMLR, 2020. URL http: //proceedings.mlr.press/v119/chen20j.html .\n- [11] Xilun Chen, Kushal Lakhotia, Barlas O˘ guz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918 , 2021.\n- [12] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 2270-2282, 2020.\n- [13] Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) , Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1269 .\n- [14] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670-680, Copenhagen, Denmark, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1070. URL https://aclanthology.org/D17-1070 .\n- [15] Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. ArXiv , abs/2209.11755, 2022.\n- [16] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science , 41(6):391-407, 1990.\n- [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .\n- [18] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. Climate-fever: A dataset for verification of real-world climate claims. arXiv preprint arXiv:2012.00614 , 2020.\n- [19] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020. URL https://proceedings.neurips.cc/ paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html .\n- [20] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Languageagnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 878-891, 2022.\n- [21] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv , abs/2101.00027, 2021.\n\n- [22] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main.552 .\n- [23] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada , pages 8536-8546, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/ a19744e268754fb0148b017647355b7b-Abstract.html .\n- [24] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. Dbpedia-entity v2: a test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1265-1268, 2017.\n- [25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pages 9726-9735. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/ CVPR42600.2020.00975 .\n- [26] Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. Cqadupstack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian document computing symposium , pages 1-8, 2015.\n- [27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research , pages 2790-2799. PMLR, 2019. URL http://proceedings.mlr.press/v97/houlsby19a.html .\n- [28] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. ArXiv , abs/2112.09118, 2021.\n- [29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 4904-4916. PMLR, 2021. URL http://proceedings.mlr.press/v139/jia21b.html .\n- [30] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main. 550 .\n- [31] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over BERT. In Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu, editors, Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , pages 39-48. ACM, 2020. doi: 10.1145/3397271.3401075. URL https://doi.org/10.1145/3397271.3401075 .\n\n- [32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_00276. URL https://aclanthology.org/Q19-1026 .\n- [33] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 6634-6647, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021. acl-long.518 .\n- [34] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In ACL , 2022.\n- [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv , abs/1907.11692, 2019.\n- [36] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4969-4983, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https: //aclanthology.org/2020.acl-main.447 .\n- [37] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www'18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018 , pages 19411942, 2018.\n- [38] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In ICLR , 2013.\n- [39] Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. ArXiv , abs/2202.08904, 2022.\n- [40] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. ArXiv , abs/2210.07316, 2022.\n- [41] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pretraining. ArXiv , abs/2201.10005, 2022.\n- [42] Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi-Phuong-Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. SELF: learning to filter noisy labels with selfensembling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview. net/forum?id=HkgsPhNYPS .\n- [43] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern'andez 'Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. ArXiv , abs/2112.07899, 2021.\n- [44] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, 2022.\n\n- [45] Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen, Sebastian Riedel, Scott Yih, Sonal Gupta, and Yashar Mehdad. Domainmatched pre-training tasks for dense retrieval. In Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022 , pages 1524-1534. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.114. URL https://doi.org/10.18653/v1/2022.findings-naacl.114 .\n- [46] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: a benchmark for knowledge intensive language tasks. In North American Chapter of the Association for Computational Linguistics , 2020.\n- [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html .\n- [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21:1-67, 2020.\n- [49] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/D19-1410 .\n- [50] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 2825-2835, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.224. URL https://aclanthology.org/2021.emnlp-main.224 .\n- [51] Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 6490-6500, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.507. URL https://aclanthology.org/2021.acl-long.507 .\n- [52] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference on Empirical Methods in Natural Language Processing , 2013.\n- [53] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\n- [54] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809-819, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https: //aclanthology.org/N18-1074 .\n- [55] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. Trec-covid: constructing a pandemic\n\ninformation retrieval test collection. In ACM SIGIR Forum , volume 54, pages 1-12. ACM New York, NY, USA, 2021.\n\n- [56] Henning Wachsmuth, Shahbaz Syed, and Benno Stein. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 241-251, 2018.\n- [57] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 7534-7550, 2020.\n- [58] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. ArXiv , abs/2207.02578, 2022.\n- [59] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 2140-2151, 2021.\n- [60] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4003-4012, Marseille, France, 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494 .\n- [61] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https://openreview. net/forum?id=zeFrfgyZln .\n- [62] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense retriever for zero-shot text retrieval. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 3557-3569, 2022.\n- [63] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369-2380, 2018.\n\n## A Dataset Details\n\nFor Common Crawl, we download the 2022-33 snapshot and cc\\_net 8 is used for preprocessing including language identification, de-duplication, language model filtering, etc. Web pages from the MS-MARCO document ranking corpus are also included. For the data filtering step, we examine each pair of passages within a web page instead of just using the title as a query. For Wikipedia, we use the version released by Petroni et al. [46]. To avoid possible data contamination, we remove text pairs that occur in the evaluation datasets based on exact string match.\n\nReddit data is collected from the year 2018 to August 2022. For the S2ORC data, we use a sample weight of 0 . 3 during training to avoid over-fitting the scientific domains.\n\nFor the BEIR benchmark, we use the 15 datasets that provide public downloads: MS MARCO [8], Trec-Covid [55], NFCorpus [5], NQ [32], HotpotQA [63], FiQA [37], ArguAna [56], Touche-2020 [4], CQADupStack [26], Quora, DBPedia [24], Scidocs [12], Fever [54], Climate-Fever [18], and Scifact [57].\n\n8 https://github.com/facebookresearch/cc\\_net\n\nTable 9: Details for each data source after filtering. The 'Others' category includes 'SimpleWiki', 'GooAQ', 'WikiHow', 'Yahoo Answers' from https://huggingface.co/datasets/ sentence-transformers/embedding-training-data .\n\n| data source   | type of text pairs                                                      | random example                                                                                                                                             | # of pairs   |\n|---------------|-------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| Wikipedia     | (entity+section title, passage)                                         | q : Lexden History p : The site on which Lexden now stands was crossed by the fortifications of iron age Colchester. . .                                   | 24 M         |\n| Reddit        | (post, upvoted comment)                                                 | q : What makes a client good quality to you? I'm putting together my ideal client . . . p : Respectful of schedules. And pays on time.. . .                | 60 M         |\n| Common Crawl  | (title, passage)                                                        | q : Central Intake Unit | Broome County p : Caseworkers from Central Intake assess the household and risk of placement. If eligible. . .                   | 69 M         |\n| Stackexchange | (title, answer) (title+description, answer)                             | q : Will killing Python made problems for Apache p : Python and Apache aren't related, unless your app is making use of Python. . . .                      | 19 M         |\n| S2ORC         | (title, abstract) (title, citation title) (abstract, citation abstract) | q : Constructive Dual DP for Reservoir Optimization p : Dynamic programming (DP) is a well established technique for optimization of reservoir manage...   | 90 M         |\n| News          | (title, passage) (highlight, passage)                                   | q : LG Display reports Q1 operating loss as. . . p : April 25 (Reuters) - South Korea's LG Display Co Ltd reported its first quarterly operating loss. . . | 3 M          |\n| Others        | misc.                                                                   | misc.                                                                                                                                                      | 6 M          |\n| All above     | -                                                                       | -                                                                                                                                                          | ∼ 270 M      |\n\n## B Implementation Details\n\nWe list the hyperparameters in Table 11. Since some evaluation datasets have long texts, we freeze the position embeddings during both pre-training and fine-tuning and set the maximum text length to 512 for evaluation.\n\nFor the Quora duplicate retrieval task in the BEIR benchmark, we add prefix ' query: ' to all the questions. For other retrieval tasks, we use ' query: ' and ' passage: ' prefixes correspondingly.\n\nThe MS-MARCO results in Table 12 use document titles provided by RocketQA [50]. This evaluation setup is consistent with most state-of-the-art dense retrievers. However, the MS-MARCO data from the BEIR benchmark does not have titles, so the results are expected to be lower.\n\nTable 10: Model configurations.\n\n|          |   # layers |   hidden size | # params   |\n|----------|------------|---------------|------------|\n| E5 small |         12 |           384 | 33 M       |\n| E5 base  |         12 |           768 | 110 M      |\n| E5 large |         24 |          1024 | 330 M      |\n\nTable 11: Hyperparameters for contrastive pre-training and fine-tuning.\n\n|                | pre-training   | pre-training   | pre-training   | fine-tuning   | fine-tuning   | fine-tuning   |\n|----------------|----------------|----------------|----------------|---------------|---------------|---------------|\n|                | E5-PT small    | E5-PT base     | E5-PT large    | E5 small      | E5 base       | E5 large      |\n| learning rate  | 3 × 10 - 4     | 2 × 10 - 4     | 10 - 4         | 3 × 10 - 5    | 2 × 10 - 5    | 10 - 5        |\n| GPUs           | 16             | 32             | 64             | 8             | 8             | 8             |\n| warmup steps   | 1000           | 1000           | 1000           | 400           | 400           | 400           |\n| batch size     | 32 K           | 32 K           | 32 K           | 256           | 256           | 256           |\n| max steps      | 20 K           | 20 K           | 20 K           | n.a.          | n.a.          | n.a.          |\n| max length     | 128            | 128            | 128            | 192           | 192           | 192           |\n| epochs         | n.a.           | n.a.           | n.a.           | 3             | 3             | 3             |\n| τ              | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| α              | n.a.           | n.a.           | n.a.           | 0 . 2         | 0 . 2         | 0 . 2         |\n| weight decay   | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| hard negatives | 0              | 0              | 0              | 7             | 7             | 7             |\n\nIn-domain Evaluation We report results for in-domain datasets in Table 12. These results can help illustrate the benefits brought by contrastive pre-training when abundant in-domain labeled data are\n\navailable. For MS-MARCO passage ranking, MRR@10 and Recall@1k are reported. For the NQ dataset, Recall@20 and Recall@100 are the main metrics.\n\nTable 12: In-domain results. 'target pre-train' refers to intermediate pre-training on the target corpus before supervised fine-tuning. For NQ, we use the passage retrieval setting from DPR [30].\n\n|                 | target pre-train?   | MS-MARCO   | MS-MARCO   | NQ   | NQ    |\n|-----------------|---------------------|------------|------------|------|-------|\n|                 | target pre-train?   | MRR@10     | R@1k       | R@20 | R@100 |\n| ANCE [61]       | ✗                   | 33.0       | 95.9       | 81.9 | 87.5  |\n| RocketQAv2 [50] | ✗                   | 38.8       | 98.1       | 83.7 | 89.0  |\n| SimLM [58]      | ✓                   | 41.1       | 98.7       | 85.2 | 89.7  |\n| E5 small        | ✗                   | 37.5       | 98.1       | 84.6 | 89.8  |\n| E5 base         | ✗                   | 38.5       | 98.5       | 86.1 | 90.7  |\n| E5 large        | ✗                   | 39.4       | 98.7       | 86.4 | 90.5  |\n\n## C Negative Results\n\nHere are some attempts that we eventually give up on:\n\nAdding BM25 hard negatives Similar to DPR [30], we add one BM25 hard negative for each positive pair during training. When using 15 Mdata, this strategy improves the overall results by ∼ 0 . 5 points on the BEIR benchmark. However, running the BM25 algorithm over a 250 M+ dataset is too time-consuming even with multi-node and multi-process parallelism.\n\nUsing RoBERTa instead of BERT for initialization Though RoBERTa shows consistent gains on many NLP tasks, we empirically find that RoBERTa performs worse than BERT initialization on most of the BEIR benchmark datasets.\n\nAuxiliary MLM objective We add a masked language modeling loss for 25 %of the training text pairs. The numbers are on par with removing this auxiliary objective, but the training cost goes up.\n\nTable 13: Results for each dataset in the MTEB benchmark [40]. The numbers for the Retrieval category are not included here since the datasets are the same as the BEIR benchmark.\n\n|                                    | unsupervised   | unsupervised   | unsupervised   | supervised   | supervised   | supervised   |\n|------------------------------------|----------------|----------------|----------------|--------------|--------------|--------------|\n|                                    | E5-PT small    | E5-PT base     | E5-PT large    | E5 small     | E5 base      | E5 large     |\n| AmazonCounterfactualClassification | 71.7           | 73.6           | 70.4           | 76.2         | 79.7         | 77.7         |\n| AmazonPolarityClassification       | 76.1           | 77.0           | 83.2           | 87.5         | 88.0         | 90.1         |\n| AmazonReviewsClassification        | 35.0           | 35.8           | 37.4           | 42.6         | 42.7         | 43.0         |\n| Banking77Classification            | 82.1           | 82.9           | 83.5           | 81.9         | 83.3         | 84.1         |\n| EmotionClassification              | 42.2           | 44.2           | 43.5           | 46.9         | 49.4         | 48.1         |\n| ImdbClassification                 | 67.9           | 67.3           | 77.7           | 75.6         | 76.0         | 82.1         |\n| MassiveIntentClassification        | 70.2           | 71.1           | 70.8           | 72.2         | 72.3         | 73.2         |\n| MassiveScenarioClassification      | 74.6           | 75.4           | 75.9           | 75.8         | 76.8         | 77.4         |\n| MTOPDomainClassification           | 91.3           | 92.3           | 93.2           | 92.1         | 93.2         | 93.9         |\n| MTOPIntentClassification           | 71.9           | 74.0           | 74.2           | 73.2         | 74.8         | 76.4         |\n| ToxicConversationsClassification   | 67.0           | 67.4           | 66.1           | 72.8         | 74.1         | 70.6         |\n| TweetSentimentExtractionClass.     | 54.4           | 53.3           | 52.5           | 63.3         | 61.4         | 61.2         |\n| ArxivClusteringP2P                 | 47.9           | 49.3           | 49.4           | 44.1         | 44.6         | 46.2         |\n| ArxivClusteringS2S                 | 39.9           | 42.8           | 43.6           | 37.1         | 40.5         | 41.4         |\n| BiorxivClusteringP2P               | 38.5           | 38.8           | 39.2           | 35.8         | 36.2         | 37.6         |\n| BiorxivClusteringS2S               | 35.4           | 36.5           | 36.7           | 31.9         | 32.7         | 35.1         |\n| MedrxivClusteringP2P               | 34.4           | 33.7           | 33.3           | 31.3         | 31.5         | 32.3         |\n| MedrxivClusteringS2S               | 32.0           | 32.1           | 32.2           | 28.2         | 28.3         | 29.7         |\n| RedditClustering                   | 46.9           | 49.3           | 52.4           | 42.9         | 48.2         | 50.7         |\n| RedditClusteringP2P                | 60.2           | 64.4           | 64.6           | 56.4         | 62.2         | 61.4         |\n| StackExchangeClustering            | 57.7           | 60.2           | 63.3           | 59.1         | 63.9         | 65.0         |\n| StackExchangeClusteringP2P         | 32.0           | 34.0           | 34.7           | 30.3         | 32.6         | 33.6         |\n| TwentyNewsgroupsClustering         | 34.4           | 36.2           | 37.9           | 37.5         | 42.6         | 43.8         |\n| SprintDuplicateQuestions           | 91.6           | 90.8           | 92.0           | 95.3         | 94.9         | 95.4         |\n| TwitterSemEval2015                 | 60.0           | 62.8           | 64.7           | 74.2         | 74.4         | 76.1         |\n| TwitterURLCorpus                   | 83.2           | 84.0           | 84.1           | 85.8         | 86.0         | 86.3         |\n| AskUbuntuDupQuestions              | 57.8           | 57.6           | 58.3           | 59.4         | 59.7         | 60.1         |\n| MindSmallReranking                 | 29.0           | 29.6           | 29.2           | 29.6         | 30.1         | 30.8         |\n| SciDocsRR                          | 81.1           | 82.6           | 84.3           | 79.8         | 82.9         | 83.9         |\n| StackOverflowDupQuestions          | 44.4           | 44.2           | 45.8           | 49.1         | 50.1         | 51.3         |\n| BIOSSES                            | 69.2           | 71.9           | 69.7           | 84.2         | 85.1         | 84.7         |\n| SICK-R                             | 66.6           | 68.7           | 69.7           | 78.9         | 79.7         | 80.5         |\n| STS12                              | 60.7           | 57.9           | 54.7           | 75.2         | 74.2         | 75.9         |\n| STS13                              | 71.1           | 73.5           | 74.0           | 81.8         | 83.3         | 85.2         |\n| STS14                              | 64.2           | 64.0           | 65.3           | 78.5         | 78.5         | 80.5         |\n| STS15                              | 74.3           | 75.4           | 75.8           | 87.5         | 88.4         | 88.8         |\n| STS16                              | 76.6           | 79.8           | 80.1           | 84.6         | 84.2         | 85.3         |\n| STS17                              | 78.3           | 77.2           | 76.0           | 87.9         | 87.2         | 89.4         |\n| STS22                              | 59.2           | 56.2           | 62.8           | 63.8         | 62.9         | 63.0         |\n| STSBenchmark                       | 67.7           | 70.5           | 70.9           | 86.4         | 86.2         | 87.2         |\n| SummEval                           | 32.7           | 31.1           | 32.6           | 31.4         | 31.0         | 31.0         |",
  "tables": [
    {
      "index": 0,
      "markdown": "|               |   BM25 |   SimCSE | LaPraDor †   |   Contriever | cpt 300M   |   E5-PT small |   E5-PT base |   E5-PT large |\n|---------------|--------|----------|--------------|--------------|------------|---------------|--------------|---------------|\n| MS MARCO      |   22.8 |      9.4 | 16.9 ∗       |         20.6 | 19.9       |          25.4 |         26   |          26.2 |\n| Trec-Covid    |   65.6 |     26.2 | 22.7         |         27.4 | 52.9       |          52   |         61   |          61.8 |\n| NFCorpus      |   32.5 |      9.9 | 31.1         |         31.7 | 32.0       |          29.3 |         35.8 |          33.7 |\n| NQ            |   32.9 |     11.7 | 18.1         |         25.4 | -          |          37.3 |         39   |          41.7 |\n| HotpotQA      |   60.3 |     19.8 | 30.3         |         48.1 | 51.5       |          46   |         52.4 |          52.2 |\n| FiQA          |   23.6 |      9.8 | 20.3         |         24.5 | 34.1       |          38.3 |         40   |          43.2 |\n| ArguAna       |   31.5 |     38.3 | 45.9         |         37.9 | 38.7       |          42.5 |         42.2 |          44.4 |\n| Touche-2020   |   36.7 |      8.9 | 9.4          |         19.3 | 21.0       |          19.9 |         16.9 |          19.8 |\n| CQADupStack   |   29.9 |     13.2 | 22.0         |         28.4 | -          |          35   |         35.4 |          38.9 |\n| Quora         |   78.9 |     78   | 78.7         |         83.5 | 68.1       |          85.8 |         85.7 |          86.1 |\n| DBPedia       |   31.3 |     15   | 25.0         |         29.2 | 27.2       |          34.5 |         35.4 |          37.1 |\n| Scidocs       |   15.8 |      5.5 | 13.3         |         14.9 | -          |          19.9 |         21.1 |          21.8 |\n| Fever         |   75.3 |     21.1 | 36.8         |         68.2 | 57.1       |          62.5 |         63.4 |          68.6 |\n| Climate-Fever |   21.3 |     11.8 | 13.8         |         15.5 | 15.8       |          14.5 |         15.4 |          15.7 |\n| Scifact       |   66.5 |     25.7 | 55.5         |         64.9 | 65.4       |          68.5 |         73.7 |          72.3 |\n| Average       |   41.7 |     20.3 | 29.3         |         36   | -          |          40.8 |         42.9 |          44.2 |\n| Best on       |    5   |      0   | 1            |          0   | 0          |           0   |          2   |           7   |"
    },
    {
      "index": 1,
      "markdown": "|               |   ANCE |   GTR base |   ColBERT |   Contriever | cpt 300M   |   GTR large |   E5 small |   E5 base |   E5 large |\n|---------------|--------|------------|-----------|--------------|------------|-------------|------------|-----------|------------|\n| MS MARCO      |   38.8 |       42   |      40.1 |         40.7 | -          |        43   |       42.3 |      43.1 |       44.1 |\n| Trec-Covid    |   65.4 |       53.9 |      67.7 |         59.6 | 67.9       |        55.7 |       76.8 |      79.6 |       78.3 |\n| NFCorpus      |   23.7 |       30.8 |      30.5 |         32.8 | 33.2       |        32.9 |       33.9 |      36.6 |       36.1 |\n| NQ            |   44.6 |       49.5 |      52.4 |         49.8 | -          |        54.7 |       58.7 |      60   |       62.9 |\n| HotpotQA      |   45.6 |       53.5 |      59.3 |         63.8 | 59.4       |        57.9 |       56.3 |      62.2 |       63.3 |\n| FiQA          |   29.5 |       34.9 |      31.7 |         32.9 | 38.4       |        42.4 |       34.8 |      36.4 |       38.6 |\n| ArguAna       |   41.5 |       51.1 |      23.3 |         44.6 | 47.0       |        52.5 |       46.7 |      51.4 |       49.4 |\n| Touche-2020   |   24   |       20.5 |      20.2 |         23   | 28.5       |        21.9 |       26.8 |      28.3 |       27.2 |\n| CQADupStack   |   29.6 |       35.7 |      35   |         34.5 | -          |        38.4 |       36.1 |      38.9 |       39.4 |\n| Quora         |   85.2 |       88.1 |      85.4 |         86.5 | 70.6       |        89   |       87.7 |      87.9 |       88.2 |\n| DBPedia       |   28.1 |       34.7 |      39.2 |         41.3 | 36.2       |        39.1 |       38.6 |      41   |       42.4 |\n| Scidocs       |   12.2 |       14.9 |      14.5 |         16.5 | -          |        15.8 |       16.4 |      19   |       20.1 |\n| Fever         |   66.9 |       66   |      77.1 |         75.8 | 72.1       |        71.2 |       53.5 |      58.2 |       65   |\n| Climate-Fever |   19.8 |       24.1 |      18.4 |         23.7 | 18.5       |        26.2 |       15.8 |      15.4 |       22.4 |\n| Scifact       |   50.7 |       60   |      67.1 |         67.7 | 67.2       |        63.9 |       65.6 |      73.1 |       72.6 |\n| Average       |   40.5 |       44   |      44.4 |         46.6 | -          |        47   |       46   |      48.7 |       50   |\n| Best on       |    0   |        0   |       1   |          1   | 1          |         4   |        0   |       3   |        5   |"
    },
    {
      "index": 2,
      "markdown": "| # of datasets →     | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|---------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised models |             |             |                |            |            |          |           |          |\n| Glove               | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| BERT                | 61.7        | 30.1        | 56.3           | 43.4       | 10.6       | 54.4     | 29.8      | 38.3     |\n| SimCSE-BERT-unsup   | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| E5-PT small         | 67.0        | 41.7        | 78.2           | 53.1       | 40.8       | 68.8     | 32.7      | 54.3     |\n| E5-PT base          | 67.9        | 43.4        | 79.2           | 53.5       | 42.9       | 69.5     | 31.1      | 55.6     |\n| E5-PT large         | 69.0        | 44.3        | 80.3           | 54.4       | 44.2       | 69.9     | 32.6      | 56.6     |\n| Supervised models   |             |             |                |            |            |          |           |          |\n| SimCSE-BERT-sup     | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| BERT-FT base        | 68.7        | 33.9        | 82.6           | 50.5       | 41.5       | 79.2     | 29.0      | 55.2     |\n| Contriever          | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR large           | 67.1        | 41.6        | 85.3           | 55.4       | 47.4       | 78.2     | 29.5      | 58.3     |\n| Sentence-T5 large   | 72.3        | 41.7        | 85.0           | 54.0       | 36.7       | 81.8     | 29.6      | 57.1     |\n| E5 small            | 71.7        | 39.5        | 85.1           | 54.5       | 46.0       | 80.9     | 31.4      | 58.9     |\n| E5 base             | 72.6        | 42.1        | 85.1           | 55.7       | 48.7       | 81.0     | 31.0      | 60.4     |\n| E5 large            | 73.1        | 43.3        | 85.9           | 56.5       | 50.0       | 82.1     | 31.0      | 61.4     |\n| Larger models       |             |             |                |            |            |          |           |          |\n| GTR xxl             | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl     | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |"
    },
    {
      "index": 3,
      "markdown": "|            | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Zero-shot   | Full Fine-tune   | Full Fine-tune   |\n|------------|-------------|-------------|-------------|-------------|-------------|------------------|------------------|\n|            | Majority    | BERT base   | E5 small    | E5 base     | E5 large    | BERT base        | BERT large       |\n| SST-2 [52] | 50.9        | 58.9        | 79.7        | 81.3        | 85.3        | 93.5             | 94.9             |"
    },
    {
      "index": 4,
      "markdown": "| batch size   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------|--------|---------|-----------|-----------|-------|\n| 32k          |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| 8k           |       33.3 | 38.5 |   37.6 |    85.7 |      34   |      71.8 |  50.2 |\n| 1k           |       28.2 | 33.1 |   30.4 |    84   |      30.1 |      69.1 |  45.8 |"
    },
    {
      "index": 5,
      "markdown": "| Fine-tuned on   |   Retrieval |   STS |   Classification |   Summ. |   MTEB Avg |\n|-----------------|-------------|-------|------------------|---------|------------|\n| No fine-tuning  |        42.9 |  69.5 |             67.9 |    31.1 |       55.6 |\n| MS-MARCO + NQ   |        50.3 |  78.3 |             68.3 |    30.6 |       59   |\n| NLI             |        38.3 |  81.1 |             72.6 |    31.6 |       57.3 |\n| All above       |        48.7 |  81   |             73.1 |    31   |       60.4 |"
    },
    {
      "index": 6,
      "markdown": "| # of pairs   |            |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|--------------|------------|------------|------|--------|---------|-----------|-----------|-------|\n| 1M           | w/o filter |       23   | 15.1 |   18.5 |    83.1 |      18.2 |      51.4 |  34.9 |\n| 1M           | w/ filter  |       26.8 | 22.7 |   24.5 |    85   |      27.5 |      57.5 |  40.7 |\n| All          | w/o filter |       34.5 | 35.4 |   39.1 |    85.7 |      32.9 |      72.5 |  50   |\n| All          | w/ filter  |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |"
    },
    {
      "index": 7,
      "markdown": "|             | # negatives   |   NFCorpus |   NQ |   FiQA |   Quora |   DBPedia |   Scifact |   Avg |\n|-------------|---------------|------------|------|--------|---------|-----------|-----------|-------|\n| In batch    | 32k           |       35.8 | 39   |   40   |    85.7 |      35.4 |      73.7 |  51.6 |\n| + pre-batch | 64k           |       29.4 | 27.2 |   29.4 |    84.6 |      25   |      64.3 |  43.3 |\n| MoCo        | 130k          |       29.7 | 36.1 |   32   |    81.6 |      29.9 |      63.6 |  45.5 |"
    },
    {
      "index": 8,
      "markdown": "| data source   | type of text pairs                                                      | random example                                                                                                                                             | # of pairs   |\n|---------------|-------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|\n| Wikipedia     | (entity+section title, passage)                                         | q : Lexden History p : The site on which Lexden now stands was crossed by the fortifications of iron age Colchester. . .                                   | 24 M         |\n| Reddit        | (post, upvoted comment)                                                 | q : What makes a client good quality to you? I'm putting together my ideal client . . . p : Respectful of schedules. And pays on time.. . .                | 60 M         |\n| Common Crawl  | (title, passage)                                                        | q : Central Intake Unit | Broome County p : Caseworkers from Central Intake assess the household and risk of placement. If eligible. . .                   | 69 M         |\n| Stackexchange | (title, answer) (title+description, answer)                             | q : Will killing Python made problems for Apache p : Python and Apache aren't related, unless your app is making use of Python. . . .                      | 19 M         |\n| S2ORC         | (title, abstract) (title, citation title) (abstract, citation abstract) | q : Constructive Dual DP for Reservoir Optimization p : Dynamic programming (DP) is a well established technique for optimization of reservoir manage...   | 90 M         |\n| News          | (title, passage) (highlight, passage)                                   | q : LG Display reports Q1 operating loss as. . . p : April 25 (Reuters) - South Korea's LG Display Co Ltd reported its first quarterly operating loss. . . | 3 M          |\n| Others        | misc.                                                                   | misc.                                                                                                                                                      | 6 M          |\n| All above     | -                                                                       | -                                                                                                                                                          | ∼ 270 M      |"
    },
    {
      "index": 9,
      "markdown": "|          |   # layers |   hidden size | # params   |\n|----------|------------|---------------|------------|\n| E5 small |         12 |           384 | 33 M       |\n| E5 base  |         12 |           768 | 110 M      |\n| E5 large |         24 |          1024 | 330 M      |"
    },
    {
      "index": 10,
      "markdown": "|                | pre-training   | pre-training   | pre-training   | fine-tuning   | fine-tuning   | fine-tuning   |\n|----------------|----------------|----------------|----------------|---------------|---------------|---------------|\n|                | E5-PT small    | E5-PT base     | E5-PT large    | E5 small      | E5 base       | E5 large      |\n| learning rate  | 3 × 10 - 4     | 2 × 10 - 4     | 10 - 4         | 3 × 10 - 5    | 2 × 10 - 5    | 10 - 5        |\n| GPUs           | 16             | 32             | 64             | 8             | 8             | 8             |\n| warmup steps   | 1000           | 1000           | 1000           | 400           | 400           | 400           |\n| batch size     | 32 K           | 32 K           | 32 K           | 256           | 256           | 256           |\n| max steps      | 20 K           | 20 K           | 20 K           | n.a.          | n.a.          | n.a.          |\n| max length     | 128            | 128            | 128            | 192           | 192           | 192           |\n| epochs         | n.a.           | n.a.           | n.a.           | 3             | 3             | 3             |\n| τ              | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| α              | n.a.           | n.a.           | n.a.           | 0 . 2         | 0 . 2         | 0 . 2         |\n| weight decay   | 0 . 01         | 0 . 01         | 0 . 01         | 0 . 01        | 0 . 01        | 0 . 01        |\n| hard negatives | 0              | 0              | 0              | 7             | 7             | 7             |"
    },
    {
      "index": 11,
      "markdown": "|                 | target pre-train?   | MS-MARCO   | MS-MARCO   | NQ   | NQ    |\n|-----------------|---------------------|------------|------------|------|-------|\n|                 | target pre-train?   | MRR@10     | R@1k       | R@20 | R@100 |\n| ANCE [61]       | ✗                   | 33.0       | 95.9       | 81.9 | 87.5  |\n| RocketQAv2 [50] | ✗                   | 38.8       | 98.1       | 83.7 | 89.0  |\n| SimLM [58]      | ✓                   | 41.1       | 98.7       | 85.2 | 89.7  |\n| E5 small        | ✗                   | 37.5       | 98.1       | 84.6 | 89.8  |\n| E5 base         | ✗                   | 38.5       | 98.5       | 86.1 | 90.7  |\n| E5 large        | ✗                   | 39.4       | 98.7       | 86.4 | 90.5  |"
    },
    {
      "index": 12,
      "markdown": "|                                    | unsupervised   | unsupervised   | unsupervised   | supervised   | supervised   | supervised   |\n|------------------------------------|----------------|----------------|----------------|--------------|--------------|--------------|\n|                                    | E5-PT small    | E5-PT base     | E5-PT large    | E5 small     | E5 base      | E5 large     |\n| AmazonCounterfactualClassification | 71.7           | 73.6           | 70.4           | 76.2         | 79.7         | 77.7         |\n| AmazonPolarityClassification       | 76.1           | 77.0           | 83.2           | 87.5         | 88.0         | 90.1         |\n| AmazonReviewsClassification        | 35.0           | 35.8           | 37.4           | 42.6         | 42.7         | 43.0         |\n| Banking77Classification            | 82.1           | 82.9           | 83.5           | 81.9         | 83.3         | 84.1         |\n| EmotionClassification              | 42.2           | 44.2           | 43.5           | 46.9         | 49.4         | 48.1         |\n| ImdbClassification                 | 67.9           | 67.3           | 77.7           | 75.6         | 76.0         | 82.1         |\n| MassiveIntentClassification        | 70.2           | 71.1           | 70.8           | 72.2         | 72.3         | 73.2         |\n| MassiveScenarioClassification      | 74.6           | 75.4           | 75.9           | 75.8         | 76.8         | 77.4         |\n| MTOPDomainClassification           | 91.3           | 92.3           | 93.2           | 92.1         | 93.2         | 93.9         |\n| MTOPIntentClassification           | 71.9           | 74.0           | 74.2           | 73.2         | 74.8         | 76.4         |\n| ToxicConversationsClassification   | 67.0           | 67.4           | 66.1           | 72.8         | 74.1         | 70.6         |\n| TweetSentimentExtractionClass.     | 54.4           | 53.3           | 52.5           | 63.3         | 61.4         | 61.2         |\n| ArxivClusteringP2P                 | 47.9           | 49.3           | 49.4           | 44.1         | 44.6         | 46.2         |\n| ArxivClusteringS2S                 | 39.9           | 42.8           | 43.6           | 37.1         | 40.5         | 41.4         |\n| BiorxivClusteringP2P               | 38.5           | 38.8           | 39.2           | 35.8         | 36.2         | 37.6         |\n| BiorxivClusteringS2S               | 35.4           | 36.5           | 36.7           | 31.9         | 32.7         | 35.1         |\n| MedrxivClusteringP2P               | 34.4           | 33.7           | 33.3           | 31.3         | 31.5         | 32.3         |\n| MedrxivClusteringS2S               | 32.0           | 32.1           | 32.2           | 28.2         | 28.3         | 29.7         |\n| RedditClustering                   | 46.9           | 49.3           | 52.4           | 42.9         | 48.2         | 50.7         |\n| RedditClusteringP2P                | 60.2           | 64.4           | 64.6           | 56.4         | 62.2         | 61.4         |\n| StackExchangeClustering            | 57.7           | 60.2           | 63.3           | 59.1         | 63.9         | 65.0         |\n| StackExchangeClusteringP2P         | 32.0           | 34.0           | 34.7           | 30.3         | 32.6         | 33.6         |\n| TwentyNewsgroupsClustering         | 34.4           | 36.2           | 37.9           | 37.5         | 42.6         | 43.8         |\n| SprintDuplicateQuestions           | 91.6           | 90.8           | 92.0           | 95.3         | 94.9         | 95.4         |\n| TwitterSemEval2015                 | 60.0           | 62.8           | 64.7           | 74.2         | 74.4         | 76.1         |\n| TwitterURLCorpus                   | 83.2           | 84.0           | 84.1           | 85.8         | 86.0         | 86.3         |\n| AskUbuntuDupQuestions              | 57.8           | 57.6           | 58.3           | 59.4         | 59.7         | 60.1         |\n| MindSmallReranking                 | 29.0           | 29.6           | 29.2           | 29.6         | 30.1         | 30.8         |\n| SciDocsRR                          | 81.1           | 82.6           | 84.3           | 79.8         | 82.9         | 83.9         |\n| StackOverflowDupQuestions          | 44.4           | 44.2           | 45.8           | 49.1         | 50.1         | 51.3         |\n| BIOSSES                            | 69.2           | 71.9           | 69.7           | 84.2         | 85.1         | 84.7         |\n| SICK-R                             | 66.6           | 68.7           | 69.7           | 78.9         | 79.7         | 80.5         |\n| STS12                              | 60.7           | 57.9           | 54.7           | 75.2         | 74.2         | 75.9         |\n| STS13                              | 71.1           | 73.5           | 74.0           | 81.8         | 83.3         | 85.2         |\n| STS14                              | 64.2           | 64.0           | 65.3           | 78.5         | 78.5         | 80.5         |\n| STS15                              | 74.3           | 75.4           | 75.8           | 87.5         | 88.4         | 88.8         |\n| STS16                              | 76.6           | 79.8           | 80.1           | 84.6         | 84.2         | 85.3         |\n| STS17                              | 78.3           | 77.2           | 76.0           | 87.9         | 87.2         | 89.4         |\n| STS22                              | 59.2           | 56.2           | 62.8           | 63.8         | 62.9         | 63.0         |\n| STSBenchmark                       | 67.7           | 70.5           | 70.9           | 86.4         | 86.2         | 87.2         |\n| SummEval                           | 32.7           | 31.1           | 32.6           | 31.4         | 31.0         | 31.0         |"
    }
  ],
  "stats": {
    "pages": 17,
    "chunksCreated": 104,
    "totalCharacters": 73943,
    "totalWords": 10502,
    "numTables": 13,
    "processingTimeMs": 52302
  }
}