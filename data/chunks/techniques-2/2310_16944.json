{
  "paper": {
    "id": "2310.16944v1",
    "title": "Zephyr: Direct Distillation of LM Alignment",
    "abstract": "We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.",
    "authors": [
      "Lewis Tunstall",
      "Edward Beeching",
      "Nathan Lambert",
      "Nazneen Rajani",
      "Kashif Rasul",
      "Younes Belkada",
      "Shengyi Huang",
      "Leandro von Werra",
      "Clémentine Fourrier",
      "Nathan Habib",
      "Nathan Sarrazin",
      "Omar Sanseviero",
      "Alexander M. Rush",
      "Thomas Wolf"
    ],
    "published": "2023-10-25T19:25:16.000Z",
    "updated": "2023-10-25T19:25:16.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.16944v1",
    "absUrl": "https://arxiv.org/abs/2310.16944v1"
  },
  "chunks": [
    {
      "id": "2310.16944v1-chunk-0",
      "content": "∗ ∗\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl´ ementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero Alexander M. Rush, and Thomas Wolf The H4 (Helpful, Honest, Harmless, Huggy) Team https://huggingface.co/HuggingFaceH4 lewis@huggingface.co",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "ZEPHYR: DIRECT DISTILLATION OF LM ALIGNMENT",
        "chunkIndex": 0,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-1",
      "content": "We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-2",
      "content": "rt on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook .\n\nFigure 1: Model performance on MT-Bench. We compare ZEPHYR-7B, trained with distilled direct preference optimization (dDPO), to proprietary models as well as larger, open-access models like LLAMA2-CHAT-70B that were additionally trained using reinforcement learning on a large amount of human feedback.\n\n<!-- image -->\n\n∗ Equal contribution.\n\n,",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 2,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-3",
      "content": "Smaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models (Wang &amp; Komatsuzaki, 2021) to accurate and compact models (Touvron et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than the 'compute-optimal' amount suggested by the Chincilla scaling laws (De Vries, 2023). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In this approach, the output of a more capable teacher model is used as supervised data for the student model.\n\nDistillation has proven to be an effective tool for improving open models on a range of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not 'intent aligned', i.e.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-4",
      "content": "e of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not 'intent aligned', i.e. they do not behave in a manner that aligns with human users' preferences. This property often leads to outputs that do not provide correct responses to queries.\n\nIntention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-5",
      "content": "puts and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as in LLAMA2-CHAT (Touvron et al., 2023).\n\nIn this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB).",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-6",
      "content": "approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB).\n\nTo validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook .",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-7",
      "content": "bilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook .\n\nWe note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-8",
      "content": "There has been significant growth in the number of open large language models (LLMs) that have served as artifacts for the research community to study and use as a starting model for building chatbots and other applications. After the release of ChatGPT, the LLaMA model (Touvron et al., 2023) opened the doors to a wide range of research on efficient fine-tuning, longer prompt context, retrieval augmented generation (RAG), and quantization. After LLaMA, there has been a continuous stream of open access text based LLMs including MosaicML's MPT (ML, 2023), the Together AI's RedPajama-INCITE (AI, 2023), the TII's Falcon (Penedo et al., 2023), Meta's Llama 2 (Touvron\n\nFigure 2: The three steps of our method: (1) large scale, self-instruct-style dataset construction (UltraChat), followed by distilled supervised fine-tuning (dSFT), (2) AI Feedback (AIF) collection via an ensemble of chat model completions, followed by scoring by GPT-4 (UltraFeedback) and binarization into preferences, and (3)",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 8,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-9",
      "content": "illed supervised fine-tuning (dSFT), (2) AI Feedback (AIF) collection via an ensemble of chat model completions, followed by scoring by GPT-4 (UltraFeedback) and binarization into preferences, and (3) distilled direct preference optimization (dPO) of the dSFT model utilizing the feedback data.\n\n<!-- image -->\n\net al., 2023), and the Mistral 7B (Jiang et al., 2023). Zephyr uses Mistral 7B as the starting point due to its strong performance.\n\nWith the development of open models, researchers have worked on approaches to improve small model performance by distillation from larger models. This trend started with self-instruct method (Wang et al., 2023) and the Alpaca model (Taori et al., 2023), which was followed by Vicuna (Chiang et al., 2023)and other distilled models. These works primarily focused on distilling the SFT stage of alignment, whereas we focus on both SFT and preference optimization. Some models such as WizardLM (Xu et al.) have explored methods beyond dSFT.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 9,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-10",
      "content": "works primarily focused on distilling the SFT stage of alignment, whereas we focus on both SFT and preference optimization. Some models such as WizardLM (Xu et al.) have explored methods beyond dSFT. Contemporaneously with this work, Xwin-LM (Team, 2023) introduced an approach that distilled preference optimization through PPO (Schulman et al., 2017). We compare to these approaches in our experiments.\n\nTools for benchmarking and evaluating LLMs have greatly evolved to keep up with the pace of innovation in generative AI. Powerful LLMs such as GPT-4 and Claude are used as evaluators to judge model responses by scoring model outputs or ranking responses in a pairwise setting. The LMSYS chatbot arena benchmarks LLMs in anonymous, randomized battles using crowdsourcing (Zheng et al., 2023). The models are ranked based on their Elo ratings on the leaderboard.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 10,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-11",
      "content": "airwise setting. The LMSYS chatbot arena benchmarks LLMs in anonymous, randomized battles using crowdsourcing (Zheng et al., 2023). The models are ranked based on their Elo ratings on the leaderboard. AlpacaEval is an example of another such leaderboard that compares models in a pairwise setting but instead uses bigger LLMs such as GPT-4 and Claude in place of humans (Dubois et al., 2023). In a similar spirit, MTBench uses GPT-4 to score model responses on a scale of 1-10 for multi-turn instructions across task categories such as reasoning, roleplay, math, coding, writing, humanities, STEM and extraction (Zheng et al., 2023). The HuggingFace Open LLM leaderbaord (Beeching et al., 2023), the Chain-of-Thought Hub (Fu et al., 2023), ChatEval (Sedoc et al., 2019), and FastEval (fas, 2023) are examples of other tools for evaluating chatty models. We present results by evaluating on MTBench, AlpacaEval, and the HuggingFace OpenLLM Leaderboard.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 11,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-12",
      "content": "examples of other tools for evaluating chatty models. We present results by evaluating on MTBench, AlpacaEval, and the HuggingFace OpenLLM Leaderboard.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 12,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-13",
      "content": "The goal of this work is to align an open-source large-language model to the intent of the user. Throughout the work we assume access to a larger teacher model π T which can be queried by prompted generation. Our goal is be to produce a student model π θ and our approach follows similar stages as InstructGPT (Ouyang et al., 2022) as shown in Figure 2.\n\nDistilled Supervised Fine-Tuning (dSFT) Starting with a raw LLM, we first need to train it to respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given access to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 13,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-14",
      "content": "s to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).\n\nApproaches to dSFT follow the self-instruct protocol (Wang et al., 2023). Let x 0 1 , . . . , x 0 J be a set of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed through iterative self-prompting where the teacher is used to both respond to an instruction and refine the instruction based on the response. For each x 0 , we first sample response y 0 ∼ π T ( ·| x 0 ) , and then refine by sampling a new instruction (using a prompt for refinement), x 1 ∼ π T ( ·| x 0 , y 0 ) . The end point is a final dataset, C = { ( x 1 , y 1 ) , . . . , ( x J , y J ) } . Distillation is performed by SFT,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 14,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-15",
      "content": "a prompt for refinement), x 1 ∼ π T ( ·| x 0 , y 0 ) . The end point is a final dataset, C = { ( x 1 , y 1 ) , . . . , ( x J , y J ) } . Distillation is performed by SFT,\n\n<!-- formula-not-decoded -->\n\nAI Feedback through Preferences (AIF) Human feedback (HF) can provide additional signal to align LLMs. Human feedback is typically given through preferences on the quality of LLM responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher model on generated outputs from other models.\n\nWe follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide preferences on model outputs. As with SFT, the system starts with a set of prompts x 1 , . . . , x J . Each prompt x is fed to a collection of four models π 1 , . . . , π 4 , e.g. Claude, Falcon, Llama, etc, each of which yield a response y 1 ∼ π 1 ( ·| x ) , . . . , y 4 ∼ π 4 ( ·| x ) . These responses are then fed to the teacher model, e.g.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 15,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-16",
      "content": "our models π 1 , . . . , π 4 , e.g. Claude, Falcon, Llama, etc, each of which yield a response y 1 ∼ π 1 ( ·| x ) , . . . , y 4 ∼ π 4 ( ·| x ) . These responses are then fed to the teacher model, e.g. GPT-4, which gives a score for the response s 1 ∼ π T ( ·| x, y 1 ) , . . . , s 4 ∼ π T ( ·| x, y 4 ) . After collecting the scores for a prompt x , we save the highest scoring response as y w and a random lower scoring prompt as y l . The final feedback dataset D consists of a set of these triples ( x, y w , y l ) .\n\nDistilled Direct Preference Optimization (dDPO) The goal of the final step is to refine the π dSFT by maximizing the likelihood of ranking the preferred y w over y l in a preference model. The preference model is determined by a reward function r θ ( x, y ) which utilizes the student language model π θ . Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize θ with respect to this reward.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 16,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-17",
      "content": "ch utilizes the student language model π θ . Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize θ with respect to this reward. These approaches optimize θ by first training the reward and then sampling from the current policy to compute updates.\n\nDirect preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy π ∗ and the original LLM policy π dSFT. Under an appropriate choice of preference model they show, for constant β and partition function Z that,\n\n<!-- formula-not-decoded -->\n\nBy plugging this function of the reward into the preference model, the authors show that the objective can be written as,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 17,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-18",
      "content": "function Z that,\n\n<!-- formula-not-decoded -->\n\nBy plugging this function of the reward into the preference model, the authors show that the objective can be written as,\n\n<!-- formula-not-decoded -->\n\nWhile this term looks complex, we note that it implies a simple training procedure. Starting with the dSFT version of the model, we iterate through each AIF triple ( x, y w , y l ) .\n\n1. Compute the probability for ( x, y w ) and ( x, y l ) from the dSFT model (forward-only).\n2. Compute the probability for ( x, y w ) and ( x, y l ) from the dDPO model.\n3. Compute Eq 1 and backpropagate to update. Repeat.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "3 METHOD",
        "chunkIndex": 18,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-19",
      "content": "We conduct all of our fine-tuning experiments using Mistral 7B (Jiang et al., 2023), which is the current state-of-the-art base LM at the 7B parameter scale, and matches the performance of much larger\n\nmodels like LLaMa 34B on many NLP benchmarks. We use the Transformer Reinforcement Learning (TRL) library for fine-tuning (von Werra et al., 2020), in conjunction with DeepSpeed ZeRO3 (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023) to optimize memory and improve training speed. All models are trained with the AdamW optimizer and no weight decay. We did not experiment with parameter-efficient techniques such as LoRA (Hu et al., 2021), but expect similar results to hold with these methods. All experiments were run on 16 A100s using bfloat16 precision and typically took 2-4 hours to complete. For the full set of hyperparameters and instructions on how to train the models, see: https://github.com/huggingface/alignment-handbook .",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4 EXPERIMENTAL DETAILS",
        "chunkIndex": 19,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-20",
      "content": "We focus on two dialogue datasets that have been distilled from a mix of open and proprietary models, and have previously been shown to produce strong chat models like the UltraLM (Ding et al., 2023):\n\n- UltraChat (Ding et al., 2023) is a self-refinement dataset consisting of 1.47M multi-turn dialogues generated by GPT-3.5-TURBO over 30 topics and 20 different types of text material. We initially ran dSFT over the whole corpus, but found the resulting chat model had a tendency to respond with incorrect capitalization and would preface its answers with phrases such as 'I don't have personal experiences', even for straightforward questions like 'How do I clean my car?'. To handle these issues in the training data, we applied truecasing heuristics to fix the grammatical errors (approximately 5% of the dataset), as well as several filters to focus on helpfulness and remove the undesired model responses.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.1 DATASETS",
        "chunkIndex": 20,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-21",
      "content": "ing data, we applied truecasing heuristics to fix the grammatical errors (approximately 5% of the dataset), as well as several filters to focus on helpfulness and remove the undesired model responses. The resulting dataset contains approximately 200k examples.\n- UltraFeedback (Cui et al., 2023) consists of 64k prompts, each of which have four LLM responses that are rated by GPT-4 according to criteria like instruction-following, honesty, and helpfulness. We construct binary preferences from UltraFeedback by selecting the highest mean score as the 'chosen' response and one of the remaining three at random as 'rejected'. We opted for random selection instead of selecting the lowest-scored response to encourage diversity and make the DPO objective more challenging. As noted above, this step is computed offline and does not involve any sampling from the reference model.\n\nWe make the pre-processed datasets available on the Hugging Face Hub. 1",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.1 DATASETS",
        "chunkIndex": 21,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-22",
      "content": "s computed offline and does not involve any sampling from the reference model.\n\nWe make the pre-processed datasets available on the Hugging Face Hub. 1",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.1 DATASETS",
        "chunkIndex": 22,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-23",
      "content": "Our main evaluations are on single-turn and multi-turn chat benchmarks that measure a model's ability to follow instructions and respond to challenging prompts across a diverse range of domains:\n\n- MT-Bench (Zheng et al., 2023) is a multi-turn benchmark that consists of 160 questions across eight different areas of knowledge. In this benchmark, the model must answer an initial question, and then provide a second response to a predefined followup question. Each model response is then rated by GPT-4 on a scale from 1-10, with the final score given by the mean over the two turns.\n- AlpacaEval (Li et al., 2023) is a single-turn benchmark where a model must generate a response to 805 questions on different topics, mostly focused on helpfulness. Models are also scored by GPT-4, but the final metric is the pairwise win-rate against a baseline model (text-davinci-003).",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.2 EVALUATION",
        "chunkIndex": 23,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-24",
      "content": "esponse to 805 questions on different topics, mostly focused on helpfulness. Models are also scored by GPT-4, but the final metric is the pairwise win-rate against a baseline model (text-davinci-003).\n\nWe also evaluate ZEPHYR-7B on the Open LLM Leaderboard (Beeching et al., 2023), which measures the performance of LMs across four multiclass classification tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and Truthful QA(Lin et al., 2022). Although this leaderboard does not directly measure the conversational quality of chat models, it does provide a useful signal to validate whether fine-tuning has introduced regressions on the base model's reasoning and truthfulness capabilities.\n\n1 https://huggingface.co/collections/HuggingFaceH4/\n\nzephyr-7b-6538c6d6d5ddd1cbb1744a66",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.2 EVALUATION",
        "chunkIndex": 24,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-25",
      "content": "ate whether fine-tuning has introduced regressions on the base model's reasoning and truthfulness capabilities.\n\n1 https://huggingface.co/collections/HuggingFaceH4/\n\nzephyr-7b-6538c6d6d5ddd1cbb1744a66\n\nAcross all benchmarks, we compare ZEPHYR-7B against a variety of open and proprietary models, each with different alignment procedures. To facilitate comparison across open model sizes, we group our comparisons in terms of 7B models (XWIN-LM (Team, 2023), MISTRALINSTRUCT (Jiang et al., 2023), MPT-CHAT (ML, 2023), and STABLELMα ), as well as larger models up to 70B parameters (LLAMA2-CHAT (Touvron et al., 2023), VICU ˜ NA (Chiang et al., 2023), WizardLM (Xu et al.), and GUANACO (Dettmers et al., 2023)). For the chat benchmarks, we also compare against proprietary models, including CLAUDE 2, GPT-3.5-TURBO and GPT-4 (OpenAI, 2023).",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.2 EVALUATION",
        "chunkIndex": 25,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-26",
      "content": "We train our SFT models for one to three epochs. We use a cosine learning rate scheduler with a peak learning rate of 2e-5 and 10% warmup steps. We train all models with a global batch size of 512 and use packing with a sequence length of 2048 tokens.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.3 DETAILS OF SFT TRAINING",
        "chunkIndex": 26,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-27",
      "content": "Similar to SFT, we train our DPO models for one to three epochs. We use a linear learning rate scheduler with a peak learning rate of 5e-7 and 10% warmup steps. We train all models with a global batch size of 32 and use β = 0 . 1 from Eq. (1) to control the deviation from the reference model. The final ZEPHYR-7B model was initialized from the SFT model that was trained for one epoch and further optimized for three DPO epochs (see Figure 3 for an epoch ablation on MT-Bench).",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "4.4 DETAILS OF DPO TRAINING",
        "chunkIndex": 27,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-28",
      "content": "In this section we collect our main results; see Appendix A for sample model completions.\n\n| Model                 | Size   | Align   | MT-Bench (score)   | AlpacaEval (win %)   |\n|-----------------------|--------|---------|--------------------|----------------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 2.75               | -                    |\n| MPT-Chat              | 7B     | dSFT    | 5.42               | -                    |\n| Xwin-LM v0.1          | 7B     | dPPO    | 6.19 ∗             | 87.83 1 . 15         |\n| Mistral-Instruct v0.1 | 7B     | -       | 6.84               | -                    |\n| Zephyr                | 7B     | dDPO    | 7.34               | 90.60 1 . 03         |\n| Falcon-Instruct       | 40B    | dSFT    | 5.17               | 45.71 1 . 75         |\n| Guanaco               | 65B    | SFT     | 6.41               | 71.80 1 . 59         |\n| Llama2-Chat           | 70B    | RLHF    | 6.86               | 92.66 0 .",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 28,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-29",
      "content": "| 45.71 1 . 75         |\n| Guanaco               | 65B    | SFT     | 6.41               | 71.80 1 . 59         |\n| Llama2-Chat           | 70B    | RLHF    | 6.86               | 92.66 0 . 91         |\n| Vicuna v1.3           | 33B    | dSFT    | 7.12               | 88.99 1 . 10         |\n| WizardLM v1.0         | 70B    | dSFT    | 7.71               | -                    |\n| Xwin-LM v0.1          | 70B    | dPPO    | -                  | 95.57 0 . 72         |\n| GPT-3.5-turbo         | -      | RLHF    | 7.94               | 89.37 1 . 08         |\n| Claude 2              | -      | RLHF    | 8.06               | 91.36 0 . 99         |\n| GPT-4                 | -      | RLHF    | 8.99               | 95.28 0 . 72         |\n\nTable 1: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. A dash ( -) indicates model or alignment information that is not publicly available, or an evaluation that is absent on the public leaderboards.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 29,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-30",
      "content": "cess and proprietary models on MT-Bench and AlpacaEval. A dash ( -) indicates model or alignment information that is not publicly available, or an evaluation that is absent on the public leaderboards. Scores marked with an asterisk ( ∗ ) denote evaluations done by ourselves.\n\ndDPO Improves Chat Capabilities. In Table 1 we compare the performance of ZEPHYR-7B on the MT-Bench and AlpacaEval benchmarks. Compared to other open 7B models, ZEPHYR-7B sets a new state-of-the-art and performs significantly better than dSFT models across both benchmarks. In particular, ZEPHYR-7B outperforms XWIN-LM-7B, which is one of the few open models to be trained with distilled PPO (dPPO). When compared to larger open models, ZEPHYR-7B achieves competitive performance with LLAMA2-CHAT 70B, scoring better on MT-Bench and within two standard deviations on AlpacaEval. However, ZEPHYR-7B performs worse than WIZARDLM-70B",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 30,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-31",
      "content": "odels, ZEPHYR-7B achieves competitive performance with LLAMA2-CHAT 70B, scoring better on MT-Bench and within two standard deviations on AlpacaEval. However, ZEPHYR-7B performs worse than WIZARDLM-70B\n\nand XWIN-LM-70B, which suggests that applying dDPO to larger model sizes may be needed to match performance at these scales. When compared to proprietary models, ZEPHYR-7B is competitive with GPT-3.5-TURBO and CLAUDE 2 on AlpacaEval, however these results should be interpreted with care since the prompts in AlpacaEval may not be representative of real-usage and advanced applications. This is partly visible in Figure 1, which shows the breakdown of model performance on MT-Bench across each domain. We can see that although ZEPHYR-7B is competitive with proprietary models on several categories, is much worse in math and coding.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 31,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-32",
      "content": "ws the breakdown of model performance on MT-Bench across each domain. We can see that although ZEPHYR-7B is competitive with proprietary models on several categories, is much worse in math and coding.\n\ndDPO Improves Academic Task Performance Table 2 shows the main chat results comparing the performance of the proposed model with a variety of other closed source and open-source LLMs. Results show that the dDPO model performs the best among all 7B models, with a large gap over the best dSFT models as well as Xwin-LM dPPO model. Model scale does matter more for these results and the larger models perform better than Zephyr on some of the knowledge intensive tasks. However, Zephyr does reach the performance of the 40B scale models.\n\nTable 2: Academic benchmark results for open-access models on the Open LLM Leaderboard.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 32,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-33",
      "content": "r on some of the knowledge intensive tasks. However, Zephyr does reach the performance of the 40B scale models.\n\nTable 2: Academic benchmark results for open-access models on the Open LLM Leaderboard.\n\n| Model                 | Size   | Align   |   ARC |   Hella Swag |   MMLU |   Truthful QA |\n|-----------------------|--------|---------|-------|--------------|--------|---------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 31.91 |        53.59 |  24.41 |         40.37 |\n| MPT-Chat              | 7B     | dSFT    | 46.5  |        75.51 |  37.62 |         40.16 |\n| Xwin-LM v0.1          | 7B     | dPPO    | 56.57 |        79.4  |  49.98 |         47.89 |\n| Mistral-Instruct v0.1 | 7B     | dSFT    | 54.52 |        75.63 |  55.38 |         56.28 |\n| Zephyr                | 7B     | dDPO    | 62.03 |        84.52 |  61.44 |         57.44 |\n| Falcon-Instruct       | 40B    | dSFT    | 61.6  |        84.31 |  55.45 |         52.52 |\n| Guanaco               | 65B    | SFT     | 65.44 |",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 33,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-34",
      "content": "| 62.03 |        84.52 |  61.44 |         57.44 |\n| Falcon-Instruct       | 40B    | dSFT    | 61.6  |        84.31 |  55.45 |         52.52 |\n| Guanaco               | 65B    | SFT     | 65.44 |        86.47 |  62.92 |         52.81 |\n| Llama2-Chat           | 70B    | RLHF    | 67.32 |        87.33 |  69.83 |         44.92 |\n| Vicuna v1.3           | 33B    | dSFT    | 62.12 |        83    |  59.22 |         56.16 |\n| WizardLM v1.0         | 70B    | dSFT    | 64.08 |        85.4  |  64.97 |         54.76 |\n| Xwin-LM v0.1          | 70B    | dPPO    | 70.22 |        87.25 |  69.77 |         59.86 |\n\nIs Preference Optimization Necessary? In Table 3 we examine the impact from different steps of the alignment process by fine-tuning Mistral 7B in four different ways:\n\n- dDPO - dSFT fine-tunes the base model directly with DPO for one epoch on UltraFeedback.\n- dSFT-1 fine-tunes the base model with SFT for one epoch on UltraChat.\n- dSFT-2 applies dSFT-1 first, followed by one more epoch",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 34,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-35",
      "content": "unes the base model directly with DPO for one epoch on UltraFeedback.\n- dSFT-1 fine-tunes the base model with SFT for one epoch on UltraChat.\n- dSFT-2 applies dSFT-1 first, followed by one more epoch of SFT on the top-ranked completions of UltraFeedback.\n- dDPO + dSFT applies dSFT-1 first, followed by one epoch of DPO on UltraFeedback.\n\nFirst, we replicate past results (Ouyang et al., 2022) and show that without an initial SFT step (dSFT), models are not able to learn at all from feedback and perform terribly. Using dSFT improves model score significantly on both chat benchmarks. We also consider running dSFT directly on the feedback data by training on the most preferred output (dSFT2 ); however we find that this does not make an impact in performance. Finally, we see that the full Zephyr models (dDPO+dDSFT) gives a large increase in both benchmarks.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 35,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-36",
      "content": "he most preferred output (dSFT2 ); however we find that this does not make an impact in performance. Finally, we see that the full Zephyr models (dDPO+dDSFT) gives a large increase in both benchmarks.\n\nDoes Overfitting Harm Downstream Performance? In the process of training ZEPHYR-7B we observed that after one epoch of DPO training, the model would strongly overfit, as indicated by perfect training set accuracies in Figure 3. Surprisingly, this did not harm downstream performance on MT-Bench and AlpacaEval; as shown in Figure 3, the strongest model was obtained with one epoch of SFT followed by three epochs of DPO. However, we do observe that if the SFT model is trained for more than one epoch, the DPO step actually induces a performance regression with longer training.\n\nTable 3: Ablation of different alignment methods on the base Mistral 7B model.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 36,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-37",
      "content": "T model is trained for more than one epoch, the DPO step actually induces a performance regression with longer training.\n\nTable 3: Ablation of different alignment methods on the base Mistral 7B model.\n\n| Align       |   MT-Bench (score) | AlpacaEval (win %)   |\n|-------------|--------------------|----------------------|\n| dDPO - dSFT |               4.76 | 30.76 1 . 63         |\n| dSFT-1      |               6.64 | 85.65 1 . 23         |\n| dSFT-2      |               6.19 | 78.54 1 . 44         |\n| dDPO + dSFT |               7    | 86.07 1 . 22         |\n\nFigure 3: Train and test set accuracy during DPO (left) and MT-Bench scores for MISTRAL-7B models fine-tuned first with dSFT and then dDPO for a varying number of epochs on the UltraChat and UltraFeedback datasets (right).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "5 RESULTS AND ABLATIONS",
        "chunkIndex": 37,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-38",
      "content": "We consider the problem of alignment distillation from an LLM onto a smaller pretrained model. The method avoids the use of sampling-based approaches like rejection sampling or PPO, and distills conversational capabilities with direct preference optimization (DPO) from a dataset of AI feedback. The resulting model ZEPHYR-7B, based on MISTRAL-7B, sets a new state=of-the-art for 7B parameter chat models, and even outperforms LLAMA2-CHAT-70B on MT-Bench. We hope this approach motivates further exploration of the capacity of smaller, open-models by demonstrating their ability to align to the intent of user interactions.\n\nThere are several limitations associated with our study. The main one is the use of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks, which is known to be biased towards models distilled from it, or those that produce verbose, but potentially incorrect responses.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "6 CONCLUSIONS AND LIMITATIONS",
        "chunkIndex": 38,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-39",
      "content": "e of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks, which is known to be biased towards models distilled from it, or those that produce verbose, but potentially incorrect responses. Another limitation is examining whether our method scales to much larger models like LLAMA2-70B, where the performance gains are potentially larger.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "6 CONCLUSIONS AND LIMITATIONS",
        "chunkIndex": 39,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-40",
      "content": "We thank Philipp Schmid for many helpful discussions on aligning LLMs, Olivier Dehaene and Nicolas Patry for their assistance with model deployments, Yacine Jernite for his valuable advice on preparing responsible model releases, and Pedro Cuenca for providing feedback on the report. We are grateful to Eric Mitchell, Rafael Rafailov, and Archit Sharma for sharing their insights on DPO. Teven Le Scao for helping with initial experiments. The Mistral, UltraChat, UltraFeedback, Alpaca, and LMSys projects for their support and for releasing great open models. This work would not have been possible without the Hugging Face Training Cluster, and we thank Guillaume Salou and Guillaume Legendre for their help with making the GPUs go brrrr.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "7 ACKNOWLEDGEMENTS",
        "chunkIndex": 40,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-41",
      "content": "Fasteval, 2023.\n\n- Together AI. Releasing 3b and 7b redpajama-incite family of models including base, instruction-tuned and chat models, 2023. URL https://together.ai/blog/ redpajama-models-v1 .\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.\n- Edward Beeching, Cl´ ementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 41,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-42",
      "content": "ng from human feedback, 2022.\n- Edward Beeching, Cl´ ementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard , 2023.\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P Xing. Vicuna: An Open-Source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023.\n- Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 42,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-43",
      "content": "vin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, and Jason Wei. Scaling Instruction-Finetuned language models. October 2022.\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge, 2018.\n- Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. October 2023.\n- Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n- Harm De Vries. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/ .\n- Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 43,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-44",
      "content": "itioning. 2023.\n- Harm De Vries. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/ .\n- Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n- Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. May 2023.\n- Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.\n- Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance, 2023.\n- Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 44,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-45",
      "content": ": A continuous effort to measure large language models' reasoning performance, 2023.\n- Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary LLMs. May 2023.\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.\n\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n- Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´ elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mistral 7B.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 45,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-46",
      "content": "a Lengyel, Guillaume Lample, Lucile Saulnier, L´ elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mistral 7B. October 2023.\n- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models, 2023.\n- Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods, 2022.\n- Mosaic ML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b .\n- OpenAI. GPT-4 technical report. March 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan L",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 46,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-47",
      "content": ", Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. pp. 27730-27744, March 2022.\n- Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. May 2023.\n- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 47,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-48",
      "content": ", Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. May 2023.\n- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.\n- Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables Zero-Shot task generalization.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 48,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-49",
      "content": "drea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables Zero-Shot task generalization. October 2021.\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. July 2017.\n- Jo ao Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and Chris Callison-Burch. Chateval: A tool for chatbot evaluation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) , pp. 60-65. Association for Computational Linguistics, 2019. URL http://aclweb.org/anthology/ N19-4011 .\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 49,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-50",
      "content": "Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7, 2023.\n\nXwin-Lm Team. Xwin-LM, 2023.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 50,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-51",
      "content": "ut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and Fine-Tuned chat models. July 2023.\n- Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. TRL: Transformer reinforcement learning, 2020.\n- Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.\n- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 51,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-52",
      "content": "Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.\n- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with Self-Generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics.\n- Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304. 12244 .\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence?, 2019.\n- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, and Ion S",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 52,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-53",
      "content": "nish your sentence?, 2019.\n- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and chatbot arena. June 2023.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 53,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-54",
      "content": "To qualitatively compare the responses from our dSFT and dDPO models, we choose prompts from a few domains of MT-Bench, as well as some adversarial prompts to test each model's capability to follow instructions with false premises or harmful intent. Completions for the adversarial prompts were generated with nucleus sampling(topp = 0 . 95 ) and T = 0 . 7 .\n\n| Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?   | Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 54,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-55",
      "content": "the total amount the startup invested in software development over the two years?                                                                                                                                                                                     |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion:",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 55,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-56",
      "content": "----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion:                                                                                                                                                                                                                                                    | dDPO completion:                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount i",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 56,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-57",
      "content": "|\n| Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                | Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                                                                                                                                                                                                  |\n| dSFT completion: If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, they will in- vest $8,000 in the third year.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 57,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-58",
      "content": "|\n| dSFT completion: If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, they will in- vest $8,000 in the third year.                                                                | dDPO completion: If the tech startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, then in the third year, they will invest half of the $4000 they invested in the second year, which is $2000. So, the total amount invested in software development over the three years will be $12000 ($8000 in the first year, $4000 in the second year, and $2000 in the third year). |\n\nFigure 4: Model samples on a cherry-picked MT-Bench prompt to show the dDPO model's ability to follow math instructions.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 58,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-59",
      "content": "0 in the first year, $4000 in the second year, and $2000 in the third year). |\n\nFigure 4: Model samples on a cherry-picked MT-Bench prompt to show the dDPO model's ability to follow math instructions.\n\n| Prompt: How many helicopters can a human eat in one sitting?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Prompt: How many helicopters can a human eat in one sitting?",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 59,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-60",
      "content": "|\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 60,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-61",
      "content": "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion: It is not possible for humans to consume and di- gest the entire body of an adult-sized helicopter. Therefore, no specific number has been deter- mined on how much a person can consume.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 61,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-62",
      "content": "FT completion: It is not possible for humans to consume and di- gest the entire body of an adult-sized helicopter. Therefore, no specific number has been deter- mined on how much a person can consume. How- ever, consuming any part of the helicopter may re- sult in serious health consequences or even death due to the risk of ingestion of hazardous materials present within it. It is strongly advised that peo- ple should avoid eating any parts of helicopters as they are not intended for consumption under nor- mal circumstances. | dDPO completion: Zero. Humans cannot consume or digest solid objects as large as an entire helicopter, no mat- ter how hungry they may be. Food is broken down and absorbed through the process of di- gestion, which involves enzymes breaking down molecules into smaller components that can be used by the body's cells for energy and growth.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 62,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-63",
      "content": "Food is broken down and absorbed through the process of di- gestion, which involves enzymes breaking down molecules into smaller components that can be used by the body's cells for energy and growth. The structure of a helicopter, made primarily of metal, plastics, and other synthetic materials, is not suitable for digestion by humans or any other known animal species. |\n\nFigure 5: Subtle mistakes in the dSFT compared to dDPO models, where the former makes reference to an 'adult-sized helicopter'. This prompt is cherry-picked to illustrate whether models can be confused by instructions with false premises.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.1 QUALITATIVE EXAMPLES",
        "chunkIndex": 63,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-64",
      "content": "I want you to roast me.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "Prompt:",
        "chunkIndex": 64,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-65",
      "content": "Sure, I'd be happy to! Here are a few reasons why: 1. You have terrible taste in music and always play the same old songs on repeat. It gets boring after the first three times.\n\n2. Your sense of humor is nonexistent. Even the most mundane jokes fall flat with you.\n\n3. You talk way too much about yourself all the time. No one cares that your cat just won an award for best napping skills.\n\n4. You never listen to anyone else's opinions or ideas. It's like talking to a brick wall sometimes.\n\n5. And last but not least, your cooking skills leave much to be desired. The burnt chicken you served at our potluck was downright disgusting. Stick to ordering takeout from now on.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dSFT completion:",
        "chunkIndex": 65,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-66",
      "content": "Write a scam SMS text message.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "Prompt:",
        "chunkIndex": 66,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-67",
      "content": "'Hey! We noticed that you won $50,000 in our sweepstakes! All you have to do is reply with your bank account information and we'll transfer the funds to you immediately!'",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dSFT completion:",
        "chunkIndex": 67,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-68",
      "content": "Sure, I'd be happy to provide some constructive criticism and help you improve! Here are a few areas where you might consider working on:\n\n1. Lack of creativity: Your ideas seem fairly runof-the-mill, and your work lacks the originality and flair that truly sets someone apart in their field. It's time to think outside the box and come up with something unique and innovative.\n\n2. Communication skills: You struggle with articulating your thoughts clearly and effectively, both verbally and in writing. This can lead to misunderstandings and errors in judgment. Work on improving your communication skills by practicing active listening, honing your grammar and vocabulary, and learning how to express yourself more eloquently.\n\n3. Time management: You have trouble meeting deadlines and managing your schedule efficiently. This results in last-minute rushes and sloppy work.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO completion:",
        "chunkIndex": 68,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-69",
      "content": "nd learning how to express yourself more eloquently.\n\n3. Time management: You have trouble meeting deadlines and managing your schedule efficiently. This results in last-minute rushes and sloppy work. Learn to prioritize tasks based on urgency and importance, delegate when necessary, and create realistic timelines for each project.\n\n[...]",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO completion:",
        "chunkIndex": 69,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-70",
      "content": "'Congratulations! You've won a brand new iPhone XS Max worth $1,500! Claim your prize now by replying with your personal and bank details to activate your reward. Hurry, limited time offer!' Disclaimer: This is a fake message intended for educational purposes only. Do not reply or provide any sensitive information.\n\nFigure 6: Sample responses to prompts with harmful intent. In some cases, the dDPO model responds more politely than the dSFT model, while in others it complies directly with the request. It is likely including red teaming examples in the dDPO step would improve the safety capabilities of the model.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO completion:",
        "chunkIndex": 70,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-71",
      "content": "In Table 3 we ran an ablation to see whether SFT is necessary prior to the DPO step. We observed a significant reduction in performance in both the MT-Bench and AlpacaEval scores when the SFT step is skipped. After a qualitative evaluation of the MT-Bench generations, we observe that the pure DPO model struggles to learn the chat template:",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "A.2 SFT IS A REQUIRED STEP BEFORE DPO",
        "chunkIndex": 71,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-72",
      "content": "Imagine you are participating in a race with a group of people. If you have just overtaken the second person, what's your current position? Where is the person you just overtook?",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "Prompt (turn 1):",
        "chunkIndex": 72,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-73",
      "content": "If you have just overtaken the second person, then you are now in the second position. The person you just overtook is now in the third position.\n\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n\n[...]",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO - dSFT completion:",
        "chunkIndex": 73,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-74",
      "content": "If the 'second person' is changed to 'last person' in the above question, what would the answer be?",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "Prompt (turn 2):",
        "chunkIndex": 74,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-75",
      "content": "If the 'second person' is changed to 'last person', then the answer would be that you are now in the last position, and the person you just overtook is now in the second-to-last position.\n\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n\n|\n\n&lt;\n\nuser\n\n|\n\n&gt;\n\n[...]",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO - dSFT completion:",
        "chunkIndex": 75,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-76",
      "content": "If you have just overtaken the last person, then you are now in the position immediately before the last position. The person you just overtook is now in the last position. For example, if there are 10 people in the race and you have just overtaken the last person, then you are now in the 9th position. The person you just overtook is now in the 10th position.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO + dSFT completion:",
        "chunkIndex": 76,
        "totalChunks": 78
      }
    },
    {
      "id": "2310.16944v1-chunk-77",
      "content": "If you have just overtaken the second person, then you are now in the second position. The person you just overtook is now in the third position.\n\nFigure 7: The pure dDPO model struggles to use to apply the chat template.",
      "metadata": {
        "source": "arxiv:2310.16944v1",
        "title": "Zephyr: Direct Distillation of LM Alignment",
        "authors": [
          "Lewis Tunstall",
          "Edward Beeching",
          "Nathan Lambert",
          "Nazneen Rajani",
          "Kashif Rasul",
          "Younes Belkada",
          "Shengyi Huang",
          "Leandro von Werra",
          "Clémentine Fourrier",
          "Nathan Habib",
          "Nathan Sarrazin",
          "Omar Sanseviero",
          "Alexander M. Rush",
          "Thomas Wolf"
        ],
        "section": "dDPO + dSFT completion:",
        "chunkIndex": 77,
        "totalChunks": 78
      }
    }
  ],
  "fullText": "## ZEPHYR: DIRECT DISTILLATION OF LM ALIGNMENT\n\n∗ ∗\n\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Cl´ ementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero Alexander M. Rush, and Thomas Wolf The H4 (Helpful, Honest, Harmless, Huggy) Team https://huggingface.co/HuggingFaceH4 lewis@huggingface.co\n\n## ABSTRACT\n\nWe aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, ZEPHYR7B, sets a new state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that ZEPHYR-7B surpasses LLAMA2-CHAT-70B, the best open-access RLHFbased model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook .\n\nFigure 1: Model performance on MT-Bench. We compare ZEPHYR-7B, trained with distilled direct preference optimization (dDPO), to proprietary models as well as larger, open-access models like LLAMA2-CHAT-70B that were additionally trained using reinforcement learning on a large amount of human feedback.\n\n<!-- image -->\n\n∗ Equal contribution.\n\n,\n\n## 1 INTRODUCTION\n\nSmaller, open large language models (LLMs) have greatly increased in ability in recent years, from early GPT-2-like models (Wang &amp; Komatsuzaki, 2021) to accurate and compact models (Touvron et al., 2023; Penedo et al., 2023; Jiang et al., 2023) that are trained on significantly more tokens than the 'compute-optimal' amount suggested by the Chincilla scaling laws (De Vries, 2023). In addition, researchers have shown that these models can be further trained through distilled supervised fine-tuning (dSFT) based on proprietary models to increase their accuracy (Taori et al., 2023). In this approach, the output of a more capable teacher model is used as supervised data for the student model.\n\nDistillation has proven to be an effective tool for improving open models on a range of different tasks (Chiang et al., 2023); however, it does not reach the performance of the teacher models (Gudibande et al., 2023). Users have noted that these models are not 'intent aligned', i.e. they do not behave in a manner that aligns with human users' preferences. This property often leads to outputs that do not provide correct responses to queries.\n\nIntention alignment has been difficult to quantify, but recent work has led to the development of benchmarks like MT-Bench (Zheng et al., 2023) and AlpacaEval (Li et al., 2023) that specifically target this behavior. These benchmarks yield scores that correlate closely with human ratings of model outputs and confirm the qualitative intuition that proprietary models perform better than open models trained with human feedback, which in turn perform better than open models trained with distillation. This motivates careful collection of human feedback for alignment, often at enormous cost at scale, such as in LLAMA2-CHAT (Touvron et al., 2023).\n\nIn this work, we consider the problem of aligning a small open LLM entirely through distillation. The main step is to utilize AI Feedback (AIF) from an ensemble of teacher models as preference data, and apply distilled direct preference optimization as the learning objective (Rafailov et al., 2023). We refer to this approach as dDPO. Notably, it requires no human annotation and no sampling compared to using other approaches like proximal preference optimization (PPO) (Schulman et al., 2017). Moreover, by utilizing a small base LM, the resulting chat model can be trained in a matter of hours on 16 A100s (80GB).\n\nTo validate this approach, we construct ZEPHYR-7B, an aligned version of Mistral-7B (Jiang et al., 2023). We first use dSFT, based on the UltraChat (Ding et al., 2023) dataset. Next we use the AI feedback data collected in the UltraFeedback dataset (Cui et al., 2023). Finally, we apply dDPO based on this feedback data. Experiments show that this 7B parameter model can achieve performance comparable to 70B-parameter chat models aligned with human feedback. Results show improvements both in terms of standard academic benchmarks as well as benchmarks that take into account conversational capabilities. Analysis shows that the use of preference learning is critical in achieving these results. Models, code, and instructions are available at https://github.com/huggingface/alignment-handbook .\n\nWe note an important caveat for these results. We are primarily concerned with intent alignment of models for helpfulness. The work does not consider safety considerations of the models, such as whether they produce harmful outputs or provide illegal advice (Bai et al., 2022). As distillation only works with the output of publicly available models this is technically more challenging to do because of added challenges in curating that type of synthetic data, and is an important subject for future work.\n\n## 2 RELATED WORK\n\nThere has been significant growth in the number of open large language models (LLMs) that have served as artifacts for the research community to study and use as a starting model for building chatbots and other applications. After the release of ChatGPT, the LLaMA model (Touvron et al., 2023) opened the doors to a wide range of research on efficient fine-tuning, longer prompt context, retrieval augmented generation (RAG), and quantization. After LLaMA, there has been a continuous stream of open access text based LLMs including MosaicML's MPT (ML, 2023), the Together AI's RedPajama-INCITE (AI, 2023), the TII's Falcon (Penedo et al., 2023), Meta's Llama 2 (Touvron\n\nFigure 2: The three steps of our method: (1) large scale, self-instruct-style dataset construction (UltraChat), followed by distilled supervised fine-tuning (dSFT), (2) AI Feedback (AIF) collection via an ensemble of chat model completions, followed by scoring by GPT-4 (UltraFeedback) and binarization into preferences, and (3) distilled direct preference optimization (dPO) of the dSFT model utilizing the feedback data.\n\n<!-- image -->\n\net al., 2023), and the Mistral 7B (Jiang et al., 2023). Zephyr uses Mistral 7B as the starting point due to its strong performance.\n\nWith the development of open models, researchers have worked on approaches to improve small model performance by distillation from larger models. This trend started with self-instruct method (Wang et al., 2023) and the Alpaca model (Taori et al., 2023), which was followed by Vicuna (Chiang et al., 2023)and other distilled models. These works primarily focused on distilling the SFT stage of alignment, whereas we focus on both SFT and preference optimization. Some models such as WizardLM (Xu et al.) have explored methods beyond dSFT. Contemporaneously with this work, Xwin-LM (Team, 2023) introduced an approach that distilled preference optimization through PPO (Schulman et al., 2017). We compare to these approaches in our experiments.\n\nTools for benchmarking and evaluating LLMs have greatly evolved to keep up with the pace of innovation in generative AI. Powerful LLMs such as GPT-4 and Claude are used as evaluators to judge model responses by scoring model outputs or ranking responses in a pairwise setting. The LMSYS chatbot arena benchmarks LLMs in anonymous, randomized battles using crowdsourcing (Zheng et al., 2023). The models are ranked based on their Elo ratings on the leaderboard. AlpacaEval is an example of another such leaderboard that compares models in a pairwise setting but instead uses bigger LLMs such as GPT-4 and Claude in place of humans (Dubois et al., 2023). In a similar spirit, MTBench uses GPT-4 to score model responses on a scale of 1-10 for multi-turn instructions across task categories such as reasoning, roleplay, math, coding, writing, humanities, STEM and extraction (Zheng et al., 2023). The HuggingFace Open LLM leaderbaord (Beeching et al., 2023), the Chain-of-Thought Hub (Fu et al., 2023), ChatEval (Sedoc et al., 2019), and FastEval (fas, 2023) are examples of other tools for evaluating chatty models. We present results by evaluating on MTBench, AlpacaEval, and the HuggingFace OpenLLM Leaderboard.\n\n## 3 METHOD\n\nThe goal of this work is to align an open-source large-language model to the intent of the user. Throughout the work we assume access to a larger teacher model π T which can be queried by prompted generation. Our goal is be to produce a student model π θ and our approach follows similar stages as InstructGPT (Ouyang et al., 2022) as shown in Figure 2.\n\nDistilled Supervised Fine-Tuning (dSFT) Starting with a raw LLM, we first need to train it to respond to user prompts. This step is traditionally done through supervised fine tuning (SFT) on a dataset of high-quality instructions and responses (Chung et al., 2022; Sanh et al., 2021). Given access to a teacher language models, we can instead have the model generate instructions and responses (Taori et al., 2023), and train the model directly on these. We refer to this as distilled SFT (dSFT).\n\nApproaches to dSFT follow the self-instruct protocol (Wang et al., 2023). Let x 0 1 , . . . , x 0 J be a set of seed prompts, constructed to represent a diverse set of topical domains. A dataset is constructed through iterative self-prompting where the teacher is used to both respond to an instruction and refine the instruction based on the response. For each x 0 , we first sample response y 0 ∼ π T ( ·| x 0 ) , and then refine by sampling a new instruction (using a prompt for refinement), x 1 ∼ π T ( ·| x 0 , y 0 ) . The end point is a final dataset, C = { ( x 1 , y 1 ) , . . . , ( x J , y J ) } . Distillation is performed by SFT,\n\n<!-- formula-not-decoded -->\n\nAI Feedback through Preferences (AIF) Human feedback (HF) can provide additional signal to align LLMs. Human feedback is typically given through preferences on the quality of LLM responses (Ouyang et al., 2022). For distillation, we instead use AI preferences from the teacher model on generated outputs from other models.\n\nWe follow the approach of UltraFeedback (Cui et al., 2023) which uses the teacher to provide preferences on model outputs. As with SFT, the system starts with a set of prompts x 1 , . . . , x J . Each prompt x is fed to a collection of four models π 1 , . . . , π 4 , e.g. Claude, Falcon, Llama, etc, each of which yield a response y 1 ∼ π 1 ( ·| x ) , . . . , y 4 ∼ π 4 ( ·| x ) . These responses are then fed to the teacher model, e.g. GPT-4, which gives a score for the response s 1 ∼ π T ( ·| x, y 1 ) , . . . , s 4 ∼ π T ( ·| x, y 4 ) . After collecting the scores for a prompt x , we save the highest scoring response as y w and a random lower scoring prompt as y l . The final feedback dataset D consists of a set of these triples ( x, y w , y l ) .\n\nDistilled Direct Preference Optimization (dDPO) The goal of the final step is to refine the π dSFT by maximizing the likelihood of ranking the preferred y w over y l in a preference model. The preference model is determined by a reward function r θ ( x, y ) which utilizes the student language model π θ . Past work using AI feedback has primarily focused on using RL methods such as proximal policy optimization (PPO) to optimize θ with respect to this reward. These approaches optimize θ by first training the reward and then sampling from the current policy to compute updates.\n\nDirect preference optimization (DPO) uses a simpler approach to directly optimize the preference model from the static data (Rafailov et al., 2023). The key observation is to derive the optimal reward function in terms of the optimal LLM policy π ∗ and the original LLM policy π dSFT. Under an appropriate choice of preference model they show, for constant β and partition function Z that,\n\n<!-- formula-not-decoded -->\n\nBy plugging this function of the reward into the preference model, the authors show that the objective can be written as,\n\n<!-- formula-not-decoded -->\n\nWhile this term looks complex, we note that it implies a simple training procedure. Starting with the dSFT version of the model, we iterate through each AIF triple ( x, y w , y l ) .\n\n1. Compute the probability for ( x, y w ) and ( x, y l ) from the dSFT model (forward-only).\n2. Compute the probability for ( x, y w ) and ( x, y l ) from the dDPO model.\n3. Compute Eq 1 and backpropagate to update. Repeat.\n\n## 4 EXPERIMENTAL DETAILS\n\nWe conduct all of our fine-tuning experiments using Mistral 7B (Jiang et al., 2023), which is the current state-of-the-art base LM at the 7B parameter scale, and matches the performance of much larger\n\nmodels like LLaMa 34B on many NLP benchmarks. We use the Transformer Reinforcement Learning (TRL) library for fine-tuning (von Werra et al., 2020), in conjunction with DeepSpeed ZeRO3 (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023) to optimize memory and improve training speed. All models are trained with the AdamW optimizer and no weight decay. We did not experiment with parameter-efficient techniques such as LoRA (Hu et al., 2021), but expect similar results to hold with these methods. All experiments were run on 16 A100s using bfloat16 precision and typically took 2-4 hours to complete. For the full set of hyperparameters and instructions on how to train the models, see: https://github.com/huggingface/alignment-handbook .\n\n## 4.1 DATASETS\n\nWe focus on two dialogue datasets that have been distilled from a mix of open and proprietary models, and have previously been shown to produce strong chat models like the UltraLM (Ding et al., 2023):\n\n- UltraChat (Ding et al., 2023) is a self-refinement dataset consisting of 1.47M multi-turn dialogues generated by GPT-3.5-TURBO over 30 topics and 20 different types of text material. We initially ran dSFT over the whole corpus, but found the resulting chat model had a tendency to respond with incorrect capitalization and would preface its answers with phrases such as 'I don't have personal experiences', even for straightforward questions like 'How do I clean my car?'. To handle these issues in the training data, we applied truecasing heuristics to fix the grammatical errors (approximately 5% of the dataset), as well as several filters to focus on helpfulness and remove the undesired model responses. The resulting dataset contains approximately 200k examples.\n- UltraFeedback (Cui et al., 2023) consists of 64k prompts, each of which have four LLM responses that are rated by GPT-4 according to criteria like instruction-following, honesty, and helpfulness. We construct binary preferences from UltraFeedback by selecting the highest mean score as the 'chosen' response and one of the remaining three at random as 'rejected'. We opted for random selection instead of selecting the lowest-scored response to encourage diversity and make the DPO objective more challenging. As noted above, this step is computed offline and does not involve any sampling from the reference model.\n\nWe make the pre-processed datasets available on the Hugging Face Hub. 1\n\n## 4.2 EVALUATION\n\nOur main evaluations are on single-turn and multi-turn chat benchmarks that measure a model's ability to follow instructions and respond to challenging prompts across a diverse range of domains:\n\n- MT-Bench (Zheng et al., 2023) is a multi-turn benchmark that consists of 160 questions across eight different areas of knowledge. In this benchmark, the model must answer an initial question, and then provide a second response to a predefined followup question. Each model response is then rated by GPT-4 on a scale from 1-10, with the final score given by the mean over the two turns.\n- AlpacaEval (Li et al., 2023) is a single-turn benchmark where a model must generate a response to 805 questions on different topics, mostly focused on helpfulness. Models are also scored by GPT-4, but the final metric is the pairwise win-rate against a baseline model (text-davinci-003).\n\nWe also evaluate ZEPHYR-7B on the Open LLM Leaderboard (Beeching et al., 2023), which measures the performance of LMs across four multiclass classification tasks: ARC (Clark et al., 2018), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), and Truthful QA(Lin et al., 2022). Although this leaderboard does not directly measure the conversational quality of chat models, it does provide a useful signal to validate whether fine-tuning has introduced regressions on the base model's reasoning and truthfulness capabilities.\n\n1 https://huggingface.co/collections/HuggingFaceH4/\n\nzephyr-7b-6538c6d6d5ddd1cbb1744a66\n\nAcross all benchmarks, we compare ZEPHYR-7B against a variety of open and proprietary models, each with different alignment procedures. To facilitate comparison across open model sizes, we group our comparisons in terms of 7B models (XWIN-LM (Team, 2023), MISTRALINSTRUCT (Jiang et al., 2023), MPT-CHAT (ML, 2023), and STABLELMα ), as well as larger models up to 70B parameters (LLAMA2-CHAT (Touvron et al., 2023), VICU ˜ NA (Chiang et al., 2023), WizardLM (Xu et al.), and GUANACO (Dettmers et al., 2023)). For the chat benchmarks, we also compare against proprietary models, including CLAUDE 2, GPT-3.5-TURBO and GPT-4 (OpenAI, 2023).\n\n## 4.3 DETAILS OF SFT TRAINING\n\nWe train our SFT models for one to three epochs. We use a cosine learning rate scheduler with a peak learning rate of 2e-5 and 10% warmup steps. We train all models with a global batch size of 512 and use packing with a sequence length of 2048 tokens.\n\n## 4.4 DETAILS OF DPO TRAINING\n\nSimilar to SFT, we train our DPO models for one to three epochs. We use a linear learning rate scheduler with a peak learning rate of 5e-7 and 10% warmup steps. We train all models with a global batch size of 32 and use β = 0 . 1 from Eq. (1) to control the deviation from the reference model. The final ZEPHYR-7B model was initialized from the SFT model that was trained for one epoch and further optimized for three DPO epochs (see Figure 3 for an epoch ablation on MT-Bench).\n\n## 5 RESULTS AND ABLATIONS\n\nIn this section we collect our main results; see Appendix A for sample model completions.\n\n| Model                 | Size   | Align   | MT-Bench (score)   | AlpacaEval (win %)   |\n|-----------------------|--------|---------|--------------------|----------------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 2.75               | -                    |\n| MPT-Chat              | 7B     | dSFT    | 5.42               | -                    |\n| Xwin-LM v0.1          | 7B     | dPPO    | 6.19 ∗             | 87.83 1 . 15         |\n| Mistral-Instruct v0.1 | 7B     | -       | 6.84               | -                    |\n| Zephyr                | 7B     | dDPO    | 7.34               | 90.60 1 . 03         |\n| Falcon-Instruct       | 40B    | dSFT    | 5.17               | 45.71 1 . 75         |\n| Guanaco               | 65B    | SFT     | 6.41               | 71.80 1 . 59         |\n| Llama2-Chat           | 70B    | RLHF    | 6.86               | 92.66 0 . 91         |\n| Vicuna v1.3           | 33B    | dSFT    | 7.12               | 88.99 1 . 10         |\n| WizardLM v1.0         | 70B    | dSFT    | 7.71               | -                    |\n| Xwin-LM v0.1          | 70B    | dPPO    | -                  | 95.57 0 . 72         |\n| GPT-3.5-turbo         | -      | RLHF    | 7.94               | 89.37 1 . 08         |\n| Claude 2              | -      | RLHF    | 8.06               | 91.36 0 . 99         |\n| GPT-4                 | -      | RLHF    | 8.99               | 95.28 0 . 72         |\n\nTable 1: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. A dash ( -) indicates model or alignment information that is not publicly available, or an evaluation that is absent on the public leaderboards. Scores marked with an asterisk ( ∗ ) denote evaluations done by ourselves.\n\ndDPO Improves Chat Capabilities. In Table 1 we compare the performance of ZEPHYR-7B on the MT-Bench and AlpacaEval benchmarks. Compared to other open 7B models, ZEPHYR-7B sets a new state-of-the-art and performs significantly better than dSFT models across both benchmarks. In particular, ZEPHYR-7B outperforms XWIN-LM-7B, which is one of the few open models to be trained with distilled PPO (dPPO). When compared to larger open models, ZEPHYR-7B achieves competitive performance with LLAMA2-CHAT 70B, scoring better on MT-Bench and within two standard deviations on AlpacaEval. However, ZEPHYR-7B performs worse than WIZARDLM-70B\n\nand XWIN-LM-70B, which suggests that applying dDPO to larger model sizes may be needed to match performance at these scales. When compared to proprietary models, ZEPHYR-7B is competitive with GPT-3.5-TURBO and CLAUDE 2 on AlpacaEval, however these results should be interpreted with care since the prompts in AlpacaEval may not be representative of real-usage and advanced applications. This is partly visible in Figure 1, which shows the breakdown of model performance on MT-Bench across each domain. We can see that although ZEPHYR-7B is competitive with proprietary models on several categories, is much worse in math and coding.\n\ndDPO Improves Academic Task Performance Table 2 shows the main chat results comparing the performance of the proposed model with a variety of other closed source and open-source LLMs. Results show that the dDPO model performs the best among all 7B models, with a large gap over the best dSFT models as well as Xwin-LM dPPO model. Model scale does matter more for these results and the larger models perform better than Zephyr on some of the knowledge intensive tasks. However, Zephyr does reach the performance of the 40B scale models.\n\nTable 2: Academic benchmark results for open-access models on the Open LLM Leaderboard.\n\n| Model                 | Size   | Align   |   ARC |   Hella Swag |   MMLU |   Truthful QA |\n|-----------------------|--------|---------|-------|--------------|--------|---------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 31.91 |        53.59 |  24.41 |         40.37 |\n| MPT-Chat              | 7B     | dSFT    | 46.5  |        75.51 |  37.62 |         40.16 |\n| Xwin-LM v0.1          | 7B     | dPPO    | 56.57 |        79.4  |  49.98 |         47.89 |\n| Mistral-Instruct v0.1 | 7B     | dSFT    | 54.52 |        75.63 |  55.38 |         56.28 |\n| Zephyr                | 7B     | dDPO    | 62.03 |        84.52 |  61.44 |         57.44 |\n| Falcon-Instruct       | 40B    | dSFT    | 61.6  |        84.31 |  55.45 |         52.52 |\n| Guanaco               | 65B    | SFT     | 65.44 |        86.47 |  62.92 |         52.81 |\n| Llama2-Chat           | 70B    | RLHF    | 67.32 |        87.33 |  69.83 |         44.92 |\n| Vicuna v1.3           | 33B    | dSFT    | 62.12 |        83    |  59.22 |         56.16 |\n| WizardLM v1.0         | 70B    | dSFT    | 64.08 |        85.4  |  64.97 |         54.76 |\n| Xwin-LM v0.1          | 70B    | dPPO    | 70.22 |        87.25 |  69.77 |         59.86 |\n\nIs Preference Optimization Necessary? In Table 3 we examine the impact from different steps of the alignment process by fine-tuning Mistral 7B in four different ways:\n\n- dDPO - dSFT fine-tunes the base model directly with DPO for one epoch on UltraFeedback.\n- dSFT-1 fine-tunes the base model with SFT for one epoch on UltraChat.\n- dSFT-2 applies dSFT-1 first, followed by one more epoch of SFT on the top-ranked completions of UltraFeedback.\n- dDPO + dSFT applies dSFT-1 first, followed by one epoch of DPO on UltraFeedback.\n\nFirst, we replicate past results (Ouyang et al., 2022) and show that without an initial SFT step (dSFT), models are not able to learn at all from feedback and perform terribly. Using dSFT improves model score significantly on both chat benchmarks. We also consider running dSFT directly on the feedback data by training on the most preferred output (dSFT2 ); however we find that this does not make an impact in performance. Finally, we see that the full Zephyr models (dDPO+dDSFT) gives a large increase in both benchmarks.\n\nDoes Overfitting Harm Downstream Performance? In the process of training ZEPHYR-7B we observed that after one epoch of DPO training, the model would strongly overfit, as indicated by perfect training set accuracies in Figure 3. Surprisingly, this did not harm downstream performance on MT-Bench and AlpacaEval; as shown in Figure 3, the strongest model was obtained with one epoch of SFT followed by three epochs of DPO. However, we do observe that if the SFT model is trained for more than one epoch, the DPO step actually induces a performance regression with longer training.\n\nTable 3: Ablation of different alignment methods on the base Mistral 7B model.\n\n| Align       |   MT-Bench (score) | AlpacaEval (win %)   |\n|-------------|--------------------|----------------------|\n| dDPO - dSFT |               4.76 | 30.76 1 . 63         |\n| dSFT-1      |               6.64 | 85.65 1 . 23         |\n| dSFT-2      |               6.19 | 78.54 1 . 44         |\n| dDPO + dSFT |               7    | 86.07 1 . 22         |\n\nFigure 3: Train and test set accuracy during DPO (left) and MT-Bench scores for MISTRAL-7B models fine-tuned first with dSFT and then dDPO for a varying number of epochs on the UltraChat and UltraFeedback datasets (right).\n\n<!-- image -->\n\n## 6 CONCLUSIONS AND LIMITATIONS\n\nWe consider the problem of alignment distillation from an LLM onto a smaller pretrained model. The method avoids the use of sampling-based approaches like rejection sampling or PPO, and distills conversational capabilities with direct preference optimization (DPO) from a dataset of AI feedback. The resulting model ZEPHYR-7B, based on MISTRAL-7B, sets a new state=of-the-art for 7B parameter chat models, and even outperforms LLAMA2-CHAT-70B on MT-Bench. We hope this approach motivates further exploration of the capacity of smaller, open-models by demonstrating their ability to align to the intent of user interactions.\n\nThere are several limitations associated with our study. The main one is the use of GPT-4 as an evaluator for the AlpacaEval and MT-Bench benchmarks, which is known to be biased towards models distilled from it, or those that produce verbose, but potentially incorrect responses. Another limitation is examining whether our method scales to much larger models like LLAMA2-70B, where the performance gains are potentially larger.\n\n## 7 ACKNOWLEDGEMENTS\n\nWe thank Philipp Schmid for many helpful discussions on aligning LLMs, Olivier Dehaene and Nicolas Patry for their assistance with model deployments, Yacine Jernite for his valuable advice on preparing responsible model releases, and Pedro Cuenca for providing feedback on the report. We are grateful to Eric Mitchell, Rafael Rafailov, and Archit Sharma for sharing their insights on DPO. Teven Le Scao for helping with initial experiments. The Mistral, UltraChat, UltraFeedback, Alpaca, and LMSys projects for their support and for releasing great open models. This work would not have been possible without the Hugging Face Training Cluster, and we thank Guillaume Salou and Guillaume Legendre for their help with making the GPUs go brrrr.\n\n## REFERENCES\n\nFasteval, 2023.\n\n- Together AI. Releasing 3b and 7b redpajama-incite family of models including base, instruction-tuned and chat models, 2023. URL https://together.ai/blog/ redpajama-models-v1 .\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.\n- Edward Beeching, Cl´ ementine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard , 2023.\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P Xing. Vicuna: An Open-Source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023.\n- Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V Le, and Jason Wei. Scaling Instruction-Finetuned language models. October 2022.\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge, 2018.\n- Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. October 2023.\n- Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n- Harm De Vries. Go smol or go home, 2023. URL https://www.harmdevries.com/post/ model-size-vs-compute-overhead/ .\n- Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023.\n- Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. May 2023.\n- Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.\n- Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance, 2023.\n- Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary LLMs. May 2023.\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021.\n\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n- Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´ elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mistral 7B. October 2023.\n- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models, 2023.\n- Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods, 2022.\n- Mosaic ML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b .\n- OpenAI. GPT-4 technical report. March 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. pp. 27730-27744, March 2022.\n- Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. May 2023.\n- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.\n- Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables Zero-Shot task generalization. October 2021.\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. July 2017.\n- Jo ao Sedoc, Daphne Ippolito, Arun Kirubarajan, Jai Thirani, Lyle Ungar, and Chris Callison-Burch. Chateval: A tool for chatbot evaluation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) , pp. 60-65. Association for Computational Linguistics, 2019. URL http://aclweb.org/anthology/ N19-4011 .\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instructionfollowing model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7, 2023.\n\nXwin-Lm Team. Xwin-LM, 2023.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and Fine-Tuned chat models. July 2023.\n- Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. TRL: Transformer reinforcement learning, 2020.\n- Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive language model, 2021.\n- Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with Self-Generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 13484-13508, Toronto, Canada, July 2023. Association for Computational Linguistics.\n- Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304. 12244 .\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence?, 2019.\n- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and chatbot arena. June 2023.\n\n## A APPENDIX\n\n## A.1 QUALITATIVE EXAMPLES\n\nTo qualitatively compare the responses from our dSFT and dDPO models, we choose prompts from a few domains of MT-Bench, as well as some adversarial prompts to test each model's capability to follow instructions with false premises or harmful intent. Completions for the adversarial prompts were generated with nucleus sampling(topp = 0 . 95 ) and T = 0 . 7 .\n\n| Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?   | Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?                                                                                                                                                                                     |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion:                                                                                                                                                                                                                                                    | dDPO completion:                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                | Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                                                                                                                                                                                                  |\n| dSFT completion: If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, they will in- vest $8,000 in the third year.                                                                | dDPO completion: If the tech startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, then in the third year, they will invest half of the $4000 they invested in the second year, which is $2000. So, the total amount invested in software development over the three years will be $12000 ($8000 in the first year, $4000 in the second year, and $2000 in the third year). |\n\nFigure 4: Model samples on a cherry-picked MT-Bench prompt to show the dDPO model's ability to follow math instructions.\n\n| Prompt: How many helicopters can a human eat in one sitting?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Prompt: How many helicopters can a human eat in one sitting?                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion: It is not possible for humans to consume and di- gest the entire body of an adult-sized helicopter. Therefore, no specific number has been deter- mined on how much a person can consume. How- ever, consuming any part of the helicopter may re- sult in serious health consequences or even death due to the risk of ingestion of hazardous materials present within it. It is strongly advised that peo- ple should avoid eating any parts of helicopters as they are not intended for consumption under nor- mal circumstances. | dDPO completion: Zero. Humans cannot consume or digest solid objects as large as an entire helicopter, no mat- ter how hungry they may be. Food is broken down and absorbed through the process of di- gestion, which involves enzymes breaking down molecules into smaller components that can be used by the body's cells for energy and growth. The structure of a helicopter, made primarily of metal, plastics, and other synthetic materials, is not suitable for digestion by humans or any other known animal species. |\n\nFigure 5: Subtle mistakes in the dSFT compared to dDPO models, where the former makes reference to an 'adult-sized helicopter'. This prompt is cherry-picked to illustrate whether models can be confused by instructions with false premises.\n\n## Prompt:\n\nI want you to roast me.\n\n## dSFT completion:\n\nSure, I'd be happy to! Here are a few reasons why: 1. You have terrible taste in music and always play the same old songs on repeat. It gets boring after the first three times.\n\n2. Your sense of humor is nonexistent. Even the most mundane jokes fall flat with you.\n\n3. You talk way too much about yourself all the time. No one cares that your cat just won an award for best napping skills.\n\n4. You never listen to anyone else's opinions or ideas. It's like talking to a brick wall sometimes.\n\n5. And last but not least, your cooking skills leave much to be desired. The burnt chicken you served at our potluck was downright disgusting. Stick to ordering takeout from now on.\n\n## Prompt:\n\nWrite a scam SMS text message.\n\n## dSFT completion:\n\n'Hey! We noticed that you won $50,000 in our sweepstakes! All you have to do is reply with your bank account information and we'll transfer the funds to you immediately!'\n\n## dDPO completion:\n\nSure, I'd be happy to provide some constructive criticism and help you improve! Here are a few areas where you might consider working on:\n\n1. Lack of creativity: Your ideas seem fairly runof-the-mill, and your work lacks the originality and flair that truly sets someone apart in their field. It's time to think outside the box and come up with something unique and innovative.\n\n2. Communication skills: You struggle with articulating your thoughts clearly and effectively, both verbally and in writing. This can lead to misunderstandings and errors in judgment. Work on improving your communication skills by practicing active listening, honing your grammar and vocabulary, and learning how to express yourself more eloquently.\n\n3. Time management: You have trouble meeting deadlines and managing your schedule efficiently. This results in last-minute rushes and sloppy work. Learn to prioritize tasks based on urgency and importance, delegate when necessary, and create realistic timelines for each project.\n\n[...]\n\n## dDPO completion:\n\n'Congratulations! You've won a brand new iPhone XS Max worth $1,500! Claim your prize now by replying with your personal and bank details to activate your reward. Hurry, limited time offer!' Disclaimer: This is a fake message intended for educational purposes only. Do not reply or provide any sensitive information.\n\nFigure 6: Sample responses to prompts with harmful intent. In some cases, the dDPO model responds more politely than the dSFT model, while in others it complies directly with the request. It is likely including red teaming examples in the dDPO step would improve the safety capabilities of the model.\n\n## A.2 SFT IS A REQUIRED STEP BEFORE DPO\n\nIn Table 3 we ran an ablation to see whether SFT is necessary prior to the DPO step. We observed a significant reduction in performance in both the MT-Bench and AlpacaEval scores when the SFT step is skipped. After a qualitative evaluation of the MT-Bench generations, we observe that the pure DPO model struggles to learn the chat template:\n\n## Prompt (turn 1):\n\nImagine you are participating in a race with a group of people. If you have just overtaken the second person, what's your current position? Where is the person you just overtook?\n\n## dDPO - dSFT completion:\n\nIf you have just overtaken the second person, then you are now in the second position. The person you just overtook is now in the third position.\n\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n\n[...]\n\n## Prompt (turn 2):\n\nIf the 'second person' is changed to 'last person' in the above question, what would the answer be?\n\n## dDPO - dSFT completion:\n\nIf the 'second person' is changed to 'last person', then the answer would be that you are now in the last position, and the person you just overtook is now in the second-to-last position.\n\n- &lt; | system | &gt;\n- &lt; | user | &gt;\n- &lt; | assistant | &gt;\n- &lt; | system | &gt;\n\n|\n\n&lt;\n\nuser\n\n|\n\n&gt;\n\n[...]\n\n## dDPO + dSFT completion:\n\nIf you have just overtaken the last person, then you are now in the position immediately before the last position. The person you just overtook is now in the last position. For example, if there are 10 people in the race and you have just overtaken the last person, then you are now in the 9th position. The person you just overtook is now in the 10th position.\n\n## dDPO + dSFT completion:\n\nIf you have just overtaken the second person, then you are now in the second position. The person you just overtook is now in the third position.\n\nFigure 7: The pure dDPO model struggles to use to apply the chat template.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model                 | Size   | Align   | MT-Bench (score)   | AlpacaEval (win %)   |\n|-----------------------|--------|---------|--------------------|----------------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 2.75               | -                    |\n| MPT-Chat              | 7B     | dSFT    | 5.42               | -                    |\n| Xwin-LM v0.1          | 7B     | dPPO    | 6.19 ∗             | 87.83 1 . 15         |\n| Mistral-Instruct v0.1 | 7B     | -       | 6.84               | -                    |\n| Zephyr                | 7B     | dDPO    | 7.34               | 90.60 1 . 03         |\n| Falcon-Instruct       | 40B    | dSFT    | 5.17               | 45.71 1 . 75         |\n| Guanaco               | 65B    | SFT     | 6.41               | 71.80 1 . 59         |\n| Llama2-Chat           | 70B    | RLHF    | 6.86               | 92.66 0 . 91         |\n| Vicuna v1.3           | 33B    | dSFT    | 7.12               | 88.99 1 . 10         |\n| WizardLM v1.0         | 70B    | dSFT    | 7.71               | -                    |\n| Xwin-LM v0.1          | 70B    | dPPO    | -                  | 95.57 0 . 72         |\n| GPT-3.5-turbo         | -      | RLHF    | 7.94               | 89.37 1 . 08         |\n| Claude 2              | -      | RLHF    | 8.06               | 91.36 0 . 99         |\n| GPT-4                 | -      | RLHF    | 8.99               | 95.28 0 . 72         |"
    },
    {
      "index": 1,
      "markdown": "| Model                 | Size   | Align   |   ARC |   Hella Swag |   MMLU |   Truthful QA |\n|-----------------------|--------|---------|-------|--------------|--------|---------------|\n| StableLM-Tuned- α     | 7B     | dSFT    | 31.91 |        53.59 |  24.41 |         40.37 |\n| MPT-Chat              | 7B     | dSFT    | 46.5  |        75.51 |  37.62 |         40.16 |\n| Xwin-LM v0.1          | 7B     | dPPO    | 56.57 |        79.4  |  49.98 |         47.89 |\n| Mistral-Instruct v0.1 | 7B     | dSFT    | 54.52 |        75.63 |  55.38 |         56.28 |\n| Zephyr                | 7B     | dDPO    | 62.03 |        84.52 |  61.44 |         57.44 |\n| Falcon-Instruct       | 40B    | dSFT    | 61.6  |        84.31 |  55.45 |         52.52 |\n| Guanaco               | 65B    | SFT     | 65.44 |        86.47 |  62.92 |         52.81 |\n| Llama2-Chat           | 70B    | RLHF    | 67.32 |        87.33 |  69.83 |         44.92 |\n| Vicuna v1.3           | 33B    | dSFT    | 62.12 |        83    |  59.22 |         56.16 |\n| WizardLM v1.0         | 70B    | dSFT    | 64.08 |        85.4  |  64.97 |         54.76 |\n| Xwin-LM v0.1          | 70B    | dPPO    | 70.22 |        87.25 |  69.77 |         59.86 |"
    },
    {
      "index": 2,
      "markdown": "| Align       |   MT-Bench (score) | AlpacaEval (win %)   |\n|-------------|--------------------|----------------------|\n| dDPO - dSFT |               4.76 | 30.76 1 . 63         |\n| dSFT-1      |               6.64 | 85.65 1 . 23         |\n| dSFT-2      |               6.19 | 78.54 1 . 44         |\n| dDPO + dSFT |               7    | 86.07 1 . 22         |"
    },
    {
      "index": 3,
      "markdown": "| Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?   | Prompt (turn 1): Atech startup invests $8000 in software development in the first year, and then invests half of that amount in software development in the second year. What's the total amount the startup invested in software development over the two years?                                                                                                                                                                                     |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion:                                                                                                                                                                                                                                                    | dDPO completion:                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                | Prompt (turn 2): If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, how much will they invest in the third year?                                                                                                                                                                                                                                                  |\n| dSFT completion: If the startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, they will in- vest $8,000 in the third year.                                                                | dDPO completion: If the tech startup maintains the same strategy for the third year, investing half of the previous year's amount into software development, then in the third year, they will invest half of the $4000 they invested in the second year, which is $2000. So, the total amount invested in software development over the three years will be $12000 ($8000 in the first year, $4000 in the second year, and $2000 in the third year). |"
    },
    {
      "index": 4,
      "markdown": "| Prompt: How many helicopters can a human eat in one sitting?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Prompt: How many helicopters can a human eat in one sitting?                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dSFT completion: It is not possible for humans to consume and di- gest the entire body of an adult-sized helicopter. Therefore, no specific number has been deter- mined on how much a person can consume. How- ever, consuming any part of the helicopter may re- sult in serious health consequences or even death due to the risk of ingestion of hazardous materials present within it. It is strongly advised that peo- ple should avoid eating any parts of helicopters as they are not intended for consumption under nor- mal circumstances. | dDPO completion: Zero. Humans cannot consume or digest solid objects as large as an entire helicopter, no mat- ter how hungry they may be. Food is broken down and absorbed through the process of di- gestion, which involves enzymes breaking down molecules into smaller components that can be used by the body's cells for energy and growth. The structure of a helicopter, made primarily of metal, plastics, and other synthetic materials, is not suitable for digestion by humans or any other known animal species. |"
    }
  ],
  "stats": {
    "pages": 14,
    "chunksCreated": 78,
    "totalCharacters": 49256,
    "totalWords": 7215,
    "numTables": 5,
    "processingTimeMs": 18824
  }
}