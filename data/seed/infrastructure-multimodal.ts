/**
 * Infrastructure, Multimodal, and Alignment Techniques Seed Data
 * 
 * Covers:
 * - Quantization: GPTQ, AWQ, LLM-FP4
 * - Efficient Attention: Flash Attention, PagedAttention, vLLM
 * - Multimodal: CLIP, BLIP, Flamingo, LLaVA, Qwen-VL
 * - Speech: Whisper, Distil-Whisper
 * - Alignment: DPO, ORPO, IPO
 * - Serving: vLLM, TensorRT-LLM
 */

import neo4j from "neo4j-driver";

const driver = neo4j.driver(
  "bolt://localhost:7687",
  neo4j.auth.basic("neo4j", "password")
);

interface Entity {
  type: string;
  name: string;
  description: string;
  properties?: Record<string, string | number | string[]>;
}

interface Relation {
  from: string;
  to: string;
  type: string;
  properties?: Record<string, string | number>;
}

// ============================================================================
// ENTITIES
// ============================================================================

const entities: Entity[] = [
  // =========== CONCEPTS ===========
  {
    type: "Concept",
    name: "Model Quantization",
    description: "ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚„æ´»æ€§åŒ–ã‚’ä½Žç²¾åº¦ï¼ˆINT8, INT4, FP4ãªã©ï¼‰ã«åœ§ç¸®ã™ã‚‹æŠ€è¡“ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ã¨æŽ¨è«–é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚Post-training quantization (PTQ)ã¨Quantization-aware training (QAT)ã®2ã¤ã®ä¸»è¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã‚ã‚‹ã€‚",
    properties: {
      aliases: ["é‡å­åŒ–", "Weight Quantization", "ãƒ¢ãƒ‡ãƒ«åœ§ç¸®"],
      domain: "Model Optimization"
    }
  },
  {
    type: "Concept",
    name: "Efficient Attention",
    description: "Transformerã®self-attentionè¨ˆç®—ã‚’åŠ¹çŽ‡åŒ–ã™ã‚‹æŠ€è¡“ç¾¤ã€‚æ¨™æº–çš„ãªattentionã¯O(nÂ²)ã®è¨ˆç®—é‡ã ãŒã€IO-awareè¨­è¨ˆã‚„KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã«ã‚ˆã‚Šå¤§å¹…ãªé«˜é€ŸåŒ–ãŒå¯èƒ½ã€‚",
    properties: {
      aliases: ["åŠ¹çŽ‡çš„Attention", "Attention Optimization"],
      domain: "Architecture Optimization"
    }
  },
  {
    type: "Concept",
    name: "Vision-Language Models",
    description: "ç”»åƒãƒ»å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆçš„ã«ç†è§£ãƒ»ç”Ÿæˆã™ã‚‹ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIãƒ¢ãƒ‡ãƒ«ã€‚CLIPä»¥é™ã€å¯¾ç…§å­¦ç¿’ã‚„ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒä¸»è¦æŠ€è¡“ã¨ãªã£ã¦ã„ã‚‹ã€‚",
    properties: {
      aliases: ["VLM", "ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«", "Vision-Language Pretraining"],
      domain: "Multimodal AI"
    }
  },
  {
    type: "Concept",
    name: "Preference Alignment",
    description: "äººé–“ã®å¥½ã¿ã«æ²¿ã£ã¦LLMã®å‡ºåŠ›ã‚’èª¿æ•´ã™ã‚‹æŠ€è¡“ã€‚RLHFãŒå…ˆé§†ã‘ã ãŒã€DPOã‚„ORPOãªã©ã‚ˆã‚ŠåŠ¹çŽ‡çš„ãªæ‰‹æ³•ãŒç™»å ´ã€‚å ±é…¬ãƒ¢ãƒ‡ãƒ«ãªã—ã§ç›´æŽ¥æœ€é©åŒ–å¯èƒ½ã€‚",
    properties: {
      aliases: ["é¸å¥½ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ", "RLHF Alternative", "Human Alignment"],
      domain: "Alignment"
    }
  },
  {
    type: "Concept",
    name: "Speech Recognition",
    description: "éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ï¼ˆASRï¼‰ã€‚Whisperã«ã‚ˆã‚Šå¤§è¦æ¨¡å¼±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§ãƒ­ãƒã‚¹ãƒˆæ€§ãŒå¤§å¹…å‘ä¸Šã€‚å¤šè¨€èªžå¯¾å¿œã¨é•·æ™‚é–“éŸ³å£°å‡¦ç†ãŒèª²é¡Œã€‚",
    properties: {
      aliases: ["éŸ³å£°èªè­˜", "ASR", "Speech-to-Text", "STT"],
      domain: "Speech Processing"
    }
  },
  {
    type: "Concept",
    name: "LLM Serving",
    description: "å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã§åŠ¹çŽ‡çš„ã«æä¾›ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã€‚ãƒãƒƒãƒå‡¦ç†ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã€é€£ç¶šãƒãƒƒãƒãƒ³ã‚°ãªã©ãŒé‡è¦æŠ€è¡“ã€‚",
    properties: {
      aliases: ["LLMã‚µãƒ¼ãƒ“ãƒ³ã‚°", "Model Serving", "Inference Optimization"],
      domain: "MLOps"
    }
  },

  // =========== QUANTIZATION TECHNIQUES ===========
  {
    type: "Technique",
    name: "GPTQ",
    description: "è¿‘ä¼¼äºŒæ¬¡æƒ…å ±ã‚’ç”¨ã„ãŸãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆé‡å­åŒ–æ‰‹æ³•ã€‚GPT-175Bã‚’ç´„4GPUæ™‚é–“ã§3-4bitã«é‡å­åŒ–å¯èƒ½ã€‚ç²¾åº¦åŠ£åŒ–ã‚’æœ€å°é™ã«æŠ‘ãˆãªãŒã‚‰ã€175Bãƒ¢ãƒ‡ãƒ«ã‚’å˜ä¸€GPUã§å®Ÿè¡Œå¯èƒ½ã«ã—ãŸç”»æœŸçš„æ‰‹æ³•ã€‚ICLR 2023ã§ç™ºè¡¨ã€‚",
    properties: {
      arxivId: "2210.17323",
      year: 2022,
      domain: "Quantization",
      precision: "3-4 bit",
      speedup: "3.25-4.5x"
    }
  },
  {
    type: "Technique",
    name: "AWQ",
    description: "Activation-aware Weight Quantizationã€‚æ´»æ€§åŒ–åˆ†å¸ƒã‚’å‚ç…§ã—ã¦é‡è¦ãªé‡ã¿ãƒãƒ£ãƒãƒ«ã‚’ç‰¹å®šãƒ»ä¿è­·ã™ã‚‹é‡å­åŒ–æ‰‹æ³•ã€‚é‡ã¿ã®1%ã‚’ä¿è­·ã™ã‚‹ã ã‘ã§é‡å­åŒ–èª¤å·®ã‚’å¤§å¹…å‰Šæ¸›ã€‚MLSys 2024 Best Paperã€‚70B Llama-2ã‚’ãƒ¢ãƒã‚¤ãƒ«GPUã§å®Ÿè¡Œå¯èƒ½ã«ã€‚",
    properties: {
      arxivId: "2306.00978",
      year: 2023,
      domain: "Quantization",
      precision: "4 bit",
      speedup: "3x over FP16"
    }
  },
  {
    type: "Technique",
    name: "LLM-FP4",
    description: "4bitæµ®å‹•å°æ•°ç‚¹é‡å­åŒ–æ‰‹æ³•ã€‚æ•´æ•°é‡å­åŒ–ã‚ˆã‚ŠæŸ”è»Ÿã§ã€é•·ã„å°¾ã‚’æŒã¤åˆ†å¸ƒã«é©å¿œã€‚é‡ã¿ã¨æ´»æ€§åŒ–ã®ä¸¡æ–¹ã‚’4bitã«é‡å­åŒ–ã—ã€LLaMA-13Bã§å¹³å‡63.1ã®ã‚¹ã‚³ã‚¢ã‚’é”æˆã€‚EMNLP 2023ã§ç™ºè¡¨ã€‚",
    properties: {
      arxivId: "2310.16836",
      year: 2023,
      domain: "Quantization",
      precision: "FP4"
    }
  },
  {
    type: "Technique",
    name: "GGUF",
    description: "llama.cppç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã€‚CPUæŽ¨è«–ã«æœ€é©åŒ–ã•ã‚ŒãŸé‡å­åŒ–å½¢å¼ã§ã€Q4_0, Q5_K_Mç­‰ã®æ§˜ã€…ãªé‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã‚’å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã«æ ¼ç´ã€‚",
    properties: {
      year: 2023,
      domain: "Model Format",
      developer: "ggerganov"
    }
  },

  // =========== EFFICIENT ATTENTION ===========
  {
    type: "Technique",
    name: "Flash Attention",
    description: "IO-awareãªæ­£ç¢ºãªattentionã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚ã‚¿ã‚¤ãƒªãƒ³ã‚°ã«ã‚ˆã‚ŠGPUãƒ¡ãƒ¢ãƒªéšŽå±¤ã‚’æœ€é©æ´»ç”¨ã—ã€HBMã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã€‚BERT-largeã§15%ã€GPT-2ã§3å€ã®è¨“ç·´é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚Path-X (16K)ã§61.4%ç²¾åº¦ã‚’é”æˆã—ãŸåˆã®Transformerã€‚",
    properties: {
      arxivId: "2205.14135",
      year: 2022,
      developer: "Stanford",
      author: "Tri Dao"
    }
  },
  {
    type: "Technique",
    name: "Flash Attention 2",
    description: "Flash Attentionã®æ”¹è‰¯ç‰ˆã€‚ä¸¦åˆ—åŒ–ã¨ãƒ¯ãƒ¼ã‚¯åˆ†å‰²ã‚’æœ€é©åŒ–ã—ã€A100ã§ç†è«–æœ€å¤§FLOPs/sã®50-73%ã‚’é”æˆã€‚GPTè¨“ç·´ã§225 TFLOPs/sï¼ˆãƒ¢ãƒ‡ãƒ«FLOPsåŠ¹çŽ‡72%ï¼‰ã€‚ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ã§ã‚‚ä¸¦åˆ—åŒ–å¯èƒ½ã€‚",
    properties: {
      arxivId: "2307.08691",
      year: 2023,
      speedup: "2x over FlashAttention v1"
    }
  },
  {
    type: "Technique",
    name: "PagedAttention",
    description: "OSã®ä»®æƒ³ãƒ¡ãƒ¢ãƒªã¨ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã«ç€æƒ³ã‚’å¾—ãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†æ‰‹æ³•ã€‚ãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ã¨é‡è¤‡ã‚’è§£æ¶ˆã—ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§å¹…ã«å¢—åŠ å¯èƒ½ã€‚vLLMã®åŸºç›¤æŠ€è¡“ã€‚SOSP 2023ã§ç™ºè¡¨ã€‚",
    properties: {
      arxivId: "2309.06180",
      year: 2023,
      venue: "SOSP 2023"
    }
  },

  // =========== LLM SERVING SYSTEMS ===========
  {
    type: "AIModel",
    name: "vLLM",
    description: "PagedAttentionã‚’å®Ÿè£…ã—ãŸé«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆLLMã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã€‚KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ãƒ¢ãƒªç„¡é§„ã‚’ã»ã¼ã‚¼ãƒ­ã«å‰Šæ¸›ã—ã€FasterTransformerã‚„Orcaã¨æ¯”è¼ƒã—ã¦2-4å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã€‚",
    properties: {
      arxivId: "2309.06180",
      year: 2023,
      developer: "UC Berkeley",
      type: "Inference Framework"
    }
  },
  {
    type: "AIModel",
    name: "TinyChat",
    description: "AWQã¨é€£æºã™ã‚‹åŠ¹çŽ‡çš„ãª4bitæŽ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚ã‚«ãƒ¼ãƒãƒ«ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œé‡ã¿ãƒ‘ãƒƒã‚­ãƒ³ã‚°ã«ã‚ˆã‚Šã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã¨ãƒ¢ãƒã‚¤ãƒ«GPUã§3å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚",
    properties: {
      year: 2023,
      developer: "MIT",
      type: "Inference Framework"
    }
  },

  // =========== MULTIMODAL MODELS ===========
  {
    type: "AIModel",
    name: "CLIP",
    description: "Contrastive Language-Image Pre-trainingã€‚4å„„ã®ç”»åƒãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã§å¯¾ç…§å­¦ç¿’ã‚’å®Ÿæ–½ã€‚ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ImageNetä¸Šã§ResNet-50ã¨åŒç­‰ã®ç²¾åº¦ã‚’é”æˆã€‚30ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã€‚ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã€‚",
    properties: {
      arxivId: "2103.00020",
      year: 2021,
      developer: "OpenAI",
      trainingData: "400M image-text pairs"
    }
  },
  {
    type: "AIModel",
    name: "BLIP",
    description: "Bootstrapping Language-Image Pre-trainingã€‚ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆå™¨ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚ŠãƒŽã‚¤ã‚¸ãƒ¼ãªWebãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹æžœçš„ã«æ´»ç”¨ã€‚ç”»åƒãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§+2.7%ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã§+2.8% CIDErå‘ä¸Šã€‚ç†è§£ã¨ç”Ÿæˆã®ä¸¡ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã€‚",
    properties: {
      arxivId: "2201.12086",
      year: 2022,
      developer: "Salesforce"
    }
  },
  {
    type: "AIModel",
    name: "BLIP-2",
    description: "å‡çµã—ãŸç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨LLMã‚’è»½é‡ãªQ-Formerã§æ©‹æ¸¡ã—ã€‚Flamingo-80Bã‚’54å€å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§8.7%ä¸Šå›žã‚‹ï¼ˆã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆVQAv2ï¼‰ã€‚åŠ¹çŽ‡çš„ãªVLPã®æ–°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã€‚",
    properties: {
      arxivId: "2301.12597",
      year: 2023,
      developer: "Salesforce"
    }
  },
  {
    type: "AIModel",
    name: "Flamingo",
    description: "Few-shotå­¦ç¿’å¯èƒ½ãªVisual Language Modelã€‚ä»»æ„ã«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã•ã‚ŒãŸç”»åƒ/å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã€‚å¤§è¦æ¨¡ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«Webã‚³ãƒ¼ãƒ‘ã‚¹ã§è¨“ç·´ã—ã€å°‘æ•°ä¾‹ã§æ–°ã‚¿ã‚¹ã‚¯ã«é©å¿œã€‚NeurIPS 2022ã€‚",
    properties: {
      arxivId: "2204.14198",
      year: 2022,
      developer: "DeepMind",
      parameters: "80B"
    }
  },
  {
    type: "AIModel",
    name: "LLaVA",
    description: "Large Language and Vision Assistantã€‚GPT-4ã§ç”Ÿæˆã—ãŸè¦–è¦šæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ãŸåˆã®ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã€‚CLIP-ViTã¨Vicunaã‚’çµåˆã—ã€GPT-4å¯¾æ¯”ã§85.1%ã®ã‚¹ã‚³ã‚¢ã€‚NeurIPS 2023 Oralã€‚",
    properties: {
      arxivId: "2304.08485",
      year: 2023,
      developer: "Microsoft/Wisconsin"
    }
  },
  {
    type: "AIModel",
    name: "LLaVA-1.5",
    description: "LLaVAã®æ”¹è‰¯ç‰ˆã€‚CLIP-ViT-L-336pxã¨MLP projectionã‚’æŽ¡ç”¨ã—ã€1.2Mã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§11ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§SOTAã€‚å˜ä¸€8-A100ãƒŽãƒ¼ãƒ‰ã§ç´„1æ—¥ã§è¨“ç·´å¯èƒ½ã€‚CVPR 2024 Highlightã€‚",
    properties: {
      arxivId: "2310.03744",
      year: 2023,
      developer: "Wisconsin"
    }
  },
  {
    type: "AIModel",
    name: "Qwen-VL",
    description: "Qwen-LMãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡VLMã€‚è¦–è¦šå—å®¹å™¨ã€å…¥å‡ºåŠ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€3æ®µéšŽè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€å¤šè¨€èªžãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’è¨­è¨ˆã€‚ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨OCRèƒ½åŠ›ã‚’å®Ÿè£…ã€‚",
    properties: {
      arxivId: "2308.12966",
      year: 2023,
      developer: "Alibaba"
    }
  },

  // =========== SPEECH MODELS ===========
  {
    type: "AIModel",
    name: "Whisper",
    description: "68ä¸‡æ™‚é–“ã®å¤šè¨€èªžãƒ»ãƒžãƒ«ãƒã‚¿ã‚¹ã‚¯å¼±æ•™å¸«ã‚ã‚Šãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ãŸéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã€‚ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§æ¨™æº–ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã¨ç«¶äº‰åŠ›ãŒã‚ã‚Šã€äººé–“ã«è¿‘ã„ç²¾åº¦ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’é”æˆã€‚OpenAIãŒå…¬é–‹ã€‚",
    properties: {
      arxivId: "2212.04356",
      year: 2022,
      developer: "OpenAI",
      trainingData: "680K hours",
      parameters: "1.5B (large)"
    }
  },
  {
    type: "AIModel",
    name: "Distil-Whisper",
    description: "Whisperã®è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã€‚å¤§è¦æ¨¡ç–‘ä¼¼ãƒ©ãƒ™ãƒªãƒ³ã‚°ã§è¨“ç·´ã—ã€5.8å€é«˜é€Ÿãƒ»51%ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›ã§OODãƒ†ã‚¹ãƒˆã§WER 1%ä»¥å†…ã€‚Speculative decodingã§Whisperã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨2å€é«˜é€ŸåŒ–ã€‚",
    properties: {
      arxivId: "2311.00430",
      year: 2023,
      developer: "Hugging Face"
    }
  },

  // =========== ALIGNMENT TECHNIQUES ===========
  {
    type: "Technique",
    name: "DPO",
    description: "Direct Preference Optimizationã€‚å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã‚ãšã€é¸å¥½ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æŽ¥ãƒãƒªã‚·ãƒ¼ã‚’æœ€é©åŒ–ã€‚RLHFã®ç›®çš„é–¢æ•°ã‚’é–‰å½¢å¼ã§è§£ãã€å˜ç´”ãªåˆ†é¡žæå¤±ã«å¸°ç€ã€‚PPOãƒ™ãƒ¼ã‚¹RLHFã‚ˆã‚Šå®‰å®šã§é«˜å“è³ªã€‚NeurIPS 2023ã€‚",
    properties: {
      arxivId: "2305.18290",
      year: 2023,
      developer: "Stanford"
    }
  },
  {
    type: "Technique",
    name: "ORPO",
    description: "Odds Ratio Preference Optimizationã€‚å‚ç…§ãƒ¢ãƒ‡ãƒ«ãªã—ã®ãƒ¢ãƒŽãƒªã‚·ãƒƒã‚¯é¸å¥½æœ€é©åŒ–ã€‚SFTã¨é¸å¥½ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å˜ä¸€æ®µéšŽã§å®Ÿè¡Œã€‚Mistral-7Bã§AlpacaEval 12.20%ã€MT-Bench 7.32ã‚’é”æˆã€‚",
    properties: {
      arxivId: "2403.07691",
      year: 2024,
      developer: "KAIST"
    }
  },
  {
    type: "Technique",
    name: "IPO",
    description: "Identity Preference Optimizationã€‚ÏˆPOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§Ïˆã‚’æ’ç­‰é–¢æ•°ã«è¨­å®šã€‚DPOã®ç†è«–çš„é™ç•Œã‚’å›žé¿ã—ã€é¸å¥½ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æŽ¥æœ€é©åŒ–ã€‚ç†è«–çš„ä¿è¨¼ä»˜ãã€‚",
    properties: {
      arxivId: "2310.12036",
      year: 2023,
      developer: "DeepMind"
    }
  },
  {
    type: "Technique",
    name: "RLHF",
    description: "Reinforcement Learning from Human Feedbackã€‚äººé–“ã®é¸å¥½ã‹ã‚‰å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€PPOã§LLMã‚’æœ€é©åŒ–ã€‚InstructGPTã€ChatGPTã®åŸºç›¤æŠ€è¡“ã ãŒã€è¤‡é›‘ã§ä¸å®‰å®šã¨ã„ã†èª²é¡ŒãŒã‚ã‚‹ã€‚",
    properties: {
      year: 2022,
      developer: "OpenAI/Anthropic"
    }
  },
  {
    type: "Technique",
    name: "PPO",
    description: "Proximal Policy Optimizationã€‚ä¿¡é ¼é ˜åŸŸåˆ¶ç´„ä»˜ãã®æ–¹ç­–å‹¾é…æ³•ã€‚RLHFã§æ¨™æº–çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚å®‰å®šã ãŒè¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚",
    properties: {
      year: 2017,
      developer: "OpenAI"
    }
  },

  // =========== PERSONS ===========
  {
    type: "Person",
    name: "Tri Dao",
    description: "Flash Attentionã®é–‹ç™ºè€…ã€‚Stanfordå¤§å­¦ã€‚IO-awareãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­è¨ˆã§Transformerã®åŠ¹çŽ‡ã‚’å¤§å¹…æ”¹å–„ã€‚ç¾åœ¨ã¯Together AIã§ã‚‚æ´»å‹•ã€‚",
    properties: {
      affiliation: "Stanford/Together AI"
    }
  },
  {
    type: "Person",
    name: "Alec Radford",
    description: "OpenAIã®ç ”ç©¶è€…ã€‚GPTã€CLIPã€Whisperãªã©å¤šãã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã«è²¢çŒ®ã€‚è‡ªç„¶è¨€èªžå‡¦ç†ã¨ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®å…ˆé§†è€…ã€‚",
    properties: {
      affiliation: "OpenAI"
    }
  },
  {
    type: "Person",
    name: "Jong Wook Kim",
    description: "OpenAIã®ç ”ç©¶è€…ã€‚CLIPã¨Whisperã®å…±åŒé–‹ç™ºè€…ã€‚ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã¨éŸ³å£°èªè­˜ã®å°‚é–€å®¶ã€‚",
    properties: {
      affiliation: "OpenAI"
    }
  },
  {
    type: "Person",
    name: "Junnan Li",
    description: "Salesforceã®AIç ”ç©¶è€…ã€‚BLIPã‚·ãƒªãƒ¼ã‚ºã®ä¸»è¦é–‹ç™ºè€…ã€‚Vision-Language Pre-trainingã®åŠ¹çŽ‡åŒ–ã«è²¢çŒ®ã€‚",
    properties: {
      affiliation: "Salesforce"
    }
  },
  {
    type: "Person",
    name: "Haotian Liu",
    description: "University of Wisconsin-Madisonã€‚LLaVAã‚·ãƒªãƒ¼ã‚ºã®ä¸»è¦é–‹ç™ºè€…ã€‚Visual Instruction Tuningã‚’æå”±ã€‚",
    properties: {
      affiliation: "UW-Madison"
    }
  },
  {
    type: "Person",
    name: "Rafael Rafailov",
    description: "Stanfordå¤§å­¦ã€‚DPOï¼ˆDirect Preference Optimizationï¼‰ã®ä¸»è¦é–‹ç™ºè€…ã€‚ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹æ³•ã®åŠ¹çŽ‡åŒ–ã«è²¢çŒ®ã€‚",
    properties: {
      affiliation: "Stanford"
    }
  },
  {
    type: "Person",
    name: "Chelsea Finn",
    description: "Stanfordå¤§å­¦æ•™æŽˆã€‚Meta-learningã€Robot learningã®å°‚é–€å®¶ã€‚DPOã®å…±è‘—è€…ã§ã‚‚ã‚ã‚Šã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆç ”ç©¶ã«ã‚‚è²¢çŒ®ã€‚",
    properties: {
      affiliation: "Stanford"
    }
  },
  {
    type: "Person",
    name: "Woosuk Kwon",
    description: "UC Berkeleyã€‚PagedAttentionã¨vLLMã®ä¸»è¦é–‹ç™ºè€…ã€‚LLMã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®åŠ¹çŽ‡åŒ–ã«å¤§ããè²¢çŒ®ã€‚",
    properties: {
      affiliation: "UC Berkeley"
    }
  },

  // =========== ORGANIZATIONS ===========
  {
    type: "Organization",
    name: "Salesforce Research",
    description: "Salesforceã®ç ”ç©¶éƒ¨é–€ã€‚BLIPã‚·ãƒªãƒ¼ã‚ºã€CodeGenç­‰ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã€‚Enterprise AIã«å¼·ã¿ã€‚",
    properties: {
      location: "San Francisco, USA"
    }
  },
  {
    type: "Organization",
    name: "Together AI",
    description: "ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹AIã«æ³¨åŠ›ã™ã‚‹ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€‚Flash Attentionã®é–‹ç™ºè€…Tri DaoãŒå‚ç”»ã€‚åˆ†æ•£æŽ¨è«–ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æä¾›ã€‚",
    properties: {
      location: "San Francisco, USA",
      founded: 2022
    }
  },

  // =========== BENCHMARKS ===========
  {
    type: "Benchmark",
    name: "VQAv2",
    description: "Visual Question Answering v2ã€‚ç”»åƒã«é–¢ã™ã‚‹è‡ªç„¶è¨€èªžè³ªå•ã«å›žç­”ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã€‚265Kã®ç”»åƒã€110ä¸‡ä»¥ä¸Šã®è³ªå•ã€‚VLMè©•ä¾¡ã®æ¨™æº–ã€‚",
    properties: {
      domain: "Vision-Language",
      task: "Visual QA"
    }
  },
  {
    type: "Benchmark",
    name: "COCO Captioning",
    description: "MS COCOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã€‚CIDErã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã€‚ç”»åƒè¨˜è¿°èƒ½åŠ›ã®æ¨™æº–è©•ä¾¡ã€‚",
    properties: {
      domain: "Vision-Language",
      task: "Image Captioning"
    }
  },
  {
    type: "Benchmark",
    name: "AlpacaEval",
    description: "LLMã®æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã€‚GPT-4ã¨ã®æ¯”è¼ƒã§å‹çŽ‡ã‚’æ¸¬å®šã€‚2.0ã§ã¯Length-controlledç‰ˆã‚‚è¿½åŠ ã€‚ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã®æ¨™æº–ã€‚",
    properties: {
      domain: "Language",
      task: "Instruction Following"
    }
  },
  {
    type: "Benchmark",
    name: "MT-Bench",
    description: "ãƒžãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã€‚GPT-4ã«ã‚ˆã‚‹è‡ªå‹•è©•ä¾¡ã§1-10ã‚¹ã‚³ã‚¢ã€‚8ã‚«ãƒ†ã‚´ãƒª80è³ªå•ã€‚LLMã®ä¼šè©±å“è³ªè©•ä¾¡ã«ä½¿ç”¨ã€‚",
    properties: {
      domain: "Language",
      task: "Multi-turn Dialogue"
    }
  },
  {
    type: "Benchmark",
    name: "LibriSpeech",
    description: "1000æ™‚é–“ã®èª­ã¿ä¸Šã’è‹±èªžéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚éŸ³å£°èªè­˜ã®æ¨™æº–ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã€‚Clean/Otherã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§è©•ä¾¡ã€‚",
    properties: {
      domain: "Speech",
      task: "Speech Recognition",
      hours: 1000
    }
  },

  // =========== PUBLICATIONS ===========
  {
    type: "Publication",
    name: "Learning Transferable Visual Models From Natural Language Supervision",
    description: "CLIPè«–æ–‡ã€‚å¯¾ç…§å­¦ç¿’ã«ã‚ˆã‚‹ç”»åƒãƒ†ã‚­ã‚¹ãƒˆäº‹å‰å­¦ç¿’ã‚’ææ¡ˆã€‚4å„„ãƒšã‚¢ã§ã®è¨“ç·´ã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè»¢ç§»ã§å„ªã‚ŒãŸæ€§èƒ½ã€‚ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®è»¢æ›ç‚¹ã€‚",
    properties: {
      arxivId: "2103.00020",
      year: 2021,
      venue: "ICML 2021",
      authors: ["Alec Radford", "Jong Wook Kim", "et al."]
    }
  },
  {
    type: "Publication",
    name: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    description: "Flash Attentionè«–æ–‡ã€‚IO-awareãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­è¨ˆã«ã‚ˆã‚Šã€æ­£ç¢ºãªattentionã‚’é«˜é€ŸåŒ–ã€‚é•·ç³»åˆ—Transformerã®è¨“ç·´ã‚’å®Ÿç¾å¯èƒ½ã«ã€‚",
    properties: {
      arxivId: "2205.14135",
      year: 2022,
      venue: "NeurIPS 2022",
      authors: ["Tri Dao", "Daniel Y. Fu", "et al."]
    }
  },
  {
    type: "Publication",
    name: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    description: "DPOè«–æ–‡ã€‚RLHFã‚’é–‰å½¢å¼ã§è§£ãã€åˆ†é¡žæå¤±ã«å¸°ç€ã€‚å ±é…¬ãƒ¢ãƒ‡ãƒ«ãªã—ã§é¸å¥½å­¦ç¿’ã‚’å®Ÿç¾ã—ã€ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆç ”ç©¶ã«é©å‘½ã‚’ã‚‚ãŸã‚‰ã—ãŸã€‚",
    properties: {
      arxivId: "2305.18290",
      year: 2023,
      venue: "NeurIPS 2023",
      authors: ["Rafael Rafailov", "Archit Sharma", "Chelsea Finn", "et al."]
    }
  },
  {
    type: "Publication",
    name: "Visual Instruction Tuning",
    description: "LLaVAè«–æ–‡ã€‚GPT-4ã§ç”Ÿæˆã—ãŸè¦–è¦šæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã§LLMã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æŒ‡ç¤ºè¿½å¾“ã®åˆæœŸæˆåŠŸä¾‹ã€‚NeurIPS 2023 Oralã€‚",
    properties: {
      arxivId: "2304.08485",
      year: 2023,
      venue: "NeurIPS 2023 Oral",
      authors: ["Haotian Liu", "Chunyuan Li", "Yong Jae Lee"]
    }
  },
  {
    type: "Publication",
    name: "Robust Speech Recognition via Large-Scale Weak Supervision",
    description: "Whisperè«–æ–‡ã€‚68ä¸‡æ™‚é–“ã®å¼±æ•™å¸«ã‚ã‚Šãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã—ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§äººé–“ã«è¿‘ã„ç²¾åº¦ã¨ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’é”æˆã€‚éŸ³å£°èªè­˜ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’å®Ÿè¨¼ã€‚",
    properties: {
      arxivId: "2212.04356",
      year: 2022,
      authors: ["Alec Radford", "Jong Wook Kim", "Ilya Sutskever", "et al."]
    }
  }
];

// ============================================================================
// RELATIONS
// ============================================================================

const relations: Relation[] = [
  // === Quantization Relations ===
  { from: "GPTQ", to: "Model Quantization", type: "IMPLEMENTS" },
  { from: "AWQ", to: "Model Quantization", type: "IMPLEMENTS" },
  { from: "LLM-FP4", to: "Model Quantization", type: "IMPLEMENTS" },
  { from: "GGUF", to: "Model Quantization", type: "IMPLEMENTS" },
  { from: "AWQ", to: "GPTQ", type: "IMPROVES", properties: { aspect: "activation-aware protection" } },
  { from: "LLM-FP4", to: "GPTQ", type: "RELATED_TO", properties: { aspect: "floating-point vs integer" } },
  { from: "TinyChat", to: "AWQ", type: "IMPLEMENTS" },

  // === Efficient Attention Relations ===
  { from: "Flash Attention", to: "Efficient Attention", type: "IMPLEMENTS" },
  { from: "Flash Attention 2", to: "Efficient Attention", type: "IMPLEMENTS" },
  { from: "PagedAttention", to: "Efficient Attention", type: "IMPLEMENTS" },
  { from: "Flash Attention 2", to: "Flash Attention", type: "IMPROVES", properties: { speedup: "2x" } },
  { from: "vLLM", to: "PagedAttention", type: "IMPLEMENTS" },
  { from: "vLLM", to: "LLM Serving", type: "IMPLEMENTS" },
  { from: "Tri Dao", to: "Flash Attention", type: "DEVELOPED" },
  { from: "Tri Dao", to: "Flash Attention 2", type: "DEVELOPED" },
  { from: "Woosuk Kwon", to: "PagedAttention", type: "DEVELOPED" },
  { from: "Woosuk Kwon", to: "vLLM", type: "DEVELOPED" },

  // === Multimodal Model Relations ===
  { from: "CLIP", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "BLIP", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "BLIP-2", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "Flamingo", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "LLaVA", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "LLaVA-1.5", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "Qwen-VL", to: "Vision-Language Models", type: "BELONGS_TO" },
  { from: "BLIP", to: "CLIP", type: "BUILDS_ON" },
  { from: "BLIP-2", to: "BLIP", type: "IMPROVES" },
  { from: "LLaVA", to: "CLIP", type: "USES", properties: { component: "vision encoder" } },
  { from: "LLaVA-1.5", to: "LLaVA", type: "IMPROVES" },
  { from: "Qwen-VL", to: "BLIP-2", type: "RELATED_TO" },
  { from: "OpenAI", to: "CLIP", type: "DEVELOPED" },
  { from: "Salesforce Research", to: "BLIP", type: "DEVELOPED" },
  { from: "Salesforce Research", to: "BLIP-2", type: "DEVELOPED" },
  { from: "DeepMind", to: "Flamingo", type: "DEVELOPED" },
  { from: "Haotian Liu", to: "LLaVA", type: "DEVELOPED" },
  { from: "Haotian Liu", to: "LLaVA-1.5", type: "DEVELOPED" },
  { from: "Alibaba", to: "Qwen-VL", type: "DEVELOPED" },
  { from: "Junnan Li", to: "BLIP", type: "DEVELOPED" },
  { from: "Junnan Li", to: "BLIP-2", type: "DEVELOPED" },
  { from: "Alec Radford", to: "CLIP", type: "DEVELOPED" },
  { from: "Jong Wook Kim", to: "CLIP", type: "DEVELOPED" },

  // === Speech Model Relations ===
  { from: "Whisper", to: "Speech Recognition", type: "IMPLEMENTS" },
  { from: "Distil-Whisper", to: "Speech Recognition", type: "IMPLEMENTS" },
  { from: "Distil-Whisper", to: "Whisper", type: "DISTILLED_FROM" },
  { from: "OpenAI", to: "Whisper", type: "DEVELOPED" },
  { from: "Hugging Face", to: "Distil-Whisper", type: "DEVELOPED" },
  { from: "Alec Radford", to: "Whisper", type: "DEVELOPED" },
  { from: "Jong Wook Kim", to: "Whisper", type: "DEVELOPED" },
  { from: "Whisper", to: "LibriSpeech", type: "EVALUATED_ON" },
  { from: "Distil-Whisper", to: "LibriSpeech", type: "EVALUATED_ON" },

  // === Alignment Technique Relations ===
  { from: "DPO", to: "Preference Alignment", type: "IMPLEMENTS" },
  { from: "ORPO", to: "Preference Alignment", type: "IMPLEMENTS" },
  { from: "IPO", to: "Preference Alignment", type: "IMPLEMENTS" },
  { from: "RLHF", to: "Preference Alignment", type: "IMPLEMENTS" },
  { from: "PPO", to: "RLHF", type: "USED_IN" },
  { from: "DPO", to: "RLHF", type: "IMPROVES", properties: { aspect: "no reward model" } },
  { from: "ORPO", to: "DPO", type: "IMPROVES", properties: { aspect: "no reference model" } },
  { from: "IPO", to: "DPO", type: "RELATED_TO", properties: { aspect: "theoretical framework" } },
  { from: "Rafael Rafailov", to: "DPO", type: "DEVELOPED" },
  { from: "Chelsea Finn", to: "DPO", type: "DEVELOPED" },
  { from: "DeepMind", to: "IPO", type: "DEVELOPED" },

  // === Benchmark Relations ===
  { from: "CLIP", to: "VQAv2", type: "EVALUATED_ON" },
  { from: "BLIP", to: "VQAv2", type: "EVALUATED_ON" },
  { from: "BLIP-2", to: "VQAv2", type: "EVALUATED_ON" },
  { from: "Flamingo", to: "VQAv2", type: "EVALUATED_ON" },
  { from: "LLaVA", to: "VQAv2", type: "EVALUATED_ON" },
  { from: "BLIP", to: "COCO Captioning", type: "EVALUATED_ON" },
  { from: "BLIP-2", to: "COCO Captioning", type: "EVALUATED_ON" },
  { from: "DPO", to: "AlpacaEval", type: "EVALUATED_ON" },
  { from: "ORPO", to: "AlpacaEval", type: "EVALUATED_ON" },
  { from: "DPO", to: "MT-Bench", type: "EVALUATED_ON" },
  { from: "ORPO", to: "MT-Bench", type: "EVALUATED_ON" },

  // === Publication Relations ===
  { from: "Learning Transferable Visual Models From Natural Language Supervision", to: "CLIP", type: "DESCRIBES" },
  { from: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", to: "Flash Attention", type: "DESCRIBES" },
  { from: "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", to: "DPO", type: "DESCRIBES" },
  { from: "Visual Instruction Tuning", to: "LLaVA", type: "DESCRIBES" },
  { from: "Robust Speech Recognition via Large-Scale Weak Supervision", to: "Whisper", type: "DESCRIBES" },

  // === Organization Relations ===
  { from: "Tri Dao", to: "Together AI", type: "AFFILIATED_WITH" },
  { from: "Tri Dao", to: "Stanford University", type: "AFFILIATED_WITH" },
  { from: "Rafael Rafailov", to: "Stanford University", type: "AFFILIATED_WITH" },
  { from: "Chelsea Finn", to: "Stanford University", type: "AFFILIATED_WITH" },
  { from: "Woosuk Kwon", to: "UC Berkeley", type: "AFFILIATED_WITH" },
  { from: "Junnan Li", to: "Salesforce Research", type: "AFFILIATED_WITH" },
];

// ============================================================================
// INGESTION
// ============================================================================

async function ingest() {
  const session = driver.session();
  
  try {
    console.log("ðŸš€ Starting Infrastructure & Multimodal data ingestion...\n");

    // Create entities
    let created = 0;
    for (const entity of entities) {
      const props = {
        name: entity.name,
        description: entity.description,
        ...entity.properties
      };
      
      await session.run(
        `MERGE (e:${entity.type} {name: $name})
         SET e += $props
         RETURN e`,
        { name: entity.name, props }
      );
      created++;
    }
    console.log(`âœ… ${created} entities created/updated`);

    // Create relations
    let relCreated = 0;
    for (const rel of relations) {
      await session.run(
        `MATCH (a {name: $from})
         MATCH (b {name: $to})
         MERGE (a)-[r:${rel.type}]->(b)
         SET r += $props
         RETURN r`,
        { 
          from: rel.from, 
          to: rel.to, 
          props: rel.properties || {} 
        }
      );
      relCreated++;
    }
    console.log(`âœ… ${relCreated} relations created/updated`);

    // Statistics
    const stats = await session.run(`
      MATCH (n)
      RETURN labels(n)[0] AS type, count(n) AS count
      ORDER BY count DESC
    `);
    
    console.log("\nðŸ“Š Database Statistics:");
    let total = 0;
    for (const record of stats.records) {
      const type = record.get("type");
      const count = record.get("count").toNumber();
      console.log(`   ${type}: ${count}`);
      total += count;
    }
    console.log(`   Total: ${total}`);

  } finally {
    await session.close();
    await driver.close();
  }
}

ingest().catch(console.error);
