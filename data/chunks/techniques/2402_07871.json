{
  "paper": {
    "id": "2402.07871v1",
    "title": "Scaling Laws for Fine-Grained Mixture of Experts",
    "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.",
    "authors": [
      "Jakub Krajewski",
      "Jan Ludziejewski",
      "Kamil Adamczewski",
      "Maciej Pióro",
      "Michał Krutul",
      "Szymon Antoniak",
      "Kamil Ciebiera",
      "Krystian Król",
      "Tomasz Odrzygóźdź",
      "Piotr Sankowski",
      "Marek Cygan",
      "Sebastian Jaszczur"
    ],
    "published": "2024-02-12T18:33:47.000Z",
    "updated": "2024-02-12T18:33:47.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2402.07871v1",
    "absUrl": "https://arxiv.org/abs/2402.07871v1"
  },
  "chunks": [
    {
      "id": "2402.07871v1-chunk-0",
      "content": "| Jakub Krajewski ∗ University of Warsaw IDEAS NCBR   | Jan Ludziejewski ∗ University of Warsaw IDEAS NCBR   | Kamil Adamczewski IDEAS NCBR                   | Maciej Pi´ oro IPPT PAN IDEAS NCBR                   |\n|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------|------------------------------------------------------|\n| Michał Krutul University of Warsaw IDEAS NCBR       | Szymon Antoniak University of Warsaw IDEAS NCBR      | Kamil Ciebiera University of Warsaw IDEAS NCBR | Krystian Kr´ ol University of Warsaw IDEAS NCBR      |\n| Tomasz Odrzyg´ o´ zd´ z TradeLink                   | Piotr Sankowski University of Warsaw IDEAS NCBR      | Marek Cygan University of Warsaw Nomagic       | Sebastian Jaszczur ∗ University of Warsaw IDEAS NCBR |",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS",
        "chunkIndex": 0,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-1",
      "content": "Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-2",
      "content": "In recent years, we have witnessed Large Language Models (LLMs) achieve exceptional performance in tasks across numerous domains (Chowdhery et al., 2022; Yin et al., 2023; Agostinelli et al., 2023). However, training those massive models incurs high computational costs, measured in millions of GPU-hours (Touvron et al., 2023b), enabled only by enormous budgets (Scao et al., 2023) and leading to non-negligible carbon footprints (Faiz et al., 2024). To combat these obstacles, the research community has been striving to increase the efficiency of LLMs. One promising approach that has lately been gaining visibility is the use of Mixture of Experts (MoE) methods. Models such as Switch (Fedus et al., 2022) and Mixtral (Jiang et al., 2024) have already demonstrated that it is possible to achieve comparable effectiveness with significantly lower computational costs.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 2,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-3",
      "content": "els such as Switch (Fedus et al., 2022) and Mixtral (Jiang et al., 2024) have already demonstrated that it is possible to achieve comparable effectiveness with significantly lower computational costs.\n\nIn the context of the current trend of increasing budgets for training language models, a question arises: will MoE models continue to be attractive in the future? This is an important issue, as other studies have stated that the gap in efficiency between MoE and standard Transformers narrows at\n\nContributions: Jakub implemented fine-grained MoE, ran experiments, and oversaw the course of the project. Jan designed and implemented the scaling laws, also optimized and tuned the fine-grained MoE implementation. Kamil A. provided significant advice on many aspects of the project. Maciej experimented with the block design and, with Michał, provided considerable technical support. Szymon, Kamil C., Krystian, and Tomasz contributed to the project and the engineering in various ways.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-4",
      "content": "iej experimented with the block design and, with Michał, provided considerable technical support. Szymon, Kamil C., Krystian, and Tomasz contributed to the project and the engineering in various ways. Marek, along with Piotr, provided high-level scientific advice. Sebastian came up with the initial idea, started the project, and supervised it while setting the research direction and leading experiments and analyses. Correspondence to &lt; s.jaszczur@uw.edu.pl &gt; . ∗ Equal contribution.\n\nFigure 1: Mixture-of-Experts can be always considered more efficient than dense Transformers, regardless of the model size. (a) Compute Optimal scaling curves for MoE and standard Transformers. The dashed line represents a dense Transformer. Colors denote optimal granularity for the given FLOPs training budget. (b) Relative number of FLOPs needed to train Transformer and Vanilla MoE (MoE with G = 1 ) to achieve the performance of MoE with compute optimal G .\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-5",
      "content": "r the given FLOPs training budget. (b) Relative number of FLOPs needed to train Transformer and Vanilla MoE (MoE with G = 1 ) to achieve the performance of MoE with compute optimal G .\n\n<!-- image -->\n\nscale (Artetxe et al., 2022) or even that traditional dense models may outperform MoE as the size of the models increases (Clark et al., 2022).\n\nIn this paper, we argue that previous claims lose their validity when we relax certain implicit assumptions regarding the training process, present in previous research. In particular, we refer to the fixed training duration and the constant size of experts in MoE models.\n\nOur results suggest that a compute-optimal MoE model trained with a budget of 10 20 FLOPs will achieve the same quality as a dense Transformer trained with a 20 × greater computing budget, with the compute savings rising steadily, exceeding 40 × when budget of 10 25 FLOPs is surpassed (see Figure 1).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-6",
      "content": "e the same quality as a dense Transformer trained with a 20 × greater computing budget, with the compute savings rising steadily, exceeding 40 × when budget of 10 25 FLOPs is surpassed (see Figure 1). Importantly, we show that the standard practice of fixing the size of experts in MoE to be the same as feed-forward layer is almost never optimal.\n\nOur main contributions are:\n\n1. Introducing a new hyperparameter - granularity. Adjusting this parameter allows us to determine the optimal size of experts in MoE models, which translates into increased efficiency.\n2. Deriving new scaling laws for MoE models that incorporate variable training duration, the number of parameters, and granularity. Such scaling laws allow us to calculate optimal training hyperparameters for MoE models.\n3. Demonstrating that, with optimal settings, MoE models can always outperform traditional Transformers at any computing budget. This is a conclusion contrary to the results from Clark et al. (2022).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-7",
      "content": "3. Demonstrating that, with optimal settings, MoE models can always outperform traditional Transformers at any computing budget. This is a conclusion contrary to the results from Clark et al. (2022).\n\nThe code used to produce the results described in this work is open-sourced at github.com/llm-random/llm-random .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-8",
      "content": "Mixture of Experts. In the context of language modeling, MoE was first introduced by Shazeer et al. (2017) as a sparsely gated layer between stacked blocks of LSTM (Hochreiter &amp; Schmidhuber, 1997). A similar technique was proposed in the context of Transformers by Shazeer et al. (2018) and Lepikhin et al. (2020). Fedus et al. (2022) proposed to route each input to only a single expert and designed a modified initialization scheme to reduce training instability. Numerous studies\n\nhave proposed to modify the original routing method. Lewis et al. (2021) used a linear assignment algorithm to postprocess token-expert mappings and ensure even expert selections. Roller et al. (2021) suggested another approach involving deterministic hash functions. Zhou et al. (2022) proposed expert choice routing, eliminating the need for additional load balancing losses. Puigcerver et al. (2023) designed a fully-differentiable Soft MoE architecture.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 8,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-9",
      "content": "functions. Zhou et al. (2022) proposed expert choice routing, eliminating the need for additional load balancing losses. Puigcerver et al. (2023) designed a fully-differentiable Soft MoE architecture.\n\nConcurrently to our work, Dai et al. (2024) proposed to modify the MoE layer by segmenting experts into smaller ones and adding shared experts to the architecture. Independently, Liu et al. (2023) suggested a unified view of sparse feed-forward layers, considering, in particular, varying the size of memory blocks. Both approaches can be interpreted as modifying granularity. However, we offer a comprehensive comparison of the relationship between training hyperparameters and derive principled selection criteria, which they lack.\n\nScaling laws. Scaling laws are empirically derived equations relating the loss of a model with variables such as the number of parameters, training samples, or the computational budget.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 9,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-10",
      "content": "ch they lack.\n\nScaling laws. Scaling laws are empirically derived equations relating the loss of a model with variables such as the number of parameters, training samples, or the computational budget. In the case of dense Transformers, scaling laws were first studied by Kaplan et al. (2020), who observed power law relationships between the final model perplexity and model and dataset size. This work was extended by Hoffmann et al. (2022) by considering variable cosine cycle lengths and formulating a modified functional form of the scaling equation.\n\nScaling laws have also been proposed for other architectures and training scenarios. Henighan et al. (2020) studied autoregressive modeling across various modalities, while Ghorbani et al. (2021) considered machine translation. Frantar et al. (2023) explored the impact of pruning on vision and language Transformers, deriving optimal sparsity for a given compute budget. Clark et al.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 10,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-11",
      "content": "al. (2021) considered machine translation. Frantar et al. (2023) explored the impact of pruning on vision and language Transformers, deriving optimal sparsity for a given compute budget. Clark et al. (2022) studied the scaling of MoE when changing model size and number of experts on a fixed dataset, concluding that routed models are more efficient only until a certain model size. In this work, we challenge that claim by considering a variable, optimal dataset size for both model families (see Section 6.3).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 11,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-12",
      "content": "Transformer. A standard decoder-only Transformer (Radford et al., 2018a;b; Kaplan et al., 2020; Brown et al., 2020) consists of an embedding layer, a stack of alternating attention and feed-forward layers, and an unembedding layer. In the model, each input token is converted by the embedding layer into a vector of size d model, the dimension maintained across all the layers in the residual stream.\n\nThe feed-forward component consists of two linear transformations and a nonlinearity ϕ in between. It can be described as FFN ( x ) = ϕ ( xW 1 + b 1 ) W 2 + b 2 , with W 1 mapping from d model to d ff, and W 2 back to the original d model. It is standard (Radford et al., 2018a; Rae et al., 2022; Touvron et al., 2023a; Jiang et al., 2023) to set the hidden dimension as d ff = 4 · d model.\n\nFeed-forward layers contain the majority of Transformer parameters and require the biggest computational budget counted in terms of FLOPs.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.1 MODEL ARCHITECTURE",
        "chunkIndex": 12,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-13",
      "content": "l., 2023) to set the hidden dimension as d ff = 4 · d model.\n\nFeed-forward layers contain the majority of Transformer parameters and require the biggest computational budget counted in terms of FLOPs. Subsequently, they are the main focus of the Mixture of Experts models considered in this work.\n\nMixture of Experts. The core idea behind MoE in Transformers is to replace the feed-forward layer with a set of N expert experts . The size of each expert is typically (Fedus et al., 2022; Zhou et al., 2022; 2023; Jiang et al., 2024) set to mirror the original dimensions of the layer, with the hidden expert dimension d expert equal to d ff . Therefore, the total number of parameters in MoE scales linearly with the number of experts. However, the computational cost remains approximately constant as each input is routed and then processed by a subset of experts.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.1 MODEL ARCHITECTURE",
        "chunkIndex": 13,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-14",
      "content": "umber of parameters in MoE scales linearly with the number of experts. However, the computational cost remains approximately constant as each input is routed and then processed by a subset of experts.\n\nFigure 2: (a) Standard MoE layer with G = 1 (b) Corresponding MoE layer with G = 2 . Each of the original experts is split into two granular ones. The split occurs in the hidden dimension of an expert. Increasing G allows for a more precise mapping between experts and tokens. Since for granularity G , the token is routed to G granular experts, the number of parameters activated per token is the same in both cases.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.1 MODEL ARCHITECTURE",
        "chunkIndex": 14,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-15",
      "content": "Dense Transformers. Large Transformer-based models are known to approximately obey the power-law relationship between final loss L , model size N, and number of training tokens D. This relationship is often called Chinchilla scaling laws described by Hoffmann et al. (2022) as\n\n<!-- formula-not-decoded -->\n\nThe power-law formula is composed of three distinct terms that characterize the intrinsic entropy of data, constraints of the model, and limitations in the training data. The term c represents the minimum possible error intrinsic to the data. The remaining two terms are suboptimality terms, which address the limitations in function representation owing to the size of the model and in data signified by the number of tokens. In the limit, with infinite data and model size, the loss is reduced to c .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.2 SCALING LAWS",
        "chunkIndex": 15,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-16",
      "content": "ess the limitations in function representation owing to the size of the model and in data signified by the number of tokens. In the limit, with infinite data and model size, the loss is reduced to c .\n\nMixture of Experts. For MoE Transformer-based models, Clark et al. (2022) formulated the final loss for a constant dataset size D of 130B tokens, allowing for variations in the expansion rate E , as:\n\n<!-- formula-not-decoded -->\n\nHowever, this result has a notable limitation as it can be applied only to the original dataset size. The scalability and effectiveness are constrained in this scenario because it is crucial to align the number of training samples with the available computational resources for optimal use. As per Kaplan et al. (2020) and Hoffmann et al. (2022), maintaining a constant dataset size while scaling up the neural network size leads to undertraining, resulting in a model that does not perform to its full potential.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.2 SCALING LAWS",
        "chunkIndex": 16,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-17",
      "content": "nt dataset size while scaling up the neural network size leads to undertraining, resulting in a model that does not perform to its full potential.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "3.2 SCALING LAWS",
        "chunkIndex": 17,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-18",
      "content": "As described in Section 3, in the standard setting, the inner dimension of each expert network, d expert , is equal to d ff, which is the same size as the feed-forward layer of the base model.\n\nIn this work, we suggest an alternative approach where the hidden dimension of the expert is not necessarily set to mirror that of the standard feed-forward layer. Instead, it can be adjusted to a value that is the most effective. This approach allows the configuration of MoE to be articulated in terms of two key hyperparameters: granularity ( G ) and expansion rate ( E ). In the following parts of this work, we will also use the term active parameters to refer to the non-embedding parameters used to produce output for a single token, except routing. The number of active parameters is denoted as N act .\n\nLet d expert be the hidden dimension of a single expert. Granularity is defined as\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "4 GRANULARITY",
        "chunkIndex": 18,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-19",
      "content": "a single token, except routing. The number of active parameters is denoted as N act .\n\nLet d expert be the hidden dimension of a single expert. Granularity is defined as\n\n<!-- formula-not-decoded -->\n\nIn other words, granularity denotes the multiplier factor for the change in the size of an expert from the original standard model, defined as G = 1 . In this work, we investigate G &gt; 1 where experts are smaller than in the standard layer.\n\nNote that increasing granularity does not affect the number of active parameters. As G increases, the number of experts that process the token grows proportionally to G . In other words, for granularity G , a token is routed to G fine-grained experts, thereby keeping the number of active parameters constant. See Fig. 2 for visualization.\n\nWe then define the expansion rate , which describes the increase in the number of parameters from a standard transformer layer to a MoE layer.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "4 GRANULARITY",
        "chunkIndex": 19,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-20",
      "content": "ctive parameters constant. See Fig. 2 for visualization.\n\nWe then define the expansion rate , which describes the increase in the number of parameters from a standard transformer layer to a MoE layer. Given that, N MoE and N ff denote the total number of parameters in a MoE layer excluding routing and the standard feed-forward layer, respectively. The expansion rate E is then defined as\n\n<!-- formula-not-decoded -->\n\nExpansion rate can also be seen as the total number of parameters in a MoE layer compared to its active parameters.\n\nThe concept of the expansion rate is intricately linked to the number of experts through the idea of granularity. Indeed, the definitions of both granularity and expansion rate extend and refine our understanding of the number of experts, symbolized as N expert .\n\n<!-- formula-not-decoded -->\n\nFor non-granular models, where G = 1 , the expansion rate is equal to the number of experts.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "4 GRANULARITY",
        "chunkIndex": 20,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-21",
      "content": "refine our understanding of the number of experts, symbolized as N expert .\n\n<!-- formula-not-decoded -->\n\nFor non-granular models, where G = 1 , the expansion rate is equal to the number of experts.\n\nIntuitively, increasing granularity for a given expansion rate gives the model more flexibility in mapping datapoints to experts, potentially improving performance. We incorporate the notion of granularity into our scaling laws in Section 5. The discussion about practical tradeoffs in changing this parameter is given in Section 6.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "4 GRANULARITY",
        "chunkIndex": 21,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-22",
      "content": "Granularity determines changes in the architecture of MoE. In this section, we answer a central question of this work: whether the granular MoE models follow scaling laws and, if so, how granularity affects them. Thus, we aim to derive a parametric scaling law for predicting the final loss value L based on granularity G , total number of non-embedding parameters N , and number of training tokens D .\n\nWe run over 100 experiments on the decoder-only Transformer architecture, with each feed-forward component replaced by a Mixture of Experts layer. Those experiments involve training models with sizes ranging from 129M to 3.7B parameters across different training durations, from 16B to 130B tokens. We consider logarithmically spaced values of granularity between 1 and 16. To constrain the search space, E = 64 is fixed, following the recommendations of Clark et al. (2022). In addition, we also run experiments with dense Transformers to compare their performance with MoE.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5 SCALING LAWS",
        "chunkIndex": 22,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-23",
      "content": "o constrain the search space, E = 64 is fixed, following the recommendations of Clark et al. (2022). In addition, we also run experiments with dense Transformers to compare their performance with MoE. The details of all architectures, the training procedure, and hyperparameter choices are described in detail in Appendix A.\n\nIn the subsequent part of this paper, we will use the notation E × N act to describe a MoE model with N act active parameters and expansion rate E.\n\nFigure 3: (a) The effect of G on L N,D ( G ) for constant N and D . Both axes are in the log-scale. The results suggest the linear relationship between log( G ) and log( L c ) . The given values are N = 64 × 25 M , D = 16 B , const = 3 . 12 . The plots for additional values of N and D can be found in Appendix F. (b) The impact of varying the number of parameters N on the loss for fixed granularity G = 4 . For other granularity values, see Appendix F.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5 SCALING LAWS",
        "chunkIndex": 23,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-24",
      "content": "or additional values of N and D can be found in Appendix F. (b) The impact of varying the number of parameters N on the loss for fixed granularity G = 4 . For other granularity values, see Appendix F. (c) The difference in the loss between training for 16B and 65B tokens for all model sizes and granularity values. The model size is reported as the expansion rate and the number of active parameters.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5 SCALING LAWS",
        "chunkIndex": 24,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-25",
      "content": "We first answer the question of whether granular models follow the scaling laws. In Figure 4(a), it can be seen that increasing granularity results in a lower loss. The returns follow approximately an exponential pattern, converging to a positive constant. The empirical relationship given by Figure 3(a) suggests the following power-law dependence of loss on a varying granularity for given N and D and constants g, h and γ that may be dependent on them,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.1 POWER LAW WITH RESPECT TO GRANULARITY",
        "chunkIndex": 25,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-26",
      "content": "As outlined in Section 3.2, the power-law given by Eq. 1 consists of three terms that describe inherent data entropy and limitations in function representation and data. This derivation is independent of the architecture. In particular, the Eq. 1 also holds for constant granularity. Empirically, we observe a power law relationship in N and D analogous to that in dense models as depicted in Figure 3(b) for a fixed value of granularity (see also Fig. 1, Kaplan et al. (2020)). Furthermore, the validity of this functional form is verified by fit in Section 5.4.\n\nSince we know that separate scaling laws are valid for given granularities, in the general form, the parameters in Eq. 1 can be dependent on the model's granularity:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.2 SCALING THE MODEL AND DATASET SIZE",
        "chunkIndex": 26,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-27",
      "content": "Following the above observation that models with constant granularity obey Chinchilla scaling laws given by Eq. 1, the key question arises as to how the general notion of granularity G can be incorporated into the joint scaling law. Moreover, the scaling law formula from Eq. 5 for constant N and D has to be representable by Eq. 4. This is because the former is a more general equation, encompassing shared hyper-parameters across all N , D , and G . It is anticipated to align with the latter, consisting of distinct power laws, each with specific parameters for different N and D values. Consequently, the objective is to identify a function that fulfills these criteria.\n\nFigure 4: Fit of the scaling laws compared to the experimental results.\n\n<!-- image -->\n\n<!-- formula-not-decoded -->\n\nIn the subsequent sections, we aim to determine which of these parameters remain independent of G and identify their functional form.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.3 THE FORM OF THE JOINT SCALING LAW",
        "chunkIndex": 27,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-28",
      "content": "perimental results.\n\n<!-- image -->\n\n<!-- formula-not-decoded -->\n\nIn the subsequent sections, we aim to determine which of these parameters remain independent of G and identify their functional form. Furthermore, we present some rationale for the structure of our formula.\n\nLower Bound. Consider the limit of Eq. 5 for N and D growing to infinity:\n\n<!-- formula-not-decoded -->\n\nwith the constant term c G dependent on granularity. This is contradictory to the fact that it captures the inherent entropy of the dataset. Lower bound of the achievable loss when training bigger models on more samples should not depend on the architecture, therefore parameter c G = c is constant for all granularities.\n\nGranularity and Number of Tokens D . As seen in Figure 3(c), the benefit of training a model on a larger dataset is almost the same for each granularity value. This suggests that there is no interaction between D and G . Therefore, we can assume that\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.3 THE FORM OF THE JOINT SCALING LAW",
        "chunkIndex": 28,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-29",
      "content": "ning a model on a larger dataset is almost the same for each granularity value. This suggests that there is no interaction between D and G . Therefore, we can assume that\n\n<!-- formula-not-decoded -->\n\nGranularity and Model Size N . We consider α to be a constant that describes how the function scales with N . In this work, we assume polynomial functional forms that rule out the potential dependency of α on G given the form of Eq. 4. Therefore, the only element dependent on G is a G :\n\n<!-- formula-not-decoded -->\n\nFinally, one could consider omitting the constant a in the equation above, and it would still reduce to 4 for constant N and D . However, this would mean that a model with infinite granularity and a small number of active parameters can achieve the perfect perplexity of the lower bound.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.3 THE FORM OF THE JOINT SCALING LAW",
        "chunkIndex": 29,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-30",
      "content": "still reduce to 4 for constant N and D . However, this would mean that a model with infinite granularity and a small number of active parameters can achieve the perfect perplexity of the lower bound. We assume that a sparse MoE (Mixture of Experts) model is unlikely to surpass the performance of an equivalent dense model that has a matching total number of parameters, all of which are active. This means that constant a can act as a marginal improvement due to granularity.\n\nSubsequently, we fit parameters in Eq. 9 to describe the scaling of MoE. For comparison, we also perform fitting for dense transformer given by Eq. 1. Similarly to Hoffmann et al. (2022), we use Huber loss (Huber, 1964), with δ = 0 . 1 . The optimization is performed using the BFGS algorithm. We include a weight decay of 5 e -4 to enhance generalization. We start with fitting parameters in Eq. 9 and then find architecture-dependent coefficients α, β, A and B in Eq. 1. We observe a good fit, with RMSE = 0 . 015 .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.3 THE FORM OF THE JOINT SCALING LAW",
        "chunkIndex": 30,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-31",
      "content": "ay of 5 e -4 to enhance generalization. We start with fitting parameters in Eq. 9 and then find architecture-dependent coefficients α, β, A and B in Eq. 1. We observe a good fit, with RMSE = 0 . 015 . The values are presented in Table 1. We depict the results in Figure 4.\n\nFigure 5: (a) Validation of the scaling laws. (b) Training loss curves for model with N = 64 × 7 M , D = 66 B tokens, measured against wall-clock time on NVIDIA A100 GPU. G = 8 leads to the best performance, as for G = 16 the routing cost dominates gains from granularity. We model the increased cost of routing by measuring FLOPs for each configuration.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.3 THE FORM OF THE JOINT SCALING LAW",
        "chunkIndex": 31,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-32",
      "content": "Table 1: Values of the fitted coefficients.\n\n| Model   |    a |     α |    b |     β | g   | γ    |    c |\n|---------|------|-------|------|-------|-----|------|------|\n| MoE     | 18.1 | 0.115 | 30.8 | 0.147 | 2.1 | 0.58 | 0.47 |\n| Dense   | 16.3 | 0.126 | 26.7 | 0.127 | -   | -    | 0.47 |\n\nWe validate the stability of the fit by excluding the top 20% of models with the lowest perplexity and finding the coefficients based on the remaining experiments. We observe that the formula remains almost unchanged in this scenario (see Table 5 in Appendix B). The validation RMSE is 0.019. Results are depicted in Figure 5 (a).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.4 FITTING THE PARAMETRIC SCALING LAW",
        "chunkIndex": 32,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-33",
      "content": "Comparing the part of the formula that approximates underfitting (that is, dependent on training tokens) in MoE ( 30 . 8 D -0 . 147 ) and Transformer ( 26 . 7 D -0 . 127 ), we can infer that MoE models need longer training to perform competitively but scale better after reaching that point. Nonetheless, this moment may still precede the compute optimal for both models. On the other hand, we can see that the exponent on dense models α = -0 . 126 scales better with a total number of parameters than the MoE counterpart α = -0 . 115 . This should not be surprising since dense models use all parameters on each token contrary to MoE, which gains a computational advantage by activating only a subset of them. Therefore, the fair comparison of the performance has to take into account FLOPs used by each model type. In the next section, we find compute-optimal granularity for a given FLOP budget.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "5.5 MOE SCALING PROPERTIES",
        "chunkIndex": 33,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-34",
      "content": "In Section 5, we show that higher granularity leads to lower loss for the same number of training steps. This is not always the case if we consider the wall-clock time. As depicted in Figure 5 (b), in practice for too high values of G (relative to d model), training can be bottlenecked by the routing cost. Practical modeling of this situation is possible by measuring FLOPs in routing. In this section we find optimal N,D,G for a given computational budget F by solving the following optimization problem,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6 OPTIMAL ALLOCATION OF COMPUTATIONAL BUDGET",
        "chunkIndex": 34,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-35",
      "content": "It is important to acknowledge that increasing granularity can lead to some challenges in training the model, namely higher computational and communication costs and a larger memory footprint. The main component responsible for higher costs is the increase in routing operations due to a larger pool of granular experts. This increase is proportional to the value of G. For standard, non-granular MoE models ( G = 1 ), the routing overhead still exists, although it has been considered negligible.\n\nTaking into account the routing operation overhead, the number of used FLOPs F is described by the following formula:\n\n<!-- formula-not-decoded -->\n\ngiven expansion rate E , granularity G , and constants that denote FLOPs per active parameter ratio, respectively, within routing ( c r ) and within the rest of the network ( c f ). The term 12 d model 2 is the number of active parameters within a transformer block, while d model EGc r is the number of active parameters within a routing network.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.1 COMPUTATIONAL COST OF GRANULARITY",
        "chunkIndex": 35,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-36",
      "content": "n the rest of the network ( c f ). The term 12 d model 2 is the number of active parameters within a transformer block, while d model EGc r is the number of active parameters within a routing network. The in-depth analysis of constants c r and c f can be found in Appendix E. We exclude embedding and unembedding from the FLOPs calculations, following Hoffmann et al. (2022).\n\nObserve that, in contrast to scenarios where routing operations are omitted, the FLOPs calculation that incorporates routing overhead relies on both d model and n blocks . Consequently, an additional condition is required to determine the scaling of d model and n blocks in relation to an increase in N , the number of parameters. It is noted that minor variations in the depth-to-width ratio are not significant (Kaplan et al., 2020). Following this analysis, we opt to adopt the assumption that d model = 64 n blocks .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.1 COMPUTATIONAL COST OF GRANULARITY",
        "chunkIndex": 36,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-37",
      "content": "arameters. It is noted that minor variations in the depth-to-width ratio are not significant (Kaplan et al., 2020). Following this analysis, we opt to adopt the assumption that d model = 64 n blocks .\n\nThe total number of parameters in the feed-forward layer, excluding the routing matrix, is 2 Ed ff d model = 8 Ed model 2 , and 4 d model 2 in attention (key, query, value, and output projection). This results in the following formula for the total number of parameters, N = d model 2 · (8 E +4) · n blocks .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.1 COMPUTATIONAL COST OF GRANULARITY",
        "chunkIndex": 37,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-38",
      "content": "Taking into consideration we need to solve the following optimization problem, given F ,\n\n<!-- formula-not-decoded -->\n\nAll these constraints are reducible to a one-dimensional optimization problem, which is, however, hard to solve analytically. Therefore we approximate the solution using Brent's method (Brent, 1971). The results of this optimization for varying FLOPs budgets are plotted in Figure 1 while the optimal configurations of parameters for selected model sizes are presented in Table 2. To validate the uncertainty of these predictions, we follow Hoffmann et al. (2022) and calculate the 10th and 90th percentiles estimated via bootstrapping data (see Appendix C for the detailed results).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.2 COMPUTE OPTIMAL FORMULA",
        "chunkIndex": 38,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-39",
      "content": "Contrary to the results from Clark et al. (2022), in Figure 1 we can see, that Mixture-of-Experts can be always considered more efficient than dense Transformers, regardless of the model size. According to our previous observations from Section 5.5, MoE models scale better with optimal training. However, for short training schedules, they may under-perform dense models. This means that for constant training time and increasing model size, there exists a point where both models will become very under-trained, in which scenario dense models surpass MoE. This shows why in Clark et al. (2022), where varying the number of training tokens has not been considered, MoE was predicted to be under-performing for models bigger than 1 T . However, when all training hyper-parameters N,D,G are properly selected to be compute-optimal for each model, the gap between dense and sparse models only increases as we scale.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.3 MOE IS ALWAYS MORE EFFICIENT",
        "chunkIndex": 39,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-40",
      "content": "dels bigger than 1 T . However, when all training hyper-parameters N,D,G are properly selected to be compute-optimal for each model, the gap between dense and sparse models only increases as we scale.\n\nTable 2: Compute optimal training hyper-parameters for MoE models. Optimal N and D follow approximately similar relation to these of Hoffmann et al. (2022) for active parameters around the range of 1 B to 10 B parameters, requiring comparably longer training for smaller models and shorter for bigger ones. Higher granularity is optimal for larger compute budgets.\n\n| N         | D       |   G |    FLOPs |   Loss |\n|-----------|---------|-----|----------|--------|\n| 64 x 100M | 4.37B   |   8 | 2.95e+18 |  3.133 |\n| 64 x 1B   | 28.94B  |  16 | 1.93e+20 |  2.491 |\n| 64 x 3B   | 72.90B  |  16 | 1.41e+21 |  2.245 |\n| 64 x 7B   | 137.60B |  32 | 6.46e+21 |  2.076 |\n| 64 x 70B  | 941.07B |  32 | 4.16e+23 |  1.694 |\n| 64 x 300B | 2.96T   |  64 | 5.69e+24 |  1.503 |\n| 64 x 1T   | 7.94T   |  64 | 4.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.3 MOE IS ALWAYS MORE EFFICIENT",
        "chunkIndex": 40,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-41",
      "content": "41e+21 |  2.245 |\n| 64 x 7B   | 137.60B |  32 | 6.46e+21 |  2.076 |\n| 64 x 70B  | 941.07B |  32 | 4.16e+23 |  1.694 |\n| 64 x 300B | 2.96T   |  64 | 5.69e+24 |  1.503 |\n| 64 x 1T   | 7.94T   |  64 | 4.97e+25 |  1.367 |",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "6.3 MOE IS ALWAYS MORE EFFICIENT",
        "chunkIndex": 41,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-42",
      "content": "Extreme Granularity. In Section 5, we argue that model performance improves with increasing granularity. This postulate largely aligns with the empirical findings of our study. Nonetheless, at exceedingly high granularity levels, such as G = 64 in models characterized by d model = 256 and E = 64 , there is an observable decline in performance. This phenomenon is particularly evident in scenarios where the number of parameters in the routing mechanism exceeds active parameters in actual experts. Additionally, as described in Section 6, the utility of such high granularity is predominantly restricted to models of substantial size. In alignment with the principles outlined by Hoffmann et al. (2022), this research focuses more on findings that can be broadly applied rather than delving into the specific details of these corner-case situations.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "7 DISCUSSION",
        "chunkIndex": 42,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-43",
      "content": "ith the principles outlined by Hoffmann et al. (2022), this research focuses more on findings that can be broadly applied rather than delving into the specific details of these corner-case situations. However, it is hypothesized that the efficiency of models with significantly high granularity could be potentially enhanced through careful expert initialization or modifications to the routing algorithm. These ideas are set aside to be investigated in future studies.\n\nVarying Expansion Rate. In this study, due to computational resources constraint, we focus on E = 64 , as recommended by Clark et al. (2022). This value of E was also used for the largest models in other works (Du et al., 2022; Zhou et al., 2022) and the best-performing configuration in Fedus et al. (2022). Nonetheless, we acknowledge the importance of considering different expansion rates, as different levels of E may be chosen based on factors like the target size of the model in memory.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "7 DISCUSSION",
        "chunkIndex": 43,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-44",
      "content": "et al. (2022). Nonetheless, we acknowledge the importance of considering different expansion rates, as different levels of E may be chosen based on factors like the target size of the model in memory. Therefore, in Appendix D, we present the results of the study for E = 16 and show that the main findings of this work are still valid in such cases.\n\nIncluding E in the formula. Another possible advancement would be to unify all of the factors N,D,G and E in one formula. While this would open the possibility of studying the relationships between coefficients in more detail, it would also be hard to practically recommend the optimal configuration in such a scenario using only FLOPs. This is because larger values of E typically lead to better performance but also incur additional memory requirements. Therefore, the choice of expansion rate may be heavily dependent on the available hardware configuration. We leave a detailed study of these factors for future work.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "7 DISCUSSION",
        "chunkIndex": 44,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-45",
      "content": "ur additional memory requirements. Therefore, the choice of expansion rate may be heavily dependent on the available hardware configuration. We leave a detailed study of these factors for future work.\n\nModeling the cost of granularity. It is important to note that the exact estimation of the training cost of MoE models is dependent on the training setup, hardware, and implementation. Specifically, increasing G can lead to higher transfer costs, depending on the adopted model of distributed training. Therefore, the precise selection of hyperparameters should be made considering these factors. In this work, we model the cost of operations using FLOPs, which is common in the Scaling Laws literature (Kaplan et al., 2020; Hoffmann et al., 2022; Frantar et al., 2023). Additionally, we would like to note that in our setup, we observe significant gains of fine-grained MoE measured as wall-clock time needed to achieve given perplexity (see Fig. 5 (b) for an example).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "7 DISCUSSION",
        "chunkIndex": 45,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-46",
      "content": "to note that in our setup, we observe significant gains of fine-grained MoE measured as wall-clock time needed to achieve given perplexity (see Fig. 5 (b) for an example).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "7 DISCUSSION",
        "chunkIndex": 46,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-47",
      "content": "This study introduces a novel hyperparameter, granularity ( G ), and underscores the significance of adjusting it for optimizing the efficiency of experts within MoE models. A central finding of this research is that a standard granularity of G = 1 is suboptimal across a broad range of FLOPs, leading to the recommendation of using higher granularity values to enhance MoE model performance and efficiency. Simultaneously, this work emphasizes the importance of varying training duration for compute-optimal settings. Consequently, both granularity and variable training length are incorporated into new scaling laws. These laws confidently demonstrate that MoE models consistently outperform dense transformers in terms of efficiency and scaling. This work not only sheds new light on the scaling laws applicable to MoE models but also provides practical guidance for improving computational efficiency in large language models.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "8 CONCLUSIONS",
        "chunkIndex": 47,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-48",
      "content": "iency and scaling. This work not only sheds new light on the scaling laws applicable to MoE models but also provides practical guidance for improving computational efficiency in large language models. The insights are critical for the development and optimization of large-scale language models, marking a significant advancement in the field.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "8 CONCLUSIONS",
        "chunkIndex": 48,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-49",
      "content": "The code used to produce the results described in this work is open-sourced and can be found at github.com/llm-random/llm-random .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "9 REPRODUCIBILITY",
        "chunkIndex": 49,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-50",
      "content": "We would like to express sincere gratitude to Piotr Miło´ s and Tomasz Trzci´ nski for valuable feedback and to Aleksandra Weglarz for her help with graphic design.\n\nThis work was funded by IDEAS NCBR, which also provided significant computational resources a supportive research environment and direction. The research was supported by PL-Grid infrastructure (grant PLG/2023/016148). We also benefited from the Entropy cluster (hosted at the Faculty of Mathematics, Informatics and Mechanics of the University of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center grant 2022/45/N/ST6/02222, and ERC Starting Grant TOTAL. Marek Cygan was partially supported by an NCBiR grant POIR.01.01.01-00-0392/17-00.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "ACKNOWLEDGMENTS",
        "chunkIndex": 50,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-51",
      "content": "- Andrea Agostinelli, Timo I. Denk, Zal´ an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023.\n- Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efficient large scale language modeling with mixtures of experts, 2022.\n- Richard P. Brent. An algorithm with guaranteed convergence for finding a zero of a function. Comput. J. , 14:422-425, 1971. URL https://api.semanticscholar.org/CorpusID: 10312755 .\n- Tom B.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 51,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-52",
      "content": "s, 2022.\n- Richard P. Brent. An algorithm with guaranteed convergence for finding a zero of a function. Comput. J. , 14:422-425, 1971. URL https://api.semanticscholar.org/CorpusID: 10312755 .\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 52,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-53",
      "content": "ra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 53,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-54",
      "content": "sandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.\n- Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models, 2022.\n- Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 54,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-55",
      "content": "i, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.\n\n- Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\n- Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models, 2024.\n- William Fedus, Barret Zoph, and Noam Shazeer.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 55,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-56",
      "content": "neda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models, 2024.\n- William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022.\n- Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models, 2023.\n- Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation, 2021.\n- Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling, 2020.\n- Sepp Hochreiter and J¨ urgen Schmidhuber.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 56,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-57",
      "content": "d, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling, 2020.\n- Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory. Neural computation , 9(8): 1735-1780, 1997.\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\n- Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):73 - 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/ aoms/1177703732 .\n- Albert Q.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 57,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-58",
      "content": "Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):73 - 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/ aoms/1177703732 .\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´ elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mistral 7b, 2023.\n- Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´ elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´ eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 58,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-59",
      "content": "Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´ eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mixtral of experts, 2024.\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\n- Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n- Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models, 2021.\n- Zeyu Leo Liu, Tim Dettmers, Xi Victoria Lin, Veselin Stoyanov, and Xian Li. Towards a unified view of sparse feed-forward network in pretraining large language model, 2023.\n- Ilya Loshchilov and Frank Hutter.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 59,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-60",
      "content": "Liu, Tim Dettmers, Xi Victoria Lin, Veselin Stoyanov, and Xian Li. Towards a unified view of sparse feed-forward network in pretraining large language model, 2023.\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\n\n- Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts, 2023.\n- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018a.\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018b. URL https://d4mucfpksywv. cloudfront.net/better-language-models/language-models.pdf .\n- Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa An",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 60,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-61",
      "content": "dan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake H",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 61,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-62",
      "content": "aniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher, 2022.\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n- Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 62,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-63",
      "content": ", Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n- Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models, 2021.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c, Daniel Hesslow, Roman Castagn´ e, Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall´ e, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoˆ ıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenc ¸on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leo",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 63,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-64",
      "content": "Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz´ alez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G´ erard Dupont, Germ´ an Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J¨ org Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu˜ noz, Maraim Masoud, Mar´ ıa Grandury, Mario ˇ Saˇ sko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 64,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-65",
      "content": "en allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu˜ noz, Maraim Masoud, Mar´ ıa Grandury, Mario ˇ Saˇ sko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L´ opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tas ¸ar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 65,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-66",
      "content": "lina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tas ¸ar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S.\n\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 66,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-67",
      "content": "r Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franc ¸ois Lavall´ ee, R´ emi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St´ ephane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur´ elie N´ ev´ eol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yu",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 67,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-68",
      "content": "a Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇ ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu˜ noz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerc",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 68,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-69",
      "content": "Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl´ ementine Fourrier, Daniel Le´ on Peri˜ n´ an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 69,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-70",
      "content": "Jain, Chuxin Xu, Cl´ ementine Fourrier, Daniel Le´ on Peri˜ n´ an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P` amies, Maria A Castillo, Marianna Nezhurina, Mario S¨ anger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th´ eo Gigant,",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 70,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-71",
      "content": "i Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th´ eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.\n\n- Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n- Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 71,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-72",
      "content": "ayer, 2017.\n- Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers, 2018.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 72,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-73",
      "content": "kel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\n\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 73,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-74",
      "content": "yan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n\n- Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2023.\n- Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\n- Yanqi Zhou, Nan Du, Yanping Huang, Daiyi Peng, Chang Lan, Da Huang, Siamak Shakeri, David So, Andrew Dai, Yifeng Lu, Zhifeng Chen, Quoc Le, Claire Cui, James Laundon, and Jeff Dean. Brainformers: Trading simplicity for efficiency, 2023.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "REFERENCES",
        "chunkIndex": 74,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-75",
      "content": "All of the models considered in this work are decoder-only Transformers trained on the C4 dataset (Raffel et al., 2023). We use GPT2 tokenizer (Radford et al., 2018a). Each batch consists of 0.5M tokens packed into 2048 sequences. Our optimizer is AdamW (Loshchilov &amp; Hutter, 2019), with a weight decay of 0 . 1 . In each training run, we use the maximum learning rate of 2e -4 , with linear warmup for 1% steps and cosine decay to 2e -5 . To improve stability, we initialize weights using the truncated normal distribution with reduced scale, as advised in Fedus et al. (2022). The models are trained using mixed precision; we always keep the attention mechanism and router in high precision. We assume the infinite data regime, as the number of training tokens for any of the runs is less than the number of tokens in the corpus. We follow Hoffmann et al. (2022) and perform our analysis on the smoothed training loss.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "A ARCHITECTURE AND TRAINING SETUP",
        "chunkIndex": 75,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-76",
      "content": "a regime, as the number of training tokens for any of the runs is less than the number of tokens in the corpus. We follow Hoffmann et al. (2022) and perform our analysis on the smoothed training loss.\n\nIn MoE, we use the Expert Choice routing algorithm, as it guarantees a balanced expert load without tuning additional hyperparameters. To maintain compatibility with autoregressive language modeling, we apply the recipe described in Zhou et al. (2022): tokens are grouped by position across different sequences. The group size is always set to 256 . We match the number of FLOPs for MoE and dense models with the same d model (meaning we activate an average of 8 d 2 model parameters per token in each MoE layer). In the router, softmax is performed over the expert dimension, while we choose tokens over the token dimension, as this leads to the best performance (as opposed to performing softmax over the token dimension). We put an additional layer normalization before the output of MoE layer.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "A ARCHITECTURE AND TRAINING SETUP",
        "chunkIndex": 76,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-77",
      "content": "ns over the token dimension, as this leads to the best performance (as opposed to performing softmax over the token dimension). We put an additional layer normalization before the output of MoE layer. This gives a small improvement for standard MoE, but is crucial for the performance of models with G &gt; 1 .\n\nTable 3 and Table 4 list the considered architecture and training variants for dense and MoE models, respectively.\n\nTable 3: Architecture and training variants (MoE models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x7M                  |       384 |          4 |         6 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "A ARCHITECTURE AND TRAINING SETUP",
        "chunkIndex": 77,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-78",
      "content": "|       384 |          4 |         6 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 130B             | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 16B, 33B,        | 1, 2, 4, 8, 16 |\n| 64x25M                 |       512 |          8 |         8 | 66B              | 1, 2, 4, 8     |\n| 64x49M                 |       640 |         10 |        10 | 16B, 33B         | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 66B              | 1, 2, 4        |\n| 64x85M                 |       768 |         12 |        12 | 33B              | 1, 2, 4        |\n\nTable 4: Architecture and training variants (dense models).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "A ARCHITECTURE AND TRAINING SETUP",
        "chunkIndex": 78,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-79",
      "content": "0 | 66B              | 1, 2, 4        |\n| 64x85M                 |       768 |         12 |        12 | 33B              | 1, 2, 4        |\n\nTable 4: Architecture and training variants (dense models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)     |\n|------------------------|-----------|------------|-----------|--------------------|\n| 3M                     |       256 |          4 |         4 | 16B, 24B, 33B, 66B |\n| 6M                     |       256 |          8 |         4 | 16B, 24B, 33B, 66B |\n| 13M                    |       512 |          4 |         8 | 16B, 24B, 33B, 66B |\n| 25M                    |       512 |          8 |         8 | 16B, 24B, 33B, 66B |\n| 49M                    |       640 |         10 |        10 | 16B, 24B, 33B, 66B |\n| 85M                    |       768 |         12 |        12 | 16B, 33B           |",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "A ARCHITECTURE AND TRAINING SETUP",
        "chunkIndex": 79,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-80",
      "content": "In this section, we provide coefficients of the scaling law fitted with 20 % of datapoints with the lowest perplexity excluded for the purpose of validation.\n\nTable 5: Values of the fitted coefficients.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "B VALIDATION OF THE SCALING LAW",
        "chunkIndex": 80,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-81",
      "content": "In this section, we assess the stability of our predictions presented in Section 6.1. Similarly to Hoffmann et al. (2022) we calculate the 10 th and 90 th percentiles estimated via bootstrapping data ( 80% of the data is sampled 100 times). See Table 6 for the details.\n\nTable 6: 10 th and 90 th percentiles estimated via bootstraping data.\n\n| N         | D                  | G        |\n|-----------|--------------------|----------|\n| 64 x 100M | (2.97B, 5.98B)     | (8, 8)   |\n| 64 x 1B   | (21.17B, 40.73B)   | (16, 16) |\n| 64 x 3B   | (50.20B, 105.88B)  | (16, 32) |\n| 64 x 7B   | (101.06B, 205.40B) | (32, 32) |\n| 64 x 70B  | (638.49B, 1.59T)   | (32, 64) |\n| 64 x 300B | (1.99T, 5.62T)     | (64, 64) |\n| 64 x 1T   | (5.29T, 16.87T)    | (64, 64) |",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "C RELIABILITY OF COMPUTE OPTIMAL FORMULA",
        "chunkIndex": 81,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-82",
      "content": "In this section, we provide results for E = 16 . The training procedure is the same as described in App. A. The models considered in this part are listed in Table 7.\n\nTable 7: Architecture and training variants (MoE models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x7M                  |       256 |          8 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 66B              | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 8B",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "D VARYING EXPANSION RATE",
        "chunkIndex": 82,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-83",
      "content": "| 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 8B               | 1, 2, 4, 8, 16 |\n\nWe fit Eq. 9 using the same procedure as described in Section 5.4. The results are detailed in Table 8.\n\nTable 8: Values of the fitted coefficients.\n\n| Model          |     a |     α |     b |     β |    g |     γ |     c |\n|----------------|-------|-------|-------|-------|------|-------|-------|\n| MoE ( E = 16 ) | 19.64 | 0.124 | 57.07 | 0.169 | 1.18 | 0.986 | 0.472 |\n\nUsing the coefficients and FLOPs calculation formulas, we can derive the compute optimal training parameters. The results are presented in Table 9.\n\nTable 9: 10 th and 90 th percentiles estimated via bootstrapping data for E = 16 .",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "D VARYING EXPANSION RATE",
        "chunkIndex": 83,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-84",
      "content": "s calculation formulas, we can derive the compute optimal training parameters. The results are presented in Table 9.\n\nTable 9: 10 th and 90 th percentiles estimated via bootstrapping data for E = 16 .\n\n| N         | D                  | G         |\n|-----------|--------------------|-----------|\n| 16 x 100M | (10.29B, 17.73B)   | (8 , 16)  |\n| 16 x 1B   | (53.74B, 103.54B)  | (16, 32)  |\n| 16 x 3B   | (106.22B, 261.04B) | (16, 32)  |\n| 16 x 7B   | (177.65B, 511.43B) | (16, 32)  |\n| 16 x 70B  | (721.60B, 3.22T)   | (32, 64)  |\n| 16 x 300B | (1.73T, 10.69T)    | (32, 64)  |\n| 16 x 1T   | (3.60T, 28.22T)    | (32, 128) |\n\nWe can observe that similarly to the case when E = 64 , larger compute budgets imply larger optimal values of G. Note that the values for 10 th and 90 th percentiles form larger intervals in this case, as in this part we run a smaller number of experiments and keep shorter training durations.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "D VARYING EXPANSION RATE",
        "chunkIndex": 84,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-85",
      "content": "ptimal values of G. Note that the values for 10 th and 90 th percentiles form larger intervals in this case, as in this part we run a smaller number of experiments and keep shorter training durations. However, we believe that this preliminary study forms a valuable addition to the results in the main part.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "D VARYING EXPANSION RATE",
        "chunkIndex": 85,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-86",
      "content": "The number of FLOPs F used in Transformer training, considering the routing operation overhead in MoE, can be described by the following formula:\n\n<!-- formula-not-decoded -->\n\nFollowing Hoffmann et al. (2022), we assume c f to be 6 . This is interpreted as 6 FLOPs for each pair of an active parameter (in linear projection) and a processed token. The breakdown of operations is as follows:\n\n- During the forward pass, 2 operations (single multiplication and single addition) are used to compute the matrix multiplication of an input and linear projection.\n- During the backward pass, 2 operations are used to compute gradients wrt. the input.\n- During the backward pass, 2 operations are used to compute gradients wrt. the weights of linear projection.\n\nIn our work, we have assumed the routing constant, c r , to be 14, with the breakdown presented below. The exact number of operations may depend on the implementation of routing, but it will be between 6 and 20.",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "E FLOPS CONSTANTS",
        "chunkIndex": 86,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-87",
      "content": ", we have assumed the routing constant, c r , to be 14, with the breakdown presented below. The exact number of operations may depend on the implementation of routing, but it will be between 6 and 20. However, our main conclusions of the paper are resistant to different assumptions of this constant.\n\n- During the forward pass, 2 operations are used to compute the expert logits based on an input and 'routing linear projection'.\n- During the backward pass, 2 operations are used to compute gradients for 'routing linear projection' wrt. the input.\n- During the backward pass, 2 operations are used to compute gradients for 'routing linear projection' wrt. the weights of linear projection.\n- During the forward pass, 2 operations are used to route input tokens to chosen experts.\n- During the forward pass, 2 operations are used to route expert outputs to chosen tokens and multiply those outputs by the routing score.\n- During the backward pass, 2 operations are used to route gradients from outpu",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "E FLOPS CONSTANTS",
        "chunkIndex": 87,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-88",
      "content": "rd pass, 2 operations are used to route expert outputs to chosen tokens and multiply those outputs by the routing score.\n- During the backward pass, 2 operations are used to route gradients from output tokens to experts.\n- During the backward pass, 2 operations are used to route gradients from experts to input tokens.\n\nSimilarly to the calculation of FLOPs for c f , FLOPs come in pairs as each multiplication is followed by an addition (used to accumulate outputs or gradients).",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "E FLOPS CONSTANTS",
        "chunkIndex": 88,
        "totalChunks": 90
      }
    },
    {
      "id": "2402.07871v1-chunk-89",
      "content": "Figure 6: Illustration of scaling N and D for constant granularity value of: (a) G = 1 (b) G = 2 (c) G = 8 (d) G = 16 .\n\n<!-- image -->\n\nFigure 7: Illustration of scaling granularity when N,D are fixed for: (a) N = 64 × 25 M , D = 16 B , const = 3 . 12 (b) N = 64 × 49 M , D = 16 B , const = 3 . 02 (c) N = 64 × 25 M , D = 32 B , const = 3 . 03 (d) N = 64 × 49 M , D = 32 B , const = 2 . 88\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.07871v1",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
          "Jakub Krajewski",
          "Jan Ludziejewski",
          "Kamil Adamczewski",
          "Maciej Pióro",
          "Michał Krutul",
          "Szymon Antoniak",
          "Kamil Ciebiera",
          "Krystian Król",
          "Tomasz Odrzygóźdź",
          "Piotr Sankowski",
          "Marek Cygan",
          "Sebastian Jaszczur"
        ],
        "section": "F ADDITIONAL VISUALIZATIONS",
        "chunkIndex": 89,
        "totalChunks": 90
      }
    }
  ],
  "fullText": "## SCALING LAWS FOR FINE-GRAINED MIXTURE OF EXPERTS\n\n| Jakub Krajewski ∗ University of Warsaw IDEAS NCBR   | Jan Ludziejewski ∗ University of Warsaw IDEAS NCBR   | Kamil Adamczewski IDEAS NCBR                   | Maciej Pi´ oro IPPT PAN IDEAS NCBR                   |\n|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------|------------------------------------------------------|\n| Michał Krutul University of Warsaw IDEAS NCBR       | Szymon Antoniak University of Warsaw IDEAS NCBR      | Kamil Ciebiera University of Warsaw IDEAS NCBR | Krystian Kr´ ol University of Warsaw IDEAS NCBR      |\n| Tomasz Odrzyg´ o´ zd´ z TradeLink                   | Piotr Sankowski University of Warsaw IDEAS NCBR      | Marek Cygan University of Warsaw Nomagic       | Sebastian Jaszczur ∗ University of Warsaw IDEAS NCBR |\n\n## ABSTRACT\n\nMixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.\n\n## 1 INTRODUCTION\n\nIn recent years, we have witnessed Large Language Models (LLMs) achieve exceptional performance in tasks across numerous domains (Chowdhery et al., 2022; Yin et al., 2023; Agostinelli et al., 2023). However, training those massive models incurs high computational costs, measured in millions of GPU-hours (Touvron et al., 2023b), enabled only by enormous budgets (Scao et al., 2023) and leading to non-negligible carbon footprints (Faiz et al., 2024). To combat these obstacles, the research community has been striving to increase the efficiency of LLMs. One promising approach that has lately been gaining visibility is the use of Mixture of Experts (MoE) methods. Models such as Switch (Fedus et al., 2022) and Mixtral (Jiang et al., 2024) have already demonstrated that it is possible to achieve comparable effectiveness with significantly lower computational costs.\n\nIn the context of the current trend of increasing budgets for training language models, a question arises: will MoE models continue to be attractive in the future? This is an important issue, as other studies have stated that the gap in efficiency between MoE and standard Transformers narrows at\n\nContributions: Jakub implemented fine-grained MoE, ran experiments, and oversaw the course of the project. Jan designed and implemented the scaling laws, also optimized and tuned the fine-grained MoE implementation. Kamil A. provided significant advice on many aspects of the project. Maciej experimented with the block design and, with Michał, provided considerable technical support. Szymon, Kamil C., Krystian, and Tomasz contributed to the project and the engineering in various ways. Marek, along with Piotr, provided high-level scientific advice. Sebastian came up with the initial idea, started the project, and supervised it while setting the research direction and leading experiments and analyses. Correspondence to &lt; s.jaszczur@uw.edu.pl &gt; . ∗ Equal contribution.\n\nFigure 1: Mixture-of-Experts can be always considered more efficient than dense Transformers, regardless of the model size. (a) Compute Optimal scaling curves for MoE and standard Transformers. The dashed line represents a dense Transformer. Colors denote optimal granularity for the given FLOPs training budget. (b) Relative number of FLOPs needed to train Transformer and Vanilla MoE (MoE with G = 1 ) to achieve the performance of MoE with compute optimal G .\n\n<!-- image -->\n\nscale (Artetxe et al., 2022) or even that traditional dense models may outperform MoE as the size of the models increases (Clark et al., 2022).\n\nIn this paper, we argue that previous claims lose their validity when we relax certain implicit assumptions regarding the training process, present in previous research. In particular, we refer to the fixed training duration and the constant size of experts in MoE models.\n\nOur results suggest that a compute-optimal MoE model trained with a budget of 10 20 FLOPs will achieve the same quality as a dense Transformer trained with a 20 × greater computing budget, with the compute savings rising steadily, exceeding 40 × when budget of 10 25 FLOPs is surpassed (see Figure 1). Importantly, we show that the standard practice of fixing the size of experts in MoE to be the same as feed-forward layer is almost never optimal.\n\nOur main contributions are:\n\n1. Introducing a new hyperparameter - granularity. Adjusting this parameter allows us to determine the optimal size of experts in MoE models, which translates into increased efficiency.\n2. Deriving new scaling laws for MoE models that incorporate variable training duration, the number of parameters, and granularity. Such scaling laws allow us to calculate optimal training hyperparameters for MoE models.\n3. Demonstrating that, with optimal settings, MoE models can always outperform traditional Transformers at any computing budget. This is a conclusion contrary to the results from Clark et al. (2022).\n\nThe code used to produce the results described in this work is open-sourced at github.com/llm-random/llm-random .\n\n## 2 RELATED WORK\n\nMixture of Experts. In the context of language modeling, MoE was first introduced by Shazeer et al. (2017) as a sparsely gated layer between stacked blocks of LSTM (Hochreiter &amp; Schmidhuber, 1997). A similar technique was proposed in the context of Transformers by Shazeer et al. (2018) and Lepikhin et al. (2020). Fedus et al. (2022) proposed to route each input to only a single expert and designed a modified initialization scheme to reduce training instability. Numerous studies\n\nhave proposed to modify the original routing method. Lewis et al. (2021) used a linear assignment algorithm to postprocess token-expert mappings and ensure even expert selections. Roller et al. (2021) suggested another approach involving deterministic hash functions. Zhou et al. (2022) proposed expert choice routing, eliminating the need for additional load balancing losses. Puigcerver et al. (2023) designed a fully-differentiable Soft MoE architecture.\n\nConcurrently to our work, Dai et al. (2024) proposed to modify the MoE layer by segmenting experts into smaller ones and adding shared experts to the architecture. Independently, Liu et al. (2023) suggested a unified view of sparse feed-forward layers, considering, in particular, varying the size of memory blocks. Both approaches can be interpreted as modifying granularity. However, we offer a comprehensive comparison of the relationship between training hyperparameters and derive principled selection criteria, which they lack.\n\nScaling laws. Scaling laws are empirically derived equations relating the loss of a model with variables such as the number of parameters, training samples, or the computational budget. In the case of dense Transformers, scaling laws were first studied by Kaplan et al. (2020), who observed power law relationships between the final model perplexity and model and dataset size. This work was extended by Hoffmann et al. (2022) by considering variable cosine cycle lengths and formulating a modified functional form of the scaling equation.\n\nScaling laws have also been proposed for other architectures and training scenarios. Henighan et al. (2020) studied autoregressive modeling across various modalities, while Ghorbani et al. (2021) considered machine translation. Frantar et al. (2023) explored the impact of pruning on vision and language Transformers, deriving optimal sparsity for a given compute budget. Clark et al. (2022) studied the scaling of MoE when changing model size and number of experts on a fixed dataset, concluding that routed models are more efficient only until a certain model size. In this work, we challenge that claim by considering a variable, optimal dataset size for both model families (see Section 6.3).\n\n## 3 BACKGROUND\n\n## 3.1 MODEL ARCHITECTURE\n\nTransformer. A standard decoder-only Transformer (Radford et al., 2018a;b; Kaplan et al., 2020; Brown et al., 2020) consists of an embedding layer, a stack of alternating attention and feed-forward layers, and an unembedding layer. In the model, each input token is converted by the embedding layer into a vector of size d model, the dimension maintained across all the layers in the residual stream.\n\nThe feed-forward component consists of two linear transformations and a nonlinearity ϕ in between. It can be described as FFN ( x ) = ϕ ( xW 1 + b 1 ) W 2 + b 2 , with W 1 mapping from d model to d ff, and W 2 back to the original d model. It is standard (Radford et al., 2018a; Rae et al., 2022; Touvron et al., 2023a; Jiang et al., 2023) to set the hidden dimension as d ff = 4 · d model.\n\nFeed-forward layers contain the majority of Transformer parameters and require the biggest computational budget counted in terms of FLOPs. Subsequently, they are the main focus of the Mixture of Experts models considered in this work.\n\nMixture of Experts. The core idea behind MoE in Transformers is to replace the feed-forward layer with a set of N expert experts . The size of each expert is typically (Fedus et al., 2022; Zhou et al., 2022; 2023; Jiang et al., 2024) set to mirror the original dimensions of the layer, with the hidden expert dimension d expert equal to d ff . Therefore, the total number of parameters in MoE scales linearly with the number of experts. However, the computational cost remains approximately constant as each input is routed and then processed by a subset of experts.\n\nFigure 2: (a) Standard MoE layer with G = 1 (b) Corresponding MoE layer with G = 2 . Each of the original experts is split into two granular ones. The split occurs in the hidden dimension of an expert. Increasing G allows for a more precise mapping between experts and tokens. Since for granularity G , the token is routed to G granular experts, the number of parameters activated per token is the same in both cases.\n\n<!-- image -->\n\n## 3.2 SCALING LAWS\n\nDense Transformers. Large Transformer-based models are known to approximately obey the power-law relationship between final loss L , model size N, and number of training tokens D. This relationship is often called Chinchilla scaling laws described by Hoffmann et al. (2022) as\n\n<!-- formula-not-decoded -->\n\nThe power-law formula is composed of three distinct terms that characterize the intrinsic entropy of data, constraints of the model, and limitations in the training data. The term c represents the minimum possible error intrinsic to the data. The remaining two terms are suboptimality terms, which address the limitations in function representation owing to the size of the model and in data signified by the number of tokens. In the limit, with infinite data and model size, the loss is reduced to c .\n\nMixture of Experts. For MoE Transformer-based models, Clark et al. (2022) formulated the final loss for a constant dataset size D of 130B tokens, allowing for variations in the expansion rate E , as:\n\n<!-- formula-not-decoded -->\n\nHowever, this result has a notable limitation as it can be applied only to the original dataset size. The scalability and effectiveness are constrained in this scenario because it is crucial to align the number of training samples with the available computational resources for optimal use. As per Kaplan et al. (2020) and Hoffmann et al. (2022), maintaining a constant dataset size while scaling up the neural network size leads to undertraining, resulting in a model that does not perform to its full potential.\n\n## 4 GRANULARITY\n\nAs described in Section 3, in the standard setting, the inner dimension of each expert network, d expert , is equal to d ff, which is the same size as the feed-forward layer of the base model.\n\nIn this work, we suggest an alternative approach where the hidden dimension of the expert is not necessarily set to mirror that of the standard feed-forward layer. Instead, it can be adjusted to a value that is the most effective. This approach allows the configuration of MoE to be articulated in terms of two key hyperparameters: granularity ( G ) and expansion rate ( E ). In the following parts of this work, we will also use the term active parameters to refer to the non-embedding parameters used to produce output for a single token, except routing. The number of active parameters is denoted as N act .\n\nLet d expert be the hidden dimension of a single expert. Granularity is defined as\n\n<!-- formula-not-decoded -->\n\nIn other words, granularity denotes the multiplier factor for the change in the size of an expert from the original standard model, defined as G = 1 . In this work, we investigate G &gt; 1 where experts are smaller than in the standard layer.\n\nNote that increasing granularity does not affect the number of active parameters. As G increases, the number of experts that process the token grows proportionally to G . In other words, for granularity G , a token is routed to G fine-grained experts, thereby keeping the number of active parameters constant. See Fig. 2 for visualization.\n\nWe then define the expansion rate , which describes the increase in the number of parameters from a standard transformer layer to a MoE layer. Given that, N MoE and N ff denote the total number of parameters in a MoE layer excluding routing and the standard feed-forward layer, respectively. The expansion rate E is then defined as\n\n<!-- formula-not-decoded -->\n\nExpansion rate can also be seen as the total number of parameters in a MoE layer compared to its active parameters.\n\nThe concept of the expansion rate is intricately linked to the number of experts through the idea of granularity. Indeed, the definitions of both granularity and expansion rate extend and refine our understanding of the number of experts, symbolized as N expert .\n\n<!-- formula-not-decoded -->\n\nFor non-granular models, where G = 1 , the expansion rate is equal to the number of experts.\n\nIntuitively, increasing granularity for a given expansion rate gives the model more flexibility in mapping datapoints to experts, potentially improving performance. We incorporate the notion of granularity into our scaling laws in Section 5. The discussion about practical tradeoffs in changing this parameter is given in Section 6.\n\n## 5 SCALING LAWS\n\nGranularity determines changes in the architecture of MoE. In this section, we answer a central question of this work: whether the granular MoE models follow scaling laws and, if so, how granularity affects them. Thus, we aim to derive a parametric scaling law for predicting the final loss value L based on granularity G , total number of non-embedding parameters N , and number of training tokens D .\n\nWe run over 100 experiments on the decoder-only Transformer architecture, with each feed-forward component replaced by a Mixture of Experts layer. Those experiments involve training models with sizes ranging from 129M to 3.7B parameters across different training durations, from 16B to 130B tokens. We consider logarithmically spaced values of granularity between 1 and 16. To constrain the search space, E = 64 is fixed, following the recommendations of Clark et al. (2022). In addition, we also run experiments with dense Transformers to compare their performance with MoE. The details of all architectures, the training procedure, and hyperparameter choices are described in detail in Appendix A.\n\nIn the subsequent part of this paper, we will use the notation E × N act to describe a MoE model with N act active parameters and expansion rate E.\n\nFigure 3: (a) The effect of G on L N,D ( G ) for constant N and D . Both axes are in the log-scale. The results suggest the linear relationship between log( G ) and log( L c ) . The given values are N = 64 × 25 M , D = 16 B , const = 3 . 12 . The plots for additional values of N and D can be found in Appendix F. (b) The impact of varying the number of parameters N on the loss for fixed granularity G = 4 . For other granularity values, see Appendix F. (c) The difference in the loss between training for 16B and 65B tokens for all model sizes and granularity values. The model size is reported as the expansion rate and the number of active parameters.\n\n<!-- image -->\n\n## 5.1 POWER LAW WITH RESPECT TO GRANULARITY\n\nWe first answer the question of whether granular models follow the scaling laws. In Figure 4(a), it can be seen that increasing granularity results in a lower loss. The returns follow approximately an exponential pattern, converging to a positive constant. The empirical relationship given by Figure 3(a) suggests the following power-law dependence of loss on a varying granularity for given N and D and constants g, h and γ that may be dependent on them,\n\n<!-- formula-not-decoded -->\n\n## 5.2 SCALING THE MODEL AND DATASET SIZE\n\nAs outlined in Section 3.2, the power-law given by Eq. 1 consists of three terms that describe inherent data entropy and limitations in function representation and data. This derivation is independent of the architecture. In particular, the Eq. 1 also holds for constant granularity. Empirically, we observe a power law relationship in N and D analogous to that in dense models as depicted in Figure 3(b) for a fixed value of granularity (see also Fig. 1, Kaplan et al. (2020)). Furthermore, the validity of this functional form is verified by fit in Section 5.4.\n\nSince we know that separate scaling laws are valid for given granularities, in the general form, the parameters in Eq. 1 can be dependent on the model's granularity:\n\n<!-- formula-not-decoded -->\n\n## 5.3 THE FORM OF THE JOINT SCALING LAW\n\nFollowing the above observation that models with constant granularity obey Chinchilla scaling laws given by Eq. 1, the key question arises as to how the general notion of granularity G can be incorporated into the joint scaling law. Moreover, the scaling law formula from Eq. 5 for constant N and D has to be representable by Eq. 4. This is because the former is a more general equation, encompassing shared hyper-parameters across all N , D , and G . It is anticipated to align with the latter, consisting of distinct power laws, each with specific parameters for different N and D values. Consequently, the objective is to identify a function that fulfills these criteria.\n\nFigure 4: Fit of the scaling laws compared to the experimental results.\n\n<!-- image -->\n\n<!-- formula-not-decoded -->\n\nIn the subsequent sections, we aim to determine which of these parameters remain independent of G and identify their functional form. Furthermore, we present some rationale for the structure of our formula.\n\nLower Bound. Consider the limit of Eq. 5 for N and D growing to infinity:\n\n<!-- formula-not-decoded -->\n\nwith the constant term c G dependent on granularity. This is contradictory to the fact that it captures the inherent entropy of the dataset. Lower bound of the achievable loss when training bigger models on more samples should not depend on the architecture, therefore parameter c G = c is constant for all granularities.\n\nGranularity and Number of Tokens D . As seen in Figure 3(c), the benefit of training a model on a larger dataset is almost the same for each granularity value. This suggests that there is no interaction between D and G . Therefore, we can assume that\n\n<!-- formula-not-decoded -->\n\nGranularity and Model Size N . We consider α to be a constant that describes how the function scales with N . In this work, we assume polynomial functional forms that rule out the potential dependency of α on G given the form of Eq. 4. Therefore, the only element dependent on G is a G :\n\n<!-- formula-not-decoded -->\n\nFinally, one could consider omitting the constant a in the equation above, and it would still reduce to 4 for constant N and D . However, this would mean that a model with infinite granularity and a small number of active parameters can achieve the perfect perplexity of the lower bound. We assume that a sparse MoE (Mixture of Experts) model is unlikely to surpass the performance of an equivalent dense model that has a matching total number of parameters, all of which are active. This means that constant a can act as a marginal improvement due to granularity.\n\nSubsequently, we fit parameters in Eq. 9 to describe the scaling of MoE. For comparison, we also perform fitting for dense transformer given by Eq. 1. Similarly to Hoffmann et al. (2022), we use Huber loss (Huber, 1964), with δ = 0 . 1 . The optimization is performed using the BFGS algorithm. We include a weight decay of 5 e -4 to enhance generalization. We start with fitting parameters in Eq. 9 and then find architecture-dependent coefficients α, β, A and B in Eq. 1. We observe a good fit, with RMSE = 0 . 015 . The values are presented in Table 1. We depict the results in Figure 4.\n\nFigure 5: (a) Validation of the scaling laws. (b) Training loss curves for model with N = 64 × 7 M , D = 66 B tokens, measured against wall-clock time on NVIDIA A100 GPU. G = 8 leads to the best performance, as for G = 16 the routing cost dominates gains from granularity. We model the increased cost of routing by measuring FLOPs for each configuration.\n\n<!-- image -->\n\n## 5.4 FITTING THE PARAMETRIC SCALING LAW\n\nTable 1: Values of the fitted coefficients.\n\n| Model   |    a |     α |    b |     β | g   | γ    |    c |\n|---------|------|-------|------|-------|-----|------|------|\n| MoE     | 18.1 | 0.115 | 30.8 | 0.147 | 2.1 | 0.58 | 0.47 |\n| Dense   | 16.3 | 0.126 | 26.7 | 0.127 | -   | -    | 0.47 |\n\nWe validate the stability of the fit by excluding the top 20% of models with the lowest perplexity and finding the coefficients based on the remaining experiments. We observe that the formula remains almost unchanged in this scenario (see Table 5 in Appendix B). The validation RMSE is 0.019. Results are depicted in Figure 5 (a).\n\n## 5.5 MOE SCALING PROPERTIES\n\nComparing the part of the formula that approximates underfitting (that is, dependent on training tokens) in MoE ( 30 . 8 D -0 . 147 ) and Transformer ( 26 . 7 D -0 . 127 ), we can infer that MoE models need longer training to perform competitively but scale better after reaching that point. Nonetheless, this moment may still precede the compute optimal for both models. On the other hand, we can see that the exponent on dense models α = -0 . 126 scales better with a total number of parameters than the MoE counterpart α = -0 . 115 . This should not be surprising since dense models use all parameters on each token contrary to MoE, which gains a computational advantage by activating only a subset of them. Therefore, the fair comparison of the performance has to take into account FLOPs used by each model type. In the next section, we find compute-optimal granularity for a given FLOP budget.\n\n## 6 OPTIMAL ALLOCATION OF COMPUTATIONAL BUDGET\n\nIn Section 5, we show that higher granularity leads to lower loss for the same number of training steps. This is not always the case if we consider the wall-clock time. As depicted in Figure 5 (b), in practice for too high values of G (relative to d model), training can be bottlenecked by the routing cost. Practical modeling of this situation is possible by measuring FLOPs in routing. In this section we find optimal N,D,G for a given computational budget F by solving the following optimization problem,\n\n<!-- formula-not-decoded -->\n\n## 6.1 COMPUTATIONAL COST OF GRANULARITY\n\nIt is important to acknowledge that increasing granularity can lead to some challenges in training the model, namely higher computational and communication costs and a larger memory footprint. The main component responsible for higher costs is the increase in routing operations due to a larger pool of granular experts. This increase is proportional to the value of G. For standard, non-granular MoE models ( G = 1 ), the routing overhead still exists, although it has been considered negligible.\n\nTaking into account the routing operation overhead, the number of used FLOPs F is described by the following formula:\n\n<!-- formula-not-decoded -->\n\ngiven expansion rate E , granularity G , and constants that denote FLOPs per active parameter ratio, respectively, within routing ( c r ) and within the rest of the network ( c f ). The term 12 d model 2 is the number of active parameters within a transformer block, while d model EGc r is the number of active parameters within a routing network. The in-depth analysis of constants c r and c f can be found in Appendix E. We exclude embedding and unembedding from the FLOPs calculations, following Hoffmann et al. (2022).\n\nObserve that, in contrast to scenarios where routing operations are omitted, the FLOPs calculation that incorporates routing overhead relies on both d model and n blocks . Consequently, an additional condition is required to determine the scaling of d model and n blocks in relation to an increase in N , the number of parameters. It is noted that minor variations in the depth-to-width ratio are not significant (Kaplan et al., 2020). Following this analysis, we opt to adopt the assumption that d model = 64 n blocks .\n\nThe total number of parameters in the feed-forward layer, excluding the routing matrix, is 2 Ed ff d model = 8 Ed model 2 , and 4 d model 2 in attention (key, query, value, and output projection). This results in the following formula for the total number of parameters, N = d model 2 · (8 E +4) · n blocks .\n\n## 6.2 COMPUTE OPTIMAL FORMULA\n\nTaking into consideration we need to solve the following optimization problem, given F ,\n\n<!-- formula-not-decoded -->\n\nAll these constraints are reducible to a one-dimensional optimization problem, which is, however, hard to solve analytically. Therefore we approximate the solution using Brent's method (Brent, 1971). The results of this optimization for varying FLOPs budgets are plotted in Figure 1 while the optimal configurations of parameters for selected model sizes are presented in Table 2. To validate the uncertainty of these predictions, we follow Hoffmann et al. (2022) and calculate the 10th and 90th percentiles estimated via bootstrapping data (see Appendix C for the detailed results).\n\n## 6.3 MOE IS ALWAYS MORE EFFICIENT\n\nContrary to the results from Clark et al. (2022), in Figure 1 we can see, that Mixture-of-Experts can be always considered more efficient than dense Transformers, regardless of the model size. According to our previous observations from Section 5.5, MoE models scale better with optimal training. However, for short training schedules, they may under-perform dense models. This means that for constant training time and increasing model size, there exists a point where both models will become very under-trained, in which scenario dense models surpass MoE. This shows why in Clark et al. (2022), where varying the number of training tokens has not been considered, MoE was predicted to be under-performing for models bigger than 1 T . However, when all training hyper-parameters N,D,G are properly selected to be compute-optimal for each model, the gap between dense and sparse models only increases as we scale.\n\nTable 2: Compute optimal training hyper-parameters for MoE models. Optimal N and D follow approximately similar relation to these of Hoffmann et al. (2022) for active parameters around the range of 1 B to 10 B parameters, requiring comparably longer training for smaller models and shorter for bigger ones. Higher granularity is optimal for larger compute budgets.\n\n| N         | D       |   G |    FLOPs |   Loss |\n|-----------|---------|-----|----------|--------|\n| 64 x 100M | 4.37B   |   8 | 2.95e+18 |  3.133 |\n| 64 x 1B   | 28.94B  |  16 | 1.93e+20 |  2.491 |\n| 64 x 3B   | 72.90B  |  16 | 1.41e+21 |  2.245 |\n| 64 x 7B   | 137.60B |  32 | 6.46e+21 |  2.076 |\n| 64 x 70B  | 941.07B |  32 | 4.16e+23 |  1.694 |\n| 64 x 300B | 2.96T   |  64 | 5.69e+24 |  1.503 |\n| 64 x 1T   | 7.94T   |  64 | 4.97e+25 |  1.367 |\n\n## 7 DISCUSSION\n\nExtreme Granularity. In Section 5, we argue that model performance improves with increasing granularity. This postulate largely aligns with the empirical findings of our study. Nonetheless, at exceedingly high granularity levels, such as G = 64 in models characterized by d model = 256 and E = 64 , there is an observable decline in performance. This phenomenon is particularly evident in scenarios where the number of parameters in the routing mechanism exceeds active parameters in actual experts. Additionally, as described in Section 6, the utility of such high granularity is predominantly restricted to models of substantial size. In alignment with the principles outlined by Hoffmann et al. (2022), this research focuses more on findings that can be broadly applied rather than delving into the specific details of these corner-case situations. However, it is hypothesized that the efficiency of models with significantly high granularity could be potentially enhanced through careful expert initialization or modifications to the routing algorithm. These ideas are set aside to be investigated in future studies.\n\nVarying Expansion Rate. In this study, due to computational resources constraint, we focus on E = 64 , as recommended by Clark et al. (2022). This value of E was also used for the largest models in other works (Du et al., 2022; Zhou et al., 2022) and the best-performing configuration in Fedus et al. (2022). Nonetheless, we acknowledge the importance of considering different expansion rates, as different levels of E may be chosen based on factors like the target size of the model in memory. Therefore, in Appendix D, we present the results of the study for E = 16 and show that the main findings of this work are still valid in such cases.\n\nIncluding E in the formula. Another possible advancement would be to unify all of the factors N,D,G and E in one formula. While this would open the possibility of studying the relationships between coefficients in more detail, it would also be hard to practically recommend the optimal configuration in such a scenario using only FLOPs. This is because larger values of E typically lead to better performance but also incur additional memory requirements. Therefore, the choice of expansion rate may be heavily dependent on the available hardware configuration. We leave a detailed study of these factors for future work.\n\nModeling the cost of granularity. It is important to note that the exact estimation of the training cost of MoE models is dependent on the training setup, hardware, and implementation. Specifically, increasing G can lead to higher transfer costs, depending on the adopted model of distributed training. Therefore, the precise selection of hyperparameters should be made considering these factors. In this work, we model the cost of operations using FLOPs, which is common in the Scaling Laws literature (Kaplan et al., 2020; Hoffmann et al., 2022; Frantar et al., 2023). Additionally, we would like to note that in our setup, we observe significant gains of fine-grained MoE measured as wall-clock time needed to achieve given perplexity (see Fig. 5 (b) for an example).\n\n## 8 CONCLUSIONS\n\nThis study introduces a novel hyperparameter, granularity ( G ), and underscores the significance of adjusting it for optimizing the efficiency of experts within MoE models. A central finding of this research is that a standard granularity of G = 1 is suboptimal across a broad range of FLOPs, leading to the recommendation of using higher granularity values to enhance MoE model performance and efficiency. Simultaneously, this work emphasizes the importance of varying training duration for compute-optimal settings. Consequently, both granularity and variable training length are incorporated into new scaling laws. These laws confidently demonstrate that MoE models consistently outperform dense transformers in terms of efficiency and scaling. This work not only sheds new light on the scaling laws applicable to MoE models but also provides practical guidance for improving computational efficiency in large language models. The insights are critical for the development and optimization of large-scale language models, marking a significant advancement in the field.\n\n## 9 REPRODUCIBILITY\n\nThe code used to produce the results described in this work is open-sourced and can be found at github.com/llm-random/llm-random .\n\n## ACKNOWLEDGMENTS\n\nWe would like to express sincere gratitude to Piotr Miło´ s and Tomasz Trzci´ nski for valuable feedback and to Aleksandra Weglarz for her help with graphic design.\n\nThis work was funded by IDEAS NCBR, which also provided significant computational resources a supportive research environment and direction. The research was supported by PL-Grid infrastructure (grant PLG/2023/016148). We also benefited from the Entropy cluster (hosted at the Faculty of Mathematics, Informatics and Mechanics of the University of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center grant 2022/45/N/ST6/02222, and ERC Starting Grant TOTAL. Marek Cygan was partially supported by an NCBiR grant POIR.01.01.01-00-0392/17-00.\n\n## REFERENCES\n\n- Andrea Agostinelli, Timo I. Denk, Zal´ an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Christian Frank. Musiclm: Generating music from text, 2023.\n- Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efficient large scale language modeling with mixtures of experts, 2022.\n- Richard P. Brent. An algorithm with guaranteed convergence for finding a zero of a function. Comput. J. , 14:422-425, 1971. URL https://api.semanticscholar.org/CorpusID: 10312755 .\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.\n- Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models, 2022.\n- Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.\n\n- Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.\n- Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan Chen, and Lei Jiang. Llmcarbon: Modeling the end-to-end carbon footprint of large language models, 2024.\n- William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022.\n- Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models, 2023.\n- Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation, 2021.\n- Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling, 2020.\n- Sepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory. Neural computation , 9(8): 1735-1780, 1997.\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022.\n- Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics , 35(1):73 - 101, 1964. doi: 10.1214/aoms/1177703732. URL https://doi.org/10.1214/ aoms/1177703732 .\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´ elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mistral 7b, 2023.\n- Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´ elio Renard Lavaud, Lucile Saulnier, MarieAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´ eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ ee Lacroix, and William El Sayed. Mixtral of experts, 2024.\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\n- Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020.\n- Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models, 2021.\n- Zeyu Leo Liu, Tim Dettmers, Xi Victoria Lin, Veselin Stoyanov, and Xian Li. Towards a unified view of sparse feed-forward network in pretraining large language model, 2023.\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\n\n- Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of experts, 2023.\n- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018a.\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2018b. URL https://d4mucfpksywv. cloudfront.net/better-language-models/language-models.pdf .\n- Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher, 2022.\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n- Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models, 2021.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c, Daniel Hesslow, Roman Castagn´ e, Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall´ e, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoˆ ıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenc ¸on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz´ alez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G´ erard Dupont, Germ´ an Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J¨ org Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu˜ noz, Maraim Masoud, Mar´ ıa Grandury, Mario ˇ Saˇ sko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L´ opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tas ¸ar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S.\n\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franc ¸ois Lavall´ ee, R´ emi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St´ ephane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur´ elie N´ ev´ eol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇ ek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu˜ noz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl´ ementine Fourrier, Daniel Le´ on Peri˜ n´ an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P` amies, Maria A Castillo, Marianna Nezhurina, Mario S¨ anger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th´ eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023.\n\n- Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n- Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-tensorflow: Deep learning for supercomputers, 2018.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\n\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n\n- Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2023.\n- Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.\n- Yanqi Zhou, Nan Du, Yanping Huang, Daiyi Peng, Chang Lan, Da Huang, Siamak Shakeri, David So, Andrew Dai, Yifeng Lu, Zhifeng Chen, Quoc Le, Claire Cui, James Laundon, and Jeff Dean. Brainformers: Trading simplicity for efficiency, 2023.\n\n## A ARCHITECTURE AND TRAINING SETUP\n\nAll of the models considered in this work are decoder-only Transformers trained on the C4 dataset (Raffel et al., 2023). We use GPT2 tokenizer (Radford et al., 2018a). Each batch consists of 0.5M tokens packed into 2048 sequences. Our optimizer is AdamW (Loshchilov &amp; Hutter, 2019), with a weight decay of 0 . 1 . In each training run, we use the maximum learning rate of 2e -4 , with linear warmup for 1% steps and cosine decay to 2e -5 . To improve stability, we initialize weights using the truncated normal distribution with reduced scale, as advised in Fedus et al. (2022). The models are trained using mixed precision; we always keep the attention mechanism and router in high precision. We assume the infinite data regime, as the number of training tokens for any of the runs is less than the number of tokens in the corpus. We follow Hoffmann et al. (2022) and perform our analysis on the smoothed training loss.\n\nIn MoE, we use the Expert Choice routing algorithm, as it guarantees a balanced expert load without tuning additional hyperparameters. To maintain compatibility with autoregressive language modeling, we apply the recipe described in Zhou et al. (2022): tokens are grouped by position across different sequences. The group size is always set to 256 . We match the number of FLOPs for MoE and dense models with the same d model (meaning we activate an average of 8 d 2 model parameters per token in each MoE layer). In the router, softmax is performed over the expert dimension, while we choose tokens over the token dimension, as this leads to the best performance (as opposed to performing softmax over the token dimension). We put an additional layer normalization before the output of MoE layer. This gives a small improvement for standard MoE, but is crucial for the performance of models with G &gt; 1 .\n\nTable 3 and Table 4 list the considered architecture and training variants for dense and MoE models, respectively.\n\nTable 3: Architecture and training variants (MoE models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x7M                  |       384 |          4 |         6 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 130B             | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 16B, 33B,        | 1, 2, 4, 8, 16 |\n| 64x25M                 |       512 |          8 |         8 | 66B              | 1, 2, 4, 8     |\n| 64x49M                 |       640 |         10 |        10 | 16B, 33B         | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 66B              | 1, 2, 4        |\n| 64x85M                 |       768 |         12 |        12 | 33B              | 1, 2, 4        |\n\nTable 4: Architecture and training variants (dense models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)     |\n|------------------------|-----------|------------|-----------|--------------------|\n| 3M                     |       256 |          4 |         4 | 16B, 24B, 33B, 66B |\n| 6M                     |       256 |          8 |         4 | 16B, 24B, 33B, 66B |\n| 13M                    |       512 |          4 |         8 | 16B, 24B, 33B, 66B |\n| 25M                    |       512 |          8 |         8 | 16B, 24B, 33B, 66B |\n| 49M                    |       640 |         10 |        10 | 16B, 24B, 33B, 66B |\n| 85M                    |       768 |         12 |        12 | 16B, 33B           |\n\n## B VALIDATION OF THE SCALING LAW\n\nIn this section, we provide coefficients of the scaling law fitted with 20 % of datapoints with the lowest perplexity excluded for the purpose of validation.\n\nTable 5: Values of the fitted coefficients.\n\n<!-- formula-not-decoded -->\n\n## C RELIABILITY OF COMPUTE OPTIMAL FORMULA\n\nIn this section, we assess the stability of our predictions presented in Section 6.1. Similarly to Hoffmann et al. (2022) we calculate the 10 th and 90 th percentiles estimated via bootstrapping data ( 80% of the data is sampled 100 times). See Table 6 for the details.\n\nTable 6: 10 th and 90 th percentiles estimated via bootstraping data.\n\n| N         | D                  | G        |\n|-----------|--------------------|----------|\n| 64 x 100M | (2.97B, 5.98B)     | (8, 8)   |\n| 64 x 1B   | (21.17B, 40.73B)   | (16, 16) |\n| 64 x 3B   | (50.20B, 105.88B)  | (16, 32) |\n| 64 x 7B   | (101.06B, 205.40B) | (32, 32) |\n| 64 x 70B  | (638.49B, 1.59T)   | (32, 64) |\n| 64 x 300B | (1.99T, 5.62T)     | (64, 64) |\n| 64 x 1T   | (5.29T, 16.87T)    | (64, 64) |\n\n## D VARYING EXPANSION RATE\n\nIn this section, we provide results for E = 16 . The training procedure is the same as described in App. A. The models considered in this part are listed in Table 7.\n\nTable 7: Architecture and training variants (MoE models).\n\n| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x7M                  |       256 |          8 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 66B              | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 8B               | 1, 2, 4, 8, 16 |\n\nWe fit Eq. 9 using the same procedure as described in Section 5.4. The results are detailed in Table 8.\n\nTable 8: Values of the fitted coefficients.\n\n| Model          |     a |     α |     b |     β |    g |     γ |     c |\n|----------------|-------|-------|-------|-------|------|-------|-------|\n| MoE ( E = 16 ) | 19.64 | 0.124 | 57.07 | 0.169 | 1.18 | 0.986 | 0.472 |\n\nUsing the coefficients and FLOPs calculation formulas, we can derive the compute optimal training parameters. The results are presented in Table 9.\n\nTable 9: 10 th and 90 th percentiles estimated via bootstrapping data for E = 16 .\n\n| N         | D                  | G         |\n|-----------|--------------------|-----------|\n| 16 x 100M | (10.29B, 17.73B)   | (8 , 16)  |\n| 16 x 1B   | (53.74B, 103.54B)  | (16, 32)  |\n| 16 x 3B   | (106.22B, 261.04B) | (16, 32)  |\n| 16 x 7B   | (177.65B, 511.43B) | (16, 32)  |\n| 16 x 70B  | (721.60B, 3.22T)   | (32, 64)  |\n| 16 x 300B | (1.73T, 10.69T)    | (32, 64)  |\n| 16 x 1T   | (3.60T, 28.22T)    | (32, 128) |\n\nWe can observe that similarly to the case when E = 64 , larger compute budgets imply larger optimal values of G. Note that the values for 10 th and 90 th percentiles form larger intervals in this case, as in this part we run a smaller number of experiments and keep shorter training durations. However, we believe that this preliminary study forms a valuable addition to the results in the main part.\n\n## E FLOPS CONSTANTS\n\nThe number of FLOPs F used in Transformer training, considering the routing operation overhead in MoE, can be described by the following formula:\n\n<!-- formula-not-decoded -->\n\nFollowing Hoffmann et al. (2022), we assume c f to be 6 . This is interpreted as 6 FLOPs for each pair of an active parameter (in linear projection) and a processed token. The breakdown of operations is as follows:\n\n- During the forward pass, 2 operations (single multiplication and single addition) are used to compute the matrix multiplication of an input and linear projection.\n- During the backward pass, 2 operations are used to compute gradients wrt. the input.\n- During the backward pass, 2 operations are used to compute gradients wrt. the weights of linear projection.\n\nIn our work, we have assumed the routing constant, c r , to be 14, with the breakdown presented below. The exact number of operations may depend on the implementation of routing, but it will be between 6 and 20. However, our main conclusions of the paper are resistant to different assumptions of this constant.\n\n- During the forward pass, 2 operations are used to compute the expert logits based on an input and 'routing linear projection'.\n- During the backward pass, 2 operations are used to compute gradients for 'routing linear projection' wrt. the input.\n- During the backward pass, 2 operations are used to compute gradients for 'routing linear projection' wrt. the weights of linear projection.\n- During the forward pass, 2 operations are used to route input tokens to chosen experts.\n- During the forward pass, 2 operations are used to route expert outputs to chosen tokens and multiply those outputs by the routing score.\n- During the backward pass, 2 operations are used to route gradients from output tokens to experts.\n- During the backward pass, 2 operations are used to route gradients from experts to input tokens.\n\nSimilarly to the calculation of FLOPs for c f , FLOPs come in pairs as each multiplication is followed by an addition (used to accumulate outputs or gradients).\n\n## F ADDITIONAL VISUALIZATIONS\n\nFigure 6: Illustration of scaling N and D for constant granularity value of: (a) G = 1 (b) G = 2 (c) G = 8 (d) G = 16 .\n\n<!-- image -->\n\nFigure 7: Illustration of scaling granularity when N,D are fixed for: (a) N = 64 × 25 M , D = 16 B , const = 3 . 12 (b) N = 64 × 49 M , D = 16 B , const = 3 . 02 (c) N = 64 × 25 M , D = 32 B , const = 3 . 03 (d) N = 64 × 49 M , D = 32 B , const = 2 . 88\n\n<!-- image -->",
  "tables": [
    {
      "index": 0,
      "markdown": "| Jakub Krajewski ∗ University of Warsaw IDEAS NCBR   | Jan Ludziejewski ∗ University of Warsaw IDEAS NCBR   | Kamil Adamczewski IDEAS NCBR                   | Maciej Pi´ oro IPPT PAN IDEAS NCBR                   |\n|-----------------------------------------------------|------------------------------------------------------|------------------------------------------------|------------------------------------------------------|\n| Michał Krutul University of Warsaw IDEAS NCBR       | Szymon Antoniak University of Warsaw IDEAS NCBR      | Kamil Ciebiera University of Warsaw IDEAS NCBR | Krystian Kr´ ol University of Warsaw IDEAS NCBR      |\n| Tomasz Odrzyg´ o´ zd´ z TradeLink                   | Piotr Sankowski University of Warsaw IDEAS NCBR      | Marek Cygan University of Warsaw Nomagic       | Sebastian Jaszczur ∗ University of Warsaw IDEAS NCBR |"
    },
    {
      "index": 1,
      "markdown": "| Model   |    a |     α |    b |     β | g   | γ    |    c |\n|---------|------|-------|------|-------|-----|------|------|\n| MoE     | 18.1 | 0.115 | 30.8 | 0.147 | 2.1 | 0.58 | 0.47 |\n| Dense   | 16.3 | 0.126 | 26.7 | 0.127 | -   | -    | 0.47 |"
    },
    {
      "index": 2,
      "markdown": "| N         | D       |   G |    FLOPs |   Loss |\n|-----------|---------|-----|----------|--------|\n| 64 x 100M | 4.37B   |   8 | 2.95e+18 |  3.133 |\n| 64 x 1B   | 28.94B  |  16 | 1.93e+20 |  2.491 |\n| 64 x 3B   | 72.90B  |  16 | 1.41e+21 |  2.245 |\n| 64 x 7B   | 137.60B |  32 | 6.46e+21 |  2.076 |\n| 64 x 70B  | 941.07B |  32 | 4.16e+23 |  1.694 |\n| 64 x 300B | 2.96T   |  64 | 5.69e+24 |  1.503 |\n| 64 x 1T   | 7.94T   |  64 | 4.97e+25 |  1.367 |"
    },
    {
      "index": 3,
      "markdown": "| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x7M                  |       384 |          4 |         6 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 16B, 33B, 66B    | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 130B             | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 16B, 33B,        | 1, 2, 4, 8, 16 |\n| 64x25M                 |       512 |          8 |         8 | 66B              | 1, 2, 4, 8     |\n| 64x49M                 |       640 |         10 |        10 | 16B, 33B         | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 66B              | 1, 2, 4        |\n| 64x85M                 |       768 |         12 |        12 | 33B              | 1, 2, 4        |"
    },
    {
      "index": 4,
      "markdown": "| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)     |\n|------------------------|-----------|------------|-----------|--------------------|\n| 3M                     |       256 |          4 |         4 | 16B, 24B, 33B, 66B |\n| 6M                     |       256 |          8 |         4 | 16B, 24B, 33B, 66B |\n| 13M                    |       512 |          4 |         8 | 16B, 24B, 33B, 66B |\n| 25M                    |       512 |          8 |         8 | 16B, 24B, 33B, 66B |\n| 49M                    |       640 |         10 |        10 | 16B, 24B, 33B, 66B |\n| 85M                    |       768 |         12 |        12 | 16B, 33B           |"
    },
    {
      "index": 5,
      "markdown": "| N         | D                  | G        |\n|-----------|--------------------|----------|\n| 64 x 100M | (2.97B, 5.98B)     | (8, 8)   |\n| 64 x 1B   | (21.17B, 40.73B)   | (16, 16) |\n| 64 x 3B   | (50.20B, 105.88B)  | (16, 32) |\n| 64 x 7B   | (101.06B, 205.40B) | (32, 32) |\n| 64 x 70B  | (638.49B, 1.59T)   | (32, 64) |\n| 64 x 300B | (1.99T, 5.62T)     | (64, 64) |\n| 64 x 1T   | (5.29T, 16.87T)    | (64, 64) |"
    },
    {
      "index": 6,
      "markdown": "| #parameters (nonemb)   |   d model |   n blocks |   n heads | D (in #tokens)   | G              |\n|------------------------|-----------|------------|-----------|------------------|----------------|\n| 64x3M                  |       256 |          4 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x7M                  |       256 |          8 |         4 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x13M                 |       512 |          4 |         8 | 66B              | 1, 2, 4        |\n| 64x25M                 |       512 |          8 |         8 | 8B, 16B, 33B     | 1, 2, 4, 8, 16 |\n| 64x49M                 |       640 |         10 |        10 | 8B               | 1, 2, 4, 8, 16 |"
    },
    {
      "index": 7,
      "markdown": "| Model          |     a |     α |     b |     β |    g |     γ |     c |\n|----------------|-------|-------|-------|-------|------|-------|-------|\n| MoE ( E = 16 ) | 19.64 | 0.124 | 57.07 | 0.169 | 1.18 | 0.986 | 0.472 |"
    },
    {
      "index": 8,
      "markdown": "| N         | D                  | G         |\n|-----------|--------------------|-----------|\n| 16 x 100M | (10.29B, 17.73B)   | (8 , 16)  |\n| 16 x 1B   | (53.74B, 103.54B)  | (16, 32)  |\n| 16 x 3B   | (106.22B, 261.04B) | (16, 32)  |\n| 16 x 7B   | (177.65B, 511.43B) | (16, 32)  |\n| 16 x 70B  | (721.60B, 3.22T)   | (32, 64)  |\n| 16 x 300B | (1.73T, 10.69T)    | (32, 64)  |\n| 16 x 1T   | (3.60T, 28.22T)    | (32, 128) |"
    }
  ],
  "stats": {
    "pages": 20,
    "chunksCreated": 90,
    "totalCharacters": 61925,
    "totalWords": 9812,
    "numTables": 9,
    "processingTimeMs": 27699
  }
}