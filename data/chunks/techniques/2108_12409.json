{
  "paper": {
    "id": "2108.12409v2",
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.",
    "authors": [
      "Ofir Press",
      "Noah A. Smith",
      "Mike Lewis"
    ],
    "published": "2021-08-27T17:35:06.000Z",
    "updated": "2022-04-22T18:20:48.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2108.12409v2",
    "absUrl": "https://arxiv.org/abs/2108.12409v2"
  },
  "chunks": [
    {
      "id": "2108.12409v2-chunk-0",
      "content": "Ofir Press 1 , 2 Noah A. Smith 1 , 3 Mike Lewis 2\n\n1\n\n2\n\nPaul G. Allen School of Computer Science &amp; Engineering, University of Washington Facebook AI Research\n\n3 Allen Institute for AI\n\nofirp@cs.washington.edu",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION",
        "chunkIndex": 0,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-1",
      "content": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-2",
      "content": "trapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark. 1",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 2,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-3",
      "content": "When constructing a transformer-based language model, a major design decision is the length of training sequences, denoted L herein, which has to date been equivalent to the length of inference sequences. More context, achieved by larger L , improves predictions at inference time. But longer sequences are more expensive to train on. 2\n\nBefore transformers, RNN language models were trained on shorterL sequences and assumed to generalize to longer contexts at inference time (Mikolov et al., 2010; Mikolov &amp; Zweig, 2012; Zaremba et al., 2014). Vaswani et al. (2017), introducing the transformer, speculated that it 'may [...] extrapolate to sequence lengths longer than the ones encountered during training.' We define extrapolation as a model's ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-4",
      "content": "ing.' We define extrapolation as a model's ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained. We find that transformer language models (LMs) that use sinusoidal position embeddings have very weak extrapolation abilities; see Figure 1.\n\nWe demonstrate that this failure to extrapolate is caused by the position embedding method. As shown in Figure 1, recent alternatives to the original sinusoidal position method (Su et al., 2021; Raffel et al., 2020) have improved extrapolation. However, the better of these, the T5 bias, is considerably slower than the sinusoidal approach and uses extra memory and parameters (Figure 2).\n\nWe therefore introduce Attention with Linear Biases (ALiBi) to facilitate efficient extrapolation. ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-5",
      "content": "inear Biases (ALiBi) to facilitate efficient extrapolation. ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Our simple approach eliminates position embeddings.\n\n1 Code &amp; models: https://github.com/ofirpress/attention\\_with\\_linear\\_biases\n\n2 Figure 7 in the appendix plots training speed, in words per second, against L .\n\n<!-- image -->\n\nInference Input Tokens\n\nFigure 1: Extrapolation: as the (validation-set's) input sequence gets longer ( x -axis), current position methods (sinusoidal, rotary, and T5) show degraded perplexity ( y -axis, lower is better), but our method (§3) does not. Models were trained on WikiText-103 with sequences of L = 512 (left) or L = 1,024 (right) tokens. T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables 2 and 3 in the appendix.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-6",
      "content": "WikiText-103 with sequences of L = 512 (left) or L = 1,024 (right) tokens. T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables 2 and 3 in the appendix.\n\nCompared to a sinusoidal model trained on the same input length, our method requires no additional runtime or parameters and incurs a negligible (0-0.7%) memory increase. ALiBi can be implemented by changing only a few lines of existing transformer code.\n\nUsing ALiBi, a transformer LM can be trained on shortL sequences and therefore at much lower cost, and it can still be reliably applied to long sequences at runtime. For example, a 1.3 billion parameter LM trained on L = 1024 tokens with ALiBi achieves the same perplexity as a sinusoidal model trained on L = 2048 when both are tested on sequences of 2048 tokens, even though our model is 11% faster and uses 11% less memory.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-7",
      "content": "tokens with ALiBi achieves the same perplexity as a sinusoidal model trained on L = 2048 when both are tested on sequences of 2048 tokens, even though our model is 11% faster and uses 11% less memory.\n\nThough performance peaks at around two times the number of tokens that the model was trained on, ALiBi maintains strong performance even on sequences of length 10,000. In recently explored settings where NLP training examples are given as context to an LM (Brown et al., 2020), our approach will allow exposure to more examples. Additionally, it enables generation of longer outputs.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-8",
      "content": "We show for the first time that the sinusoidal position method, which technically should be able to extrapolate, in practice has very limited extrapolation capabilities. Though the rotary position method improves over the sinusoidal one, it still does not achieve satisfying results. Holding everything else constant, we are the first to observe that the T5 bias method leads to better extrapolation than either of these, and so we conclude that extrapolation ability depends heavily on the position embedding. Unfortunately, the T5 bias is computationally costly (Figure 2).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2 CURRENT APPROACHES DO NOT EXTRAPOLATE EFFICIENTLY",
        "chunkIndex": 8,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-9",
      "content": "A transformer LM receives a list of tokens and outputs a probability distribution representing its prediction for the next token. We call the input list the current input subsequence since the inputs to language models are typically subsequences from (much longer) training or evaluation sequences. During both training and perplexity evaluation (i.e., scoring a fixed sequence), many predictions can be calculated at once; this is done using a 'causal mask' that ensures each position's prediction is influenced only by tokens to its left. Let L be the length of each input subsequence during training; it includes L predictions, which on average have access to L +1 2 tokens of (left) context. To explore a model's extrapolation abilities, we are interested in cases where sequences of length L valid &gt; L are considered at evaluation time.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.1 BACKGROUND AND EXPERIMENTAL SETUP",
        "chunkIndex": 9,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-10",
      "content": "age have access to L +1 2 tokens of (left) context. To explore a model's extrapolation abilities, we are interested in cases where sequences of length L valid &gt; L are considered at evaluation time. When L differs between inference and training, we use L to refer to the length of subsequences during training and L valid to refer to their length at validation.\n\nTraining Memory\n\nFigure 2: A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1% during training and 3% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table 1 in the appendix for exact numbers.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.1 BACKGROUND AND EXPERIMENTAL SETUP",
        "chunkIndex": 10,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-11",
      "content": "ng. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table 1 in the appendix for exact numbers.\n\n<!-- image -->\n\nNonoverlapping Inference To train on or evaluate a sequence longer than L tokens, it is typical to segment the sequence into L -length subsequences and train on or evaluate them independently. Unless otherwise stated, we use nonoverlapping inference to report perplexity scores.\n\nExtrapolation During Inference Formally, the functions that define a transformer layer are agnostic to input length; 3 they map from some arbitrary, unfixed number of input vectors to the same number of output vectors. When transformers are applied to data that is inherently sequential, like text, positional information is injected into the inputs in various ways.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.1 BACKGROUND AND EXPERIMENTAL SETUP",
        "chunkIndex": 11,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-12",
      "content": "input vectors to the same number of output vectors. When transformers are applied to data that is inherently sequential, like text, positional information is injected into the inputs in various ways.\n\nVaswani et al. (2017) discussed two options for embedding positions into vectors to be added to word embeddings: learning embeddings for specific positions and unlearned sinusoidal embeddings. They observed similar performance between these two but preferred the sinusoidal approach, which they argued might extrapolate to longer input sequences during inference. We find that this model cannot extrapolate to more than a few dozen tokens beyond L . 4\n\nExperiment Setup We first test the extrapolation abilities of various position methods on the WikiText-103 corpus (Merity et al., 2016) using the transformer language model of Baevski &amp; Auli (2018). We use this model because of its prominent role in recent language modeling developments (Khandelwal et al., 2020; Press et al., 2021).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.1 BACKGROUND AND EXPERIMENTAL SETUP",
        "chunkIndex": 12,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-13",
      "content": "ing the transformer language model of Baevski &amp; Auli (2018). We use this model because of its prominent role in recent language modeling developments (Khandelwal et al., 2020; Press et al., 2021). The training set is about 103 million tokens from English Wikipedia (half a gigabyte). The model has 16 transformer layers of dimension 1024 , with 8 heads, and a feedforward inner dimension of 4096 . This model ties the word embedding and softmax matrices (Press &amp; Wolf, 2017; Inan et al., 2017). In our experiments, other than varying the position method and training subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs (205).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.1 BACKGROUND AND EXPERIMENTAL SETUP",
        "chunkIndex": 13,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-14",
      "content": "Sinusoidal Position Embeddings Sinusoidal position embeddings (Vaswani et al., 2017; §3.5) are constant, non-learned vectors that are added to token embeddings on input to the first layer of the transformer. They are frequently used in transformer language modeling (Baevski &amp; Auli, 2018; Lewis et al., 2021) and machine translation (Vaswani et al., 2017; Ott et al., 2018) models. We first consider the unmodified model of Baevski &amp; Auli (2018), which uses sinusoidal position embeddings, and train it on L = 512 tokens; we then run inference with it on the validation set on L + k tokens, with k ranging from 0 to 15,000. Figure 1 (left) and the corresponding Table 2 (in the appendix) show that while the model improves perplexity up to k = 20 , performance stops improving and stays steady from k = 20 to k = 50 and then begins degrading. Similar results are obtained for a model trained with L = 1024 tokens (Figure 1 (right) and Table 3 in the appendix).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 14,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-15",
      "content": "stops improving and stays steady from k = 20 to k = 50 and then begins degrading. Similar results are obtained for a model trained with L = 1024 tokens (Figure 1 (right) and Table 3 in the appendix). That model improves for up to L valid = L +50 tokens, after which performance declines.\n\n3 These include the embedding lookup, feedforward sublayer, and softmax layer, which act independently on vector inputs, as well as the attention sublayers, whose parameters do not depend on input length (and which must handle variable-length inputs, e.g., due to causal masking).\n\n4 The learned positional embedding approach does not have a way to encode positions greater than L ; it therefore has no ability to extrapolate.\n\nRotary Position Embeddings The rotary method was introduced by Su et al. (2021) and has recently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPTJ (Wang &amp; Komatsuzaki, 2021).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 15,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-16",
      "content": "tion Embeddings The rotary method was introduced by Su et al. (2021) and has recently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPTJ (Wang &amp; Komatsuzaki, 2021). Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.\n\nUnlike the sinusoidal or learned positional embedding approach, the rotary method injects position information into the model at every layer, not just at the initial one. In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 16,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-17",
      "content": "transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method (§3).\n\nWe apply the rotary position embedding method to our Baevski &amp; Auli baseline. 5 The perplexity results (Figure 1 and Appendix Tables 2 and 3) are better than the sinusoidal approach: the model with L = 512 ( L = 1024 ) improves perplexity with up to k = 200 ( k = 100 ) more tokens than it saw during training, but this comes at the cost of slower training and inference (Figure 2).\n\nT5 Bias Though most models use trained or sinusoidal position embeddings, the T5 model of Raffel et al.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 17,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-18",
      "content": "it saw during training, but this comes at the cost of slower training and inference (Figure 2).\n\nT5 Bias Though most models use trained or sinusoidal position embeddings, the T5 model of Raffel et al. (2020) uses a relative position method (Shaw et al., 2018; Huang et al., 2019) that adds no position information to word embeddings (as in the previous method). Instead, it modifies the way attention values are computed. We refer to this as the 'T5 bias' method. 6 To compute attention values in the unmodified transformer, we compute the dot product of every query with every relevant key and then softmax these attention values. In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 18,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-19",
      "content": "tion values. In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key. Therefore, all query-key scores where the query and key distance are zero (i.e., the query and key represent the same token) get a specific learned bias, all scores where the query and key are one word away get a different learned bias, and so on, up to a certain point, from where multiple different distances share the same learned bias (which might be beneficial for extrapolation). As in the rotary method, the T5 bias injects position information into the model at every layer and integrates no explicit position information into the self-attention value vectors.\n\nRaffel et al. (2020) propose that the T5 bias may allow extrapolation, but they did not report experiments testing this. Here, we show that the T5 bias does allow language models to extrapolate.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 19,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-20",
      "content": "ors.\n\nRaffel et al. (2020) propose that the T5 bias may allow extrapolation, but they did not report experiments testing this. Here, we show that the T5 bias does allow language models to extrapolate. We do this by again modifying the Baevski &amp; Auli model, this time to insert the T5 bias into it. 7\n\nAs Figure 1 shows, the T5 bias improves perplexity with longer sequences than the ones it was trained on, i.e., k = 600 ( k = 800 ) extra tokens for a model trained on L = 512 ( L = 1024 ) input tokens. Unfortunately, this impressive performance comes at a cost: training is at least twice as slow as with the sinusoidal model. Therefore, this model's extrapolation ability provides no efficiency advantage. For example, to do inference on 1024 tokens, we could either train the sinusoidal model with L = 1024 or train the T5 bias model on L = 512 tokens and extrapolate to 1024 for inference.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 20,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-21",
      "content": "ncy advantage. For example, to do inference on 1024 tokens, we could either train the sinusoidal model with L = 1024 or train the T5 bias model on L = 512 tokens and extrapolate to 1024 for inference. However, the L = 1024 sinusoidal model runs at 28.5k words per second (WPS), while the L = 512 T5 bias model runs at 14.4k WPS (Appendix Table 1), so there is no speedup when training on shorter sequences with this method. 8\n\n5 Our rotary method implementation is based on the code in https://github.com/JunnYu/ RoFormer\\_pytorch , which is linked to from the official repository of Su et al. (2021): ( https: //github.com/ZhuiyiTechnology/roformer ). After we finished running our experiments with the rotary method, we were informed that the runtime of the code linked above could be optimized, making it only 2% slower than the sinusoidal approach. This optimization would not change extrapolation performance.\n\n6 This method is similar to the one used in Parikh et al. (2016, Equation 7).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 21,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-22",
      "content": "zed, making it only 2% slower than the sinusoidal approach. This optimization would not change extrapolation performance.\n\n6 This method is similar to the one used in Parikh et al. (2016, Equation 7).\n\n7 Our T5 bias implementation is based on the one used in HuggingFace Transformers (Wolf et al., 2020), which in turn is based on the official Mesh Tensorflow T5 code.\n\n8 Narang et al. (2021) benchmarked the T5 bias as being just 8.7% slower than the sinusoidal approach; thus, while always incurring a runtime penalty, this method's runtime could be faster depending on the choice of hardware and software frameworks used. Narang et al. used the Tensorflow T5 library running on TPUs, while we used the PyTorch Fairseq library running on GPUs.\n\nFigure 3: When computing attention scores for each head, our linearly biased attention method, ALiBi, adds a constant bias (right) to each attention score ( q i · k j , left).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 22,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-23",
      "content": "ibrary running on GPUs.\n\nFigure 3: When computing attention scores for each head, our linearly biased attention method, ALiBi, adds a constant bias (right) to each attention score ( q i · k j , left). As in the unmodified attention sublayer, the softmax function is then applied to these scores, and the rest of the computation is unmodified. mis a head-specific scalar that is set and not learned throughout training. We show that our method for setting m values generalizes to multiple text domains, models and training compute budgets. When using ALiBi, we do not add positional embeddings at the bottom of the network.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "2.2 MEASURING EXTRAPOLATION",
        "chunkIndex": 23,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-24",
      "content": "In the transformer model of Vaswani et al. (2017), position embeddings are added to the word embeddings at the bottom of the network. For an input subsequence of length L , the attention sublayer computes the attention scores for the i th query q i ∈ R 1 × d , ( 1 ≤ i ≤ L ) in each head, given the first i keys K ∈ R i × d , where d is the head dimension:\n\n<!-- formula-not-decoded -->\n\nThese attention scores are then multiplied by the values to return the output of the attention sublayer. 9\n\nWhen using ALiBi, we do not add position embeddings at any point in the network. The only modification we apply is after the query-key dot product, where we add a static, non-learned bias: 10\n\n<!-- formula-not-decoded -->\n\nwhere scalar m is a head-specific slope fixed before training. Figure 3 offers a visualization.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 24,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-25",
      "content": "er the query-key dot product, where we add a static, non-learned bias: 10\n\n<!-- formula-not-decoded -->\n\nwhere scalar m is a head-specific slope fixed before training. Figure 3 offers a visualization.\n\nFor our models with 8 heads, the slopes that we used are the geometric sequence: 1 2 1 , 1 2 2 , ..., 1 2 8 . For models that require 16 heads, we interpolate those 8 slopes by geometrically averaging every consecutive pair, resulting in the geometric sequence that starts at 1 √ 2 and has the ratio of 1 √ 2 : 1 2 0 . 5 , 1 2 1 , 1 2 1 . 5 , ..., 1 2 8 . In general, for n heads, our set of slopes is the geometric sequence that starts at 2 -8 n and uses that same value as its ratio.\n\nIn §4, we observe that this set of slopes works on a wide variety of text domains and model sizes. Therefore, we do not believe that it is necessary to tune these slope values every time a new model is trained on a new dataset.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 25,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-26",
      "content": "set of slopes works on a wide variety of text domains and model sizes. Therefore, we do not believe that it is necessary to tune these slope values every time a new model is trained on a new dataset. This makes our method similar to the sinusoidal approach, where the hyperparameters (the start and end of the geometric progression of wavelengths) were set once by Vaswani et al. (2017) and then reused in different models of different sizes on different datasets.\n\nALiBi has an inductive bias towards recency; it penalizes attention scores between distant query-key pairs, with the penalty increasing as the distance between a key and a query grows. The different heads increase their penalties at different rates, depending on the slope magnitude.\n\nWe initially experimented with making the slopes trainable, but this did not yield strong extrapolation results. 11 Abrief manual exploration of around ten slope sets led us to discover the set of slopes that we finally picked.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 26,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-27",
      "content": "ith making the slopes trainable, but this did not yield strong extrapolation results. 11 Abrief manual exploration of around ten slope sets led us to discover the set of slopes that we finally picked. Our main insight from this exploration is that the slope sets that work best are those with slopes in the (0 , 1) range, with the slopes' density increasing as we get closer to 0 . We also found our method to be robust to slope choice. Even randomly sampling from the exponential distribution worked well in some cases (although that method had high variance).\n\nSince ALiBi is a relative position method, we add position information at every layer to the keys and queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that these properties might be beneficial for extrapolation.\n\n9 For simplicity we omit the key, query, value and final output projections, dropout, and the scaling factor. √",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 27,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-28",
      "content": "ry methods. We hypothesize that these properties might be beneficial for extrapolation.\n\n9 For simplicity we omit the key, query, value and final output projections, dropout, and the scaling factor. √\n\n10 The ALiBi bias is not multiplied by the d k scaling factor from Equation 1 of Vaswani et al. (2017).\n\n11 In our experiments, trainable slopes also slowed down the training speed by 3%.\n\nImplementation. ALiBi is easy to implement, with all changes accomplished in a few lines of code. We implement it by modifying the mask matrix by adding the linear biases to it (in practice, when training a transformer LM, query q i attends only to keys 1 to i ; this is implemented by adding a mask matrix to the query-key dot product before the softmax operation is applied). This means that there is no runtime penalty when using our method since we add no operations to the network.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 28,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-29",
      "content": "adding a mask matrix to the query-key dot product before the softmax operation is applied). This means that there is no runtime penalty when using our method since we add no operations to the network.\n\nCompared to the sinusoidal model trained on the same input lengths, AliBi incurs a memory increase (up to 100MB in some of our experiments): in the unmodified transformer, the mask is of size L × L ; when using ALiBi, the mask is a slightly larger n × L × L (where n is the number of heads) since the linear biases added for each head uses a different slope. But, as we show, ALiBi enables training on much smaller sequences while still achieving (and occasionally surpassing) results obtained using sinusoidal embeddings on longer sequences, which saves multiple gigabytes of memory.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "3 ATTENTION WITH LINEAR BIASES (ALIBI)",
        "chunkIndex": 29,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-30",
      "content": "We first show that on WikiText103 ALiBi is efficient and enables training models with short input subsequences that outperform strong baselines even when the ALiBi models extrapolate to more than six times the number of tokens that they were trained on. We then take the same hyperparameters for our method (the set of slopes) that worked on WikiText-103 and show that - with no modification - they provide strong results on a dataset in a very different domain: books. Finally, we show that a 1.3B parameter model trained with AliBi on a much larger (461 GB) dataset with much more compute provides a superior alternative to the sinusoidal method since it achieves similar perplexity scores while running faster and using less memory (since it is trained on shorter inputs).\n\nWhile multiple alternatives to the position methods presented in Vaswani et al.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4 RESULTS",
        "chunkIndex": 30,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-31",
      "content": "achieves similar perplexity scores while running faster and using less memory (since it is trained on shorter inputs).\n\nWhile multiple alternatives to the position methods presented in Vaswani et al. (2017) have been proposed, few have been adopted in large (1B or more parameter) LMs since that setting is much more challenging than the smaller scale experiments. GPT-3 and Jurassic-1 (Lieber et al., 2021) use the learned position embedding method from Vaswani et al., and GPT-J uses the rotary method. Our results on the 1.3B parameter model show our method's ability to generalize to larger models, dataset sizes and training durations without retuning the hyperparameter.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4 RESULTS",
        "chunkIndex": 31,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-32",
      "content": "Figure 4: ALiBi models trained and evaluated on varying sequence lengths on the WikiText-103 validation set and the sinusoidal baseline (not evaluated on longer sequences). All of our models outperform the sinusoidal ones even when trained on fewer tokens. Appendix Table 5 has exact perplexities, more ALiBi models (trained on fewer tokens), and results for rotary and T5 bias models.\n\n<!-- image -->\n\nWe first develop our method on the WikiText-103 corpus (Merity et al., 2016), replacing the sinusoidal position embeddings in the language model of Baevski &amp; Auli (2018) with ALiBi.\n\nFigure 4 (and the corresponding Appendix Table 5) show our results for models trained with varying numbers of input subsequence tokens ( L ), extrapolating to longer subsequence lengths on the validation dataset. Our first observation is that, without extrapolation, for every L , our models outperform those using the sinusoidal method, sometimes by a significant amount.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 32,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-33",
      "content": "uence lengths on the validation dataset. Our first observation is that, without extrapolation, for every L , our models outperform those using the sinusoidal method, sometimes by a significant amount. For example, the Baevski &amp; Auli model achieves 18.67 ± 0.24 (std. dev.) perplexity when trained with L = 3072 input tokens, but our L = 3072 model achieves 17.60 perplexity (when both models evaluate with L valid = 3072).\n\nOur second observation is that all of our models can extrapolate, and they obtain improved perplexity scores when handling more tokens than they observed during training. For example, our model trained on 512 tokens (which achieves 19.73 perplexity when evaluating subsequences of length 512 in the development set) achieves a perplexity score of 18.40 on the development set when extrapolating to subsequences of length 3072. Surprisingly, this surpasses the score that the L = 3072 sinusoidal model obtains on the development set by a statistically significant margin.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 33,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-34",
      "content": "t set when extrapolating to subsequences of length 3072. Surprisingly, this surpasses the score that the L = 3072 sinusoidal model obtains on the development set by a statistically significant margin. Note that all our models trained on L = 512 to L = 2048 outperform the sinusoidal baseline trained on L = 3072 when extrapolating to L valid = 3072 even though those models all take much less time to train since they train on shorter subsequences (Appendix Figure 8 compares training speed to perplexity for these models)! The L = 512 model is 1.84 times faster to train and yet still outperforms the L = 3072 sinusoidal model when extrapolating to L valid = 3072 . In addition, training the L = 3072 sinusoidal model requires a GPU with more than 16 GB of memory to fit the large attention matrices, which our L = 512 outperforms even though it can be trained on a GPU with much less memory due to much smaller attention matrices.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 34,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-35",
      "content": "U with more than 16 GB of memory to fit the large attention matrices, which our L = 512 outperforms even though it can be trained on a GPU with much less memory due to much smaller attention matrices.\n\nAdditionally, Table 5 (in the appendix) also shows that, for L s of 1024 and 3072, our method performs better than the rotary and T5 bias models even when L valid = L (i.e., no extrapolation is occurring). Figure 1 (and the corresponding Appendix Tables 2 and 3) more broadly explore our method vs. the other position methods. They show that the T5 bias (the best of the baselines) improves perplexity until L valid is around 2 L , but on the WikiText-103 dataset our method continually improves perplexity until at least around 3 L , with the L = 512 model improving perplexity even when L valid exceeds 12k tokens. Even when unable to improve perplexity given longer sequences, ALiBi always maintains strong performance as more tokens are added.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 35,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-36",
      "content": "512 model improving perplexity even when L valid exceeds 12k tokens. Even when unable to improve perplexity given longer sequences, ALiBi always maintains strong performance as more tokens are added.\n\nAppendix Table 6 shows that our results on the validation set also transfer to the test set of WikiText103. Currently, almost all models that present results on WikiText-103 use sliding window evaluation (defined in §B) to compute perplexities. We apply that method to our (and to the sinusoidal, rotary and T5 bias) models in Appendix Table 7. We find that our L = 3072 model surpasses the performance of Transformer-XL (Dai et al., 2019), the Sandwich (Press et al., 2020), and Shortformer (Press et al., 2021) models. Our results are similar to the ones obtained with staged training (Press et al., 2021) but fall short of results obtained by Routing Transformer (Roy et al., 2020) and kNN-LM (Khandelwal et al., 2020).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 36,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-37",
      "content": "ur results are similar to the ones obtained with staged training (Press et al., 2021) but fall short of results obtained by Routing Transformer (Roy et al., 2020) and kNN-LM (Khandelwal et al., 2020). The methods used in those models are orthogonal to ours, and we hypothesize that combining them with ours might lead to even larger performance increases.\n\nAfter developing our method on WikiText-103, in Appendix Section A.3, we run one set of experiments on a different domain (books) using a similar model architecture and without modifying any of the ALiBi hyperparameters (the slopes) and show that our results fully transfer to this new domain. Our models are able to both surpass the sinusoidal baseline when not extrapolating while also outperforming it when extrapolating to longer sequences.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS",
        "chunkIndex": 37,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-38",
      "content": "Our final set of experiments investigates whether ALiBi transfers to a larger model trained with a larger computational budget on a larger dataset than the ones we previously used. We show that our method achieves strong results in this more challenging setting, obtaining similar performance to the sinusoidal baseline while using significantly less memory, since we train on shorter subsequences.\n\nThe dataset we choose is a combination of the datasets used to train the RoBERTa (Liu et al., 2019) implementation of BERT (Devlin et al., 2019) and the English part of the CC-100 corpus introduced in Conneau et al. (2020), for a total of 461 GB. The RoBERTa training corpus-i.e., the Toronto Book Corpus (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWebText (Gokaslan &amp; Cohen, 2019) and Stories (Trinh &amp; Le, 2018))-is 161 gigabytes, and the English part of the CC-100 corpus is 300 gigabytes. The validation set contains 649K tokens.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 38,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-39",
      "content": "16), OpenWebText (Gokaslan &amp; Cohen, 2019) and Stories (Trinh &amp; Le, 2018))-is 161 gigabytes, and the English part of the CC-100 corpus is 300 gigabytes. The validation set contains 649K tokens.\n\nOur models for this dataset have 25 transformer layers with 16 heads and a dimension of 2048, with an 8192 hidden dimension of the feedforward sublayers. These models have 1.3B parameters. We train our models for one epoch, which is 50k updates on 128 V100 GPUs.\n\nIn Figure 5 (left), we compare the validation perplexity for L valid = 1024 throughout the training process for an ALiBi model trained with L = 512 compared to the sinusoidal model trained with L = 1024. Since our model is trained on shorter sequences, it is 7% faster and uses 1.6 GB less\n\nFigure 5: On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 39,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-40",
      "content": "igure 5: On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens. The ALiBi models obtain strong results even though they use 6%-11% less memory since they train on shorter sequences. Appendix Table 11 shows memory use and end-of-training perplexities.\n\n<!-- image -->\n\nmemory. We halt training of the sinusoidal baseline when our model reaches the end of its training (one epoch). At that time, our model is just 0.06 perplexity away from the baseline even though it was trained on sequences that are half the length of those the baseline used and requires less memory.\n\nIn Figure 5 (right), results become even more impressive, showing that our model trained on L = 1024 outperforms by 0.09 perplexity the sinusoidal model trained on L = 2048 (when evaluating with L valid = 2048) even though our model uses 3.1 GB less memory.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 40,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-41",
      "content": "showing that our model trained on L = 1024 outperforms by 0.09 perplexity the sinusoidal model trained on L = 2048 (when evaluating with L valid = 2048) even though our model uses 3.1 GB less memory. Our model maintains a lead in perplexity over the sinusoidal model during the entire training process. By sampling five evenly distributed points across the training process, we compute that our L = 1024 model reaches a given perplexity value, on average, 11% faster than the sinusoidal model does.\n\nSince our models in these comparisons use much less memory, they allow for stacking more layers, which would further improve performance (with negligible, if any, runtime cost). To keep our experiments as straightforward as possible, however, we do not add layers to our models.\n\nAppendix Table 12 presents additional results comparing our models to the sinusoidal baseline when both are trained on the same L , showing that ALiBi performs similarly to the sinusoidal baseline when not extrapolating",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 41,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-42",
      "content": "resents additional results comparing our models to the sinusoidal baseline when both are trained on the same L , showing that ALiBi performs similarly to the sinusoidal baseline when not extrapolating. This contrasts with the results presented on the smaller datasets, where ALiBi consistently outperforms other position methods even when not extrapolating, suggesting that ALiBi's inductive bias provides additional benefits for lower-resource language modeling.\n\n<!-- image -->\n\nValidation Input Length (\n\nLvalid\n\n)\n\nFigure 6: The ALiBi and sinusoidal models (with both L = 512 and 1024) trained for 50k updates (1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. ALiBi achieves the best results at around 2 L but maintains strong performance even up to 10000 tokens in these experiments.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 42,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-43",
      "content": "(1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. ALiBi achieves the best results at around 2 L but maintains strong performance even up to 10000 tokens in these experiments.\n\nFigure 6 shows that our models trained on L = 512 and L = 1024 achieve the best results when extrapolating to about double the tokens that they were trained on. Specifically, the L = 512 model (that obtains 9.79 perplexity when L valid = 512) achieves its best score (9.3) when extrapolating to\n\n1012 tokens, and the L = 1024 model (that obtains 9.16 perplexity when L valid = 1024) achieves its best score (8.9) when extrapolating to 2024 tokens.\n\nOne possible explanation is that the subsequences the model observes during training are up to L tokens long. When performing inference on subsequences of length 2 L , half of the subsequences the model consumes are as long as the examples seen during training.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 43,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-44",
      "content": "bserves during training are up to L tokens long. When performing inference on subsequences of length 2 L , half of the subsequences the model consumes are as long as the examples seen during training. When inference is performed on subsequences of length 2 L +1 or longer, less than half of the predictions the model makes are on subsequences of lengths seen during training, and that might degrade performance.\n\nThe sinusoidal model cannot extrapolate at all in this setting, with its performance degrading for both the L = 512 and 1024 models as soon as one token more than L is added during evaluation.\n\nIn Appendix B, we find that ALiBi's edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse. We posit that future work building on ALiBi might achieve further gains by more efficiently exploiting longer histories.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "4.2 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 44,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-45",
      "content": "In parallel with our work, Wennberg &amp; Henter (2021) introduce a relative position method that, like our method, adds a bias to attention scores that is a function of the distance between the key and query elements. Unlike our ALiBi method, which uses a non-learned linear function, their method uses a radial-basis function, with multiple trainable parameters (in our experiments, this led to a slight decrease in runtime). In addition, they present experiments on text classification, not on language modeling. They do not explore extrapolation. The Distance Aware Transformer (Wu et al., 2021) multiplies attention scores by a bias that is a function of the distance between the key and query. This function uses a different, learned parameter in every head. They show results only on text classification. In our experiments (not presented), multiplying attention scores by the bias (instead of adding, as in ALiBi) degraded performance.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 45,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-46",
      "content": "ameter in every head. They show results only on text classification. In our experiments (not presented), multiplying attention scores by the bias (instead of adding, as in ALiBi) degraded performance.\n\nTransformer-XL (Dai et al., 2019) presented a language model that uses a cache and can attend to more tokens during inference than it was trained on (by increasing the length of the cache). However, this work presents results only where output length is limited to the L (the training length), and their relative position method is very slow (Press et al., 2021). The Longformer (Beltagy et al., 2020) adapts models trained on shorter sequences to document-level tasks. However, to achieve this they had to partially train their models on longer sequences. Our ALiBi method enables extrapolation without any additional training on longer sequences.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 46,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-47",
      "content": "document-level tasks. However, to achieve this they had to partially train their models on longer sequences. Our ALiBi method enables extrapolation without any additional training on longer sequences.\n\nTo our knowledge, extrapolation has not been previously explored in transformer language modeling, but it has been investigated previously and concurrently with transformers on other tasks, such as machine translation (Rosendahl et al., 2019; Neishi &amp; Yoshinaga, 2019; Newman et al., 2020; Kiyono et al., 2021), sequence-to-sequence models trained on an artificial dataset (Hupkes et al., 2020), pretrained sequence-to-sequence models tested on arithmetic tasks (Nogueira et al., 2021, Appendix C), models trained with reinforcement learning (Lampinen et al., 2021), image, speech recognition, and machine translation models (Likhomanenko et al., 2021), and protein structure prediction (Jumper et al., 2021, Appendix 1.5).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 47,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-48",
      "content": "and machine translation models (Likhomanenko et al., 2021), and protein structure prediction (Jumper et al., 2021, Appendix 1.5).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 48,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-49",
      "content": "We showed that the sinusoidal position embedding approach does not enable transformers to extrapolate to inputs longer than the ones they were trained on. We then established that extrapolation in transformers can be enabled by just changing the position method. We showed that our ALiBi method offers an extremely simple replacement for existing position approaches and allow models to extrapolate. In addition, when not extrapolating, our method achieves either better perplexity than the sinusoidal method (in models smaller than 1B parameters, trained on less data) or similar perplexity (in larger, billion parameter models trained on much more data). ALiBi is simple to implement and does not slow down runtime or require extra parameters (but does occasionally require a negligible amount of extra memory). Using our method, we sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "6 CONCLUSION",
        "chunkIndex": 49,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-50",
      "content": "We thank Tim Dettmers, Gabriel Ilharco, Jungo Kasai, Hao Peng, Sewon Min, Sofia Serrano, Sam Shleifer, Luke Zettlemoyer, Julian Michael, Nikolaos Pappas, Yizhong Wang, and the anonymous reviewers for their valuable feedback and fruitful discussions.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "ACKNOWLEDGMENTS",
        "chunkIndex": 50,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-51",
      "content": "- Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. CoRR , abs/1809.10853, 2018. URL http://arxiv.org/abs/1809.10853 .\n- Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150 , 2020.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´ an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 51,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-52",
      "content": "e few-shot learners, 2020.\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´ an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020. doi: 10.18653/v1/2020.acl-main.747. URL http://dx.doi.org/10.18653/v1/2020.acl-main.747 .\n- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285 .\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 52,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-53",
      "content": "nce, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285 .\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423 .\n- Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus , 2019.\n- Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam M. Shazeer, Andrew M. Dai, M. Hoffman, M. Dinculescu, and D. Eck. Music transformer: Generating music with long-term structure.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 53,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-54",
      "content": "uang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam M. Shazeer, Andrew M. Dai, M. Hoffman, M. Dinculescu, and D. Eck. Music transformer: Generating music with long-term structure. In ICLR , 2019.\n- Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research , 67:757-795, April 2020. doi: 10.1613/jair.1.11674. URL https://doi.org/10.1613/jair.1. 11674 .\n- Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In ICLR , 2017. URL https://openreview.net/ forum?id=r1aPbsFle .\n- J. Jumper, Richard Evans, A. Pritzel, Tim Green, Michael Figurnov, O. Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Z´ ıdek, Anna Potapenko, A. Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, A. Cowie, B. Romera-Paredes, Stanislav Nikolov, Rishub Jain, J. Adler, T. Back, Stig Petersen, D.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 54,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-55",
      "content": "ss Bates, Augustin Z´ ıdek, Anna Potapenko, A. Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, A. Cowie, B. Romera-Paredes, Stanislav Nikolov, Rishub Jain, J. Adler, T. Back, Stig Petersen, D. Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, S. Bodenstein, D. Silver, Oriol Vinyals, A. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with alphafold. Nature , 596:583 - 589, 2021.\n- Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR) , 2020.\n\n- Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. ArXiv , abs/2109.05644, 2021.\n- Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, and Felix Hill.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 55,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-56",
      "content": "i, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. ArXiv , abs/2109.05644, 2021.\n- Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, and Felix Hill. Towards mental time travel: a hierarchical memory for reinforcement learning agents. CoRR , abs/2105.14039, 2021. URL https://arxiv.org/abs/2105.14039 .\n- Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models, 2021.\n- Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, August 2021.\n- Tatiana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve, and Alex Rogozhnikov. CAPE: encoding relative positions with continuous augmented positional embeddings. CoRR , abs/2106.03143, 2021.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 56,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-57",
      "content": "iana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve, and Alex Rogozhnikov. CAPE: encoding relative positions with continuous augmented positional embeddings. CoRR , abs/2106.03143, 2021. URL https://arxiv.org/abs/2106.03143 .\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\n- Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n- Tomas Mikolov and G. Zweig. Context dependent recurrent neural network language model. 2012 IEEE Spoken Language Technology Workshop (SLT) , pp. 234-239, 2012.\n- Tomas Mikolov, M. Karafi´ at, L. Burget, J. Cernock´ y, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH , 2010.\n- Sebastian Nagel. Cc-news.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 57,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-58",
      "content": "(SLT) , pp. 234-239, 2012.\n- Tomas Mikolov, M. Karafi´ at, L. Burget, J. Cernock´ y, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH , 2010.\n- Sebastian Nagel. Cc-news. https://commoncrawl.org/2016/10/ news-dataset-available/ , 2016.\n- Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across implementations and applications?, 2021.\n- Masato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , pp. 328-338, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1031.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 58,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-59",
      "content": "ceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , pp. 328-338, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1031. URL https://aclanthology.org/K19-1031 .\n- Benjamin Newman, John Hewitt, Percy Liang, and Christopher D. Manning. The eos decision and length extrapolation. In BlackBoxNLP@EMNLP , 2020. URL https://nlp.stanford. edu/pubs/newman2020extrapolation.pdf .\n- Rodrigo Nogueira, Zhiying Jiang, and Jimmy J. Li. Investigating the limitations of the transformers with simple arithmetic tasks. ArXiv , abs/2102.13019, 2021.\n- Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation (WMT) , 2018.\n- Ankur Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 59,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-60",
      "content": "ings of the Third Conference on Machine Translation (WMT) , 2018.\n- Ankur Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2249-2255, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https: //aclanthology.org/D16-1244 .\n- Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers , pp. 157-163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/E17-2025 .",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 60,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-61",
      "content": "Association for Computational Linguistics: Volume 2, Short Papers , pp. 157-163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/E17-2025 .\n\n- Ofir Press, Noah A. Smith, and Omer Levy. Improving transformer models by reordering their sublayers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 2996-3005, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.270. URL https://www.aclweb.org/anthology/2020. acl-main.270 .\n- Ofir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 5493-5505, Online, August 2021. Association for Computational Linguistics.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 61,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-62",
      "content": "tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 5493-5505, Online, August 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.acl-long.427 .\n- Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id= SylKikSYDH .\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research , 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html .\n- Jan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional encodings for neural machine translation.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 62,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-63",
      "content": ", 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html .\n- Jan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional encodings for neural machine translation. In International Workshop on Spoken Language Translation , Hong Kong, China, November 2019.\n- Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers, 2020.\n- Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pp. 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18-2074 .\n- Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 63,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-64",
      "content": "Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18-2074 .\n- Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n- Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning, 2018.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n- Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\n- Ulme Wennberg and Gustav Eje Henter.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 64,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-65",
      "content": ".\n- Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\n- Ulme Wennberg and Gustav Eje Henter. The case for translation-invariant self-attention in transformer-based language models, 2021.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6 .",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 65,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-66",
      "content": "l Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6 .\n\n- Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. DA-transformer: Distance-aware transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2059-2068, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.166. URL https://aclanthology.org/2021.naacl-main.166 .\n- Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization, 2014.\n- Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pp.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 66,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-67",
      "content": ", and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pp. 19-27, 2015.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "REFERENCES",
        "chunkIndex": 67,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-68",
      "content": "The training speed of transformer LMs gets slower as the input subsequence length L increases. Figure 7 visualizes this.\n\nFigure 7: Training speed of our model and the sinusoidal baseline trained on different amounts of input subsequence tokens L .\n\n<!-- image -->\n\nTable 1 contains the runtimes and memory use statistics for models using the various position methods discussed in this work.\n\nTable 1: The speed (during training and evaluation, in words per second) and memory usage (during training) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline on WikiText103. Training and inference are batched, and speeds are shown for one V100 GPU.\n\n| Position Method   | Train Length   | Speed ( ↑ )   | Speed ( ↑ )   | Memory ( ↓ )   |\n|-------------------|----------------|---------------|---------------|----------------|\n|                   |                | Train         | Eval.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 68,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-69",
      "content": "| Speed ( ↑ )   | Speed ( ↑ )   | Memory ( ↓ )   |\n|-------------------|----------------|---------------|---------------|----------------|\n|                   |                | Train         | Eval.         |                |\n|                   | 512            | 28.5k         | 82.1k         | 15.3 GB        |\n| Sinusoidal        | 1024           | 26.0k         | 77.8k         | 19.2 GB        |\n|                   | 3072           | 15.3k         | 42.4k         | 15.1 GB        |\n|                   | 512            | 20.0k         | 43.4k         | 17.8 GB        |\n| Rotary            | 1024           | 17.7k         | 39.4k         | 22.8 GB        |\n|                   | 3072           | 11.5k         | 29.5k         | 17.8 GB        |\n|                   | 512            | 14.4k         | 21.8k         | 16.9 GB        |\n| T5 Bias           | 1024           | 13.0k         | 20.2k         | 20.9 GB        |\n|                   | 3072           | 4.3k          | 4.9k",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 69,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-70",
      "content": "| 21.8k         | 16.9 GB        |\n| T5 Bias           | 1024           | 13.0k         | 20.2k         | 20.9 GB        |\n|                   | 3072           | 4.3k          | 4.9k          | 15.9 GB        |\n|                   | 512            | 28.3k         | 85.8k         | 15.3 GB        |\n| ALiBi             | 1024           | 25.8k         | 76.4k         | 19.3 GB        |\n|                   | 3072           | 15.5k         | 42.2k         | 15.2 GB        |\n\nTables 2, 3, and 4 show the perplexity and runtime of models using the sinusoidal, rotary T5 bias, and ALiBi position methods when extrapolating to sequences longer than the ones they were trained on. The models used in these tables were trained on L = 512 , 1024 and 3072 tokens.\n\nTable 2: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 512 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 70,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-71",
      "content": "e 2: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 512 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 512      | 20.05        | 15046        | 20.07     | 10839     | 19.65     | 11724     | 19.73     | 14726     |\n| 513      | 19.98        | 14925        | 20.01     | 10806     | 19.57     | 10491     | 19.62     | 14965     |\n| 522      | 19.93        | 15116        | 20.02     | 11295     | 19.57     | 9970      | 19.64     | 15316     |\n| 532      | 19.9",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 71,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-72",
      "content": "1     | 10806     | 19.57     | 10491     | 19.62     | 14965     |\n| 522      | 19.93        | 15116        | 20.02     | 11295     | 19.57     | 9970      | 19.64     | 15316     |\n| 532      | 19.91        | 15358        | 19.98     | 10854     | 19.53     | 10382     | 19.61     | 15383     |\n| 542      | 19.91        | 15076        | 19.94     | 10795     | 19.47     | 12270     | 19.57     | 15301     |\n| 552      | 19.91        | 16394        | 19.93     | 12267     | 19.47     | 13000     | 19.54     | 16540     |\n| 562      | 19.91        | 16646        | 19.87     | 12481     | 19.39     | 12201     | 19.49     | 16385     |\n| 572      | 19.95        | 16934        | 19.83     | 12668     | 19.36     | 12851     | 19.46     | 16881     |\n| 582      | 20.13        | 16961        | 19.88     | 12594     | 19.41     | 13904     | 19.48     | 17064     |\n| 592      | 20.18        | 17243        | 19.84     | 13007     | 19.36     | 13706     | 19.43     | 17289     |\n| 602      |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 72,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-73",
      "content": "19.88     | 12594     | 19.41     | 13904     | 19.48     | 17064     |\n| 592      | 20.18        | 17243        | 19.84     | 13007     | 19.36     | 13706     | 19.43     | 17289     |\n| 602      | 20.40        | 17502        | 19.81     | 12788     | 19.33     | 14102     | 19.38     | 17141     |\n| 612      | 20.59        | 17637        | 19.81     | 12601     | 19.27     | 14573     | 19.38     | 17661     |\n| 712      | 24.86        | 15614        | 19.79     | 12676     | 19.10     | 13818     | 19.14     | 15637     |\n| 812      | 30.82        | 17151        | 20.17     | 13954     | 18.94     | 14377     | 18.99     | 17210     |\n| 912      | 37.42        | 17200        | 20.73     | 13887     | 18.86     | 15345     | 18.88     | 17619     |\n| 1012     | 43.54        | 16304        | 21.37     | 13759     | 18.79     | 14240     | 18.73     | 16059     |\n| 1112     | 50.36        | 16424        | 22.01     | 13891     | 18.77     | 14014     | 18.68     | 16659     |\n| 1212",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 73,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-74",
      "content": "| 21.37     | 13759     | 18.79     | 14240     | 18.73     | 16059     |\n| 1112     | 50.36        | 16424        | 22.01     | 13891     | 18.77     | 14014     | 18.68     | 16659     |\n| 1212     | 58.01        | 17294        | 23.02     | 15245     | 18.87     | 14589     | 18.67     | 17372     |\n| 1312     | 63.62        | 15314        | 23.93     | 13698     | 18.84     | 13138     | 18.60     | 15698     |\n| 1412     | 70.75        | 15663        | 24.81     | 13928     | 18.87     | 12857     | 18.59     | 15860     |\n| 1512     | 76.23        | 15812        | 25.99     | 14248     | 18.91     | 13752     | 18.52     | 16225     |\n| 2512     | 132.41       | 15254        | 31.58     | 13456     | 20.41     | 9948      | 18.41     | 15204     |\n| 3512     | 178.97       | 13293        | 35.54     | 11850     | 22.91     | 7847      | 18.40     | 13329     |\n| 4512     | 209.37       | 11767        | 39.15     | 10485     | 25.91     | 6146      | 18.41     | 11738     |\n|",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 74,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-75",
      "content": "3        | 35.54     | 11850     | 22.91     | 7847      | 18.40     | 13329     |\n| 4512     | 209.37       | 11767        | 39.15     | 10485     | 25.91     | 6146      | 18.41     | 11738     |\n| 5512     | 240.44       | 10168        | 43.14     | 9020      | 29.54     | 5309      | 18.36     | 9986      |\n| 6512     | 271.40       | 9052         | 47.81     | 8108      | 34.48     | 4680      | 18.35     | 9022      |\n| 7512     | 293.02       | 8315         | 51.12     | 7483      | 39.29     | 4102      | 18.33     | 8324      |\n| 8512     | 305.65       | 7259         | 54.98     | 6718      | 43.08     | 3660      | 18.34     | 7366      |\n| 9512     | 336.02       | 6672         | 57.85     | 6211      | 48.90     | 3370      | 18.34     | 6555      |\n| 10512    | 341.53       | 6126         | 60.77     | 5575      | 52.95     | 3010      | 18.32     | 6030      |\n| 11512    | 362.74       | 5994         | 66.62     | 5445      | 61.38     | 2873      | 18.32     | 5882",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 75,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-76",
      "content": "6126         | 60.77     | 5575      | 52.95     | 3010      | 18.32     | 6030      |\n| 11512    | 362.74       | 5994         | 66.62     | 5445      | 61.38     | 2873      | 18.32     | 5882      |\n| 12512    | 373.17       | 5421         | 69.70     | 4988      | 64.94     | 2602      | 18.31     | 5287      |\n| 13512    | 382.91       | 5174         | 73.27     | 4692      | OOM       | -         | 18.31     | 4962      |\n| 14512    | 399.98       | 4351         | 75.52     | 3969      | OOM       | -         | 18.31     | 4352      |\n| 15512    | 406.01       | 4291         | 79.25     | 4103      | OOM       | -         | 18.31     | 4289      |\n\nTable 3: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 1024 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 76,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-77",
      "content": "d evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 1024     | 19.34        | 17002        | 19.33     | 14690     | 18.80     | 14973     | 18.66     | 16951     |\n| 1025     | 19.33        | 16630        | 19.34     | 14423     | 18.82     | 14635     | 18.67     | 16690     |\n| 1034     | 19.27        | 16589        | 19.28     | 14351     | 18.74     | 14435     | 18.60     | 16707     |\n| 1044     | 19.26        | 16760        | 19.27     | 14491     | 18.72     | 14644     | 18.60     | 16667",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 77,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-78",
      "content": "| 16589        | 19.28     | 14351     | 18.74     | 14435     | 18.60     | 16707     |\n| 1044     | 19.26        | 16760        | 19.27     | 14491     | 18.72     | 14644     | 18.60     | 16667     |\n| 1054     | 19.23        | 16747        | 19.26     | 14503     | 18.71     | 14800     | 18.58     | 16833     |\n| 1064     | 19.21        | 16676        | 19.22     | 14623     | 18.70     | 14498     | 18.55     | 16941     |\n| 1074     | 19.19        | 16879        | 19.19     | 14464     | 18.65     | 14670     | 18.49     | 16936     |\n| 1084     | 19.22        | 16942        | 19.23     | 14650     | 18.70     | 14607     | 18.56     | 17090     |\n| 1094     | 19.24        | 16771        | 19.22     | 14629     | 18.69     | 14517     | 18.54     | 16880     |\n| 1104     | 19.28        | 16870        | 19.27     | 14837     | 18.69     | 14635     | 18.52     | 17009     |\n| 1114     | 19.29        | 16795        | 19.27     | 14879     | 18.69     | 14540     | 18.52     |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 78,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-79",
      "content": "| 16870        | 19.27     | 14837     | 18.69     | 14635     | 18.52     | 17009     |\n| 1114     | 19.29        | 16795        | 19.27     | 14879     | 18.69     | 14540     | 18.52     | 17050     |\n| 1124     | 19.26        | 17312        | 19.18     | 15121     | 18.62     | 14480     | 18.46     | 17571     |\n| 1224     | 20.54        | 17901        | 19.38     | 15584     | 18.58     | 14956     | 18.40     | 18013     |\n| 1324     | 23.13        | 16308        | 19.96     | 14386     | 18.52     | 13726     | 18.33     | 16422     |\n| 1424     | 26.45        | 16217        | 21.27     | 14385     | 18.48     | 13516     | 18.28     | 16121     |\n| 1524     | 29.82        | 16377        | 22.59     | 14693     | 18.42     | 13587     | 18.22     | 16659     |\n| 1624     | 34.27        | 15928        | 24.34     | 14228     | 18.40     | 12979     | 18.17     | 16053     |\n| 1724     | 38.24        | 16640        | 25.66     | 14686     | 18.35     | 12976     | 18.15",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 79,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-80",
      "content": "34.27        | 15928        | 24.34     | 14228     | 18.40     | 12979     | 18.17     | 16053     |\n| 1724     | 38.24        | 16640        | 25.66     | 14686     | 18.35     | 12976     | 18.15     | 16607     |\n| 1824     | 42.23        | 16840        | 27.63     | 14918     | 18.30     | 13071     | 18.08     | 16846     |\n| 1924     | 46.46        | 15071        | 29.64     | 13452     | 18.31     | 11843     | 18.08     | 15118     |\n| 2024     | 51.09        | 15591        | 31.17     | 13706     | 18.34     | 11906     | 18.05     | 15557     |\n| 3024     | 96.46        | 13639        | 35.67     | 12256     | 18.62     | 8480      | 17.92     | 13668     |\n| 4024     | 144.00       | 12441        | 44.30     | 11203     | 19.44     | 7443      | 17.95     | 12402     |\n| 5024     | 182.31       | 11431        | 48.31     | 10324     | 20.47     | 6384      | 17.92     | 11394     |\n| 6024     | 214.02       | 10238        | 54.78     | 9117      | 21.76     | 5577      | 18",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 80,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-81",
      "content": "| 182.31       | 11431        | 48.31     | 10324     | 20.47     | 6384      | 17.92     | 11394     |\n| 6024     | 214.02       | 10238        | 54.78     | 9117      | 21.76     | 5577      | 18.01     | 10119     |\n| 7024     | 261.86       | 8785         | 62.83     | 7950      | 23.64     | 4867      | 17.93     | 8779      |\n| 8024     | 284.88       | 8132         | 64.91     | 7355      | 25.79     | 4377      | 17.96     | 8086      |\n| 9024     | 310.04       | 7045         | 71.91     | 6380      | 27.54     | 3787      | 17.98     | 7001      |\n| 10024    | 337.48       | 6633         | 77.70     | 6016      | 29.54     | 3582      | 17.97     | 6583      |\n| 11024    | 358.43       | 5722         | 81.15     | 5219      | 31.94     | 3170      | 18.02     | 5641      |\n| 12024    | 375.95       | 5560         | 87.51     | 5072      | 33.35     | 2940      | 18.01     | 5294      |\n| 13024    | 393.57       | 4691         | 94.74     | 4383      | OOM       | -",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 81,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-82",
      "content": "2024    | 375.95       | 5560         | 87.51     | 5072      | 33.35     | 2940      | 18.01     | 5294      |\n| 13024    | 393.57       | 4691         | 94.74     | 4383      | OOM       | -         | 17.98     | 4621      |\n| 14024    | 403.52       | 4905         | 96.10     | 4546      | OOM       | -         | 18.01     | 4827      |\n| 15024    | 431.66       | 4518         | 99.78     | 3878      | OOM       | -         | 17.96     | 4447      |\n| 16024    | 453.32       | 4239         | 106.99    | 4170      | OOM       | -         | 17.98     | 4153      |\n\nTable 4: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 3072 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 82,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-83",
      "content": "d evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n|        | Sinusoidal    | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi       | ALiBi     |\n|--------|---------------|--------------|-----------|-----------|-----------|-----------|-------------|-----------|\n| Inputs | PPL ( ↓ )     | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ )   | WPS ( ↑ ) |\n| 3072   | 18.67         | 13380        | 18.57     | 12548     | 18.01     | 8828      | 17.60       | 13866     |\n| 3073   | 18.67         | 13773        | 18.57     | 12474     | 18.01     | 8483      | 17.59       | 13793     |\n| 3082   | 18.62         | 13741        | 18.54     | 12388     | 17.95     | 8698      | 17.59       | 13778     |\n| 3092   | 18.60         | 13742        | 18.48     | 12458     | 17.92     | 8361      | 17.55",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 83,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-84",
      "content": "| 13741        | 18.54     | 12388     | 17.95     | 8698      | 17.59       | 13778     |\n| 3092   | 18.60         | 13742        | 18.48     | 12458     | 17.92     | 8361      | 17.55       | 13783     |\n| 3102   | 18.65         | 13701        | 18.52     | 12365     | 17.94     | 8764      | 17.59       | 13747     |\n| 3112   | 18.64         | 13809        | 18.51     | 12449     | 17.96     | 8665      | 17.59       | 13827     |\n| 3122   | 18.68         | 13722        | 18.52     | 12432     | 17.98     | 8437      | 17.58       | 13795     |\n| 3132   | 18.67         | 13825        | 18.54     | 12490     | 17.97     | 8653      | 17.58       | 13784     |\n| 3142   | 18.69         | 13543        | 18.52     | 12230     | 17.97     | 8282      | 17.61       | 13572     |\n| 3152   | 18.66         | 13520        | 18.56     | 12240     | 17.98     | 8608      | 17.59       | 13523     |\n| 3162   | 18.71         | 13501        | 18.56     | 12253     | 18.04     | 8589      |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 84,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-85",
      "content": "| 18.66         | 13520        | 18.56     | 12240     | 17.98     | 8608      | 17.59       | 13523     |\n| 3162   | 18.71         | 13501        | 18.56     | 12253     | 18.04     | 8589      | 17.62       | 13598     |\n| 3172   | 18.72         | 13563        | 18.55     | 12297     | 17.99     | 8583      | 17.59       | 13625     |\n| 3272   | 18.87         | 13453        | 18.55     | 12148     | 17.93     | 8144      | 17.59       | 13482     |\n| 3372   | 19.46         | 13533        | 18.50     | 12254     | 17.88     | 8442      | 17.52       | 13565     |\n| 3472   | 20.55         | 13047        | 18.52     | 11868     | 17.95     | 7857      | 17.54       | 13107     |\n| 3572   | 21.84         | 13128        | 18.50     | 11882     | 17.86     | 7814      | 17.50       | 13170     |\n| 3672   | 23.04         | 13106        | 18.49     | 11859     | 17.87     | 7719      | 17.48       | 13196     |\n| 3772   | 24.47         | 13287        | 18.54     | 11942     | 17.85     |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 85,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-86",
      "content": "|\n| 3672   | 23.04         | 13106        | 18.49     | 11859     | 17.87     | 7719      | 17.48       | 13196     |\n| 3772   | 24.47         | 13287        | 18.54     | 11942     | 17.85     | 7579      | 17.49       | 13312     |\n| 3872   | 25.85         | 12621        | 18.40     | 11272     | 17.82     | 7581      | 17.41       | 12566     |\n| 3972   | 27.21         | 12379        | 18.48     | 11151     | 17.84     | 7483      | 17.41       | 12324     |\n| 4072   | 28.59         | 12178        | 18.59     | 11019     | 17.88     | 6974      | 17.48       | 12212     |\n| 5072   | 45.53         | 11076        | 18.80     | 9887      | 17.76     | 6230      | 17.33       | 10938     |\n| 6072   | 65.01         | 10114        | 19.50     | 9049      | 17.68     | 5554      | 17.26       | 10133     |\n| 7072   | 85.96         | 8647         | 20.60     | 7861      | 17.83     | 4820      | 17.22       | 8670      |\n| 8072   | 102.74        | 7755         | 21.60     | 6991      |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 86,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-87",
      "content": "| 10133     |\n| 7072   | 85.96         | 8647         | 20.60     | 7861      | 17.83     | 4820      | 17.22       | 8670      |\n| 8072   | 102.74        | 7755         | 21.60     | 6991      | 18.06     | 4281      | 17.30       | 7729      |\n| 9072   | 125.99        | 6953         | 22.14     | 6360      | 18.12     | 3823      | 17.26       | 6939      |\n| 10072  | 133.68        | 6646         | 23.21     | 6068      | 18.37     | 3579      | 17.28       | 6597      |\n| 11072  | 161.29        | 5663         | 24.39     | 5158      | 18.64     | 3119      | 17.26       | 5585      |\n| 12072  | 169.55        | 5567         | 26.70     | 5111      | 18.93     | 2920      | 17.24       | 5397      |\n| 13072  | 189.43        | 5044         | 29.33     | 4658      | 19.10     | 2735      | 17.15       | 4809      |\n| 14072  | 203.86        | 4915         | 32.21     | 4616      | OOM       | -         | 17.22       | 4866      |\n| 16072  |               | 4382         | 34.51     |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 87,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-88",
      "content": "| 17.15       | 4809      |\n| 14072  | 203.86        | 4915         | 32.21     | 4616      | OOM       | -         | 17.22       | 4866      |\n| 16072  |               | 4382         | 34.51     | 4099      | OOM       | -         |             | 4491      |\n| 15072  | 221.14 231.29 | 4561         | 33.47     | 4292      | OOM       | -         | 17.23 17.22 | 4312      |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.1 INTRODUCTION",
        "chunkIndex": 88,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-89",
      "content": "Figure 8: The training speed and validation perplexity (with L valid = 3072) for ALiBi models and the sinusoidal model trained with L = 3072. All our models trained on 512 or more tokens achieve better perplexity than the sinusoidal model even though all of them (except the L = 3072) require less time and memory to train.\n\n<!-- image -->\n\nFigure 8 depicts a cross section of Figure 4, showing our models with different train lengths and the sinusoidal baseline, all evaluated on L valid = 3072 tokens. We observe that all our models with 512 ≤ L &lt; 3072 are faster to train than the sinusoidal model with L = 3072, but they all achieve greater perplexity scores on the validation set. Our model with L = 3072 trains just as fast as the sinusoidal one but bests its score by more than one perplexity point; (the standard deviation for the the sinusoidal model with L = 3072 is 0.24).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 89,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-90",
      "content": "t. Our model with L = 3072 trains just as fast as the sinusoidal one but bests its score by more than one perplexity point; (the standard deviation for the the sinusoidal model with L = 3072 is 0.24).\n\nTable 5 shows the perplexity values obtained when 8 different ALiBi models, trained on L values between 64 and 3072, extrapolating to L valid values longer than the ones they were trained on. In addition, we present results for the sinusoidal, rotary and T5 bias models, with L valid = L .\n\nTable 5: Perplexity when ALiBi extrapolates on the WikiText-103 development set. ∗ For results we present for the sinusoidal, rotary and T5 bias models, L = L valid (so we do not test the extrapolation abilities of those baselines here).\n\n| ALiBi        | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 90,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-91",
      "content": "ion Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| Train Length | 64                  | 128                 | 256                 | 512                 | 1024                | 1536                | 2048                |             3072    |\n| 64           | 28.46               | 24.70               | 22.88               | 22.09               | 21.73               | 21.63               | 21.59               |               21.53 |\n| 128          | -                   | 23.98               | 21.70               | 20.67               | 20.36               | 20.29               | 20.31               |               20.28 |\n| 256          | -                   | -                   | 21.29               | 19.89",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 91,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-92",
      "content": "| 20.36               | 20.29               | 20.31               |               20.28 |\n| 256          | -                   | -                   | 21.29               | 19.89               | 19.29               | 19.13               | 19.10               |               19.03 |\n| 512          | -                   | -                   | -                   | 19.73               | 18.81               | 18.50               | 18.48               |               18.4  |\n| 1024         | -                   | -                   | -                   | -                   | 18.66               | 18.20               | 18.05               |               17.96 |\n| 1536         | -                   | -                   | -                   | -                   | -                   | 18.12               | 17.90               |               17.72 |\n| 2048         | -                   | -                   | -                   | -                   | -                   | -",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 92,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-93",
      "content": "| 18.12               | 17.90               |               17.72 |\n| 2048         | -                   | -                   | -                   | -                   | -                   | -                   | 17.91               |               17.64 |\n| 3072         | -                   | -                   | -                   | -                   | -                   | -                   | -                   |               17.6  |\n| Sinusoidal ∗ | 28.03               | 23.81               | 21.45               | 20.05               | 19.34               | 19.05               | 18.87               |               18.67 |\n| Rotary ∗     | -                   | -                   | -                   | 20.07               | 19.33               | -                   | -                   |               18.57 |\n| T5 Bias ∗    | -                   | -                   | -                   | 19.65               | 18.80               | -                   | -",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 93,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-94",
      "content": "|               18.57 |\n| T5 Bias ∗    | -                   | -                   | -                   | 19.65               | 18.80               | -                   | -                   |               18.01 |\n\nTable 6 compares ALiBi to the sinusoidal, rotary and T5 bias baselines on the test set of WikiText103, and Table 7 compares ALiBi to the current state of the art models on that test set.\n\nTable 6: Test perplexity and runtime on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods.\n\n| Model                         | Param. ↓   | Train   | Inference   | Inference   | Inference   |\n|-------------------------------|------------|---------|-------------|-------------|-------------|\n|                               |            | Speed ↑ | Speed ↑     | Valid ↓     | Test ↓      |\n| Sinusoidal, L = 3072          | 247M       | 15.3k   | 13.6k       | 18.67       | 19.38       |\n| Rotary, L = 3072              | 2",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 94,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-95",
      "content": "| Speed ↑ | Speed ↑     | Valid ↓     | Test ↓      |\n| Sinusoidal, L = 3072          | 247M       | 15.3k   | 13.6k       | 18.67       | 19.38       |\n| Rotary, L = 3072              | 247M       | 11.5k   | 12.2k       | 18.57       | 19.28       |\n| T5 Bias, L = 3072             | 247M       | 4.3k    | 7.3k        | 18.01       | 18.73       |\n| ALiBi L = 512, L valid = 3072 | 247M       | 28.3k   | 13.6k       | 18.40       | 19.08       |\n| L = 3072, L valid = 3072      | 247M       | 15.5k   | 13.6k       | 17.60       | 18.30       |\n\nTable 7: Valid and test perplexity scores on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods with sliding window evaluation (§B and S=512 following (Baevski &amp; Auli, 2018; Khandelwal et al., 2020; Press et al., 2021)). The sinusoidal model presents our results from training and inference with the model of Baevski &amp; Auli.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 95,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-96",
      "content": "nd S=512 following (Baevski &amp; Auli, 2018; Khandelwal et al., 2020; Press et al., 2021)). The sinusoidal model presents our results from training and inference with the model of Baevski &amp; Auli.\n\n| Model                                      | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------------|------------|-----------|----------|\n| Adaptive Inputs (Baevski &Auli, 2018)      | 247M       | 17.97     |    18.7  |\n| Transformer-XL (Dai et al., 2019)          | 257M       | -         |    18.3  |\n| Shortformer (Press et al., 2021)           | 247M       | 17.47     |    18.15 |\n| Sandwich Transformer (Press et al., 2020)  | 247M       | -         |    17.96 |\n| Staged Training (Press et al., 2021)       | 247M       | -         |    17.56 |\n| Compressive Transformer (Rae et al., 2020) | 329M       | -         |    17.1  |\n| Routing Transformer (Roy et al., 2020)     | -          | -         |    15.8  |\n| kNN-LM (Khandelwal et al., 2020)           | 247M",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 96,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-97",
      "content": "nsformer (Rae et al., 2020) | 329M       | -         |    17.1  |\n| Routing Transformer (Roy et al., 2020)     | -          | -         |    15.8  |\n| kNN-LM (Khandelwal et al., 2020)           | 247M       | 15.81     |    15.79 |\n| Sinusoidal, L = 3072                       | 247M       | 17.95     |    18.67 |\n| Rotary, L = 3072                           | 247M       | 17.98     |    18.72 |\n| T5 Bias, L = 3072                          | 247M       | 17.37     |    18.12 |\n| ALiBi L = 512, L valid = 3072              | 247M       | 18.30     |    19.01 |\n| L = 3072, L valid = 3072                   | 247M       | 16.97     |    17.66 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.2 ALIBI RESULTS ON WIKITEXT-103",
        "chunkIndex": 97,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-98",
      "content": "To ensure that our results are not specific to the WikiText-103 corpus, we next apply our model and the baselines to a different domain while using a similar model architecture and the same ALiBi slopes as those used in the previous subsection.\n\nWeemphasize that our set of slopes was chosen by running experiments on the WikiText-103 corpus, and here we apply that set of slopes to a model trained on a very different text domain. Throughout the entire process of developing this method, we ran only one set of experiments on this domain using the previously selected set of slopes.\n\nSpecifically, we use the Toronto BooksCorpus (Zhu et al., 2015), which has been used to train BERT (Devlin et al., 2019) (in conjuction with the English Wikipedia). The corpus is about 700M tokens (2.9 GB).\n\nWe use the same train/validation/test split as Khandelwal et al. (2020) and their tokenization, which uses BERT's vocabulary of 29K byte-pair encodings.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.3 RESULTS ON THE TORONTO BOOK CORPUS",
        "chunkIndex": 98,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-99",
      "content": "ia). The corpus is about 700M tokens (2.9 GB).\n\nWe use the same train/validation/test split as Khandelwal et al. (2020) and their tokenization, which uses BERT's vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103's, we replace the adaptive word embedding and softmax of Baevski &amp; Auli (2018) with a tied word embedding and softmax matrix (Press &amp; Wolf, 2017; Inan et al., 2017).\n\nOur results in Figure 9 (and Table 8) replicate our success on the WikiText-103 dataset. Our model surpasses the sinusoidal baseline when trained on the same amount of input tokens ( L ) and, in\n\nFigure 9: ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus. Our models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even when trained on much shorter sequences.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.3 RESULTS ON THE TORONTO BOOK CORPUS",
        "chunkIndex": 99,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-100",
      "content": "different input lengths on the Toronto BookCorpus. Our models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even when trained on much shorter sequences.\n\n<!-- image -->\n\naddition, our model is able to extrapolate to longer sequences at inference. This occurs even though our set of slopes was not tuned on this dataset. This result establishes the generality of ALiBi and the particular set of slopes we found and suggests that they may be used on different text domains without further hyperparameter tuning.\n\nTables 9 and 10 present the perplexities for our ALiBi models, the baselines, and the current state of the art on the Toronto BookCorpus validation and test sets. Our results here mirror our results on WikiText-103: we improve over the sinusoidal baseline even when AliBi is trained on fewer tokens.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.3 RESULTS ON THE TORONTO BOOK CORPUS",
        "chunkIndex": 100,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-101",
      "content": "of the art on the Toronto BookCorpus validation and test sets. Our results here mirror our results on WikiText-103: we improve over the sinusoidal baseline even when AliBi is trained on fewer tokens.\n\nTable 8: ALiBi models extrapolating on the Toronto BookCorpus development set. ∗ For the results of the sinusoidal models, L = L valid (so we do not test the extrapolation abilities of those models here).\n\n|              | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|\n| Train Length | 512                 | 1024                |             3072    |\n| 512          | 14.29               | 13.64               |               13.55 |\n| 1024         | -                   | 13.86               |               13.52 |\n| 3072         | -                   | -                   |               13.15 |\n| Sinusoidal ∗ | 14.80               | 14.73               |               14.46 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.3 RESULTS ON THE TORONTO BOOK CORPUS",
        "chunkIndex": 101,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-102",
      "content": "|               13.52 |\n| 3072         | -                   | -                   |               13.15 |\n| Sinusoidal ∗ | 14.80               | 14.73               |               14.46 |\n\nTable 9: Validation and test perplexities on the Toronto Book Corpus dataset.\n\n| Model                               | Param.   |   Valid ↓ |   Test ↓ |\n|-------------------------------------|----------|-----------|----------|\n| Sinusoidal, L = 3072                | 247M     |     14.46 |    11.67 |\n| ALiBi L train = 512, L valid = 3072 | 247M     |     13.55 |    10.98 |\n| L train = 3072, L valid = 3072      | 247M     |     13.15 |    10.73 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.3 RESULTS ON THE TORONTO BOOK CORPUS",
        "chunkIndex": 102,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-103",
      "content": "Table 11 compares our 1.3 billion parameter ALiBi models when extrapolating to two times the number of tokens that they were trained on. We use the sinusoidal model as our baseline, and train it for the same amount of time as we train the ALiBi model that we compare it to (and so since our ALiBi models run faster in this setting, the sinusoidal models complete less updates).\n\nTable 10: Validation and test perplexities on the Toronto Book Corpus dataset with a sliding window (§B). Following (Baevski &amp; Auli, 2018; Khandelwal et al., 2020; Press et al., 2020; 2021), we set the sliding window stride S =512.\n\n| Model                                | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------|------------|-----------|----------|\n| kNN-LM (Khandelwal et al., 2020)     | 247M       | 14.20     |    10.89 |\n| Shortformer (Press et al., 2021)     | 247M       | 13.40     |    10.88 |\n| Sandwich (Press et al., 2020)        | 247M       | -         |    10.83 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.4 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 103,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-104",
      "content": "020)     | 247M       | 14.20     |    10.89 |\n| Shortformer (Press et al., 2021)     | 247M       | 13.40     |    10.88 |\n| Sandwich (Press et al., 2020)        | 247M       | -         |    10.83 |\n| Staged Training (Press et al., 2021) | 247M       | 12.80     |    10.48 |\n| Sinusoidal, L = 3072                 | 247M       | 14.06     |    11.4  |\n| L = 512, L valid = 3072              | 247M       | 13.76     |    11.11 |\n| ALiBi L = 3072, L valid = 3072       | 247M       | 12.70     |    10.4  |\n\nTable 11: Perplexity, memory, and train time on the CC100+RoBERTa corpus for our ALiBi models and the sinusoidal baseline. We run our L = 512 (1024) model and the sinusoidal model with L = 1024 (2048) for the same amount of time. We show that our models achieve strong results even though they use 6-11% less memory.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.4 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 104,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-105",
      "content": "eline. We run our L = 512 (1024) model and the sinusoidal model with L = 1024 (2048) for the same amount of time. We show that our models achieve strong results even though they use 6-11% less memory.\n\n|                            | Training   | Training   | Training   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|----------------|----------------|\n|                            | Memory ↓   | Updates    | Hours ↓    | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 46.7k      | 5.5k       | 9.24           | -              |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.30           | -              |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 44.2k      | 5.9k       | -              | 9.01           |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -              | 8.92           |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.4 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 105,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-106",
      "content": "oidal, L train = 2048 | 29.3 GB    | 44.2k      | 5.9k       | -              | 9.01           |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -              | 8.92           |\n\nTable 12 compares our 1.3 billion parameter ALiBi models to the sinusoidal baselines, with and without extrapolation, with all models completing 50,000 updates.\n\nTable 12: Perplexity, train time and memory use of the sinusoidal and ALiBi models on the CC100+RoBERTa corpus when all models are trained with 50k updates.\n\n|                            | Training   | Training   | Training   | Valid PPL ↓   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|---------------|----------------|----------------|\n|                            | Memory     | Updates    | Hours ↓    | L valid = 512 | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 512  | 24.6 GB    | 50.0k      | 5.5k       | 9.71          | 37.05          | 105.42",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.4 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 106,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-107",
      "content": "| Updates    | Hours ↓    | L valid = 512 | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 512  | 24.6 GB    | 50.0k      | 5.5k       | 9.71          | 37.05          | 105.42         |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.79          | 9.30           | 9.54           |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.15           | 48.85          |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.16           | 8.92           |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 50.0k      | 6.7k       | -             | -              | 8.83           |\n| ALiBi, L train = 2048      | 29.4 GB    | 50.0k      | 6.7k       | -             | -              | 8.84           |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "A.4 RESULTS ON THE CC100+ROBERTA CORPUS",
        "chunkIndex": 107,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-108",
      "content": "In this section we investigate why ALiBi works so effectively. We find that ALiBi's decrease in perplexity when given longer sequences is largely explained by its improved avoidance of the early token curse. We hypothesize that future work building on ALiBi might achieve further gains by more efficiently exploiting longer histories.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B ANALYSIS",
        "chunkIndex": 108,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-109",
      "content": "Figure 10: Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom; red) on a sequence of 8 words using a model with L valid = 4. Nonoverlapping evaluation is much faster since it requires just two inference passes (as opposed to the five passes required by the siding window approach). But the sliding window approach provides more context for each prediction.\n\n<!-- image -->\n\nSliding Window Inference As mentioned in Section 2, nonoverlapping inference is commonly used to evaluate sequences longer than L (the number of tokens in each training subsequence). An alternative is to use a sliding window during evaluation (Baevski &amp; Auli, 2018).\n\nA stride S is picked between 1 and L -1 , and the window is advanced by S tokens after each forward pass. 12 This means that L -S tokens from the previous subsequence are re-encoded, and only S new tokens are output.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.1 DEFINING SLIDING WINDOW EVALUATION AND THE EARLY TOKEN CURSE",
        "chunkIndex": 109,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-110",
      "content": "cked between 1 and L -1 , and the window is advanced by S tokens after each forward pass. 12 This means that L -S tokens from the previous subsequence are re-encoded, and only S new tokens are output. The advantage is that all outputs in each subsequence after the first have at least L -S previous tokens to condition on. However, since tokens must be re-encoded multiple times, this approach is much slower than the nonoverlapping one. When S = 1 , we output one token every inference pass, each using the maximal context window that the model can handle; however, this is the slowest approach. Figure 10 is a visualization of the nonoverlapping and sliding window evaluation approaches.\n\nWe use sliding window inference as a tool to analyze our models, but we note that it is normally prohibitively slow in practice (Press et al., 2021).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.1 DEFINING SLIDING WINDOW EVALUATION AND THE EARLY TOKEN CURSE",
        "chunkIndex": 110,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-111",
      "content": "lapping and sliding window evaluation approaches.\n\nWe use sliding window inference as a tool to analyze our models, but we note that it is normally prohibitively slow in practice (Press et al., 2021).\n\nEarly Token Curse Splitting an evaluation set into subsequences means that predictions occuring early in each subsequence cannot access many previous context tokens (appearing at the end of the previous subsequence). The result, referred to as the early token curse (Press et al., 2021), increases (i.e., degrades) perplexity scores. A workaround is to evaluate the model using a sliding window, giving each prediction more context. This solution is slow since it requires many more forward passes of the model.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.1 DEFINING SLIDING WINDOW EVALUATION AND THE EARLY TOKEN CURSE",
        "chunkIndex": 111,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-112",
      "content": "We presented results showing that our ALiBi method (and, to a lesser extent, the T5 bias) allows LMs to extrapolate during inference. Two reasons could explain why these methods enable LMs to achieve better perplexity given longer input subsequences:\n\n1. Performance improves because the models can use longer contexts to make more accurate predictions. For example, the average article length in the WikiText-103 corpus is about 3600 tokens; therefore, if a model trained on L = 512 tokens extrapolates to L valid = 3072 tokens during inference and achieves better results, that might be because it can spot patterns occurring across more than 512 tokens.\n2. Performance improves because longer input sequences mean the early token curse is reduced. For example, during nonoverlapping evaluation on sequences of length L valid = 1000, 10% of predictions have 100 tokens of context or less.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 112,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-113",
      "content": "onger input sequences mean the early token curse is reduced. For example, during nonoverlapping evaluation on sequences of length L valid = 1000, 10% of predictions have 100 tokens of context or less. If we rerun nonoverlapping evaluation on that model with L valid = 2000 tokens, now only 5% of predictions have 100\n\n12 Nonoverlapping inference can be viewed as sliding window inference with stride L .\n\ntokens of context or less. So, by simply being able to handle longer sequences, a model can substantially reduce the early token curse and improve performance. 13\n\nTo better understand what might be occurring, we re-evaluate the development set of WikiText-103 with our models and the sinusoidal baseline with L = 512 , 1024 , 3072 . However, this time we use sliding window evaluation with a stride of S = 1 , meaning that we move the sliding window just one token after every inference pass, giving each prediction the maximum number of context tokens that the model can use.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 113,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-114",
      "content": "aluation with a stride of S = 1 , meaning that we move the sliding window just one token after every inference pass, giving each prediction the maximum number of context tokens that the model can use.\n\nFigure 11: ALiBi models evaluated on different input lengths on WikiText-103 with sliding window evaluation (with stride S = 1 ). Unlike results shown in Figure 4, where performance improves in each of our models as we increase the validation sequence length, here performance stays relatively flat as we increase L valid . This might mean that ALiBi increases performance when L valid &gt; L not because it uses longer contexts, but because fewer tokens suffer from the early token curse. Note that as in §2, the perplexity of the sinusoidal model explodes when L valid &gt; L even when using sliding window evaluation.\n\n<!-- image -->\n\nThe results are shown in Figure 11 and in the corresponding Tables 13 (sinusoidal) and 15 (ALiBi).",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 114,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-115",
      "content": "soidal model explodes when L valid &gt; L even when using sliding window evaluation.\n\n<!-- image -->\n\nThe results are shown in Figure 11 and in the corresponding Tables 13 (sinusoidal) and 15 (ALiBi).\n\nUnsurprisingly, for the sinusoidal model, as in §2, increasing L valid causes an explosion in perplexity even when using sliding window evaluation. Our ALiBi models cannot improve perplexity when looking at longer sequences in this setting, but they keep perplexity flat when L valid increases.\n\nThis leads us to believe that our perplexity improvement when increasing L valid and using nonoverlapping evaluation is caused by explanation 2, not explanation 1. Because sliding window evaluation provides long context windows for every prediction made, it curtails the early token curse. In this setting, ALiBi's performance remains flat when L valid increases, leading us to hypothesize that the gains seen while increasing L valid in §4 were the result of larger L valid values mitigating the early",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 115,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-116",
      "content": "ing, ALiBi's performance remains flat when L valid increases, leading us to hypothesize that the gains seen while increasing L valid in §4 were the result of larger L valid values mitigating the early token curse.\n\nOur ALiBi results mirror what occurs in the model using the T5 bias: when using sliding window evaluation, perplexity remains relatively flat when evaluating longer sequences (see Table 14).\n\nOur analysis reveals that when L valid &gt; L , ALiBi might not be using contexts longer than the ones it was trained on. This highlights a research direction that could be pursued in future work.\n\nThese findings do not lessen the value of ALiBi. When L valid = L , ALiBi achieves either superior or similar results to the sinusoidal method and other alternatives even though it is simpler and requires no learned parameters.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 116,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-117",
      "content": "n the value of ALiBi. When L valid = L , ALiBi achieves either superior or similar results to the sinusoidal method and other alternatives even though it is simpler and requires no learned parameters. When evaluating L valid &gt; L tokens, even if ALiBi does not attend to more than L tokens, it yields better results than the other alternatives that can be used in this case, i.e., standard nonoverlapping inference (which is cheap, but does not perform as well) and the more accurate sliding window approach (which is very slow).\n\n13 100 tokens is an arbitrary small number used here to represent a short history context, i.e., one in which making predictions for the next output token would be harder.\n\nTable 13: Perplexities of the sinusoidal models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 117,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-118",
      "content": "edictions for the next output token would be harder.\n\nTable 13: Perplexities of the sinusoidal models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 18.35                         | 204.42                        | 264.74                        | 306.19                        |                        360.12 |\n| 1024         | -                             | 18.05                         | 206.55",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 118,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-119",
      "content": "| 264.74                        | 306.19                        |                        360.12 |\n| 1024         | -                             | 18.05                         | 206.55                        | 302.6                         |                        393.71 |\n| 3072         | -                             | -                             | -                             | -                             |                         18.03 |\n\nTable 14: Perplexities of the T5 bias models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 119,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-120",
      "content": "--|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.92                         | 18.51                         | 20.36                         | 22.62                         |                         30.77 |\n| 1024         | -                             | 17.65                         | 17.87                         | 18.51                         |                         20.66 |\n| 3072         | -                             | -                             | -                             | -                             |                         17.41 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 120,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-121",
      "content": "20.66 |\n| 3072         | -                             | -                             | -                             | -                             |                         17.41 |\n\nTable 15: Perplexities of the ALiBi models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.98                         | 17.92                         | 18.2                          | 18.28",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 121,
        "totalChunks": 123
      }
    },
    {
      "id": "2108.12409v2-chunk-122",
      "content": "| 2048                          |                       3072    |\n| 512          | 17.98                         | 17.92                         | 18.2                          | 18.28                         |                         18.3  |\n| 1024         | -                             | 17.46                         | 17.47                         | 17.62                         |                         17.92 |\n| 3072         | -                             | -                             | -                             | -                             |                         16.96 |",
      "metadata": {
        "source": "arxiv:2108.12409v2",
        "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
        "authors": [
          "Ofir Press",
          "Noah A. Smith",
          "Mike Lewis"
        ],
        "section": "B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE",
        "chunkIndex": 122,
        "totalChunks": 123
      }
    }
  ],
  "fullText": "## TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION\n\nOfir Press 1 , 2 Noah A. Smith 1 , 3 Mike Lewis 2\n\n1\n\n2\n\nPaul G. Allen School of Computer Science &amp; Engineering, University of Washington Facebook AI Research\n\n3 Allen Institute for AI\n\nofirp@cs.washington.edu\n\n## ABSTRACT\n\nSince the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark. 1\n\n## 1 INTRODUCTION\n\nWhen constructing a transformer-based language model, a major design decision is the length of training sequences, denoted L herein, which has to date been equivalent to the length of inference sequences. More context, achieved by larger L , improves predictions at inference time. But longer sequences are more expensive to train on. 2\n\nBefore transformers, RNN language models were trained on shorterL sequences and assumed to generalize to longer contexts at inference time (Mikolov et al., 2010; Mikolov &amp; Zweig, 2012; Zaremba et al., 2014). Vaswani et al. (2017), introducing the transformer, speculated that it 'may [...] extrapolate to sequence lengths longer than the ones encountered during training.' We define extrapolation as a model's ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained. We find that transformer language models (LMs) that use sinusoidal position embeddings have very weak extrapolation abilities; see Figure 1.\n\nWe demonstrate that this failure to extrapolate is caused by the position embedding method. As shown in Figure 1, recent alternatives to the original sinusoidal position method (Su et al., 2021; Raffel et al., 2020) have improved extrapolation. However, the better of these, the T5 bias, is considerably slower than the sinusoidal approach and uses extra memory and parameters (Figure 2).\n\nWe therefore introduce Attention with Linear Biases (ALiBi) to facilitate efficient extrapolation. ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query. Our simple approach eliminates position embeddings.\n\n1 Code &amp; models: https://github.com/ofirpress/attention\\_with\\_linear\\_biases\n\n2 Figure 7 in the appendix plots training speed, in words per second, against L .\n\n<!-- image -->\n\nInference Input Tokens\n\nFigure 1: Extrapolation: as the (validation-set's) input sequence gets longer ( x -axis), current position methods (sinusoidal, rotary, and T5) show degraded perplexity ( y -axis, lower is better), but our method (§3) does not. Models were trained on WikiText-103 with sequences of L = 512 (left) or L = 1,024 (right) tokens. T5 ran out of memory on our 32GB GPU. For more detail on exact perplexities and runtimes, see Tables 2 and 3 in the appendix.\n\nCompared to a sinusoidal model trained on the same input length, our method requires no additional runtime or parameters and incurs a negligible (0-0.7%) memory increase. ALiBi can be implemented by changing only a few lines of existing transformer code.\n\nUsing ALiBi, a transformer LM can be trained on shortL sequences and therefore at much lower cost, and it can still be reliably applied to long sequences at runtime. For example, a 1.3 billion parameter LM trained on L = 1024 tokens with ALiBi achieves the same perplexity as a sinusoidal model trained on L = 2048 when both are tested on sequences of 2048 tokens, even though our model is 11% faster and uses 11% less memory.\n\nThough performance peaks at around two times the number of tokens that the model was trained on, ALiBi maintains strong performance even on sequences of length 10,000. In recently explored settings where NLP training examples are given as context to an LM (Brown et al., 2020), our approach will allow exposure to more examples. Additionally, it enables generation of longer outputs.\n\n## 2 CURRENT APPROACHES DO NOT EXTRAPOLATE EFFICIENTLY\n\nWe show for the first time that the sinusoidal position method, which technically should be able to extrapolate, in practice has very limited extrapolation capabilities. Though the rotary position method improves over the sinusoidal one, it still does not achieve satisfying results. Holding everything else constant, we are the first to observe that the T5 bias method leads to better extrapolation than either of these, and so we conclude that extrapolation ability depends heavily on the position embedding. Unfortunately, the T5 bias is computationally costly (Figure 2).\n\n## 2.1 BACKGROUND AND EXPERIMENTAL SETUP\n\nA transformer LM receives a list of tokens and outputs a probability distribution representing its prediction for the next token. We call the input list the current input subsequence since the inputs to language models are typically subsequences from (much longer) training or evaluation sequences. During both training and perplexity evaluation (i.e., scoring a fixed sequence), many predictions can be calculated at once; this is done using a 'causal mask' that ensures each position's prediction is influenced only by tokens to its left. Let L be the length of each input subsequence during training; it includes L predictions, which on average have access to L +1 2 tokens of (left) context. To explore a model's extrapolation abilities, we are interested in cases where sequences of length L valid &gt; L are considered at evaluation time. When L differs between inference and training, we use L to refer to the length of subsequences during training and L valid to refer to their length at validation.\n\nTraining Memory\n\nFigure 2: A comparison of batched training, inference speed and memory use of the sinusoidal, rotary, T5 bias, and our ALiBi position methods. The speed differences between our method and the sinusoidal are within 1% during training and 3% for inference, which is insignificant on our hardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in this setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to 1024) since we break batches into multiple updates. See Table 1 in the appendix for exact numbers.\n\n<!-- image -->\n\nNonoverlapping Inference To train on or evaluate a sequence longer than L tokens, it is typical to segment the sequence into L -length subsequences and train on or evaluate them independently. Unless otherwise stated, we use nonoverlapping inference to report perplexity scores.\n\nExtrapolation During Inference Formally, the functions that define a transformer layer are agnostic to input length; 3 they map from some arbitrary, unfixed number of input vectors to the same number of output vectors. When transformers are applied to data that is inherently sequential, like text, positional information is injected into the inputs in various ways.\n\nVaswani et al. (2017) discussed two options for embedding positions into vectors to be added to word embeddings: learning embeddings for specific positions and unlearned sinusoidal embeddings. They observed similar performance between these two but preferred the sinusoidal approach, which they argued might extrapolate to longer input sequences during inference. We find that this model cannot extrapolate to more than a few dozen tokens beyond L . 4\n\nExperiment Setup We first test the extrapolation abilities of various position methods on the WikiText-103 corpus (Merity et al., 2016) using the transformer language model of Baevski &amp; Auli (2018). We use this model because of its prominent role in recent language modeling developments (Khandelwal et al., 2020; Press et al., 2021). The training set is about 103 million tokens from English Wikipedia (half a gigabyte). The model has 16 transformer layers of dimension 1024 , with 8 heads, and a feedforward inner dimension of 4096 . This model ties the word embedding and softmax matrices (Press &amp; Wolf, 2017; Inan et al., 2017). In our experiments, other than varying the position method and training subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs (205).\n\n## 2.2 MEASURING EXTRAPOLATION\n\nSinusoidal Position Embeddings Sinusoidal position embeddings (Vaswani et al., 2017; §3.5) are constant, non-learned vectors that are added to token embeddings on input to the first layer of the transformer. They are frequently used in transformer language modeling (Baevski &amp; Auli, 2018; Lewis et al., 2021) and machine translation (Vaswani et al., 2017; Ott et al., 2018) models. We first consider the unmodified model of Baevski &amp; Auli (2018), which uses sinusoidal position embeddings, and train it on L = 512 tokens; we then run inference with it on the validation set on L + k tokens, with k ranging from 0 to 15,000. Figure 1 (left) and the corresponding Table 2 (in the appendix) show that while the model improves perplexity up to k = 20 , performance stops improving and stays steady from k = 20 to k = 50 and then begins degrading. Similar results are obtained for a model trained with L = 1024 tokens (Figure 1 (right) and Table 3 in the appendix). That model improves for up to L valid = L +50 tokens, after which performance declines.\n\n3 These include the embedding lookup, feedforward sublayer, and softmax layer, which act independently on vector inputs, as well as the attention sublayers, whose parameters do not depend on input length (and which must handle variable-length inputs, e.g., due to causal masking).\n\n4 The learned positional embedding approach does not have a way to encode positions greater than L ; it therefore has no ability to extrapolate.\n\nRotary Position Embeddings The rotary method was introduced by Su et al. (2021) and has recently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPTJ (Wang &amp; Komatsuzaki, 2021). Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.\n\nUnlike the sinusoidal or learned positional embedding approach, the rotary method injects position information into the model at every layer, not just at the initial one. In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method (§3).\n\nWe apply the rotary position embedding method to our Baevski &amp; Auli baseline. 5 The perplexity results (Figure 1 and Appendix Tables 2 and 3) are better than the sinusoidal approach: the model with L = 512 ( L = 1024 ) improves perplexity with up to k = 200 ( k = 100 ) more tokens than it saw during training, but this comes at the cost of slower training and inference (Figure 2).\n\nT5 Bias Though most models use trained or sinusoidal position embeddings, the T5 model of Raffel et al. (2020) uses a relative position method (Shaw et al., 2018; Huang et al., 2019) that adds no position information to word embeddings (as in the previous method). Instead, it modifies the way attention values are computed. We refer to this as the 'T5 bias' method. 6 To compute attention values in the unmodified transformer, we compute the dot product of every query with every relevant key and then softmax these attention values. In this method, we compute the attention values as before, but then we add a learned, shared bias to each query-key score that is dependent on just the distance between the query and key. Therefore, all query-key scores where the query and key distance are zero (i.e., the query and key represent the same token) get a specific learned bias, all scores where the query and key are one word away get a different learned bias, and so on, up to a certain point, from where multiple different distances share the same learned bias (which might be beneficial for extrapolation). As in the rotary method, the T5 bias injects position information into the model at every layer and integrates no explicit position information into the self-attention value vectors.\n\nRaffel et al. (2020) propose that the T5 bias may allow extrapolation, but they did not report experiments testing this. Here, we show that the T5 bias does allow language models to extrapolate. We do this by again modifying the Baevski &amp; Auli model, this time to insert the T5 bias into it. 7\n\nAs Figure 1 shows, the T5 bias improves perplexity with longer sequences than the ones it was trained on, i.e., k = 600 ( k = 800 ) extra tokens for a model trained on L = 512 ( L = 1024 ) input tokens. Unfortunately, this impressive performance comes at a cost: training is at least twice as slow as with the sinusoidal model. Therefore, this model's extrapolation ability provides no efficiency advantage. For example, to do inference on 1024 tokens, we could either train the sinusoidal model with L = 1024 or train the T5 bias model on L = 512 tokens and extrapolate to 1024 for inference. However, the L = 1024 sinusoidal model runs at 28.5k words per second (WPS), while the L = 512 T5 bias model runs at 14.4k WPS (Appendix Table 1), so there is no speedup when training on shorter sequences with this method. 8\n\n5 Our rotary method implementation is based on the code in https://github.com/JunnYu/ RoFormer\\_pytorch , which is linked to from the official repository of Su et al. (2021): ( https: //github.com/ZhuiyiTechnology/roformer ). After we finished running our experiments with the rotary method, we were informed that the runtime of the code linked above could be optimized, making it only 2% slower than the sinusoidal approach. This optimization would not change extrapolation performance.\n\n6 This method is similar to the one used in Parikh et al. (2016, Equation 7).\n\n7 Our T5 bias implementation is based on the one used in HuggingFace Transformers (Wolf et al., 2020), which in turn is based on the official Mesh Tensorflow T5 code.\n\n8 Narang et al. (2021) benchmarked the T5 bias as being just 8.7% slower than the sinusoidal approach; thus, while always incurring a runtime penalty, this method's runtime could be faster depending on the choice of hardware and software frameworks used. Narang et al. used the Tensorflow T5 library running on TPUs, while we used the PyTorch Fairseq library running on GPUs.\n\nFigure 3: When computing attention scores for each head, our linearly biased attention method, ALiBi, adds a constant bias (right) to each attention score ( q i · k j , left). As in the unmodified attention sublayer, the softmax function is then applied to these scores, and the rest of the computation is unmodified. mis a head-specific scalar that is set and not learned throughout training. We show that our method for setting m values generalizes to multiple text domains, models and training compute budgets. When using ALiBi, we do not add positional embeddings at the bottom of the network.\n\n<!-- image -->\n\n## 3 ATTENTION WITH LINEAR BIASES (ALIBI)\n\nIn the transformer model of Vaswani et al. (2017), position embeddings are added to the word embeddings at the bottom of the network. For an input subsequence of length L , the attention sublayer computes the attention scores for the i th query q i ∈ R 1 × d , ( 1 ≤ i ≤ L ) in each head, given the first i keys K ∈ R i × d , where d is the head dimension:\n\n<!-- formula-not-decoded -->\n\nThese attention scores are then multiplied by the values to return the output of the attention sublayer. 9\n\nWhen using ALiBi, we do not add position embeddings at any point in the network. The only modification we apply is after the query-key dot product, where we add a static, non-learned bias: 10\n\n<!-- formula-not-decoded -->\n\nwhere scalar m is a head-specific slope fixed before training. Figure 3 offers a visualization.\n\nFor our models with 8 heads, the slopes that we used are the geometric sequence: 1 2 1 , 1 2 2 , ..., 1 2 8 . For models that require 16 heads, we interpolate those 8 slopes by geometrically averaging every consecutive pair, resulting in the geometric sequence that starts at 1 √ 2 and has the ratio of 1 √ 2 : 1 2 0 . 5 , 1 2 1 , 1 2 1 . 5 , ..., 1 2 8 . In general, for n heads, our set of slopes is the geometric sequence that starts at 2 -8 n and uses that same value as its ratio.\n\nIn §4, we observe that this set of slopes works on a wide variety of text domains and model sizes. Therefore, we do not believe that it is necessary to tune these slope values every time a new model is trained on a new dataset. This makes our method similar to the sinusoidal approach, where the hyperparameters (the start and end of the geometric progression of wavelengths) were set once by Vaswani et al. (2017) and then reused in different models of different sizes on different datasets.\n\nALiBi has an inductive bias towards recency; it penalizes attention scores between distant query-key pairs, with the penalty increasing as the distance between a key and a query grows. The different heads increase their penalties at different rates, depending on the slope magnitude.\n\nWe initially experimented with making the slopes trainable, but this did not yield strong extrapolation results. 11 Abrief manual exploration of around ten slope sets led us to discover the set of slopes that we finally picked. Our main insight from this exploration is that the slope sets that work best are those with slopes in the (0 , 1) range, with the slopes' density increasing as we get closer to 0 . We also found our method to be robust to slope choice. Even randomly sampling from the exponential distribution worked well in some cases (although that method had high variance).\n\nSince ALiBi is a relative position method, we add position information at every layer to the keys and queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that these properties might be beneficial for extrapolation.\n\n9 For simplicity we omit the key, query, value and final output projections, dropout, and the scaling factor. √\n\n10 The ALiBi bias is not multiplied by the d k scaling factor from Equation 1 of Vaswani et al. (2017).\n\n11 In our experiments, trainable slopes also slowed down the training speed by 3%.\n\nImplementation. ALiBi is easy to implement, with all changes accomplished in a few lines of code. We implement it by modifying the mask matrix by adding the linear biases to it (in practice, when training a transformer LM, query q i attends only to keys 1 to i ; this is implemented by adding a mask matrix to the query-key dot product before the softmax operation is applied). This means that there is no runtime penalty when using our method since we add no operations to the network.\n\nCompared to the sinusoidal model trained on the same input lengths, AliBi incurs a memory increase (up to 100MB in some of our experiments): in the unmodified transformer, the mask is of size L × L ; when using ALiBi, the mask is a slightly larger n × L × L (where n is the number of heads) since the linear biases added for each head uses a different slope. But, as we show, ALiBi enables training on much smaller sequences while still achieving (and occasionally surpassing) results obtained using sinusoidal embeddings on longer sequences, which saves multiple gigabytes of memory.\n\n## 4 RESULTS\n\nWe first show that on WikiText103 ALiBi is efficient and enables training models with short input subsequences that outperform strong baselines even when the ALiBi models extrapolate to more than six times the number of tokens that they were trained on. We then take the same hyperparameters for our method (the set of slopes) that worked on WikiText-103 and show that - with no modification - they provide strong results on a dataset in a very different domain: books. Finally, we show that a 1.3B parameter model trained with AliBi on a much larger (461 GB) dataset with much more compute provides a superior alternative to the sinusoidal method since it achieves similar perplexity scores while running faster and using less memory (since it is trained on shorter inputs).\n\nWhile multiple alternatives to the position methods presented in Vaswani et al. (2017) have been proposed, few have been adopted in large (1B or more parameter) LMs since that setting is much more challenging than the smaller scale experiments. GPT-3 and Jurassic-1 (Lieber et al., 2021) use the learned position embedding method from Vaswani et al., and GPT-J uses the rotary method. Our results on the 1.3B parameter model show our method's ability to generalize to larger models, dataset sizes and training durations without retuning the hyperparameter.\n\n## 4.1 RESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS\n\nFigure 4: ALiBi models trained and evaluated on varying sequence lengths on the WikiText-103 validation set and the sinusoidal baseline (not evaluated on longer sequences). All of our models outperform the sinusoidal ones even when trained on fewer tokens. Appendix Table 5 has exact perplexities, more ALiBi models (trained on fewer tokens), and results for rotary and T5 bias models.\n\n<!-- image -->\n\nWe first develop our method on the WikiText-103 corpus (Merity et al., 2016), replacing the sinusoidal position embeddings in the language model of Baevski &amp; Auli (2018) with ALiBi.\n\nFigure 4 (and the corresponding Appendix Table 5) show our results for models trained with varying numbers of input subsequence tokens ( L ), extrapolating to longer subsequence lengths on the validation dataset. Our first observation is that, without extrapolation, for every L , our models outperform those using the sinusoidal method, sometimes by a significant amount. For example, the Baevski &amp; Auli model achieves 18.67 ± 0.24 (std. dev.) perplexity when trained with L = 3072 input tokens, but our L = 3072 model achieves 17.60 perplexity (when both models evaluate with L valid = 3072).\n\nOur second observation is that all of our models can extrapolate, and they obtain improved perplexity scores when handling more tokens than they observed during training. For example, our model trained on 512 tokens (which achieves 19.73 perplexity when evaluating subsequences of length 512 in the development set) achieves a perplexity score of 18.40 on the development set when extrapolating to subsequences of length 3072. Surprisingly, this surpasses the score that the L = 3072 sinusoidal model obtains on the development set by a statistically significant margin. Note that all our models trained on L = 512 to L = 2048 outperform the sinusoidal baseline trained on L = 3072 when extrapolating to L valid = 3072 even though those models all take much less time to train since they train on shorter subsequences (Appendix Figure 8 compares training speed to perplexity for these models)! The L = 512 model is 1.84 times faster to train and yet still outperforms the L = 3072 sinusoidal model when extrapolating to L valid = 3072 . In addition, training the L = 3072 sinusoidal model requires a GPU with more than 16 GB of memory to fit the large attention matrices, which our L = 512 outperforms even though it can be trained on a GPU with much less memory due to much smaller attention matrices.\n\nAdditionally, Table 5 (in the appendix) also shows that, for L s of 1024 and 3072, our method performs better than the rotary and T5 bias models even when L valid = L (i.e., no extrapolation is occurring). Figure 1 (and the corresponding Appendix Tables 2 and 3) more broadly explore our method vs. the other position methods. They show that the T5 bias (the best of the baselines) improves perplexity until L valid is around 2 L , but on the WikiText-103 dataset our method continually improves perplexity until at least around 3 L , with the L = 512 model improving perplexity even when L valid exceeds 12k tokens. Even when unable to improve perplexity given longer sequences, ALiBi always maintains strong performance as more tokens are added.\n\nAppendix Table 6 shows that our results on the validation set also transfer to the test set of WikiText103. Currently, almost all models that present results on WikiText-103 use sliding window evaluation (defined in §B) to compute perplexities. We apply that method to our (and to the sinusoidal, rotary and T5 bias) models in Appendix Table 7. We find that our L = 3072 model surpasses the performance of Transformer-XL (Dai et al., 2019), the Sandwich (Press et al., 2020), and Shortformer (Press et al., 2021) models. Our results are similar to the ones obtained with staged training (Press et al., 2021) but fall short of results obtained by Routing Transformer (Roy et al., 2020) and kNN-LM (Khandelwal et al., 2020). The methods used in those models are orthogonal to ours, and we hypothesize that combining them with ours might lead to even larger performance increases.\n\nAfter developing our method on WikiText-103, in Appendix Section A.3, we run one set of experiments on a different domain (books) using a similar model architecture and without modifying any of the ALiBi hyperparameters (the slopes) and show that our results fully transfer to this new domain. Our models are able to both surpass the sinusoidal baseline when not extrapolating while also outperforming it when extrapolating to longer sequences.\n\n## 4.2 RESULTS ON THE CC100+ROBERTA CORPUS\n\nOur final set of experiments investigates whether ALiBi transfers to a larger model trained with a larger computational budget on a larger dataset than the ones we previously used. We show that our method achieves strong results in this more challenging setting, obtaining similar performance to the sinusoidal baseline while using significantly less memory, since we train on shorter subsequences.\n\nThe dataset we choose is a combination of the datasets used to train the RoBERTa (Liu et al., 2019) implementation of BERT (Devlin et al., 2019) and the English part of the CC-100 corpus introduced in Conneau et al. (2020), for a total of 461 GB. The RoBERTa training corpus-i.e., the Toronto Book Corpus (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWebText (Gokaslan &amp; Cohen, 2019) and Stories (Trinh &amp; Le, 2018))-is 161 gigabytes, and the English part of the CC-100 corpus is 300 gigabytes. The validation set contains 649K tokens.\n\nOur models for this dataset have 25 transformer layers with 16 heads and a dimension of 2048, with an 8192 hidden dimension of the feedforward sublayers. These models have 1.3B parameters. We train our models for one epoch, which is 50k updates on 128 V100 GPUs.\n\nIn Figure 5 (left), we compare the validation perplexity for L valid = 1024 throughout the training process for an ALiBi model trained with L = 512 compared to the sinusoidal model trained with L = 1024. Since our model is trained on shorter sequences, it is 7% faster and uses 1.6 GB less\n\nFigure 5: On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on 1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) tokens. The ALiBi models obtain strong results even though they use 6%-11% less memory since they train on shorter sequences. Appendix Table 11 shows memory use and end-of-training perplexities.\n\n<!-- image -->\n\nmemory. We halt training of the sinusoidal baseline when our model reaches the end of its training (one epoch). At that time, our model is just 0.06 perplexity away from the baseline even though it was trained on sequences that are half the length of those the baseline used and requires less memory.\n\nIn Figure 5 (right), results become even more impressive, showing that our model trained on L = 1024 outperforms by 0.09 perplexity the sinusoidal model trained on L = 2048 (when evaluating with L valid = 2048) even though our model uses 3.1 GB less memory. Our model maintains a lead in perplexity over the sinusoidal model during the entire training process. By sampling five evenly distributed points across the training process, we compute that our L = 1024 model reaches a given perplexity value, on average, 11% faster than the sinusoidal model does.\n\nSince our models in these comparisons use much less memory, they allow for stacking more layers, which would further improve performance (with negligible, if any, runtime cost). To keep our experiments as straightforward as possible, however, we do not add layers to our models.\n\nAppendix Table 12 presents additional results comparing our models to the sinusoidal baseline when both are trained on the same L , showing that ALiBi performs similarly to the sinusoidal baseline when not extrapolating. This contrasts with the results presented on the smaller datasets, where ALiBi consistently outperforms other position methods even when not extrapolating, suggesting that ALiBi's inductive bias provides additional benefits for lower-resource language modeling.\n\n<!-- image -->\n\nValidation Input Length (\n\nLvalid\n\n)\n\nFigure 6: The ALiBi and sinusoidal models (with both L = 512 and 1024) trained for 50k updates (1 epoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. ALiBi achieves the best results at around 2 L but maintains strong performance even up to 10000 tokens in these experiments.\n\nFigure 6 shows that our models trained on L = 512 and L = 1024 achieve the best results when extrapolating to about double the tokens that they were trained on. Specifically, the L = 512 model (that obtains 9.79 perplexity when L valid = 512) achieves its best score (9.3) when extrapolating to\n\n1012 tokens, and the L = 1024 model (that obtains 9.16 perplexity when L valid = 1024) achieves its best score (8.9) when extrapolating to 2024 tokens.\n\nOne possible explanation is that the subsequences the model observes during training are up to L tokens long. When performing inference on subsequences of length 2 L , half of the subsequences the model consumes are as long as the examples seen during training. When inference is performed on subsequences of length 2 L +1 or longer, less than half of the predictions the model makes are on subsequences of lengths seen during training, and that might degrade performance.\n\nThe sinusoidal model cannot extrapolate at all in this setting, with its performance degrading for both the L = 512 and 1024 models as soon as one token more than L is added during evaluation.\n\nIn Appendix B, we find that ALiBi's edge over sinusoidal embeddings is largely explained by its improved avoidance of the early token curse. We posit that future work building on ALiBi might achieve further gains by more efficiently exploiting longer histories.\n\n## 5 RELATED WORK\n\nIn parallel with our work, Wennberg &amp; Henter (2021) introduce a relative position method that, like our method, adds a bias to attention scores that is a function of the distance between the key and query elements. Unlike our ALiBi method, which uses a non-learned linear function, their method uses a radial-basis function, with multiple trainable parameters (in our experiments, this led to a slight decrease in runtime). In addition, they present experiments on text classification, not on language modeling. They do not explore extrapolation. The Distance Aware Transformer (Wu et al., 2021) multiplies attention scores by a bias that is a function of the distance between the key and query. This function uses a different, learned parameter in every head. They show results only on text classification. In our experiments (not presented), multiplying attention scores by the bias (instead of adding, as in ALiBi) degraded performance.\n\nTransformer-XL (Dai et al., 2019) presented a language model that uses a cache and can attend to more tokens during inference than it was trained on (by increasing the length of the cache). However, this work presents results only where output length is limited to the L (the training length), and their relative position method is very slow (Press et al., 2021). The Longformer (Beltagy et al., 2020) adapts models trained on shorter sequences to document-level tasks. However, to achieve this they had to partially train their models on longer sequences. Our ALiBi method enables extrapolation without any additional training on longer sequences.\n\nTo our knowledge, extrapolation has not been previously explored in transformer language modeling, but it has been investigated previously and concurrently with transformers on other tasks, such as machine translation (Rosendahl et al., 2019; Neishi &amp; Yoshinaga, 2019; Newman et al., 2020; Kiyono et al., 2021), sequence-to-sequence models trained on an artificial dataset (Hupkes et al., 2020), pretrained sequence-to-sequence models tested on arithmetic tasks (Nogueira et al., 2021, Appendix C), models trained with reinforcement learning (Lampinen et al., 2021), image, speech recognition, and machine translation models (Likhomanenko et al., 2021), and protein structure prediction (Jumper et al., 2021, Appendix 1.5).\n\n## 6 CONCLUSION\n\nWe showed that the sinusoidal position embedding approach does not enable transformers to extrapolate to inputs longer than the ones they were trained on. We then established that extrapolation in transformers can be enabled by just changing the position method. We showed that our ALiBi method offers an extremely simple replacement for existing position approaches and allow models to extrapolate. In addition, when not extrapolating, our method achieves either better perplexity than the sinusoidal method (in models smaller than 1B parameters, trained on less data) or similar perplexity (in larger, billion parameter models trained on much more data). ALiBi is simple to implement and does not slow down runtime or require extra parameters (but does occasionally require a negligible amount of extra memory). Using our method, we sped up the training of a 1.3 billion parameter model evaluated on the same input sequence length as GPT-3 (2048).\n\n## ACKNOWLEDGMENTS\n\nWe thank Tim Dettmers, Gabriel Ilharco, Jungo Kasai, Hao Peng, Sewon Min, Sofia Serrano, Sam Shleifer, Luke Zettlemoyer, Julian Michael, Nikolaos Pappas, Yizhong Wang, and the anonymous reviewers for their valuable feedback and fruitful discussions.\n\n## REFERENCES\n\n- Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. CoRR , abs/1809.10853, 2018. URL http://arxiv.org/abs/1809.10853 .\n- Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv:2004.05150 , 2020.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n- Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´ an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020. doi: 10.18653/v1/2020.acl-main.747. URL http://dx.doi.org/10.18653/v1/2020.acl-main.747 .\n- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285 .\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423 .\n- Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus , 2019.\n- Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam M. Shazeer, Andrew M. Dai, M. Hoffman, M. Dinculescu, and D. Eck. Music transformer: Generating music with long-term structure. In ICLR , 2019.\n- Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research , 67:757-795, April 2020. doi: 10.1613/jair.1.11674. URL https://doi.org/10.1613/jair.1. 11674 .\n- Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. In ICLR , 2017. URL https://openreview.net/ forum?id=r1aPbsFle .\n- J. Jumper, Richard Evans, A. Pritzel, Tim Green, Michael Figurnov, O. Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Z´ ıdek, Anna Potapenko, A. Bridgland, Clemens Meyer, Simon A A Kohl, Andy Ballard, A. Cowie, B. Romera-Paredes, Stanislav Nikolov, Rishub Jain, J. Adler, T. Back, Stig Petersen, D. Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, S. Bodenstein, D. Silver, Oriol Vinyals, A. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with alphafold. Nature , 596:583 - 589, 2021.\n- Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR) , 2020.\n\n- Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position embedding for transformers. ArXiv , abs/2109.05644, 2021.\n- Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, and Felix Hill. Towards mental time travel: a hierarchical memory for reinforcement learning agents. CoRR , abs/2105.14039, 2021. URL https://arxiv.org/abs/2105.14039 .\n- Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models, 2021.\n- Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, August 2021.\n- Tatiana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve, and Alex Rogozhnikov. CAPE: encoding relative positions with continuous augmented positional embeddings. CoRR , abs/2106.03143, 2021. URL https://arxiv.org/abs/2106.03143 .\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\n- Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n- Tomas Mikolov and G. Zweig. Context dependent recurrent neural network language model. 2012 IEEE Spoken Language Technology Workshop (SLT) , pp. 234-239, 2012.\n- Tomas Mikolov, M. Karafi´ at, L. Burget, J. Cernock´ y, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH , 2010.\n- Sebastian Nagel. Cc-news. https://commoncrawl.org/2016/10/ news-dataset-available/ , 2016.\n- Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across implementations and applications?, 2021.\n- Masato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , pp. 328-338, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1031. URL https://aclanthology.org/K19-1031 .\n- Benjamin Newman, John Hewitt, Percy Liang, and Christopher D. Manning. The eos decision and length extrapolation. In BlackBoxNLP@EMNLP , 2020. URL https://nlp.stanford. edu/pubs/newman2020extrapolation.pdf .\n- Rodrigo Nogueira, Zhiying Jiang, and Jimmy J. Li. Investigating the limitations of the transformers with simple arithmetic tasks. ArXiv , abs/2102.13019, 2021.\n- Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation (WMT) , 2018.\n- Ankur Parikh, Oscar T¨ ackstr¨ om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2249-2255, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https: //aclanthology.org/D16-1244 .\n- Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers , pp. 157-163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/E17-2025 .\n\n- Ofir Press, Noah A. Smith, and Omer Levy. Improving transformer models by reordering their sublayers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 2996-3005, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.270. URL https://www.aclweb.org/anthology/2020. acl-main.270 .\n- Ofir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 5493-5505, Online, August 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.acl-long.427 .\n- Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id= SylKikSYDH .\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research , 21(140):1-67, 2020. URL http: //jmlr.org/papers/v21/20-074.html .\n- Jan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional encodings for neural machine translation. In International Workshop on Spoken Language Translation , Hong Kong, China, November 2019.\n- Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers, 2020.\n- Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pp. 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18-2074 .\n- Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n- Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning, 2018.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n- Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\n- Ulme Wennberg and Gustav Eje Henter. The case for translation-invariant self-attention in transformer-based language models, 2021.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6 .\n\n- Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. DA-transformer: Distance-aware transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2059-2068, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.166. URL https://aclanthology.org/2021.naacl-main.166 .\n- Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization, 2014.\n- Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pp. 19-27, 2015.\n\n## A APPENDIX\n\n## A.1 INTRODUCTION\n\nThe training speed of transformer LMs gets slower as the input subsequence length L increases. Figure 7 visualizes this.\n\nFigure 7: Training speed of our model and the sinusoidal baseline trained on different amounts of input subsequence tokens L .\n\n<!-- image -->\n\nTable 1 contains the runtimes and memory use statistics for models using the various position methods discussed in this work.\n\nTable 1: The speed (during training and evaluation, in words per second) and memory usage (during training) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline on WikiText103. Training and inference are batched, and speeds are shown for one V100 GPU.\n\n| Position Method   | Train Length   | Speed ( ↑ )   | Speed ( ↑ )   | Memory ( ↓ )   |\n|-------------------|----------------|---------------|---------------|----------------|\n|                   |                | Train         | Eval.         |                |\n|                   | 512            | 28.5k         | 82.1k         | 15.3 GB        |\n| Sinusoidal        | 1024           | 26.0k         | 77.8k         | 19.2 GB        |\n|                   | 3072           | 15.3k         | 42.4k         | 15.1 GB        |\n|                   | 512            | 20.0k         | 43.4k         | 17.8 GB        |\n| Rotary            | 1024           | 17.7k         | 39.4k         | 22.8 GB        |\n|                   | 3072           | 11.5k         | 29.5k         | 17.8 GB        |\n|                   | 512            | 14.4k         | 21.8k         | 16.9 GB        |\n| T5 Bias           | 1024           | 13.0k         | 20.2k         | 20.9 GB        |\n|                   | 3072           | 4.3k          | 4.9k          | 15.9 GB        |\n|                   | 512            | 28.3k         | 85.8k         | 15.3 GB        |\n| ALiBi             | 1024           | 25.8k         | 76.4k         | 19.3 GB        |\n|                   | 3072           | 15.5k         | 42.2k         | 15.2 GB        |\n\nTables 2, 3, and 4 show the perplexity and runtime of models using the sinusoidal, rotary T5 bias, and ALiBi position methods when extrapolating to sequences longer than the ones they were trained on. The models used in these tables were trained on L = 512 , 1024 and 3072 tokens.\n\nTable 2: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 512 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 512      | 20.05        | 15046        | 20.07     | 10839     | 19.65     | 11724     | 19.73     | 14726     |\n| 513      | 19.98        | 14925        | 20.01     | 10806     | 19.57     | 10491     | 19.62     | 14965     |\n| 522      | 19.93        | 15116        | 20.02     | 11295     | 19.57     | 9970      | 19.64     | 15316     |\n| 532      | 19.91        | 15358        | 19.98     | 10854     | 19.53     | 10382     | 19.61     | 15383     |\n| 542      | 19.91        | 15076        | 19.94     | 10795     | 19.47     | 12270     | 19.57     | 15301     |\n| 552      | 19.91        | 16394        | 19.93     | 12267     | 19.47     | 13000     | 19.54     | 16540     |\n| 562      | 19.91        | 16646        | 19.87     | 12481     | 19.39     | 12201     | 19.49     | 16385     |\n| 572      | 19.95        | 16934        | 19.83     | 12668     | 19.36     | 12851     | 19.46     | 16881     |\n| 582      | 20.13        | 16961        | 19.88     | 12594     | 19.41     | 13904     | 19.48     | 17064     |\n| 592      | 20.18        | 17243        | 19.84     | 13007     | 19.36     | 13706     | 19.43     | 17289     |\n| 602      | 20.40        | 17502        | 19.81     | 12788     | 19.33     | 14102     | 19.38     | 17141     |\n| 612      | 20.59        | 17637        | 19.81     | 12601     | 19.27     | 14573     | 19.38     | 17661     |\n| 712      | 24.86        | 15614        | 19.79     | 12676     | 19.10     | 13818     | 19.14     | 15637     |\n| 812      | 30.82        | 17151        | 20.17     | 13954     | 18.94     | 14377     | 18.99     | 17210     |\n| 912      | 37.42        | 17200        | 20.73     | 13887     | 18.86     | 15345     | 18.88     | 17619     |\n| 1012     | 43.54        | 16304        | 21.37     | 13759     | 18.79     | 14240     | 18.73     | 16059     |\n| 1112     | 50.36        | 16424        | 22.01     | 13891     | 18.77     | 14014     | 18.68     | 16659     |\n| 1212     | 58.01        | 17294        | 23.02     | 15245     | 18.87     | 14589     | 18.67     | 17372     |\n| 1312     | 63.62        | 15314        | 23.93     | 13698     | 18.84     | 13138     | 18.60     | 15698     |\n| 1412     | 70.75        | 15663        | 24.81     | 13928     | 18.87     | 12857     | 18.59     | 15860     |\n| 1512     | 76.23        | 15812        | 25.99     | 14248     | 18.91     | 13752     | 18.52     | 16225     |\n| 2512     | 132.41       | 15254        | 31.58     | 13456     | 20.41     | 9948      | 18.41     | 15204     |\n| 3512     | 178.97       | 13293        | 35.54     | 11850     | 22.91     | 7847      | 18.40     | 13329     |\n| 4512     | 209.37       | 11767        | 39.15     | 10485     | 25.91     | 6146      | 18.41     | 11738     |\n| 5512     | 240.44       | 10168        | 43.14     | 9020      | 29.54     | 5309      | 18.36     | 9986      |\n| 6512     | 271.40       | 9052         | 47.81     | 8108      | 34.48     | 4680      | 18.35     | 9022      |\n| 7512     | 293.02       | 8315         | 51.12     | 7483      | 39.29     | 4102      | 18.33     | 8324      |\n| 8512     | 305.65       | 7259         | 54.98     | 6718      | 43.08     | 3660      | 18.34     | 7366      |\n| 9512     | 336.02       | 6672         | 57.85     | 6211      | 48.90     | 3370      | 18.34     | 6555      |\n| 10512    | 341.53       | 6126         | 60.77     | 5575      | 52.95     | 3010      | 18.32     | 6030      |\n| 11512    | 362.74       | 5994         | 66.62     | 5445      | 61.38     | 2873      | 18.32     | 5882      |\n| 12512    | 373.17       | 5421         | 69.70     | 4988      | 64.94     | 2602      | 18.31     | 5287      |\n| 13512    | 382.91       | 5174         | 73.27     | 4692      | OOM       | -         | 18.31     | 4962      |\n| 14512    | 399.98       | 4351         | 75.52     | 3969      | OOM       | -         | 18.31     | 4352      |\n| 15512    | 406.01       | 4291         | 79.25     | 4103      | OOM       | -         | 18.31     | 4289      |\n\nTable 3: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 1024 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 1024     | 19.34        | 17002        | 19.33     | 14690     | 18.80     | 14973     | 18.66     | 16951     |\n| 1025     | 19.33        | 16630        | 19.34     | 14423     | 18.82     | 14635     | 18.67     | 16690     |\n| 1034     | 19.27        | 16589        | 19.28     | 14351     | 18.74     | 14435     | 18.60     | 16707     |\n| 1044     | 19.26        | 16760        | 19.27     | 14491     | 18.72     | 14644     | 18.60     | 16667     |\n| 1054     | 19.23        | 16747        | 19.26     | 14503     | 18.71     | 14800     | 18.58     | 16833     |\n| 1064     | 19.21        | 16676        | 19.22     | 14623     | 18.70     | 14498     | 18.55     | 16941     |\n| 1074     | 19.19        | 16879        | 19.19     | 14464     | 18.65     | 14670     | 18.49     | 16936     |\n| 1084     | 19.22        | 16942        | 19.23     | 14650     | 18.70     | 14607     | 18.56     | 17090     |\n| 1094     | 19.24        | 16771        | 19.22     | 14629     | 18.69     | 14517     | 18.54     | 16880     |\n| 1104     | 19.28        | 16870        | 19.27     | 14837     | 18.69     | 14635     | 18.52     | 17009     |\n| 1114     | 19.29        | 16795        | 19.27     | 14879     | 18.69     | 14540     | 18.52     | 17050     |\n| 1124     | 19.26        | 17312        | 19.18     | 15121     | 18.62     | 14480     | 18.46     | 17571     |\n| 1224     | 20.54        | 17901        | 19.38     | 15584     | 18.58     | 14956     | 18.40     | 18013     |\n| 1324     | 23.13        | 16308        | 19.96     | 14386     | 18.52     | 13726     | 18.33     | 16422     |\n| 1424     | 26.45        | 16217        | 21.27     | 14385     | 18.48     | 13516     | 18.28     | 16121     |\n| 1524     | 29.82        | 16377        | 22.59     | 14693     | 18.42     | 13587     | 18.22     | 16659     |\n| 1624     | 34.27        | 15928        | 24.34     | 14228     | 18.40     | 12979     | 18.17     | 16053     |\n| 1724     | 38.24        | 16640        | 25.66     | 14686     | 18.35     | 12976     | 18.15     | 16607     |\n| 1824     | 42.23        | 16840        | 27.63     | 14918     | 18.30     | 13071     | 18.08     | 16846     |\n| 1924     | 46.46        | 15071        | 29.64     | 13452     | 18.31     | 11843     | 18.08     | 15118     |\n| 2024     | 51.09        | 15591        | 31.17     | 13706     | 18.34     | 11906     | 18.05     | 15557     |\n| 3024     | 96.46        | 13639        | 35.67     | 12256     | 18.62     | 8480      | 17.92     | 13668     |\n| 4024     | 144.00       | 12441        | 44.30     | 11203     | 19.44     | 7443      | 17.95     | 12402     |\n| 5024     | 182.31       | 11431        | 48.31     | 10324     | 20.47     | 6384      | 17.92     | 11394     |\n| 6024     | 214.02       | 10238        | 54.78     | 9117      | 21.76     | 5577      | 18.01     | 10119     |\n| 7024     | 261.86       | 8785         | 62.83     | 7950      | 23.64     | 4867      | 17.93     | 8779      |\n| 8024     | 284.88       | 8132         | 64.91     | 7355      | 25.79     | 4377      | 17.96     | 8086      |\n| 9024     | 310.04       | 7045         | 71.91     | 6380      | 27.54     | 3787      | 17.98     | 7001      |\n| 10024    | 337.48       | 6633         | 77.70     | 6016      | 29.54     | 3582      | 17.97     | 6583      |\n| 11024    | 358.43       | 5722         | 81.15     | 5219      | 31.94     | 3170      | 18.02     | 5641      |\n| 12024    | 375.95       | 5560         | 87.51     | 5072      | 33.35     | 2940      | 18.01     | 5294      |\n| 13024    | 393.57       | 4691         | 94.74     | 4383      | OOM       | -         | 17.98     | 4621      |\n| 14024    | 403.52       | 4905         | 96.10     | 4546      | OOM       | -         | 18.01     | 4827      |\n| 15024    | 431.66       | 4518         | 99.78     | 3878      | OOM       | -         | 17.96     | 4447      |\n| 16024    | 453.32       | 4239         | 106.99    | 4170      | OOM       | -         | 17.98     | 4153      |\n\nTable 4: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 3072 on WikiText-103 and evaluated with different values of L valid on the validation set. Bold shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\n\n|        | Sinusoidal    | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi       | ALiBi     |\n|--------|---------------|--------------|-----------|-----------|-----------|-----------|-------------|-----------|\n| Inputs | PPL ( ↓ )     | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ )   | WPS ( ↑ ) |\n| 3072   | 18.67         | 13380        | 18.57     | 12548     | 18.01     | 8828      | 17.60       | 13866     |\n| 3073   | 18.67         | 13773        | 18.57     | 12474     | 18.01     | 8483      | 17.59       | 13793     |\n| 3082   | 18.62         | 13741        | 18.54     | 12388     | 17.95     | 8698      | 17.59       | 13778     |\n| 3092   | 18.60         | 13742        | 18.48     | 12458     | 17.92     | 8361      | 17.55       | 13783     |\n| 3102   | 18.65         | 13701        | 18.52     | 12365     | 17.94     | 8764      | 17.59       | 13747     |\n| 3112   | 18.64         | 13809        | 18.51     | 12449     | 17.96     | 8665      | 17.59       | 13827     |\n| 3122   | 18.68         | 13722        | 18.52     | 12432     | 17.98     | 8437      | 17.58       | 13795     |\n| 3132   | 18.67         | 13825        | 18.54     | 12490     | 17.97     | 8653      | 17.58       | 13784     |\n| 3142   | 18.69         | 13543        | 18.52     | 12230     | 17.97     | 8282      | 17.61       | 13572     |\n| 3152   | 18.66         | 13520        | 18.56     | 12240     | 17.98     | 8608      | 17.59       | 13523     |\n| 3162   | 18.71         | 13501        | 18.56     | 12253     | 18.04     | 8589      | 17.62       | 13598     |\n| 3172   | 18.72         | 13563        | 18.55     | 12297     | 17.99     | 8583      | 17.59       | 13625     |\n| 3272   | 18.87         | 13453        | 18.55     | 12148     | 17.93     | 8144      | 17.59       | 13482     |\n| 3372   | 19.46         | 13533        | 18.50     | 12254     | 17.88     | 8442      | 17.52       | 13565     |\n| 3472   | 20.55         | 13047        | 18.52     | 11868     | 17.95     | 7857      | 17.54       | 13107     |\n| 3572   | 21.84         | 13128        | 18.50     | 11882     | 17.86     | 7814      | 17.50       | 13170     |\n| 3672   | 23.04         | 13106        | 18.49     | 11859     | 17.87     | 7719      | 17.48       | 13196     |\n| 3772   | 24.47         | 13287        | 18.54     | 11942     | 17.85     | 7579      | 17.49       | 13312     |\n| 3872   | 25.85         | 12621        | 18.40     | 11272     | 17.82     | 7581      | 17.41       | 12566     |\n| 3972   | 27.21         | 12379        | 18.48     | 11151     | 17.84     | 7483      | 17.41       | 12324     |\n| 4072   | 28.59         | 12178        | 18.59     | 11019     | 17.88     | 6974      | 17.48       | 12212     |\n| 5072   | 45.53         | 11076        | 18.80     | 9887      | 17.76     | 6230      | 17.33       | 10938     |\n| 6072   | 65.01         | 10114        | 19.50     | 9049      | 17.68     | 5554      | 17.26       | 10133     |\n| 7072   | 85.96         | 8647         | 20.60     | 7861      | 17.83     | 4820      | 17.22       | 8670      |\n| 8072   | 102.74        | 7755         | 21.60     | 6991      | 18.06     | 4281      | 17.30       | 7729      |\n| 9072   | 125.99        | 6953         | 22.14     | 6360      | 18.12     | 3823      | 17.26       | 6939      |\n| 10072  | 133.68        | 6646         | 23.21     | 6068      | 18.37     | 3579      | 17.28       | 6597      |\n| 11072  | 161.29        | 5663         | 24.39     | 5158      | 18.64     | 3119      | 17.26       | 5585      |\n| 12072  | 169.55        | 5567         | 26.70     | 5111      | 18.93     | 2920      | 17.24       | 5397      |\n| 13072  | 189.43        | 5044         | 29.33     | 4658      | 19.10     | 2735      | 17.15       | 4809      |\n| 14072  | 203.86        | 4915         | 32.21     | 4616      | OOM       | -         | 17.22       | 4866      |\n| 16072  |               | 4382         | 34.51     | 4099      | OOM       | -         |             | 4491      |\n| 15072  | 221.14 231.29 | 4561         | 33.47     | 4292      | OOM       | -         | 17.23 17.22 | 4312      |\n\n## A.2 ALIBI RESULTS ON WIKITEXT-103\n\nFigure 8: The training speed and validation perplexity (with L valid = 3072) for ALiBi models and the sinusoidal model trained with L = 3072. All our models trained on 512 or more tokens achieve better perplexity than the sinusoidal model even though all of them (except the L = 3072) require less time and memory to train.\n\n<!-- image -->\n\nFigure 8 depicts a cross section of Figure 4, showing our models with different train lengths and the sinusoidal baseline, all evaluated on L valid = 3072 tokens. We observe that all our models with 512 ≤ L &lt; 3072 are faster to train than the sinusoidal model with L = 3072, but they all achieve greater perplexity scores on the validation set. Our model with L = 3072 trains just as fast as the sinusoidal one but bests its score by more than one perplexity point; (the standard deviation for the the sinusoidal model with L = 3072 is 0.24).\n\nTable 5 shows the perplexity values obtained when 8 different ALiBi models, trained on L values between 64 and 3072, extrapolating to L valid values longer than the ones they were trained on. In addition, we present results for the sinusoidal, rotary and T5 bias models, with L valid = L .\n\nTable 5: Perplexity when ALiBi extrapolates on the WikiText-103 development set. ∗ For results we present for the sinusoidal, rotary and T5 bias models, L = L valid (so we do not test the extrapolation abilities of those baselines here).\n\n| ALiBi        | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| Train Length | 64                  | 128                 | 256                 | 512                 | 1024                | 1536                | 2048                |             3072    |\n| 64           | 28.46               | 24.70               | 22.88               | 22.09               | 21.73               | 21.63               | 21.59               |               21.53 |\n| 128          | -                   | 23.98               | 21.70               | 20.67               | 20.36               | 20.29               | 20.31               |               20.28 |\n| 256          | -                   | -                   | 21.29               | 19.89               | 19.29               | 19.13               | 19.10               |               19.03 |\n| 512          | -                   | -                   | -                   | 19.73               | 18.81               | 18.50               | 18.48               |               18.4  |\n| 1024         | -                   | -                   | -                   | -                   | 18.66               | 18.20               | 18.05               |               17.96 |\n| 1536         | -                   | -                   | -                   | -                   | -                   | 18.12               | 17.90               |               17.72 |\n| 2048         | -                   | -                   | -                   | -                   | -                   | -                   | 17.91               |               17.64 |\n| 3072         | -                   | -                   | -                   | -                   | -                   | -                   | -                   |               17.6  |\n| Sinusoidal ∗ | 28.03               | 23.81               | 21.45               | 20.05               | 19.34               | 19.05               | 18.87               |               18.67 |\n| Rotary ∗     | -                   | -                   | -                   | 20.07               | 19.33               | -                   | -                   |               18.57 |\n| T5 Bias ∗    | -                   | -                   | -                   | 19.65               | 18.80               | -                   | -                   |               18.01 |\n\nTable 6 compares ALiBi to the sinusoidal, rotary and T5 bias baselines on the test set of WikiText103, and Table 7 compares ALiBi to the current state of the art models on that test set.\n\nTable 6: Test perplexity and runtime on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods.\n\n| Model                         | Param. ↓   | Train   | Inference   | Inference   | Inference   |\n|-------------------------------|------------|---------|-------------|-------------|-------------|\n|                               |            | Speed ↑ | Speed ↑     | Valid ↓     | Test ↓      |\n| Sinusoidal, L = 3072          | 247M       | 15.3k   | 13.6k       | 18.67       | 19.38       |\n| Rotary, L = 3072              | 247M       | 11.5k   | 12.2k       | 18.57       | 19.28       |\n| T5 Bias, L = 3072             | 247M       | 4.3k    | 7.3k        | 18.01       | 18.73       |\n| ALiBi L = 512, L valid = 3072 | 247M       | 28.3k   | 13.6k       | 18.40       | 19.08       |\n| L = 3072, L valid = 3072      | 247M       | 15.5k   | 13.6k       | 17.60       | 18.30       |\n\nTable 7: Valid and test perplexity scores on WikiText-103 for two of our ALiBi models and models that use the sinusoidal, rotary and T5 bias methods with sliding window evaluation (§B and S=512 following (Baevski &amp; Auli, 2018; Khandelwal et al., 2020; Press et al., 2021)). The sinusoidal model presents our results from training and inference with the model of Baevski &amp; Auli.\n\n| Model                                      | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------------|------------|-----------|----------|\n| Adaptive Inputs (Baevski &Auli, 2018)      | 247M       | 17.97     |    18.7  |\n| Transformer-XL (Dai et al., 2019)          | 257M       | -         |    18.3  |\n| Shortformer (Press et al., 2021)           | 247M       | 17.47     |    18.15 |\n| Sandwich Transformer (Press et al., 2020)  | 247M       | -         |    17.96 |\n| Staged Training (Press et al., 2021)       | 247M       | -         |    17.56 |\n| Compressive Transformer (Rae et al., 2020) | 329M       | -         |    17.1  |\n| Routing Transformer (Roy et al., 2020)     | -          | -         |    15.8  |\n| kNN-LM (Khandelwal et al., 2020)           | 247M       | 15.81     |    15.79 |\n| Sinusoidal, L = 3072                       | 247M       | 17.95     |    18.67 |\n| Rotary, L = 3072                           | 247M       | 17.98     |    18.72 |\n| T5 Bias, L = 3072                          | 247M       | 17.37     |    18.12 |\n| ALiBi L = 512, L valid = 3072              | 247M       | 18.30     |    19.01 |\n| L = 3072, L valid = 3072                   | 247M       | 16.97     |    17.66 |\n\n## A.3 RESULTS ON THE TORONTO BOOK CORPUS\n\nTo ensure that our results are not specific to the WikiText-103 corpus, we next apply our model and the baselines to a different domain while using a similar model architecture and the same ALiBi slopes as those used in the previous subsection.\n\nWeemphasize that our set of slopes was chosen by running experiments on the WikiText-103 corpus, and here we apply that set of slopes to a model trained on a very different text domain. Throughout the entire process of developing this method, we ran only one set of experiments on this domain using the previously selected set of slopes.\n\nSpecifically, we use the Toronto BooksCorpus (Zhu et al., 2015), which has been used to train BERT (Devlin et al., 2019) (in conjuction with the English Wikipedia). The corpus is about 700M tokens (2.9 GB).\n\nWe use the same train/validation/test split as Khandelwal et al. (2020) and their tokenization, which uses BERT's vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103's, we replace the adaptive word embedding and softmax of Baevski &amp; Auli (2018) with a tied word embedding and softmax matrix (Press &amp; Wolf, 2017; Inan et al., 2017).\n\nOur results in Figure 9 (and Table 8) replicate our success on the WikiText-103 dataset. Our model surpasses the sinusoidal baseline when trained on the same amount of input tokens ( L ) and, in\n\nFigure 9: ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus. Our models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even when trained on much shorter sequences.\n\n<!-- image -->\n\naddition, our model is able to extrapolate to longer sequences at inference. This occurs even though our set of slopes was not tuned on this dataset. This result establishes the generality of ALiBi and the particular set of slopes we found and suggests that they may be used on different text domains without further hyperparameter tuning.\n\nTables 9 and 10 present the perplexities for our ALiBi models, the baselines, and the current state of the art on the Toronto BookCorpus validation and test sets. Our results here mirror our results on WikiText-103: we improve over the sinusoidal baseline even when AliBi is trained on fewer tokens.\n\nTable 8: ALiBi models extrapolating on the Toronto BookCorpus development set. ∗ For the results of the sinusoidal models, L = L valid (so we do not test the extrapolation abilities of those models here).\n\n|              | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|\n| Train Length | 512                 | 1024                |             3072    |\n| 512          | 14.29               | 13.64               |               13.55 |\n| 1024         | -                   | 13.86               |               13.52 |\n| 3072         | -                   | -                   |               13.15 |\n| Sinusoidal ∗ | 14.80               | 14.73               |               14.46 |\n\nTable 9: Validation and test perplexities on the Toronto Book Corpus dataset.\n\n| Model                               | Param.   |   Valid ↓ |   Test ↓ |\n|-------------------------------------|----------|-----------|----------|\n| Sinusoidal, L = 3072                | 247M     |     14.46 |    11.67 |\n| ALiBi L train = 512, L valid = 3072 | 247M     |     13.55 |    10.98 |\n| L train = 3072, L valid = 3072      | 247M     |     13.15 |    10.73 |\n\n## A.4 RESULTS ON THE CC100+ROBERTA CORPUS\n\nTable 11 compares our 1.3 billion parameter ALiBi models when extrapolating to two times the number of tokens that they were trained on. We use the sinusoidal model as our baseline, and train it for the same amount of time as we train the ALiBi model that we compare it to (and so since our ALiBi models run faster in this setting, the sinusoidal models complete less updates).\n\nTable 10: Validation and test perplexities on the Toronto Book Corpus dataset with a sliding window (§B). Following (Baevski &amp; Auli, 2018; Khandelwal et al., 2020; Press et al., 2020; 2021), we set the sliding window stride S =512.\n\n| Model                                | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------|------------|-----------|----------|\n| kNN-LM (Khandelwal et al., 2020)     | 247M       | 14.20     |    10.89 |\n| Shortformer (Press et al., 2021)     | 247M       | 13.40     |    10.88 |\n| Sandwich (Press et al., 2020)        | 247M       | -         |    10.83 |\n| Staged Training (Press et al., 2021) | 247M       | 12.80     |    10.48 |\n| Sinusoidal, L = 3072                 | 247M       | 14.06     |    11.4  |\n| L = 512, L valid = 3072              | 247M       | 13.76     |    11.11 |\n| ALiBi L = 3072, L valid = 3072       | 247M       | 12.70     |    10.4  |\n\nTable 11: Perplexity, memory, and train time on the CC100+RoBERTa corpus for our ALiBi models and the sinusoidal baseline. We run our L = 512 (1024) model and the sinusoidal model with L = 1024 (2048) for the same amount of time. We show that our models achieve strong results even though they use 6-11% less memory.\n\n|                            | Training   | Training   | Training   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|----------------|----------------|\n|                            | Memory ↓   | Updates    | Hours ↓    | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 46.7k      | 5.5k       | 9.24           | -              |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.30           | -              |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 44.2k      | 5.9k       | -              | 9.01           |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -              | 8.92           |\n\nTable 12 compares our 1.3 billion parameter ALiBi models to the sinusoidal baselines, with and without extrapolation, with all models completing 50,000 updates.\n\nTable 12: Perplexity, train time and memory use of the sinusoidal and ALiBi models on the CC100+RoBERTa corpus when all models are trained with 50k updates.\n\n|                            | Training   | Training   | Training   | Valid PPL ↓   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|---------------|----------------|----------------|\n|                            | Memory     | Updates    | Hours ↓    | L valid = 512 | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 512  | 24.6 GB    | 50.0k      | 5.5k       | 9.71          | 37.05          | 105.42         |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.79          | 9.30           | 9.54           |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.15           | 48.85          |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.16           | 8.92           |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 50.0k      | 6.7k       | -             | -              | 8.83           |\n| ALiBi, L train = 2048      | 29.4 GB    | 50.0k      | 6.7k       | -             | -              | 8.84           |\n\n## B ANALYSIS\n\nIn this section we investigate why ALiBi works so effectively. We find that ALiBi's decrease in perplexity when given longer sequences is largely explained by its improved avoidance of the early token curse. We hypothesize that future work building on ALiBi might achieve further gains by more efficiently exploiting longer histories.\n\n## B.1 DEFINING SLIDING WINDOW EVALUATION AND THE EARLY TOKEN CURSE\n\nFigure 10: Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom; red) on a sequence of 8 words using a model with L valid = 4. Nonoverlapping evaluation is much faster since it requires just two inference passes (as opposed to the five passes required by the siding window approach). But the sliding window approach provides more context for each prediction.\n\n<!-- image -->\n\nSliding Window Inference As mentioned in Section 2, nonoverlapping inference is commonly used to evaluate sequences longer than L (the number of tokens in each training subsequence). An alternative is to use a sliding window during evaluation (Baevski &amp; Auli, 2018).\n\nA stride S is picked between 1 and L -1 , and the window is advanced by S tokens after each forward pass. 12 This means that L -S tokens from the previous subsequence are re-encoded, and only S new tokens are output. The advantage is that all outputs in each subsequence after the first have at least L -S previous tokens to condition on. However, since tokens must be re-encoded multiple times, this approach is much slower than the nonoverlapping one. When S = 1 , we output one token every inference pass, each using the maximal context window that the model can handle; however, this is the slowest approach. Figure 10 is a visualization of the nonoverlapping and sliding window evaluation approaches.\n\nWe use sliding window inference as a tool to analyze our models, but we note that it is normally prohibitively slow in practice (Press et al., 2021).\n\nEarly Token Curse Splitting an evaluation set into subsequences means that predictions occuring early in each subsequence cannot access many previous context tokens (appearing at the end of the previous subsequence). The result, referred to as the early token curse (Press et al., 2021), increases (i.e., degrades) perplexity scores. A workaround is to evaluate the model using a sliding window, giving each prediction more context. This solution is slow since it requires many more forward passes of the model.\n\n## B.2 EXTRAPOLATION REDUCES THE EARLY TOKEN CURSE\n\nWe presented results showing that our ALiBi method (and, to a lesser extent, the T5 bias) allows LMs to extrapolate during inference. Two reasons could explain why these methods enable LMs to achieve better perplexity given longer input subsequences:\n\n1. Performance improves because the models can use longer contexts to make more accurate predictions. For example, the average article length in the WikiText-103 corpus is about 3600 tokens; therefore, if a model trained on L = 512 tokens extrapolates to L valid = 3072 tokens during inference and achieves better results, that might be because it can spot patterns occurring across more than 512 tokens.\n2. Performance improves because longer input sequences mean the early token curse is reduced. For example, during nonoverlapping evaluation on sequences of length L valid = 1000, 10% of predictions have 100 tokens of context or less. If we rerun nonoverlapping evaluation on that model with L valid = 2000 tokens, now only 5% of predictions have 100\n\n12 Nonoverlapping inference can be viewed as sliding window inference with stride L .\n\ntokens of context or less. So, by simply being able to handle longer sequences, a model can substantially reduce the early token curse and improve performance. 13\n\nTo better understand what might be occurring, we re-evaluate the development set of WikiText-103 with our models and the sinusoidal baseline with L = 512 , 1024 , 3072 . However, this time we use sliding window evaluation with a stride of S = 1 , meaning that we move the sliding window just one token after every inference pass, giving each prediction the maximum number of context tokens that the model can use.\n\nFigure 11: ALiBi models evaluated on different input lengths on WikiText-103 with sliding window evaluation (with stride S = 1 ). Unlike results shown in Figure 4, where performance improves in each of our models as we increase the validation sequence length, here performance stays relatively flat as we increase L valid . This might mean that ALiBi increases performance when L valid &gt; L not because it uses longer contexts, but because fewer tokens suffer from the early token curse. Note that as in §2, the perplexity of the sinusoidal model explodes when L valid &gt; L even when using sliding window evaluation.\n\n<!-- image -->\n\nThe results are shown in Figure 11 and in the corresponding Tables 13 (sinusoidal) and 15 (ALiBi).\n\nUnsurprisingly, for the sinusoidal model, as in §2, increasing L valid causes an explosion in perplexity even when using sliding window evaluation. Our ALiBi models cannot improve perplexity when looking at longer sequences in this setting, but they keep perplexity flat when L valid increases.\n\nThis leads us to believe that our perplexity improvement when increasing L valid and using nonoverlapping evaluation is caused by explanation 2, not explanation 1. Because sliding window evaluation provides long context windows for every prediction made, it curtails the early token curse. In this setting, ALiBi's performance remains flat when L valid increases, leading us to hypothesize that the gains seen while increasing L valid in §4 were the result of larger L valid values mitigating the early token curse.\n\nOur ALiBi results mirror what occurs in the model using the T5 bias: when using sliding window evaluation, perplexity remains relatively flat when evaluating longer sequences (see Table 14).\n\nOur analysis reveals that when L valid &gt; L , ALiBi might not be using contexts longer than the ones it was trained on. This highlights a research direction that could be pursued in future work.\n\nThese findings do not lessen the value of ALiBi. When L valid = L , ALiBi achieves either superior or similar results to the sinusoidal method and other alternatives even though it is simpler and requires no learned parameters. When evaluating L valid &gt; L tokens, even if ALiBi does not attend to more than L tokens, it yields better results than the other alternatives that can be used in this case, i.e., standard nonoverlapping inference (which is cheap, but does not perform as well) and the more accurate sliding window approach (which is very slow).\n\n13 100 tokens is an arbitrary small number used here to represent a short history context, i.e., one in which making predictions for the next output token would be harder.\n\nTable 13: Perplexities of the sinusoidal models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 18.35                         | 204.42                        | 264.74                        | 306.19                        |                        360.12 |\n| 1024         | -                             | 18.05                         | 206.55                        | 302.6                         |                        393.71 |\n| 3072         | -                             | -                             | -                             | -                             |                         18.03 |\n\nTable 14: Perplexities of the T5 bias models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.92                         | 18.51                         | 20.36                         | 22.62                         |                         30.77 |\n| 1024         | -                             | 17.65                         | 17.87                         | 18.51                         |                         20.66 |\n| 3072         | -                             | -                             | -                             | -                             |                         17.41 |\n\nTable 15: Perplexities of the ALiBi models evaluated with sliding window evaluation with stride S = 1 on the WikiText-103 validation dataset.\n\n|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.98                         | 17.92                         | 18.2                          | 18.28                         |                         18.3  |\n| 1024         | -                             | 17.46                         | 17.47                         | 17.62                         |                         17.92 |\n| 3072         | -                             | -                             | -                             | -                             |                         16.96 |",
  "tables": [
    {
      "index": 0,
      "markdown": "| Position Method   | Train Length   | Speed ( ↑ )   | Speed ( ↑ )   | Memory ( ↓ )   |\n|-------------------|----------------|---------------|---------------|----------------|\n|                   |                | Train         | Eval.         |                |\n|                   | 512            | 28.5k         | 82.1k         | 15.3 GB        |\n| Sinusoidal        | 1024           | 26.0k         | 77.8k         | 19.2 GB        |\n|                   | 3072           | 15.3k         | 42.4k         | 15.1 GB        |\n|                   | 512            | 20.0k         | 43.4k         | 17.8 GB        |\n| Rotary            | 1024           | 17.7k         | 39.4k         | 22.8 GB        |\n|                   | 3072           | 11.5k         | 29.5k         | 17.8 GB        |\n|                   | 512            | 14.4k         | 21.8k         | 16.9 GB        |\n| T5 Bias           | 1024           | 13.0k         | 20.2k         | 20.9 GB        |\n|                   | 3072           | 4.3k          | 4.9k          | 15.9 GB        |\n|                   | 512            | 28.3k         | 85.8k         | 15.3 GB        |\n| ALiBi             | 1024           | 25.8k         | 76.4k         | 19.3 GB        |\n|                   | 3072           | 15.5k         | 42.2k         | 15.2 GB        |"
    },
    {
      "index": 1,
      "markdown": "| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 512      | 20.05        | 15046        | 20.07     | 10839     | 19.65     | 11724     | 19.73     | 14726     |\n| 513      | 19.98        | 14925        | 20.01     | 10806     | 19.57     | 10491     | 19.62     | 14965     |\n| 522      | 19.93        | 15116        | 20.02     | 11295     | 19.57     | 9970      | 19.64     | 15316     |\n| 532      | 19.91        | 15358        | 19.98     | 10854     | 19.53     | 10382     | 19.61     | 15383     |\n| 542      | 19.91        | 15076        | 19.94     | 10795     | 19.47     | 12270     | 19.57     | 15301     |\n| 552      | 19.91        | 16394        | 19.93     | 12267     | 19.47     | 13000     | 19.54     | 16540     |\n| 562      | 19.91        | 16646        | 19.87     | 12481     | 19.39     | 12201     | 19.49     | 16385     |\n| 572      | 19.95        | 16934        | 19.83     | 12668     | 19.36     | 12851     | 19.46     | 16881     |\n| 582      | 20.13        | 16961        | 19.88     | 12594     | 19.41     | 13904     | 19.48     | 17064     |\n| 592      | 20.18        | 17243        | 19.84     | 13007     | 19.36     | 13706     | 19.43     | 17289     |\n| 602      | 20.40        | 17502        | 19.81     | 12788     | 19.33     | 14102     | 19.38     | 17141     |\n| 612      | 20.59        | 17637        | 19.81     | 12601     | 19.27     | 14573     | 19.38     | 17661     |\n| 712      | 24.86        | 15614        | 19.79     | 12676     | 19.10     | 13818     | 19.14     | 15637     |\n| 812      | 30.82        | 17151        | 20.17     | 13954     | 18.94     | 14377     | 18.99     | 17210     |\n| 912      | 37.42        | 17200        | 20.73     | 13887     | 18.86     | 15345     | 18.88     | 17619     |\n| 1012     | 43.54        | 16304        | 21.37     | 13759     | 18.79     | 14240     | 18.73     | 16059     |\n| 1112     | 50.36        | 16424        | 22.01     | 13891     | 18.77     | 14014     | 18.68     | 16659     |\n| 1212     | 58.01        | 17294        | 23.02     | 15245     | 18.87     | 14589     | 18.67     | 17372     |\n| 1312     | 63.62        | 15314        | 23.93     | 13698     | 18.84     | 13138     | 18.60     | 15698     |\n| 1412     | 70.75        | 15663        | 24.81     | 13928     | 18.87     | 12857     | 18.59     | 15860     |\n| 1512     | 76.23        | 15812        | 25.99     | 14248     | 18.91     | 13752     | 18.52     | 16225     |\n| 2512     | 132.41       | 15254        | 31.58     | 13456     | 20.41     | 9948      | 18.41     | 15204     |\n| 3512     | 178.97       | 13293        | 35.54     | 11850     | 22.91     | 7847      | 18.40     | 13329     |\n| 4512     | 209.37       | 11767        | 39.15     | 10485     | 25.91     | 6146      | 18.41     | 11738     |\n| 5512     | 240.44       | 10168        | 43.14     | 9020      | 29.54     | 5309      | 18.36     | 9986      |\n| 6512     | 271.40       | 9052         | 47.81     | 8108      | 34.48     | 4680      | 18.35     | 9022      |\n| 7512     | 293.02       | 8315         | 51.12     | 7483      | 39.29     | 4102      | 18.33     | 8324      |\n| 8512     | 305.65       | 7259         | 54.98     | 6718      | 43.08     | 3660      | 18.34     | 7366      |\n| 9512     | 336.02       | 6672         | 57.85     | 6211      | 48.90     | 3370      | 18.34     | 6555      |\n| 10512    | 341.53       | 6126         | 60.77     | 5575      | 52.95     | 3010      | 18.32     | 6030      |\n| 11512    | 362.74       | 5994         | 66.62     | 5445      | 61.38     | 2873      | 18.32     | 5882      |\n| 12512    | 373.17       | 5421         | 69.70     | 4988      | 64.94     | 2602      | 18.31     | 5287      |\n| 13512    | 382.91       | 5174         | 73.27     | 4692      | OOM       | -         | 18.31     | 4962      |\n| 14512    | 399.98       | 4351         | 75.52     | 3969      | OOM       | -         | 18.31     | 4352      |\n| 15512    | 406.01       | 4291         | 79.25     | 4103      | OOM       | -         | 18.31     | 4289      |"
    },
    {
      "index": 2,
      "markdown": "| Inputs   | Sinusoidal   | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi     | ALiBi     |\n|----------|--------------|--------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Inputs   | PPL ( ↓ )    | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) |\n| 1024     | 19.34        | 17002        | 19.33     | 14690     | 18.80     | 14973     | 18.66     | 16951     |\n| 1025     | 19.33        | 16630        | 19.34     | 14423     | 18.82     | 14635     | 18.67     | 16690     |\n| 1034     | 19.27        | 16589        | 19.28     | 14351     | 18.74     | 14435     | 18.60     | 16707     |\n| 1044     | 19.26        | 16760        | 19.27     | 14491     | 18.72     | 14644     | 18.60     | 16667     |\n| 1054     | 19.23        | 16747        | 19.26     | 14503     | 18.71     | 14800     | 18.58     | 16833     |\n| 1064     | 19.21        | 16676        | 19.22     | 14623     | 18.70     | 14498     | 18.55     | 16941     |\n| 1074     | 19.19        | 16879        | 19.19     | 14464     | 18.65     | 14670     | 18.49     | 16936     |\n| 1084     | 19.22        | 16942        | 19.23     | 14650     | 18.70     | 14607     | 18.56     | 17090     |\n| 1094     | 19.24        | 16771        | 19.22     | 14629     | 18.69     | 14517     | 18.54     | 16880     |\n| 1104     | 19.28        | 16870        | 19.27     | 14837     | 18.69     | 14635     | 18.52     | 17009     |\n| 1114     | 19.29        | 16795        | 19.27     | 14879     | 18.69     | 14540     | 18.52     | 17050     |\n| 1124     | 19.26        | 17312        | 19.18     | 15121     | 18.62     | 14480     | 18.46     | 17571     |\n| 1224     | 20.54        | 17901        | 19.38     | 15584     | 18.58     | 14956     | 18.40     | 18013     |\n| 1324     | 23.13        | 16308        | 19.96     | 14386     | 18.52     | 13726     | 18.33     | 16422     |\n| 1424     | 26.45        | 16217        | 21.27     | 14385     | 18.48     | 13516     | 18.28     | 16121     |\n| 1524     | 29.82        | 16377        | 22.59     | 14693     | 18.42     | 13587     | 18.22     | 16659     |\n| 1624     | 34.27        | 15928        | 24.34     | 14228     | 18.40     | 12979     | 18.17     | 16053     |\n| 1724     | 38.24        | 16640        | 25.66     | 14686     | 18.35     | 12976     | 18.15     | 16607     |\n| 1824     | 42.23        | 16840        | 27.63     | 14918     | 18.30     | 13071     | 18.08     | 16846     |\n| 1924     | 46.46        | 15071        | 29.64     | 13452     | 18.31     | 11843     | 18.08     | 15118     |\n| 2024     | 51.09        | 15591        | 31.17     | 13706     | 18.34     | 11906     | 18.05     | 15557     |\n| 3024     | 96.46        | 13639        | 35.67     | 12256     | 18.62     | 8480      | 17.92     | 13668     |\n| 4024     | 144.00       | 12441        | 44.30     | 11203     | 19.44     | 7443      | 17.95     | 12402     |\n| 5024     | 182.31       | 11431        | 48.31     | 10324     | 20.47     | 6384      | 17.92     | 11394     |\n| 6024     | 214.02       | 10238        | 54.78     | 9117      | 21.76     | 5577      | 18.01     | 10119     |\n| 7024     | 261.86       | 8785         | 62.83     | 7950      | 23.64     | 4867      | 17.93     | 8779      |\n| 8024     | 284.88       | 8132         | 64.91     | 7355      | 25.79     | 4377      | 17.96     | 8086      |\n| 9024     | 310.04       | 7045         | 71.91     | 6380      | 27.54     | 3787      | 17.98     | 7001      |\n| 10024    | 337.48       | 6633         | 77.70     | 6016      | 29.54     | 3582      | 17.97     | 6583      |\n| 11024    | 358.43       | 5722         | 81.15     | 5219      | 31.94     | 3170      | 18.02     | 5641      |\n| 12024    | 375.95       | 5560         | 87.51     | 5072      | 33.35     | 2940      | 18.01     | 5294      |\n| 13024    | 393.57       | 4691         | 94.74     | 4383      | OOM       | -         | 17.98     | 4621      |\n| 14024    | 403.52       | 4905         | 96.10     | 4546      | OOM       | -         | 18.01     | 4827      |\n| 15024    | 431.66       | 4518         | 99.78     | 3878      | OOM       | -         | 17.96     | 4447      |\n| 16024    | 453.32       | 4239         | 106.99    | 4170      | OOM       | -         | 17.98     | 4153      |"
    },
    {
      "index": 3,
      "markdown": "|        | Sinusoidal    | Sinusoidal   | Rotary    | Rotary    | T5 Bias   | T5 Bias   | ALiBi       | ALiBi     |\n|--------|---------------|--------------|-----------|-----------|-----------|-----------|-------------|-----------|\n| Inputs | PPL ( ↓ )     | WPS ( ↑ )    | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ ) | WPS ( ↑ ) | PPL ( ↓ )   | WPS ( ↑ ) |\n| 3072   | 18.67         | 13380        | 18.57     | 12548     | 18.01     | 8828      | 17.60       | 13866     |\n| 3073   | 18.67         | 13773        | 18.57     | 12474     | 18.01     | 8483      | 17.59       | 13793     |\n| 3082   | 18.62         | 13741        | 18.54     | 12388     | 17.95     | 8698      | 17.59       | 13778     |\n| 3092   | 18.60         | 13742        | 18.48     | 12458     | 17.92     | 8361      | 17.55       | 13783     |\n| 3102   | 18.65         | 13701        | 18.52     | 12365     | 17.94     | 8764      | 17.59       | 13747     |\n| 3112   | 18.64         | 13809        | 18.51     | 12449     | 17.96     | 8665      | 17.59       | 13827     |\n| 3122   | 18.68         | 13722        | 18.52     | 12432     | 17.98     | 8437      | 17.58       | 13795     |\n| 3132   | 18.67         | 13825        | 18.54     | 12490     | 17.97     | 8653      | 17.58       | 13784     |\n| 3142   | 18.69         | 13543        | 18.52     | 12230     | 17.97     | 8282      | 17.61       | 13572     |\n| 3152   | 18.66         | 13520        | 18.56     | 12240     | 17.98     | 8608      | 17.59       | 13523     |\n| 3162   | 18.71         | 13501        | 18.56     | 12253     | 18.04     | 8589      | 17.62       | 13598     |\n| 3172   | 18.72         | 13563        | 18.55     | 12297     | 17.99     | 8583      | 17.59       | 13625     |\n| 3272   | 18.87         | 13453        | 18.55     | 12148     | 17.93     | 8144      | 17.59       | 13482     |\n| 3372   | 19.46         | 13533        | 18.50     | 12254     | 17.88     | 8442      | 17.52       | 13565     |\n| 3472   | 20.55         | 13047        | 18.52     | 11868     | 17.95     | 7857      | 17.54       | 13107     |\n| 3572   | 21.84         | 13128        | 18.50     | 11882     | 17.86     | 7814      | 17.50       | 13170     |\n| 3672   | 23.04         | 13106        | 18.49     | 11859     | 17.87     | 7719      | 17.48       | 13196     |\n| 3772   | 24.47         | 13287        | 18.54     | 11942     | 17.85     | 7579      | 17.49       | 13312     |\n| 3872   | 25.85         | 12621        | 18.40     | 11272     | 17.82     | 7581      | 17.41       | 12566     |\n| 3972   | 27.21         | 12379        | 18.48     | 11151     | 17.84     | 7483      | 17.41       | 12324     |\n| 4072   | 28.59         | 12178        | 18.59     | 11019     | 17.88     | 6974      | 17.48       | 12212     |\n| 5072   | 45.53         | 11076        | 18.80     | 9887      | 17.76     | 6230      | 17.33       | 10938     |\n| 6072   | 65.01         | 10114        | 19.50     | 9049      | 17.68     | 5554      | 17.26       | 10133     |\n| 7072   | 85.96         | 8647         | 20.60     | 7861      | 17.83     | 4820      | 17.22       | 8670      |\n| 8072   | 102.74        | 7755         | 21.60     | 6991      | 18.06     | 4281      | 17.30       | 7729      |\n| 9072   | 125.99        | 6953         | 22.14     | 6360      | 18.12     | 3823      | 17.26       | 6939      |\n| 10072  | 133.68        | 6646         | 23.21     | 6068      | 18.37     | 3579      | 17.28       | 6597      |\n| 11072  | 161.29        | 5663         | 24.39     | 5158      | 18.64     | 3119      | 17.26       | 5585      |\n| 12072  | 169.55        | 5567         | 26.70     | 5111      | 18.93     | 2920      | 17.24       | 5397      |\n| 13072  | 189.43        | 5044         | 29.33     | 4658      | 19.10     | 2735      | 17.15       | 4809      |\n| 14072  | 203.86        | 4915         | 32.21     | 4616      | OOM       | -         | 17.22       | 4866      |\n| 16072  |               | 4382         | 34.51     | 4099      | OOM       | -         |             | 4491      |\n| 15072  | 221.14 231.29 | 4561         | 33.47     | 4292      | OOM       | -         | 17.23 17.22 | 4312      |"
    },
    {
      "index": 4,
      "markdown": "| ALiBi        | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|\n| Train Length | 64                  | 128                 | 256                 | 512                 | 1024                | 1536                | 2048                |             3072    |\n| 64           | 28.46               | 24.70               | 22.88               | 22.09               | 21.73               | 21.63               | 21.59               |               21.53 |\n| 128          | -                   | 23.98               | 21.70               | 20.67               | 20.36               | 20.29               | 20.31               |               20.28 |\n| 256          | -                   | -                   | 21.29               | 19.89               | 19.29               | 19.13               | 19.10               |               19.03 |\n| 512          | -                   | -                   | -                   | 19.73               | 18.81               | 18.50               | 18.48               |               18.4  |\n| 1024         | -                   | -                   | -                   | -                   | 18.66               | 18.20               | 18.05               |               17.96 |\n| 1536         | -                   | -                   | -                   | -                   | -                   | 18.12               | 17.90               |               17.72 |\n| 2048         | -                   | -                   | -                   | -                   | -                   | -                   | 17.91               |               17.64 |\n| 3072         | -                   | -                   | -                   | -                   | -                   | -                   | -                   |               17.6  |\n| Sinusoidal ∗ | 28.03               | 23.81               | 21.45               | 20.05               | 19.34               | 19.05               | 18.87               |               18.67 |\n| Rotary ∗     | -                   | -                   | -                   | 20.07               | 19.33               | -                   | -                   |               18.57 |\n| T5 Bias ∗    | -                   | -                   | -                   | 19.65               | 18.80               | -                   | -                   |               18.01 |"
    },
    {
      "index": 5,
      "markdown": "| Model                         | Param. ↓   | Train   | Inference   | Inference   | Inference   |\n|-------------------------------|------------|---------|-------------|-------------|-------------|\n|                               |            | Speed ↑ | Speed ↑     | Valid ↓     | Test ↓      |\n| Sinusoidal, L = 3072          | 247M       | 15.3k   | 13.6k       | 18.67       | 19.38       |\n| Rotary, L = 3072              | 247M       | 11.5k   | 12.2k       | 18.57       | 19.28       |\n| T5 Bias, L = 3072             | 247M       | 4.3k    | 7.3k        | 18.01       | 18.73       |\n| ALiBi L = 512, L valid = 3072 | 247M       | 28.3k   | 13.6k       | 18.40       | 19.08       |\n| L = 3072, L valid = 3072      | 247M       | 15.5k   | 13.6k       | 17.60       | 18.30       |"
    },
    {
      "index": 6,
      "markdown": "| Model                                      | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------------|------------|-----------|----------|\n| Adaptive Inputs (Baevski &Auli, 2018)      | 247M       | 17.97     |    18.7  |\n| Transformer-XL (Dai et al., 2019)          | 257M       | -         |    18.3  |\n| Shortformer (Press et al., 2021)           | 247M       | 17.47     |    18.15 |\n| Sandwich Transformer (Press et al., 2020)  | 247M       | -         |    17.96 |\n| Staged Training (Press et al., 2021)       | 247M       | -         |    17.56 |\n| Compressive Transformer (Rae et al., 2020) | 329M       | -         |    17.1  |\n| Routing Transformer (Roy et al., 2020)     | -          | -         |    15.8  |\n| kNN-LM (Khandelwal et al., 2020)           | 247M       | 15.81     |    15.79 |\n| Sinusoidal, L = 3072                       | 247M       | 17.95     |    18.67 |\n| Rotary, L = 3072                           | 247M       | 17.98     |    18.72 |\n| T5 Bias, L = 3072                          | 247M       | 17.37     |    18.12 |\n| ALiBi L = 512, L valid = 3072              | 247M       | 18.30     |    19.01 |\n| L = 3072, L valid = 3072                   | 247M       | 16.97     |    17.66 |"
    },
    {
      "index": 7,
      "markdown": "|              | Evaluation Length   | Evaluation Length   |   Evaluation Length |\n|--------------|---------------------|---------------------|---------------------|\n| Train Length | 512                 | 1024                |             3072    |\n| 512          | 14.29               | 13.64               |               13.55 |\n| 1024         | -                   | 13.86               |               13.52 |\n| 3072         | -                   | -                   |               13.15 |\n| Sinusoidal ∗ | 14.80               | 14.73               |               14.46 |"
    },
    {
      "index": 8,
      "markdown": "| Model                               | Param.   |   Valid ↓ |   Test ↓ |\n|-------------------------------------|----------|-----------|----------|\n| Sinusoidal, L = 3072                | 247M     |     14.46 |    11.67 |\n| ALiBi L train = 512, L valid = 3072 | 247M     |     13.55 |    10.98 |\n| L train = 3072, L valid = 3072      | 247M     |     13.15 |    10.73 |"
    },
    {
      "index": 9,
      "markdown": "| Model                                | Param. ↓   | Valid ↓   |   Test ↓ |\n|--------------------------------------|------------|-----------|----------|\n| kNN-LM (Khandelwal et al., 2020)     | 247M       | 14.20     |    10.89 |\n| Shortformer (Press et al., 2021)     | 247M       | 13.40     |    10.88 |\n| Sandwich (Press et al., 2020)        | 247M       | -         |    10.83 |\n| Staged Training (Press et al., 2021) | 247M       | 12.80     |    10.48 |\n| Sinusoidal, L = 3072                 | 247M       | 14.06     |    11.4  |\n| L = 512, L valid = 3072              | 247M       | 13.76     |    11.11 |\n| ALiBi L = 3072, L valid = 3072       | 247M       | 12.70     |    10.4  |"
    },
    {
      "index": 10,
      "markdown": "|                            | Training   | Training   | Training   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|----------------|----------------|\n|                            | Memory ↓   | Updates    | Hours ↓    | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 46.7k      | 5.5k       | 9.24           | -              |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.30           | -              |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 44.2k      | 5.9k       | -              | 9.01           |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -              | 8.92           |"
    },
    {
      "index": 11,
      "markdown": "|                            | Training   | Training   | Training   | Valid PPL ↓   | Valid PPL ↓    | Valid PPL ↓    |\n|----------------------------|------------|------------|------------|---------------|----------------|----------------|\n|                            | Memory     | Updates    | Hours ↓    | L valid = 512 | L valid = 1024 | L valid = 2048 |\n| Sinusoidal, L train = 512  | 24.6 GB    | 50.0k      | 5.5k       | 9.71          | 37.05          | 105.42         |\n| ALiBi, L train = 512       | 24.6 GB    | 50.0k      | 5.5k       | 9.79          | 9.30           | 9.54           |\n| Sinusoidal, L train = 1024 | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.15           | 48.85          |\n| ALiBi, L train = 1024      | 26.2 GB    | 50.0k      | 5.9k       | -             | 9.16           | 8.92           |\n| Sinusoidal, L train = 2048 | 29.3 GB    | 50.0k      | 6.7k       | -             | -              | 8.83           |\n| ALiBi, L train = 2048      | 29.4 GB    | 50.0k      | 6.7k       | -             | -              | 8.84           |"
    },
    {
      "index": 12,
      "markdown": "|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 18.35                         | 204.42                        | 264.74                        | 306.19                        |                        360.12 |\n| 1024         | -                             | 18.05                         | 206.55                        | 302.6                         |                        393.71 |\n| 3072         | -                             | -                             | -                             | -                             |                         18.03 |"
    },
    {
      "index": 13,
      "markdown": "|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.92                         | 18.51                         | 20.36                         | 22.62                         |                         30.77 |\n| 1024         | -                             | 17.65                         | 17.87                         | 18.51                         |                         20.66 |\n| 3072         | -                             | -                             | -                             | -                             |                         17.41 |"
    },
    {
      "index": 14,
      "markdown": "|              | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   | Evaluation Length ( S = 1 )   |   Evaluation Length ( S = 1 ) |\n|--------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n| Train Length | 512                           | 1024                          | 1536                          | 2048                          |                       3072    |\n| 512          | 17.98                         | 17.92                         | 18.2                          | 18.28                         |                         18.3  |\n| 1024         | -                             | 17.46                         | 17.47                         | 17.62                         |                         17.92 |\n| 3072         | -                             | -                             | -                             | -                             |                         16.96 |"
    }
  ],
  "stats": {
    "pages": 25,
    "chunksCreated": 123,
    "totalCharacters": 88328,
    "totalWords": 13629,
    "numTables": 15,
    "processingTimeMs": 65602
  }
}