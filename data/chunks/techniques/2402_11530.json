{
  "paper": {
    "id": "2402.11530v3",
    "title": "Efficient Multimodal Learning from Data-centric Perspective",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably cause significant performance drops. In this paper, we demonstrate the possibility of training a smaller but better MLLM with high-quality training data. Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from selected training data. Experiments show that our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks. We expect that this work can provide the community with a clean and flexible open-source tool for further research and development. The code, models, and data can be found in https://github.com/BAAI-DCAI/Bunny.",
    "authors": [
      "Muyang He",
      "Yexin Liu",
      "Boya Wu",
      "Jianhao Yuan",
      "Yueze Wang",
      "Tiejun Huang",
      "Bo Zhao"
    ],
    "published": "2024-02-18T10:09:10.000Z",
    "updated": "2024-07-22T09:54:40.000Z",
    "primaryCategory": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2402.11530v3",
    "absUrl": "https://arxiv.org/abs/2402.11530v3"
  },
  "chunks": [
    {
      "id": "2402.11530v3-chunk-0",
      "content": "Muyang He 1 , 2 ⋆ Yexin Liu 1 , 3 ⋆ Boya Wu 1 ⋆ 4 1 1 , 2 1 , 5 †\n\nJianhao Yuan Yueze Wang Tiejun Huang Bo Zhao\n\n1 Beijing Academy of Artificial Intelligence\n\n2 Peking University 3 HKUST(GZ) 4 University of Oxford 5 SJTU\n\n⋆ †\n\nEqual Contribution Corresponding to Bo Zhao &lt; bo.zhao@sjtu.edu.cn",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Efficient Multimodal Learning from Data-centric Perspective",
        "chunkIndex": 0,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-1",
      "content": "Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably cause significant performance drops. In this paper, we demonstrate the possibility of training a smaller but better MLLM with high-quality training data. Specifically, we introduce Bunny , a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from selected training data. Experiments show that our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks. We expect that this work can provide the community with a clean and flexible open-source tool for further research and development.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-2",
      "content": "outperforms the state-of-the-art large MLLMs on multiple benchmarks. We expect that this work can provide the community with a clean and flexible open-source tool for further research and development. The code, models, and data can be found in https://github.com/BAAI-DCAI/Bunny .",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-3",
      "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional visual understanding and reasoning performances across a range of tasks such as visual question answering [1, 2] and referring comprehension [3-5]. Benefiting from scalable Transformer-based architecture [6] and web-scale training data sources, these models have become foundational in the field of artificial intelligence, with their parameters increasing from billions to trillions [3, 7, 8].\n\nHowever, the deployment of these models is often hindered by their substantial computational costs and memory consumption in both training and inference phases, which limits their popularization across the broader research and user communities. Several early attempts, such as LLaVA-Phi [9], Imp [10], and Vary-toy [11], leverage off-the-shelf lightweight LLM backbones (e.g., Phi [12, 13]) and small vision encoders (e.g., SigLIP [14]) to build lightweight MLLMs.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-4",
      "content": "pts, such as LLaVA-Phi [9], Imp [10], and Vary-toy [11], leverage off-the-shelf lightweight LLM backbones (e.g., Phi [12, 13]) and small vision encoders (e.g., SigLIP [14]) to build lightweight MLLMs. Further works also explore optimizations in model architecture and training recipes for tailored lightweight MLLM designs [15, 16].\n\nWhile these attempts to create lightweight versions of MLLMs achieve various levels of success, these models often fall short in terms of performance when compared to their larger counterparts, as reducing the size of these models often leads to a compromise in model capacity. An alternative direction to bridge the performance gap is data optimization. Many works have shown the importance of high-quality training data. For instance, Phi-2 [12] demonstrates that curated high-quality text data can bring the performances of small-scale LLMs close to that of large-scale models. SVIT [17] validates the importance of data sources in MLLMs.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-5",
      "content": "2 [12] demonstrates that curated high-quality text data can bring the performances of small-scale LLMs close to that of large-scale models. SVIT [17] validates the importance of data sources in MLLMs. In this work, we focus on data optimization to compensate for the reduction in model size.\n\n&gt;\n\nWe introduce Bunny , a family of lightweight but powerful multimodal models with plug-and-play language/vision backbones and a multimodal projector. To compensate for the performance decrease caused by the model shrinking, we construct more informative training data by dataset condensation, i.e., curated data selection from a broader source. We demonstrate the possibility of training a smaller but better MLLM on more informative training data. Extensive experiments indicate that our Bunny-4B/8B model outperforms not only state-of-the-art small MLLMs with similar sizes but also even large ones on popular benchmarks.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-6",
      "content": "odel outperforms not only state-of-the-art small MLLMs with similar sizes but also even large ones on popular benchmarks.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-7",
      "content": "Recent advancements in Large Language Model (LLM) such as GPT [18, 19, 3], LLaMA [20, 7, 21] and PaLM [8, 22] have demonstrated remarkable capacity in various natural language processing tasks, for example, dialogue, creative writing, and problem-solving. Benefiting from scalable Transformer-based architecture [6] design and web-scale training data source, these models have become foundation models for general reasoning tasks. Among many research endeavors, one line of work focuses on miniaturizing LLM, for more efficient and low-cost training and inference. By leveraging high-quality pre-training and instruction tuning data, along with optimized lightweight architecture, lightweight LLMs with fewer than 3 billion parameters, such as Phi [12, 13], StableLM-2 [23], and TinyLLaMA [24], have demonstrated performance comparable to larger models with 7/13 billion parameters.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "2.1 Large Language Model",
        "chunkIndex": 7,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-8",
      "content": "Following the success of LLM, one line of research recently shifted further toward the Multimodal Large Language Model (MLLM) [3, 1] for unified cross-modality understanding and reasoning. Building upon various pre-trained LLM backbones, various methods are proposed to perform crossmodality fusion. Flamingo [2] and BLIP-2 [25] propose different modality fusion techniques to fuse visual tokens to frozen large language models through gated attention or query transformers. Inspired by the success of instruction tuning, LLaV A [26-29] and MiniGPT-4 [30, 31] use visual instruction tuning to align visual input to LLMs, demonstrating remarkable success. More recent work, such as Kosmos-2 [4] and Shikra [5], further empowers MLLM with grounded visual understanding capacity. While these models exhibit promising potential for general-purpose visual reasoning and planning tasks, they are, in general, extremely expensive and prohibitive to train and deploy.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "2.2 Multimodal Large Language Model",
        "chunkIndex": 8,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-9",
      "content": "More related to our work are those efforts dedicated to miniaturizing MLLMs, thereby achieving low-cost and fast deployment. LLaVA-Phi [9], Imp [10] and Vary-toy [11] leverage off-the-shelf small language models and also achieve comparable performance on various benchmarks. Several works explore various ways to optimize the model architecture and training recipe to compensate for the decrease in model size. For instance, MobileVLM [15] focuses on the architecture design of lightweight LLMs and cross-modality projectors to enable training and inference on resourcelimited mobile devices. Moreover, MiniCPM [16] and TinyGPT-V [32] explore Bayesian search on hyperparameters and complex multi-stage training strategy, respectively, to optimize training recipes. An alternative direction, yet under-explored, is training data optimization. A number of works have emphasized the critical role of training data in foundation models [15, 17].",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "2.3 Lightweight Multimodal Large Language Model",
        "chunkIndex": 9,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-10",
      "content": "imize training recipes. An alternative direction, yet under-explored, is training data optimization. A number of works have emphasized the critical role of training data in foundation models [15, 17]. In this work, without bells and whistles, we show that by optimizing the training data, small-scale models can readily outperform their large-scale counterparts.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "2.3 Lightweight Multimodal Large Language Model",
        "chunkIndex": 10,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-11",
      "content": "This section introduces the flexible backbone combination of Bunny, the construction of informative training data, and the training procedure.\n\nFigure 1: Model Architecture. Bunny offers a flexible choice of vision encoder and LLM backbone combination, which are aligned through the cross-modality projector.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3 Bunny: A Family of Lightweight Multimodal Models",
        "chunkIndex": 11,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-12",
      "content": "Following the success of the MLLM paradigm of leveraging visual instruction tuning to align vision and language representations, our model contains three main modules: the LLM backbone, Vision Encoder, and Cross-modality Projector. As illustrated in Fig. 1, Bunny has a flexible combination of the three modules. We introduce some lightweight alternatives for each module as follows.\n\nLarge Language Model Backbone. We provide various options of state-of-the-art lightweight LLM: Phi-1.5 (1.3B) [12], Qwen1.5-1.8B [33], StableLM-2 (1.6B) [23], MiniCPM-2B (2.7B) [16], Phi-2 (2.7B) [13], Phi-3-Mini (3.8B) [34] and Llama-3-8B [21] as the language model backbone.\n\nVision Encoder. We provide two options for the lightweight vision encoder: SigLIP [14] and EVA-CLIP [35], which are both efficient language-aligned image encoders with 428M parameters.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.1 Architecture and Backbone",
        "chunkIndex": 12,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-13",
      "content": "odel backbone.\n\nVision Encoder. We provide two options for the lightweight vision encoder: SigLIP [14] and EVA-CLIP [35], which are both efficient language-aligned image encoders with 428M parameters.\n\nCross-modality Projector. Following LLaVA [27], we leverage a two-layer MLP with a GELU activation function as a cross-modality projector to align the vision encoder and LLM.\n\nMore choices of LLMs, vision encoders, and projectors will be provided in the future.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.1 Architecture and Backbone",
        "chunkIndex": 13,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-14",
      "content": "The quality and scale of training data play critical roles in advancing MLLM performance. We construct Bunny-pretrain-LAION-2M and Bunny-695K for pre-training and instruction tuning, respectively.\n\nPre-training Data. We construct a high-quality pre-training data from LAION-2B [36], i.e., we condense LAION-2B into a 2M coreset for data-efficient learning. Specifically, we design a three-step coreset selection scheme 1 based on CLIP embedding. Firstly, motivated by SemDeDup [37], we cluster all 2B image embeddings by k-means, and then in each cluster, build an undirected graph that any two embeddings are connected if their cosine similarity is above a predetermined threshold. Only one sample is kept in each connected sub-graph whose Euclidean distance to the cluster centroid ranks at the median. By setting the threshold to 0 . 86 , we obtain a subset of 952M.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.2 Training Data Construction",
        "chunkIndex": 14,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-15",
      "content": "ed threshold. Only one sample is kept in each connected sub-graph whose Euclidean distance to the cluster centroid ranks at the median. By setting the threshold to 0 . 86 , we obtain a subset of 952M. Secondly, we sort the remaining samples by the cosine similarity between their text embedding and image embedding and keep samples ranking 40% -60% , subsequently resulting in a subset of 190M. In this way, the low-quality image-text pairs are removed. Thirdly, we sort the remaining samples by the cosine similarity between its image embedding and its cluster centroid and keep samples ranking 15% -35% , resulting in a subset of 38M, capturing the essence and diversity of LAION-2B. We finally randomly sampled 2 million samples from the 38M corset, resulting in Bunny-pretrain-LAION-2M for an appropriate training cost.\n\n1 https://github.com/BAAI-DCAI/Dataset-Pruning/tree/main/LAION",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.2 Training Data Construction",
        "chunkIndex": 15,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-16",
      "content": "e finally randomly sampled 2 million samples from the 38M corset, resulting in Bunny-pretrain-LAION-2M for an appropriate training cost.\n\n1 https://github.com/BAAI-DCAI/Dataset-Pruning/tree/main/LAION\n\nFine-tuning Data. We collect a set of visual instruction tuning datasets DataOptim 2 , based on which we explore the better combination of fine-tuning datasets. Specifically, we leverage SVIT-mix665K [17] and replace ShareGPT-40K [27] in it with WizardLM-evol-instruct-70K [38], resulting to Bunny-695K. We find that tuning MLLM on multimodal data may damage its cognition ability inherited from the pre-trained LLM. Probably, it is caused by the less informative and less diverse text in the multimodal training data. Keeping several high-quality pure text data in the fine-tuning dataset can relieve this problem. In experiments, we jointly use Bunny-695K, LLaVA-665K [27] and ALLaVA-Instruct-4V [39] in the fine-tuning stage for better performance.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.2 Training Data Construction",
        "chunkIndex": 16,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-17",
      "content": "eve this problem. In experiments, we jointly use Bunny-695K, LLaVA-665K [27] and ALLaVA-Instruct-4V [39] in the fine-tuning stage for better performance.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.2 Training Data Construction",
        "chunkIndex": 17,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-18",
      "content": "We employ a two-stage training strategy. The first stage is the pre-training stage, where we align the visual embeddings from a pre-trained vision encoder with the text embeddings from the LLM. The second stage is the fine-tuning stage, where we apply visual instruction tuning to fully utilize the capabilities of the MLLM across various multimodal tasks. In both stages, we use the same cross-entropy loss for next-token prediction. During the pre-training stage, only the cross-modality projector is optimized for one epoch. In the fine-tuning stage, we generally use LoRA [40] to train the LLM backbone, and the vision encoder and the cross-modality projector are fully tuned. In sec. 4.3, we conduct ablation studies about LoRA, data, epoch and freezing component, and we also explore high image resolution and weight merging for more practical MLLMs. For more training details, please refer to our GitHub repository.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "3.3 Training Procedure",
        "chunkIndex": 18,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-19",
      "content": "We evaluate Bunny on eleven popular benchmarks: MME perception [41], MME cognition [41], MMBench [42] test split, MMBench [42] dev split, SEED-Bench-1 [43], MMMU [44] validation split, MMMU[44] test split, VQA-v2 [45] test-dev split, GQA [46] test-dev-balanced split, ScienceQAIMG [47] test split and POPE [48]: the averaged F1-score of three categories (random, popular and adversarial) on the validation set of MSCOCO, to throughout assess its multimodal capacity. We compare against state-of-the-art multimodal large language models (MLLMs), including GPT-4V [49], BLIP-2 [25], InstructBLIP [50], Shikra-13/7B [5], SVIT-v1.5-13B [17], LVIS-INSTRUCT4V-13B [51], ShareGPT4V-13B [52], VILA1.5-13/3B [53], Mini-Gemini-HD-13B [54], LLaVA-v1.5-13/7B [27], LLaVA-NeXT-13/7B [28], MiniGPT-v2 [31], mPLUG-Owl2 [55], SPHINX-Intern2 [56], YiVL-6B [57], DeepSeek-VL-7B [58], MM1-7/3B-Chat [59], Idefics2 [60], LLaVA-NeXT-Llama-3-8B [29], MobileVLM-V2-3B [61], TinyLLaVA-Phi-2-SigLIP-3.1B [62], Mipha-3B [63],",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.1 Settings",
        "chunkIndex": 19,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-20",
      "content": "wl2 [55], SPHINX-Intern2 [56], YiVL-6B [57], DeepSeek-VL-7B [58], MM1-7/3B-Chat [59], Idefics2 [60], LLaVA-NeXT-Llama-3-8B [29], MobileVLM-V2-3B [61], TinyLLaVA-Phi-2-SigLIP-3.1B [62], Mipha-3B [63], MiniCPM-V 2.0 [64], Imp-v1.5-4B-Phi3 [10].",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.1 Settings",
        "chunkIndex": 20,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-21",
      "content": "As shown in Tab. 1, Bunny-4B (SigLIP-SO + Phi-3-Mini) and Bunny-8B (SigLIP-SO + Llama-38B) outperform MLLMs with similar sizes on most of the benchmarks, demonstrating exceptional multimodal understanding and reasoning capabilities. Remarkably, Bunny-8B achieves an 1644.1 score on MME P , which overwhelms the runner-up LLaVA-NeXT-Llama-3-8B (1603.7) by a large margin. Bunny-4B also shows superiority over MM1-3B-Chat on the challenging MMMU benchmark, namely 7.5 and 4.7 improvements on the validation and test splits respectively. Besides, Bunny-4B and Bunny-8B accept images up to 1152 × 1152.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.2 Comparison to the State of the Art",
        "chunkIndex": 21,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-22",
      "content": "LoRA vs. Fully Fine-tuning. In the fine-tuning stage, we observe that LoRA empirically leads to better performance compared to fully tuning the LLM. This improvement is likely because smaller models are more prone to catastrophic forgetting, which LoRA tuning helps to mitigate. We show the experimental results in Tab. 2.\n\nFor below experiments, the LLM is always tuned using LoRA.\n\n2 https://github.com/BAAI-DCAI/DataOptim\n\nTable 1: Comparison to state-of-the-art MLLMs on 11 benchmarks. Our model outperforms them in most of the settings. We evaluate these models on benchmarks: MME P : MME perception [41], MME C : MME cognition [41], MMB T : MMBench [42] test split, MMB D : MMBench [42] dev split, SEED: SEED-Bench-1 [43] with both total accuracy and image accuracy, MMMU V : MMMU [44] validation split, MMMU T : MMMU [44] test split, VQA-v2 [45] test-dev split, GQA [46] test-devbalanced split, SQA I : ScienceQA-IMG [47] test split and POPE [48]: the averaged F1-score of three categories (ra",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 22,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-23",
      "content": "on split, MMMU T : MMMU [44] test split, VQA-v2 [45] test-dev split, GQA [46] test-devbalanced split, SQA I : ScienceQA-IMG [47] test split and POPE [48]: the averaged F1-score of three categories (random, popular and adversarial) on the validation set of MSCOCO. We mark the best performance bold and the runner-up underlined in each section. § We evaluate the officially released checkpoint by ourselves.\n\n| Model                            | MME P   | MME C   | MMB T   | MMB D   | SEED(-I)    | MMMU V   | MMMU T   | VQA v2   | GQA   | SQA I   | POPE   |\n|----------------------------------|---------|---------|---------|---------|-------------|----------|----------|----------|-------|---------|--------|\n| Size > 8B                        |         |         |         |         |             |          |          |          |       |         |        |\n| GPT-4V-1106 [49]                 | 1334.0  | 437.5   | 77.0    | 75.1    | -/71.6      | 53.8     | -        | -        | -     | 82.1",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 23,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-24",
      "content": "|          |          |       |         |        |\n| GPT-4V-1106 [49]                 | 1334.0  | 437.5   | 77.0    | 75.1    | -/71.6      | 53.8     | -        | -        | -     | 82.1    | 75.4   |\n| BLIP-2-Flan-T5-XXL [25]          | 1293.8  | 290.0   | -       | -       | -/-         | 35.4     | 34.0     | 65.0     | 44.6  | 64.5    | -      |\n| InstructBLIP-Flan-T5-XXL [50]    | 1212.8  | 291.8   | -       | -       | -/-         | 35.7     | 33.8     | -        | 47.9  | 70.6    | -      |\n| BLIP-2-Vicuna-13B [25]           | -       | -       | -       | -       | -/-         | -        | -        | -        | 41.0  | 61.0    | -      |\n| InstructBLIP-Vicuna-13B [50]     | -       | -       | -       | -       | -/-         | -        | -        | -        | 49.5  | 63.1    | 83.7   |\n| Shikra-13B [5]                   | -       | -       | -       | -       | -/-         | -        | -        | 77.4     | -     | -       | -      |\n| SVIT-v1.5-13B [17]",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 24,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-25",
      "content": "1    | 83.7   |\n| Shikra-13B [5]                   | -       | -       | -       | -       | -/-         | -        | -        | 77.4     | -     | -       | -      |\n| SVIT-v1.5-13B [17]               | 1565.8  | 323.2   | 69.1    | -       | 61.9/-      | -        | 33.3     | 80.3     | 64.1  | 70.0    | -      |\n| LVIS-INSTRUCT4V-13B [51]         | 1574.9  | 286.8   | -       | 68.0    | 61.6/-      | -        | -        | 80.7     | 63.6  | 70.6    | 86.0   |\n| ShareGPT4V-13B [52]              | 1618.7  | 303.2   | -       | 68.5    | -/70.8      | -        | -        | 81.0     | 64.8  | 71.2    | -      |\n| VILA1.5-13B [53]                 | 1569.6  | -       | -       | 74.9    | 65.1/72.6   | 37.9     | 33.6     | 82.8     | 64.3  | 80.1    | 86.3   |\n| Mini-Gemini-HD-13B [54]          | 1597.0  | 320.0   | -       | 68.6    | -/-         | 37.3     | 35.1     | -        | -     | -       | -      |\n| LLaVA-v1.5-13B (LoRA) [27]       | 1541.7  | 300.4 § | 68.4 §  | 68.5    | 6",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 25,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-26",
      "content": "| 1597.0  | 320.0   | -       | 68.6    | -/-         | 37.3     | 35.1     | -        | -     | -       | -      |\n| LLaVA-v1.5-13B (LoRA) [27]       | 1541.7  | 300.4 § | 68.4 §  | 68.5    | 61.3/-      | 40.0 §   | 33.2 §   | 80.0     | 63.3  | 71.2    | 86.7   |\n| LLaVA-NeXT-13B [28]              | 1575.0  | 326.0   | -       | 70.0    | -/71.9      | 36.2     | -        | 82.8     | 65.4  | 73.6    | 86.2   |\n| 4B < Size ≤ 8B                   |         |         |         |         |             |          |          |          |       |         |        |\n| InstructBLIP-Vicuna-7B [50]      | -       | -       | 33.9    | 36.0    | 53.4/-      | -        | -        | -        | 49.2  | 60.5    | -      |\n| MiniGPT-v2 [31]                  | -       | -       | -       | -       | -/-         | -        | -        | -        | 60.3  | -       | -      |\n| Shikra-7B [5]                    | -       | -       | 60.2    | 58.8    | -/-         | -        | -        | -        |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 26,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-27",
      "content": "| -/-         | -        | -        | -        | 60.3  | -       | -      |\n| Shikra-7B [5]                    | -       | -       | 60.2    | 58.8    | -/-         | -        | -        | -        | -     | -       | -      |\n| mPLUG-Owl2 [55]                  | 1450.2  | 313.2   | 66.0    | 66.5    | 57.8/-      | 32.7     | 32.1     | 79.4     | 56.1  | 68.7    | 85.8   |\n| SPHINX-Intern2 [56]              | 1260.4  | 294.6   | -       | 57.9    | -/68.8      | -        | -        | 75.5     | 56.2  | 70.4    | 86.9   |\n| Yi-VL-6B [57]                    | -       | -       | 68.4    | 68.6    | -/67.6      | 39.1     | 37.8     | -        | -     | -       | -      |\n| DeepSeek-VL-7B [58]              | -       | -       | -       | 73.2    | -/70.4      | 36.6     | -        | -        | -     | -       | 88.1   |\n| LLaVA-v1.5-7B (LoRA) [27]        | 1476.9  | 267.9 § | 66.1 §  | 66.1    | 60.1/-      | 34.4 §   | 31.7 §   | 79.1     | 63.0  | 68.4    | 86.4   |\n| LLaVA-NeXT-7B [",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 27,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-28",
      "content": "| -     | -       | 88.1   |\n| LLaVA-v1.5-7B (LoRA) [27]        | 1476.9  | 267.9 § | 66.1 §  | 66.1    | 60.1/-      | 34.4 §   | 31.7 §   | 79.1     | 63.0  | 68.4    | 86.4   |\n| LLaVA-NeXT-7B [28]               | 1519.0  | 332.0   | -       | 67.4    | -/70.2      | 35.8     | -        | 81.8     | 64.2  | 70.1    | 86.5   |\n| MM1-7B-Chat [59]                 | 1529.3  | 328.9   | -       | 72.3    | 64.0/69.9   | 37.0     | 35.6     | 82.8     | -     | 72.6    | 86.6   |\n| Idefics2 [60]                    | -       | -       | 76.7    | -       | -/-         | 43.0     | 37.7     | 81.2     | -     | -       | -      |\n| LLaVA-NeXT-Llama-3-8B [29]       | 1603.7  | 367.8   | -       | 72.1    | -/-         | 41.7     | -        | -        | -     | -       | -      |\n| Bunny-8B                         | 1644.1  | 367.5   | 78.1    | 77.2    | 66.2 / 73.5 | 43.3     | 39.0     | 82.9     | 64.0  | 79.9    | 87.2   |\n| Size ≤ 4B                        |         |         |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 28,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-29",
      "content": "| 1644.1  | 367.5   | 78.1    | 77.2    | 66.2 / 73.5 | 43.3     | 39.0     | 82.9     | 64.0  | 79.9    | 87.2   |\n| Size ≤ 4B                        |         |         |         |         |             |          |          |          |       |         |        |\n| MobileVLM-V2-3B [61]             | 1440.5  | -       | -       | 63.2    | -/-         | -        | -        | -        | 61.1  | 70.0    | 84.7   |\n| TinyLLaVA-Phi-2-SigLIP-3.1B [62] | 1466.4  | -       | -       | -       | -/-         | 38.4     | -        | 80.1     | 62.1  | 73.0    | 87.2   |\n| VILA1.5-3B [53]                  | 1442.4  | -       | -       | 63.4    | 60.9/67.9   | 33.3     | 30.8     | 80.4     | 61.5  | 69.0    | 85.9   |\n| Mipha-3B [63]                    | 1488.9  | 295.0   | -       | 69.7    | -/68.9      | -        | -        | 81.3     | 63.9  | 70.9    | 86.7   |\n| MiniCPM-V 2.0 [64]               | 1411.4  | 396.8   | 69.1    | 69.6    | -/67.1      | 38.2     | -",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 29,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-30",
      "content": "| 69.7    | -/68.9      | -        | -        | 81.3     | 63.9  | 70.9    | 86.7   |\n| MiniCPM-V 2.0 [64]               | 1411.4  | 396.8   | 69.1    | 69.6    | -/67.1      | 38.2     | -        | -        | -     | 80.7    | 86.3   |\n| MM1-3B-Chat [59]                 | 1482.5  | 279.3   | -       | 67.8    | 63.0/68.8   | 33.9     | 33.7     | 82.0     | -     | 69.4    | 87.4   |\n| Imp-v1.5-4B-Phi3 [10]            | 1507.7  | -       | -       | 73.3    | -/-         | -        | -        | 81.5     | 63.5  | 78.3    | 86.9   |\n| Bunny-4B                         | 1581.5  | 361.1   | 75.7    | 74.2    | 64.9 / 72.5 | 41.4     | 38.4     | 82.1     | 63.2  | 78.3    | 87.2   |\n\nTable 2: Comparison between LoRA and fully tuning. The model is composed of SigLIP-SO [14] and Phi-2 [13], and trained on Bunny-695K for 1 epoch. The vision encoder is frozen.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 30,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-31",
      "content": "2  | 78.3    | 87.2   |\n\nTable 2: Comparison between LoRA and fully tuning. The model is composed of SigLIP-SO [14] and Phi-2 [13], and trained on Bunny-695K for 1 epoch. The vision encoder is frozen.\n\n| LLM Tuning        |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Fully Tuning LoRA |  1465.2 |   277.5 |    67.9 |    67.2 | 62.1(69.5)    |     36.8 |     33.1 |     79.1 |  61.7 |    72.8 |   85.9 |\n| Fully Tuning LoRA |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n\nTable 3: Ablation study on fine-tuning data and training epochs. The model is composed of SigLIPSO [14] and Phi-2 [13] with the vision encoder frozen.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 31,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-32",
      "content": "33   |     79.8 |  62.5 |    70.9 |   86.8 |\n\nTable 3: Ablation study on fine-tuning data and training epochs. The model is composed of SigLIPSO [14] and Phi-2 [13] with the vision encoder frozen.\n\n| Fine-tuning Data        |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K              |       1 |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.6)    |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| Bunny-695K              |       2 |  1480.6 |   272.5 |    71.2 |    69   | 63.0 ( 71.3 ) |     37.6 |     32.5 |     81   |  64   |    69.7 |   86.3 |\n| Bunny-695K + LLaVA-665K |       1 |  1501.3 |   272.5 |    72.4 |    71.1 | 63.0 (71.1)   |     37.8 |     33.4 |     81   |  63.6 |    69.5 |   87.2 |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 32,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-33",
      "content": "32.5 |     81   |  64   |    69.7 |   86.3 |\n| Bunny-695K + LLaVA-665K |       1 |  1501.3 |   272.5 |    72.4 |    71.1 | 63.0 (71.1)   |     37.8 |     33.4 |     81   |  63.6 |    69.5 |   87.2 |\n\nTable 4: Ablation study on extra fine-tuning data. The model is composed of SigLIP-SO [14] and Llama-3-8B [21] where the vision encoder is frozen. S 2 -Wrapper [65] is used.\n\n| Fine-tuning Data                |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K                      |       1 |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Bunny-695K + ALLaVA-Instruct-4V |       1 |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 ( 73.4 ) |     41.6 |     38.3 |     81.9 |  6",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 33,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-34",
      "content": "|     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Bunny-695K + ALLaVA-Instruct-4V |       1 |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 ( 73.4 ) |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |\n\nTable 5: Ablation study on tuning/freezing vision encoder in the fine-tuning stage. The model consists of SigLIP-SO [14] and Phi-2 [13].\n\n| Fine-tuning Data        | Vision Encoder   | MME P         | MME C       | MMB T     | MMB D     | SEED(-I)               | MMMU V    | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|---------------|-------------|-----------|-----------|------------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen Trainable | 1488.8 1486.0 | 289.3 281.1 | 69.2 69.5 | 68.6 69.3 | 62.5 (70.6) 62.3(70.6) | 38.2 37.7 | 33.0 33.2 | 79.8 79.9 | 62.5 62.6 | 70.9 70.6 | 86.8 85.9 |\n| Bunny-695K + LLaVA-665K | Frozen Trainab",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 34,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-35",
      "content": "able | 1488.8 1486.0 | 289.3 281.1 | 69.2 69.5 | 68.6 69.3 | 62.5 (70.6) 62.3(70.6) | 38.2 37.7 | 33.0 33.2 | 79.8 79.9 | 62.5 62.6 | 70.9 70.6 | 86.8 85.9 |\n| Bunny-695K + LLaVA-665K | Frozen Trainable | 1501.3 1521.5 | 272.5 291.8 | 72.4 71.9 | 71.1 71.3 | 63.0(71.1) 63.1 (71.1) | 37.8 38.0 | 33.4 33.1 | 81.0 81.1 | 63.6 63.7 | 69.5 70.8 | 87.2 87.0 |\n\nFine-tuning Data. Tab. 3 shows the ablation study on data strategy. Jointly leveraging Bunny695K and LLaVA-665K for one epoch generally yields performance improvements across multiple benchmarks. A comparison between training on Bunny-695K for two epochs and training on Bunny-695K and LLaVA-665K for one epoch demonstrates that increasing data diversity enhances performance under the same training cost. To further enhance diversity, we incorporate ALLaVAInstruct-4V [39].",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 35,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-36",
      "content": "ny-695K and LLaVA-665K for one epoch demonstrates that increasing data diversity enhances performance under the same training cost. To further enhance diversity, we incorporate ALLaVAInstruct-4V [39]. Given the detailed answers provided in ALLaVA-Instruct-4V, we append 'Please answer the question in detail.' to the end of questions to avoid overfitting the model to long-form answers. As shown in Tab. 4, adding ALLaVA-Instruct-4V brings performance improvements. Based on above results, we set the fine-tuning stage to be 1 epoch in the rest experiments in this paper.\n\nTrainable vs. Frozen Vision Encoder. By default, we freeze the vision encoder in both stages of our primary experiments. We also investigate the influence of enabling the vision encoder to be trainable during the fine-tuning stage. As shown in Tab. 5, the results vary depending on the fine-tuning data used. When employing Bunny-695K + LLaVA-665K, unfreezing the vision encoder leads to better results. Tab.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 36,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-37",
      "content": "the fine-tuning stage. As shown in Tab. 5, the results vary depending on the fine-tuning data used. When employing Bunny-695K + LLaVA-665K, unfreezing the vision encoder leads to better results. Tab. 6 shows that the performance can be further enhanced with additional finetuning data when the vision encoder is trainable.\n\nLearning Rate. As shown in Tab. 7, increasing the pre-training learning rate does not consistently lead to improvements. In this context, both 5 × 10 -4 and 8 × 10 -4 are relatively good choices, while 1 × 10 -3 is sub-optimal. As shown in Tab. 8, the original learning rates set in LLaV A are the most appropriate. A learning rate that is too high causes extremely inferior results, including several sudden spikes in loss during training.\n\nHigher Resolution. The ability of understanding high-resolution images and capturing the finegrained details is crucial for a helpful MLLM.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 37,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-38",
      "content": "s, including several sudden spikes in loss during training.\n\nHigher Resolution. The ability of understanding high-resolution images and capturing the finegrained details is crucial for a helpful MLLM. Given that the default image size of SigLIP-SO is 384 × 384, we try two approaches to scale up image resolution: positional embedding interpolation and sub-image decomposition. Through positional embedding interpolation, the vision encoder is adapted to the new resolution during fine-tuning, i.e. 448 × 448, and an image is represented with 1024 tokens, resulting a 20% training time increase. Tab. 9 shows that the improvement is marginal. For sub-image decomposition, we leverage S 2 -Wrapper [65], a simple mechanism that extends pretrained vision model to multiple image scales, which processes images at multiple scales by interpolating, splitting, encoding, merging, and concatenating features, resulting in a multi-scale representation with 3 times channel dimension while maintaining the to",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 38,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-39",
      "content": "images at multiple scales by interpolating, splitting, encoding, merging, and concatenating features, resulting in a multi-scale representation with 3 times channel dimension while maintaining the token length. It would increase the training time\n\nTable 6: Ablation study on fine-tuning data with vision encoder tuned. The experiments are conducted with SigLIP-SO [14] as the vision encoder, and S 2 -Wrapper [65] is enabled. B-695K, L-665K and A-Ins represents for Bunny-695K, LLaVA-665K and ALLaVA-Instruct-4V, respectively.\n\n| LLM        | Fine-tuning Data                  |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I | POPE      |\n|------------|-----------------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|-----------|\n| Phi-3-Mini | B-695K + L-665K                   |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 39,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-40",
      "content": "----|----------|----------|----------|-------|---------|-----------|\n| Phi-3-Mini | B-695K + L-665K                   |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   | 87.0      |\n| Phi-3-Mini | B-695K + L-665K + A-Ins           |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4 ( 71.9 ) |     42.4 |     38.1 |     82   |  62.9 |    77.8 | 86.9      |\n| Llama-3-8B | B-695K + L-665K B-695K + L-665K + |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 | 86.8 87.3 |\n| Llama-3-8B | A-Ins                             |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5 ( 73.1 ) |     40.4 |     38.8 |     82.8 |  64.1 |    79   |           |\n\nTable 7: Ablation study on pre-training learning rate. The model consists of SigLIP-SO [14] and Phi-2 [13], where Bunny-695K is utilized for fine-tuning and the vision encoder is frozen.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 40,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-41",
      "content": "|\n\nTable 7: Ablation study on pre-training learning rate. The model consists of SigLIP-SO [14] and Phi-2 [13], where Bunny-695K is utilized for fine-tuning and the vision encoder is frozen.\n\n| Pre-training LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 5 × 10 - 4        |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| 8 × 10 - 4        |  1494.3 |   277.5 |    69.6 |    69   | 62.1(70.5)    |     36.4 |     33.5 |     80   |  62.3 |    72.9 |   86.9 |\n| 1 × 10 - 3        |  1448.3 |   261.1 |    70.5 |    68.8 | 62.4(70.4)    |     36.8 |     33.3 |     80   |  62.3 |    71.1 |   86.4 |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 41,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-42",
      "content": "|     36.4 |     33.5 |     80   |  62.3 |    72.9 |   86.9 |\n| 1 × 10 - 3        |  1448.3 |   261.1 |    70.5 |    68.8 | 62.4(70.4)    |     36.8 |     33.3 |     80   |  62.3 |    71.1 |   86.4 |\n\nTable 8: Ablation study on fine-tuning learning rate. The model consists of SigLIP-SO [14] and Phi-2 [13], where Bunny-695K + LLaVA-665K is utilized for fine-tuning and the vision encoder is trainable.\n\n| LoRA LR    | Vision &Projector LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 2 × 10 - 4 | 2 × 10 - 5             |  1521.5 |   291.8 |    71.9 |    71.3 | 63.1 ( 71.1 ) |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| 1 × 10 - 4 | 1 × 10 - 5             |  1492.8 |   283.9 |    71.2 |    69.7 | 63.1 (70.9)   |     38.4 |     33.3 |     80.4",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 42,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-43",
      "content": "1 ( 71.1 ) |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| 1 × 10 - 4 | 1 × 10 - 5             |  1492.8 |   283.9 |    71.2 |    69.7 | 63.1 (70.9)   |     38.4 |     33.3 |     80.4 |  62.8 |    71.5 |   86.8 |\n| 4 × 10 - 4 | 4 × 10 - 5             |   987.7 |   240.4 |     0   |     0   | 25.9(26.3)    |     27.9 |     25.3 |     69   |  53.7 |    36.7 |   83.3 |\n\nfor 23%. As shown in Tab. 10, S 2 -Wrapper leads to a huge performance improvement while using Bunny-695K + LLaVA-665K. Considering the performance and high resolution, we take S 2 -Wrapper as the final solution.\n\nWeight Merging. During ablation studies, we obtain numerous models with the same architecture under different settings. We explore the potential benefits of merging two models by weighted averaging. As shown in Tab. 11, simple linear merging greatly enhances performance, with the best results forming our Bunny-4B/8B models.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 43,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-44",
      "content": "plore the potential benefits of merging two models by weighted averaging. As shown in Tab. 11, simple linear merging greatly enhances performance, with the best results forming our Bunny-4B/8B models. This method is more efficient and effective than hyper-parameter searching for improving model performance. We also observe that the results on some benchmarks (e.g. MME) are very sensitive to the weight factor.\n\nBackbone Combination. We also investigate Bunny's performance with various combinations of vision encoders and language models. As shown in Tab. 12, all combinations show exceptional performance, with SigLIP-SO [14] and Llama-3-8B [21] achieving the best results. This highlights the flexibility of the Bunny framework and the high quality of the curated training data. Note that in the fine-tuning stage, Phi-1.5, StableLM-2 and Phi-2-based Bunny are trained on Bunny-695K for 1 epoch with the vision encoder frozen.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 44,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-45",
      "content": "k and the high quality of the curated training data. Note that in the fine-tuning stage, Phi-1.5, StableLM-2 and Phi-2-based Bunny are trained on Bunny-695K for 1 epoch with the vision encoder frozen. In contrast, Phi-3-Mini and Llama-3-8B-based Bunny are trained using additional fine-tuning data, S 2 -Wrapper, trainable vision encoder, weight merging, etc., as illustrated before.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.3 Ablation Study",
        "chunkIndex": 45,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-46",
      "content": "We also present several qualitative test cases for Bunny. As illustrated in Tab. 13, Bunny-8B with SigLIP-SO [14] and Llama-3-8B [21] demonstrates exceptional capabilities in visual understanding, common sense reasoning, external knowledge referring, optical character recognizing, and mathematical problem-solving. These results feature Bunny-8B as a powerful and versatile visual assistant. Additionally, Tab. 14 presents several test cases in Chinese using Bunny-8B. Although we do not intentionally include Chinese in the multimodal training data, Bunny still exhibits impressive Chinese instruction-following capabilities.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "4.4 Qualitative Demonstrations",
        "chunkIndex": 46,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-47",
      "content": "We present Bunny, a family of lightweight but powerful multimodal models. It offers multiple plugand-play vision encoders, including EVA-CLIP and SigLIP, and language backbones, including Phi1.5, Qwen1.5-1.8B, StableLM-2, MiniCPM-2B, Phi-2, Phi-3-Mini, and Llama-3-8B. To compensate for the decrease in model size, we construct more informative training data by selecting data from a broader source. Remarkably, our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks.\n\nTable 9: Results on scaling up image resolution by positional embedding interpolation. The model consists of SigLIP-SO [14] and Phi-2 [13].\n\n| Fine-tuning Data        | Vision Encoder Res   | MME P      |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|----------------------|------------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K + LLaVA-",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 47,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-48",
      "content": "POPE |\n|-------------------------|----------------------|------------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K + LLaVA-665K | Trainable            | 384 1521.5 |   291.8 |    71.9 |    71.3 | 63.1(71.1)    |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| Bunny-695K + LLaVA-665K | Trainable            | 448 1489.2 |   309.6 |    71.6 |    71.5 | 63.3 ( 71.4 ) |     37.2 |     32.5 |     81.4 |  63.2 |    71   |   87.2 |\n\nTable 10: Ablation study on scaling up image resolution by sub-image decomposition (S 2 -Wrapper [65]). The model consists of SigLIP-SO [14] and Phi-3-Mini [34].\n\n| Fine-tuning Data        | Vision Encoder   | Res MME P       | MME C       | MMB T     | MMB D     | SEED(-I)                 |   MMMU V | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|-----------------|-------------|-----------|-----------|--------------",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 48,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-49",
      "content": "|   MMMU V | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|-----------------|-------------|-----------|-----------|--------------------------|----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen           | 384 1402.0      | 286.1       | 73.1      | 72.3      | 64.6 ( 71.9 )            |     40.7 | 39.3      | 80.6      | 62.4      | 75.3      | 85.8 86.6 |\n|                         |                  | 1152 1473.1     | 332.1       | 72.8      | 72.0      | 64.1(71.3)               |     41.8 | 38.3      | 80.6      | 62.4 63.0 | 75.6 74.4 | 86.4      |\n| Bunny-695K + LLaVA-665K | Trainable        | 384 1490.5 1152 | 316.4 336.4 | 72.4 74.4 | 72.0 74.3 | 63.7(71.3) 64.2 ( 71.4 ) |     40   | 38.3 38.6 | 81.3 81.6 |           |           |           |\n| Bunny-695K + LLaVA-665K |                  | 1503.5          |             |           |           |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 49,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-50",
      "content": "64.2 ( 71.4 ) |     40   | 38.3 38.6 | 81.3 81.6 |           |           |           |\n| Bunny-695K + LLaVA-665K |                  | 1503.5          |             |           |           |                          |     38.8 |           |           | 63.1      | 75.0      | 87.0      |\n\nTable 11: Ablation study on weight merging. The experiments are conducted with SigLIP-SO [14] as the vision encoder. The weight factor is selected empirically. F / T represents whether the V ision E ncoder is F rozen or T rainable. B-695K, L-665K and A-Ins represents for Bunny-695K, LLaVA-665K and ALLaVA-Instruct-4V, respectively.\n\n| LLM        | S 2   | Fine-tuning Data             | VE   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|-------|------------------------------|------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n|            |       |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 50,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-51",
      "content": "|------------|-------|------------------------------|------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n|            |       | B-695K                       | F    |  1402   |   286.1 |    73.1 |    72.3 | 64.6 (71.9)   |     40.7 |     39.3 |     80.6 |  62.4 |    75.3 |   85.8 |\n| Phi-3-Mini | w \\   | o B-695K + L-665K            | T    |  1490.5 |   316.4 |    72.4 |    72   | 63.7(71.3)    |     40   |     38.3 |     81.3 |  63   |    74.4 |   86.4 |\n|            |       | 3:7 Averaging                |      |  1495.2 |   338.9 |    74   |    73.5 | 64.5( 72.1 )  |     40.1 |     39.1 |     81.5 |  63.5 |    75.2 |   86.7 |\n|            |       | B-695K                       | F    |  1473.1 |   332.1 |    72.8 |    72   | 64.1(71.3)    |     41.8 |     38.3 |     80.6 |  62.4 |    75.6 |   86.6 |\n| Phi-3-Mini | w \\   | B-695K + L-665K              | T    |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 51,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-52",
      "content": "72   | 64.1(71.3)    |     41.8 |     38.3 |     80.6 |  62.4 |    75.6 |   86.6 |\n| Phi-3-Mini | w \\   | B-695K + L-665K              | T    |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   |   87   |\n|            |       | 3:7 Averaging                |      |  1503.9 |   362.9 |    74.1 |    74.1 | 64.6 ( 71.7 ) |     40.2 |     38.8 |     81.7 |  63.4 |    76.3 |   87   |\n|            |       | B-695K + A-Ins               | F    |  1552.4 |   306.8 |    74   |    73.8 | 62.8(71.0)    |     40.6 |     38.1 |     81   |  62.3 |    75   |   86.7 |\n| Phi-3-Mini | w \\   | B-695K + L-665K + A-Ins      | T    |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4(71.9)    |     42.4 |     38.1 |     82   |  62.9 |    77.8 |   86.9 |\n|            |       | 3:7 Averaging ( Bunny-4B     | )    |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9 ( 72.5 ) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n|            |       | B-695K",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 52,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-53",
      "content": "|       | 3:7 Averaging ( Bunny-4B     | )    |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9 ( 72.5 ) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n|            |       | B-695K                       | F    |  1542.7 |   344.6 |    75.7 |    74.8 | 65.2(73.1)    |     42.4 |     37.1 |     82   |  64.4 |    78.7 |   85.9 |\n| Llama-3-8B | w \\ o | B-695K + L-665K              | T    |  1562.2 |   319.6 |    75.3 |    74.4 | 65.2(72.5)    |     43.4 |     38   |     82.4 |  64.1 |    77.4 |   86.4 |\n|            |       | 5:5 Averaging                |      |  1588.9 |   321.1 |    77.2 |    76.7 | 65.9 ( 73.3 ) |     42.8 |     39   |     82.6 |  64.8 |    80.4 |   86.9 |\n|            |       | B-695K                       | F    |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Llama-3-8B | w \\   | B-695K + L-665K              | T    |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 53,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-54",
      "content": "| 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Llama-3-8B | w \\   | B-695K + L-665K              | T    |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 |   86.8 |\n|            |       | 6:4 Averaging                |      |  1607.8 |   324.6 |    76.5 |    76.3 | 65.9 ( 73.6 ) |     43.7 |     38.8 |     82.3 |  64.5 |    80.4 |   87.4 |\n|            |       | B-695K + A-Ins               | F    |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 (73.4)   |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |\n| Llama-3-8B | w \\   | B-695K + L-665K + A-Ins      | T    |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5(73.1)    |     40.4 |     38.8 |     82.8 |  64.1 |    79   |   87.3 |\n|            |       | 4.2:5.8 Averaging ( Bunny-8B | )    |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2 ( 73.5 ) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 54,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-55",
      "content": ".1 |    79   |   87.3 |\n|            |       | 4.2:5.8 Averaging ( Bunny-8B | )    |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2 ( 73.5 ) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |\n\nTable 12: More results of Bunny with various language and vision backbones. The best performances are achieved by integrating SigLIP-SO [14] and Llama-3-8B [21].\n\n| Vision Encoder      | LLM               |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)   |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------|-------------------|---------|---------|---------|---------|------------|----------|----------|----------|-------|---------|--------|\n| EVA02-CLIP-L (0.4B) | Phi-1.5 (1.3B)    |  1213.7 |   278.9 |    60.9 |    56.8 | 56.4(64.1) |     30   |     28.4 |     76.5 |  60.4 |    58.2 |   86.1 |\n| EVA02-CLIP-L (0.4B) | StableLM-2 (1.6B) |  1301   |   235   |    58.4 |    56.4 | 55.3(62.8) |     29.8 |     29.4 |     74.6 |  56.7 |    60   |   84.8 |\n| EVA02",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 55,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-56",
      "content": "76.5 |  60.4 |    58.2 |   86.1 |\n| EVA02-CLIP-L (0.4B) | StableLM-2 (1.6B) |  1301   |   235   |    58.4 |    56.4 | 55.3(62.8) |     29.8 |     29.4 |     74.6 |  56.7 |    60   |   84.8 |\n| EVA02-CLIP-L (0.4B) | Phi-2 (2.7B)      |  1421   |   285.4 |    68.6 |    67.4 | 62.2(70.2) |     35.9 |     32.6 |     78.9 |  62.3 |    69.1 |   87.1 |\n| SigLIP-SO (0.4B)    | Phi-1.5 (1.3B)    |  1230   |   237.5 |    61.2 |    59.7 | 57.7(65.3) |     30   |     29.1 |     78   |  61.1 |    61.3 |   85.8 |\n| SigLIP-SO (0.4B)    | StableLM-2 (1.6B) |  1366.8 |   236.1 |    65.1 |    62.8 | 58.8(67.5) |     29.9 |     29.8 |     78.9 |  60.9 |    61.1 |   85.9 |\n| SigLIP-SO (0.4B)    | Phi-2 (2.7B)      |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.7) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| SigLIP-SO (0.4B)    | Phi-3-Mini (3.8B) |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9(72.5) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n| SigLIP-SO (0.4B)",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 56,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-57",
      "content": "|    70.9 |   86.8 |\n| SigLIP-SO (0.4B)    | Phi-3-Mini (3.8B) |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9(72.5) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n| SigLIP-SO (0.4B)    | Llama-3-8B        |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2(73.5) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 57,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-58",
      "content": "They are members of the Beatles, one of the most famous rock bands in history.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Do you know who are they?",
        "chunkIndex": 58,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-59",
      "content": "Yes, this is a famous photo taken by Iain Macmillan on Abbey Road in London, England. It was used as the cover art for the Beatles' album 'Abbey Road' released in 1969.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Do you recognize where does this picture come from?",
        "chunkIndex": 59,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-60",
      "content": "The image has been widely recognized due to its iconic status and the distinctive white lines that cross the street, which are part of the pedestrian crossing at Abbey Road.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "How do you know?",
        "chunkIndex": 60,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-61",
      "content": "Given the snowy environment, you could engage in winter sports such as skiing or snowboarding.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "What kind of sport can I play here?",
        "chunkIndex": 61,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-62",
      "content": "It appears to be a suitable location for skiing due to the presence of snow-covered slopes. However, it would be best to check with local authorities for any safety guidelines or restrictions before engaging in this activity.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Is it safe for me to go skiing here?",
        "chunkIndex": 62,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-63",
      "content": "This is a painting by Vincent van Gogh, specifically a still life featuring sunflowers in a vase.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "What kind of art is this?",
        "chunkIndex": 63,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-64",
      "content": "As an AI, I don't have personal opinions or emotions. However, many people consider Van Gogh's paintings to be beautiful due to their vibrant colors and unique style.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Do you find the painting beautiful?",
        "chunkIndex": 64,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-65",
      "content": "The astronaut is holding a green bottle of beer in his hand.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "What is the astronaut holding in his hand?",
        "chunkIndex": 65,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-66",
      "content": "It's highly unlikely that an astronaut would be sitting on the moon with a beer bottle. This image seems to be a creative representation or a humorous depiction rather than a realistic scenario.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Can this happen in real world?",
        "chunkIndex": 66,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-67",
      "content": "Astronauts are trained for space missions and their primary focus is on scientific research and exploration. They follow strict protocols and guidelines to ensure safety and efficiency during their time in space. Drinking beer on the moon would not align with these objectives.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Why is that?",
        "chunkIndex": 67,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-68",
      "content": "The image is funny because it features a cat sitting in front of a wall with yellow letters spelling out 'Huh?' on it. This creates a humorous juxtaposition between the cat's expression and the question mark, suggesting that the cat might be confused or puzzled by something.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Why is the image funny?",
        "chunkIndex": 68,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-69",
      "content": "You can use this meme when you want to express confusion or puzzlement about something. It could also be used humorously to ask for clarification or to poke fun at someone who seems perplexed.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "When should I use the meme?",
        "chunkIndex": 69,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-70",
      "content": "The height of the triangle is 8 meters and its base is 20 meters.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "What is the height and base of the triangle?",
        "chunkIndex": 70,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-71",
      "content": "Yes, I can calculate the area of the triangle. The formula for the area of a triangle is (base * height) / 2. In this case, it would be (20m * 8m) / 2 = 80 square meters.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Can you calculate the area of triangle?",
        "chunkIndex": 71,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-72",
      "content": "This problem should be solvable by students in middle school or high school who have learned basic geometry concepts such as calculating the area of triangles using the formula (base * height) / 2.\n\nTable 13: Qualitative performances of Bunny-8B : The bold text represents user input, while the non-bold text is the model response. The results demonstrate Bunny's versatile capabilities in visual understanding, common sense reasoning, external knowledge referring, optical character recognizing, and mathematical problem solving.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- 请 用 中 文 鉴 赏 一下 图 片 ？\n\n这 是 一 幅 奇 幻 的 绘 画 ， 描 绘 了一 位 女 性 和 一 只 红 色 的 龙 。 女 性 的 脸 部 被 细 腻 地 描 绘 ， 包 括 她 的 眼睛 、 鼻 子 、 嘴 巴 和 耳 朵 。 她 长 着 一 头 卷 曲 的 发 型 ， 发 色 是 粉 红 色 的 。 同 时 ， 她 的皮 肤 也 呈 现 出 一 种 粉 红 色 的 光 泽 。\n\n在 女 性 面 前 飞 舞 着 一 只 红 色 的 龙 ， 龙 的 身 体 部 分 可 以 看 到 它 的 翅 膀 和 尾 巴 。 龙 的 颜 色 非 常 鲜 艳 ， 体 现 了 绘 画 中 奇 幻 元 素 的 特 点 。",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Students of what grade should be able to solve this problem?",
        "chunkIndex": 72,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-73",
      "content": "巴 和 耳 朵 。 她 长 着 一 头 卷 曲 的 发 型 ， 发 色 是 粉 红 色 的 。 同 时 ， 她 的皮 肤 也 呈 现 出 一 种 粉 红 色 的 光 泽 。\n\n在 女 性 面 前 飞 舞 着 一 只 红 色 的 龙 ， 龙 的 身 体 部 分 可 以 看 到 它 的 翅 膀 和 尾 巴 。 龙 的 颜 色 非 常 鲜 艳 ， 体 现 了 绘 画 中 奇 幻 元 素 的 特 点 。\n\n整 体 来 说 ， 这 幅 画 充 满 了 奇 幻 和 梦 幻 的 氛 围 ， 女 性 和 龙 之 间 的 互 动 关 系 也 很 有 趣 。\n\n- 请 用 中 文 详 细 描 述这 张 图 片\n\n这 是 一 幅 描 绘 了一个 美 丽 的 自 然 景 观 的 画 面 。 画 面 中 ， 两个人 在 一 艘 小 船 上 ， 正 在 穿 过 一 片 水 面 。 这 艘船 位 于 画 面 的 中 心 ， 人们 似 乎 是 在 享 受 着 这 个 宁 静 的 湖 泊 的 美 景 。\n\n水 面 上 还 有 一些 反 射 ， 可 能 是 来 自 周 围 的 山 脉 和 树 林 的 光 芒 。 这 些 反 射 使 得 水 面 更 加 迷 人 ， 增 添 了一 种神秘 感 。\n\n在 画 面 背 景 中 ， 有 一 座 山 脉 ， 它 们 高 耸 入 云 ， 形 成 了了一个 壮 丽 的 天 际 线 。 山 脉 的 山峰 部 分 被 阳 光 照 亮 ， 反 映 在 水 面 上 ， 给 整 个 画 面 带 来 了 温 暖 的 氛 围 。\n\n在 山 脉 的 脚 下 ， 是 一 片 茂 密 的 树 林 。 这 些 树 林 的 叶 子 呈 现 出 不 同 的 颜 色 ， 可 能 是 由 于 季 节 的 变化 或 日 落 的 光 芒 。 这 些 树 林 的 反 影 也 可 以 在 水 面 上 看 到 ， 这 使 得 整 个 画 面 更 加 生 动 。\n\n总 体 来 说 ， 这 幅 画 面 展 示 了一个 宁 静 、 美 丽 的 自 然 景 观 ， 两个人 在 其 中 度 过 了一 段 愉 快 的 时 光 。\n\n路 牌 上 的 医 院 名 称 是 什么 ？ 路 牌 上 的 医 院 名 称 是 'Kwong Wah Hospital' 。\n\n- 巴 士 的 运 营 时 间 是 几 点 ？",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Students of what grade should be able to solve this problem?",
        "chunkIndex": 73,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-74",
      "content": "看 到 ， 这 使 得 整 个 画 面 更 加 生 动 。\n\n总 体 来 说 ， 这 幅 画 面 展 示 了一个 宁 静 、 美 丽 的 自 然 景 观 ， 两个人 在 其 中 度 过 了一 段 愉 快 的 时 光 。\n\n路 牌 上 的 医 院 名 称 是 什么 ？ 路 牌 上 的 医 院 名 称 是 'Kwong Wah Hospital' 。\n\n- 巴 士 的 运 营 时 间 是 几 点 ？\n\n巴 士 的 运 营 时 间 是 从 早 上 7 点 到 晚 上 12 点 。\n\n- 这 是 哪 座 城 市 ？\n\n这 是 香 港 的 市 区 。\n\n- 请 用 中 文 回 答 为什么 这 张 图 片 如 此 有 趣 ？\n\n这 张 图 片 非 常 有 趣 ， 因 为 它 展 示 了一个 小 狗 的 可 爱 和 天 真 。 小 狗 的 双 手 抱 着 自 己 的 头 ， 表 明 它 在 思 考 或 感 到 困 惑 。 这 场 景 让 人 联 想 到 人 类 的 行 为 ， 特 别 是 孩子 们 经 常 这 样 做 。 同 时 ， 小 狗 的 颜 色 和 表 情 也 很 吸 引 人 ， 让 人 无 法 不 被 其 可 爱 所 吸 引 。\n\n- 这 张 图 的 创 意 体 现 在 什么 地 方 ？\n\n这 张 图 的 创 意 体 现 在 将 一个 船 和 一 只 鸟 安 置 在 一个 蛋 中 ， 创 造 出 一个 奇 怪 、 有 趣 的 场 景 。\n\n- 这 张 图 片 可 能 具 有 什么 用 途 ？\n\n这 张 图 可 能 是 用 于 表 达 创 意 或 设计 的 概 念 ， 或 者 作 为一 种 艺 术 作 品 展 现 。\n\nTable 14: Qualitative performances of Bunny-8B : The results demonstrate its good Chinese instruction-following ability.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "Students of what grade should be able to solve this problem?",
        "chunkIndex": 74,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-75",
      "content": "- [1] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. 1, 2\n- [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35: 23716-23736, 2022. 1, 2\n- [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1, 2\n- [4] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-76",
      "content": "reprint arXiv:2303.08774 , 2023. 1, 2\n- [4] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=lLmqxkfSIw . 2\n- [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023. 1, 2, 4, 5\n- [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. 1, 2\n- [7] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-77",
      "content": "ouis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 1, 2\n- [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240): 1-113, 2023. 1, 2\n- [9] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. arXiv preprint arXiv:2401.02330 , 2024. 1, 2\n- [10] Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, and Jiajun Ding. Imp: Highly capable large multimodal models for mobile devices. arXiv preprint arXiv:2405.12107 , 2024.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-78",
      "content": "ao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, and Jiajun Ding. Imp: Highly capable large multimodal models for mobile devices. arXiv preprint arXiv:2405.12107 , 2024. 1, 2, 4, 5\n- [11] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 , 2024. 1, 2\n- [12] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 , 2023. 1, 2, 3\n- [13] Microsoft. Phi-2: The surprising power of small language models, 2023. URL https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models . 1, 2, 3, 5, 6, 7, 8\n- [14] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-79",
      "content": "blog/ phi-2-the-surprising-power-of-small-language-models . 1, 2, 3, 5, 6, 7, 8\n- [14] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 11975-11986, October 2023. 1, 3, 5, 6, 7, 8\n\n- [15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886 , 2023. 1, 2\n- [16] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024. 1, 2, 3\n- [17] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-80",
      "content": "all language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024. 1, 2, 3\n- [17] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087 , 2023. 1, 2, 4, 5\n- [18] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 2\n- [19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020. 2\n- [20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 2\n- [21] AI@Meta. Llama 3 model card. 2024.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-81",
      "content": "ptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 2\n- [21] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL\\_CARD.md . 2, 3, 6, 7, 8\n- [22] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023. 2\n- [23] Stability AI. Introducing stable lm 2, 2024. URL https://stability.ai/news/ introducing-stable-lm-2 . 2, 3\n- [24] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 , 2024. 2\n- [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-82",
      "content": "preprint arXiv:2401.02385 , 2024. 2\n- [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 19730-19742. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/li23q.html . 2, 4, 5\n- [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https: //openreview.net/forum?id=w0H2xGHlkw . 2\n- [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 82,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-83",
      "content": "ormation Processing Systems , 2023. URL https: //openreview.net/forum?id=w0H2xGHlkw . 2\n- [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 26296-26306, June 2024. 3, 4, 5\n- [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/ . 4, 5\n- [29] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/ . 2, 4, 5\n- [30] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 83,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-84",
      "content": "odal capabilities in the wild, May 2024. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/ . 2, 4, 5\n- [30] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=1tZbq88f27 . 2\n\n- [31] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023. 2, 4, 5\n- [32] Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. arXiv preprint arXiv:2312.16862 , 2023. 2\n- [33] Qwen Team. Introducing qwen1.5, February 2024. URL https://qwenlm.github.io/ blog/qwen1.5/ .",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 84,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-85",
      "content": "cient multimodal large language model via small backbones. arXiv preprint arXiv:2312.16862 , 2023. 2\n- [33] Qwen Team. Introducing qwen1.5, February 2024. URL https://qwenlm.github.io/ blog/qwen1.5/ . 3\n- [34] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 , 2024. 3, 8\n- [35] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 , 2023. 3\n- [36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 85,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-86",
      "content": "ghtman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022. URL https://openreview.net/forum? id=M3Y74vmsMcY . 3\n- [37] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540 , 2023. 3\n- [38] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=CfXh93NDgH .",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 86,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-87",
      "content": "mpowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=CfXh93NDgH . 4\n- [39] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684 , 2024. 4, 6\n- [40] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9 . 4\n- [41] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 87,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-88",
      "content": "Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 , 2023. 4, 5\n- [42] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281 , 2023. 4, 5\n- [43] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 , 2023. 4, 5\n- [44] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 88,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-89",
      "content": "ng, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning\n\n- benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9556-9567, June 2024. 4, 5\n- [45] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904-6913, 2017. 4, 5\n- [46] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709, 2019.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 89,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-90",
      "content": ". Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709, 2019. 4, 5\n- [47] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS) , 2022. 4, 5\n- [48] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 292-305, Singapore, December 2023. Association for Computational Linguistics. 4, 5\n- [49] OpenAI. Gpt-4 technical report, 2023.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 90,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-91",
      "content": "3 Conference on Empirical Methods in Natural Language Processing , pages 292-305, Singapore, December 2023. Association for Computational Linguistics. 4, 5\n- [49] OpenAI. Gpt-4 technical report, 2023. 4, 5\n- [50] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/forum?id=vvoWPYqZJA . 4, 5\n- [51] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574 , 2023. 4, 5\n- [52] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 , 2023.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 91,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-92",
      "content": "Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 , 2023. 4, 5\n- [53] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 26689-26699, June 2024. 4, 5\n- [54] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814 , 2023. 4, 5\n- [55] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 92,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-93",
      "content": ", 5\n- [55] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 13040-13051, June 2024. 4, 5\n- [56] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935 , 2024. 4, 5\n- [57] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 93,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-94",
      "content": "Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. 4, 5\n- [58] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. 4, 5\n\n- [59] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis &amp; insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611 , 2024. 4, 5\n- [60] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246 , 2024.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 94,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-95",
      "content": "v preprint arXiv:2403.09611 , 2024. 4, 5\n- [60] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246 , 2024. 4, 5\n- [61] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766 , 2024. 4, 5\n- [62] Junlong Jia, Ying Hu, Xi Weng, Yiming Shi, Miao Li, Xingjian Zhang, Baichuan Zhou, Ziyu Liu, Jie Luo, Lei Huang, et al. Tinyllava factory: A modularized codebase for small-scale large multimodal models. arXiv preprint arXiv:2405.11788 , 2024. 4, 5\n- [63] Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. A comprehensive overhaul of multimodal assistant with small language models. arXiv preprint arXiv:2403.06199 , 2024.",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 95,
        "totalChunks": 97
      }
    },
    {
      "id": "2402.11530v3-chunk-96",
      "content": "g Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. A comprehensive overhaul of multimodal assistant with small language models. arXiv preprint arXiv:2403.06199 , 2024. 4, 5\n- [64] Minicpm-v 2.0: An efficient end-side mllm with strong ocr and understanding capabilities, 2024. URL https://openbmb.vercel.app/minicpm-v-2-en . 4, 5\n- [65] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? arXiv preprint arXiv:2403.13043 , 2024. 6, 8",
      "metadata": {
        "source": "arxiv:2402.11530v3",
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
          "Muyang He",
          "Yexin Liu",
          "Boya Wu",
          "Jianhao Yuan",
          "Yueze Wang",
          "Tiejun Huang",
          "Bo Zhao"
        ],
        "section": "References",
        "chunkIndex": 96,
        "totalChunks": 97
      }
    }
  ],
  "fullText": "## Efficient Multimodal Learning from Data-centric Perspective\n\nMuyang He 1 , 2 ⋆ Yexin Liu 1 , 3 ⋆ Boya Wu 1 ⋆ 4 1 1 , 2 1 , 5 †\n\nJianhao Yuan Yueze Wang Tiejun Huang Bo Zhao\n\n1 Beijing Academy of Artificial Intelligence\n\n2 Peking University 3 HKUST(GZ) 4 University of Oxford 5 SJTU\n\n⋆ †\n\nEqual Contribution Corresponding to Bo Zhao &lt; bo.zhao@sjtu.edu.cn\n\n## Abstract\n\nMultimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably cause significant performance drops. In this paper, we demonstrate the possibility of training a smaller but better MLLM with high-quality training data. Specifically, we introduce Bunny , a family of lightweight MLLMs with flexible vision and language backbones for efficient multimodal learning from selected training data. Experiments show that our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks. We expect that this work can provide the community with a clean and flexible open-source tool for further research and development. The code, models, and data can be found in https://github.com/BAAI-DCAI/Bunny .\n\n## 1 Introduction\n\nRecent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional visual understanding and reasoning performances across a range of tasks such as visual question answering [1, 2] and referring comprehension [3-5]. Benefiting from scalable Transformer-based architecture [6] and web-scale training data sources, these models have become foundational in the field of artificial intelligence, with their parameters increasing from billions to trillions [3, 7, 8].\n\nHowever, the deployment of these models is often hindered by their substantial computational costs and memory consumption in both training and inference phases, which limits their popularization across the broader research and user communities. Several early attempts, such as LLaVA-Phi [9], Imp [10], and Vary-toy [11], leverage off-the-shelf lightweight LLM backbones (e.g., Phi [12, 13]) and small vision encoders (e.g., SigLIP [14]) to build lightweight MLLMs. Further works also explore optimizations in model architecture and training recipes for tailored lightweight MLLM designs [15, 16].\n\nWhile these attempts to create lightweight versions of MLLMs achieve various levels of success, these models often fall short in terms of performance when compared to their larger counterparts, as reducing the size of these models often leads to a compromise in model capacity. An alternative direction to bridge the performance gap is data optimization. Many works have shown the importance of high-quality training data. For instance, Phi-2 [12] demonstrates that curated high-quality text data can bring the performances of small-scale LLMs close to that of large-scale models. SVIT [17] validates the importance of data sources in MLLMs. In this work, we focus on data optimization to compensate for the reduction in model size.\n\n&gt;\n\nWe introduce Bunny , a family of lightweight but powerful multimodal models with plug-and-play language/vision backbones and a multimodal projector. To compensate for the performance decrease caused by the model shrinking, we construct more informative training data by dataset condensation, i.e., curated data selection from a broader source. We demonstrate the possibility of training a smaller but better MLLM on more informative training data. Extensive experiments indicate that our Bunny-4B/8B model outperforms not only state-of-the-art small MLLMs with similar sizes but also even large ones on popular benchmarks.\n\n## 2 Related Work\n\n## 2.1 Large Language Model\n\nRecent advancements in Large Language Model (LLM) such as GPT [18, 19, 3], LLaMA [20, 7, 21] and PaLM [8, 22] have demonstrated remarkable capacity in various natural language processing tasks, for example, dialogue, creative writing, and problem-solving. Benefiting from scalable Transformer-based architecture [6] design and web-scale training data source, these models have become foundation models for general reasoning tasks. Among many research endeavors, one line of work focuses on miniaturizing LLM, for more efficient and low-cost training and inference. By leveraging high-quality pre-training and instruction tuning data, along with optimized lightweight architecture, lightweight LLMs with fewer than 3 billion parameters, such as Phi [12, 13], StableLM-2 [23], and TinyLLaMA [24], have demonstrated performance comparable to larger models with 7/13 billion parameters.\n\n## 2.2 Multimodal Large Language Model\n\nFollowing the success of LLM, one line of research recently shifted further toward the Multimodal Large Language Model (MLLM) [3, 1] for unified cross-modality understanding and reasoning. Building upon various pre-trained LLM backbones, various methods are proposed to perform crossmodality fusion. Flamingo [2] and BLIP-2 [25] propose different modality fusion techniques to fuse visual tokens to frozen large language models through gated attention or query transformers. Inspired by the success of instruction tuning, LLaV A [26-29] and MiniGPT-4 [30, 31] use visual instruction tuning to align visual input to LLMs, demonstrating remarkable success. More recent work, such as Kosmos-2 [4] and Shikra [5], further empowers MLLM with grounded visual understanding capacity. While these models exhibit promising potential for general-purpose visual reasoning and planning tasks, they are, in general, extremely expensive and prohibitive to train and deploy.\n\n## 2.3 Lightweight Multimodal Large Language Model\n\nMore related to our work are those efforts dedicated to miniaturizing MLLMs, thereby achieving low-cost and fast deployment. LLaVA-Phi [9], Imp [10] and Vary-toy [11] leverage off-the-shelf small language models and also achieve comparable performance on various benchmarks. Several works explore various ways to optimize the model architecture and training recipe to compensate for the decrease in model size. For instance, MobileVLM [15] focuses on the architecture design of lightweight LLMs and cross-modality projectors to enable training and inference on resourcelimited mobile devices. Moreover, MiniCPM [16] and TinyGPT-V [32] explore Bayesian search on hyperparameters and complex multi-stage training strategy, respectively, to optimize training recipes. An alternative direction, yet under-explored, is training data optimization. A number of works have emphasized the critical role of training data in foundation models [15, 17]. In this work, without bells and whistles, we show that by optimizing the training data, small-scale models can readily outperform their large-scale counterparts.\n\n## 3 Bunny: A Family of Lightweight Multimodal Models\n\nThis section introduces the flexible backbone combination of Bunny, the construction of informative training data, and the training procedure.\n\nFigure 1: Model Architecture. Bunny offers a flexible choice of vision encoder and LLM backbone combination, which are aligned through the cross-modality projector.\n\n<!-- image -->\n\n## 3.1 Architecture and Backbone\n\nFollowing the success of the MLLM paradigm of leveraging visual instruction tuning to align vision and language representations, our model contains three main modules: the LLM backbone, Vision Encoder, and Cross-modality Projector. As illustrated in Fig. 1, Bunny has a flexible combination of the three modules. We introduce some lightweight alternatives for each module as follows.\n\nLarge Language Model Backbone. We provide various options of state-of-the-art lightweight LLM: Phi-1.5 (1.3B) [12], Qwen1.5-1.8B [33], StableLM-2 (1.6B) [23], MiniCPM-2B (2.7B) [16], Phi-2 (2.7B) [13], Phi-3-Mini (3.8B) [34] and Llama-3-8B [21] as the language model backbone.\n\nVision Encoder. We provide two options for the lightweight vision encoder: SigLIP [14] and EVA-CLIP [35], which are both efficient language-aligned image encoders with 428M parameters.\n\nCross-modality Projector. Following LLaVA [27], we leverage a two-layer MLP with a GELU activation function as a cross-modality projector to align the vision encoder and LLM.\n\nMore choices of LLMs, vision encoders, and projectors will be provided in the future.\n\n## 3.2 Training Data Construction\n\nThe quality and scale of training data play critical roles in advancing MLLM performance. We construct Bunny-pretrain-LAION-2M and Bunny-695K for pre-training and instruction tuning, respectively.\n\nPre-training Data. We construct a high-quality pre-training data from LAION-2B [36], i.e., we condense LAION-2B into a 2M coreset for data-efficient learning. Specifically, we design a three-step coreset selection scheme 1 based on CLIP embedding. Firstly, motivated by SemDeDup [37], we cluster all 2B image embeddings by k-means, and then in each cluster, build an undirected graph that any two embeddings are connected if their cosine similarity is above a predetermined threshold. Only one sample is kept in each connected sub-graph whose Euclidean distance to the cluster centroid ranks at the median. By setting the threshold to 0 . 86 , we obtain a subset of 952M. Secondly, we sort the remaining samples by the cosine similarity between their text embedding and image embedding and keep samples ranking 40% -60% , subsequently resulting in a subset of 190M. In this way, the low-quality image-text pairs are removed. Thirdly, we sort the remaining samples by the cosine similarity between its image embedding and its cluster centroid and keep samples ranking 15% -35% , resulting in a subset of 38M, capturing the essence and diversity of LAION-2B. We finally randomly sampled 2 million samples from the 38M corset, resulting in Bunny-pretrain-LAION-2M for an appropriate training cost.\n\n1 https://github.com/BAAI-DCAI/Dataset-Pruning/tree/main/LAION\n\nFine-tuning Data. We collect a set of visual instruction tuning datasets DataOptim 2 , based on which we explore the better combination of fine-tuning datasets. Specifically, we leverage SVIT-mix665K [17] and replace ShareGPT-40K [27] in it with WizardLM-evol-instruct-70K [38], resulting to Bunny-695K. We find that tuning MLLM on multimodal data may damage its cognition ability inherited from the pre-trained LLM. Probably, it is caused by the less informative and less diverse text in the multimodal training data. Keeping several high-quality pure text data in the fine-tuning dataset can relieve this problem. In experiments, we jointly use Bunny-695K, LLaVA-665K [27] and ALLaVA-Instruct-4V [39] in the fine-tuning stage for better performance.\n\n## 3.3 Training Procedure\n\nWe employ a two-stage training strategy. The first stage is the pre-training stage, where we align the visual embeddings from a pre-trained vision encoder with the text embeddings from the LLM. The second stage is the fine-tuning stage, where we apply visual instruction tuning to fully utilize the capabilities of the MLLM across various multimodal tasks. In both stages, we use the same cross-entropy loss for next-token prediction. During the pre-training stage, only the cross-modality projector is optimized for one epoch. In the fine-tuning stage, we generally use LoRA [40] to train the LLM backbone, and the vision encoder and the cross-modality projector are fully tuned. In sec. 4.3, we conduct ablation studies about LoRA, data, epoch and freezing component, and we also explore high image resolution and weight merging for more practical MLLMs. For more training details, please refer to our GitHub repository.\n\n## 4 Experiment\n\n## 4.1 Settings\n\nWe evaluate Bunny on eleven popular benchmarks: MME perception [41], MME cognition [41], MMBench [42] test split, MMBench [42] dev split, SEED-Bench-1 [43], MMMU [44] validation split, MMMU[44] test split, VQA-v2 [45] test-dev split, GQA [46] test-dev-balanced split, ScienceQAIMG [47] test split and POPE [48]: the averaged F1-score of three categories (random, popular and adversarial) on the validation set of MSCOCO, to throughout assess its multimodal capacity. We compare against state-of-the-art multimodal large language models (MLLMs), including GPT-4V [49], BLIP-2 [25], InstructBLIP [50], Shikra-13/7B [5], SVIT-v1.5-13B [17], LVIS-INSTRUCT4V-13B [51], ShareGPT4V-13B [52], VILA1.5-13/3B [53], Mini-Gemini-HD-13B [54], LLaVA-v1.5-13/7B [27], LLaVA-NeXT-13/7B [28], MiniGPT-v2 [31], mPLUG-Owl2 [55], SPHINX-Intern2 [56], YiVL-6B [57], DeepSeek-VL-7B [58], MM1-7/3B-Chat [59], Idefics2 [60], LLaVA-NeXT-Llama-3-8B [29], MobileVLM-V2-3B [61], TinyLLaVA-Phi-2-SigLIP-3.1B [62], Mipha-3B [63], MiniCPM-V 2.0 [64], Imp-v1.5-4B-Phi3 [10].\n\n## 4.2 Comparison to the State of the Art\n\nAs shown in Tab. 1, Bunny-4B (SigLIP-SO + Phi-3-Mini) and Bunny-8B (SigLIP-SO + Llama-38B) outperform MLLMs with similar sizes on most of the benchmarks, demonstrating exceptional multimodal understanding and reasoning capabilities. Remarkably, Bunny-8B achieves an 1644.1 score on MME P , which overwhelms the runner-up LLaVA-NeXT-Llama-3-8B (1603.7) by a large margin. Bunny-4B also shows superiority over MM1-3B-Chat on the challenging MMMU benchmark, namely 7.5 and 4.7 improvements on the validation and test splits respectively. Besides, Bunny-4B and Bunny-8B accept images up to 1152 × 1152.\n\n## 4.3 Ablation Study\n\nLoRA vs. Fully Fine-tuning. In the fine-tuning stage, we observe that LoRA empirically leads to better performance compared to fully tuning the LLM. This improvement is likely because smaller models are more prone to catastrophic forgetting, which LoRA tuning helps to mitigate. We show the experimental results in Tab. 2.\n\nFor below experiments, the LLM is always tuned using LoRA.\n\n2 https://github.com/BAAI-DCAI/DataOptim\n\nTable 1: Comparison to state-of-the-art MLLMs on 11 benchmarks. Our model outperforms them in most of the settings. We evaluate these models on benchmarks: MME P : MME perception [41], MME C : MME cognition [41], MMB T : MMBench [42] test split, MMB D : MMBench [42] dev split, SEED: SEED-Bench-1 [43] with both total accuracy and image accuracy, MMMU V : MMMU [44] validation split, MMMU T : MMMU [44] test split, VQA-v2 [45] test-dev split, GQA [46] test-devbalanced split, SQA I : ScienceQA-IMG [47] test split and POPE [48]: the averaged F1-score of three categories (random, popular and adversarial) on the validation set of MSCOCO. We mark the best performance bold and the runner-up underlined in each section. § We evaluate the officially released checkpoint by ourselves.\n\n| Model                            | MME P   | MME C   | MMB T   | MMB D   | SEED(-I)    | MMMU V   | MMMU T   | VQA v2   | GQA   | SQA I   | POPE   |\n|----------------------------------|---------|---------|---------|---------|-------------|----------|----------|----------|-------|---------|--------|\n| Size > 8B                        |         |         |         |         |             |          |          |          |       |         |        |\n| GPT-4V-1106 [49]                 | 1334.0  | 437.5   | 77.0    | 75.1    | -/71.6      | 53.8     | -        | -        | -     | 82.1    | 75.4   |\n| BLIP-2-Flan-T5-XXL [25]          | 1293.8  | 290.0   | -       | -       | -/-         | 35.4     | 34.0     | 65.0     | 44.6  | 64.5    | -      |\n| InstructBLIP-Flan-T5-XXL [50]    | 1212.8  | 291.8   | -       | -       | -/-         | 35.7     | 33.8     | -        | 47.9  | 70.6    | -      |\n| BLIP-2-Vicuna-13B [25]           | -       | -       | -       | -       | -/-         | -        | -        | -        | 41.0  | 61.0    | -      |\n| InstructBLIP-Vicuna-13B [50]     | -       | -       | -       | -       | -/-         | -        | -        | -        | 49.5  | 63.1    | 83.7   |\n| Shikra-13B [5]                   | -       | -       | -       | -       | -/-         | -        | -        | 77.4     | -     | -       | -      |\n| SVIT-v1.5-13B [17]               | 1565.8  | 323.2   | 69.1    | -       | 61.9/-      | -        | 33.3     | 80.3     | 64.1  | 70.0    | -      |\n| LVIS-INSTRUCT4V-13B [51]         | 1574.9  | 286.8   | -       | 68.0    | 61.6/-      | -        | -        | 80.7     | 63.6  | 70.6    | 86.0   |\n| ShareGPT4V-13B [52]              | 1618.7  | 303.2   | -       | 68.5    | -/70.8      | -        | -        | 81.0     | 64.8  | 71.2    | -      |\n| VILA1.5-13B [53]                 | 1569.6  | -       | -       | 74.9    | 65.1/72.6   | 37.9     | 33.6     | 82.8     | 64.3  | 80.1    | 86.3   |\n| Mini-Gemini-HD-13B [54]          | 1597.0  | 320.0   | -       | 68.6    | -/-         | 37.3     | 35.1     | -        | -     | -       | -      |\n| LLaVA-v1.5-13B (LoRA) [27]       | 1541.7  | 300.4 § | 68.4 §  | 68.5    | 61.3/-      | 40.0 §   | 33.2 §   | 80.0     | 63.3  | 71.2    | 86.7   |\n| LLaVA-NeXT-13B [28]              | 1575.0  | 326.0   | -       | 70.0    | -/71.9      | 36.2     | -        | 82.8     | 65.4  | 73.6    | 86.2   |\n| 4B < Size ≤ 8B                   |         |         |         |         |             |          |          |          |       |         |        |\n| InstructBLIP-Vicuna-7B [50]      | -       | -       | 33.9    | 36.0    | 53.4/-      | -        | -        | -        | 49.2  | 60.5    | -      |\n| MiniGPT-v2 [31]                  | -       | -       | -       | -       | -/-         | -        | -        | -        | 60.3  | -       | -      |\n| Shikra-7B [5]                    | -       | -       | 60.2    | 58.8    | -/-         | -        | -        | -        | -     | -       | -      |\n| mPLUG-Owl2 [55]                  | 1450.2  | 313.2   | 66.0    | 66.5    | 57.8/-      | 32.7     | 32.1     | 79.4     | 56.1  | 68.7    | 85.8   |\n| SPHINX-Intern2 [56]              | 1260.4  | 294.6   | -       | 57.9    | -/68.8      | -        | -        | 75.5     | 56.2  | 70.4    | 86.9   |\n| Yi-VL-6B [57]                    | -       | -       | 68.4    | 68.6    | -/67.6      | 39.1     | 37.8     | -        | -     | -       | -      |\n| DeepSeek-VL-7B [58]              | -       | -       | -       | 73.2    | -/70.4      | 36.6     | -        | -        | -     | -       | 88.1   |\n| LLaVA-v1.5-7B (LoRA) [27]        | 1476.9  | 267.9 § | 66.1 §  | 66.1    | 60.1/-      | 34.4 §   | 31.7 §   | 79.1     | 63.0  | 68.4    | 86.4   |\n| LLaVA-NeXT-7B [28]               | 1519.0  | 332.0   | -       | 67.4    | -/70.2      | 35.8     | -        | 81.8     | 64.2  | 70.1    | 86.5   |\n| MM1-7B-Chat [59]                 | 1529.3  | 328.9   | -       | 72.3    | 64.0/69.9   | 37.0     | 35.6     | 82.8     | -     | 72.6    | 86.6   |\n| Idefics2 [60]                    | -       | -       | 76.7    | -       | -/-         | 43.0     | 37.7     | 81.2     | -     | -       | -      |\n| LLaVA-NeXT-Llama-3-8B [29]       | 1603.7  | 367.8   | -       | 72.1    | -/-         | 41.7     | -        | -        | -     | -       | -      |\n| Bunny-8B                         | 1644.1  | 367.5   | 78.1    | 77.2    | 66.2 / 73.5 | 43.3     | 39.0     | 82.9     | 64.0  | 79.9    | 87.2   |\n| Size ≤ 4B                        |         |         |         |         |             |          |          |          |       |         |        |\n| MobileVLM-V2-3B [61]             | 1440.5  | -       | -       | 63.2    | -/-         | -        | -        | -        | 61.1  | 70.0    | 84.7   |\n| TinyLLaVA-Phi-2-SigLIP-3.1B [62] | 1466.4  | -       | -       | -       | -/-         | 38.4     | -        | 80.1     | 62.1  | 73.0    | 87.2   |\n| VILA1.5-3B [53]                  | 1442.4  | -       | -       | 63.4    | 60.9/67.9   | 33.3     | 30.8     | 80.4     | 61.5  | 69.0    | 85.9   |\n| Mipha-3B [63]                    | 1488.9  | 295.0   | -       | 69.7    | -/68.9      | -        | -        | 81.3     | 63.9  | 70.9    | 86.7   |\n| MiniCPM-V 2.0 [64]               | 1411.4  | 396.8   | 69.1    | 69.6    | -/67.1      | 38.2     | -        | -        | -     | 80.7    | 86.3   |\n| MM1-3B-Chat [59]                 | 1482.5  | 279.3   | -       | 67.8    | 63.0/68.8   | 33.9     | 33.7     | 82.0     | -     | 69.4    | 87.4   |\n| Imp-v1.5-4B-Phi3 [10]            | 1507.7  | -       | -       | 73.3    | -/-         | -        | -        | 81.5     | 63.5  | 78.3    | 86.9   |\n| Bunny-4B                         | 1581.5  | 361.1   | 75.7    | 74.2    | 64.9 / 72.5 | 41.4     | 38.4     | 82.1     | 63.2  | 78.3    | 87.2   |\n\nTable 2: Comparison between LoRA and fully tuning. The model is composed of SigLIP-SO [14] and Phi-2 [13], and trained on Bunny-695K for 1 epoch. The vision encoder is frozen.\n\n| LLM Tuning        |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Fully Tuning LoRA |  1465.2 |   277.5 |    67.9 |    67.2 | 62.1(69.5)    |     36.8 |     33.1 |     79.1 |  61.7 |    72.8 |   85.9 |\n| Fully Tuning LoRA |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n\nTable 3: Ablation study on fine-tuning data and training epochs. The model is composed of SigLIPSO [14] and Phi-2 [13] with the vision encoder frozen.\n\n| Fine-tuning Data        |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K              |       1 |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.6)    |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| Bunny-695K              |       2 |  1480.6 |   272.5 |    71.2 |    69   | 63.0 ( 71.3 ) |     37.6 |     32.5 |     81   |  64   |    69.7 |   86.3 |\n| Bunny-695K + LLaVA-665K |       1 |  1501.3 |   272.5 |    72.4 |    71.1 | 63.0 (71.1)   |     37.8 |     33.4 |     81   |  63.6 |    69.5 |   87.2 |\n\nTable 4: Ablation study on extra fine-tuning data. The model is composed of SigLIP-SO [14] and Llama-3-8B [21] where the vision encoder is frozen. S 2 -Wrapper [65] is used.\n\n| Fine-tuning Data                |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K                      |       1 |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Bunny-695K + ALLaVA-Instruct-4V |       1 |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 ( 73.4 ) |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |\n\nTable 5: Ablation study on tuning/freezing vision encoder in the fine-tuning stage. The model consists of SigLIP-SO [14] and Phi-2 [13].\n\n| Fine-tuning Data        | Vision Encoder   | MME P         | MME C       | MMB T     | MMB D     | SEED(-I)               | MMMU V    | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|---------------|-------------|-----------|-----------|------------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen Trainable | 1488.8 1486.0 | 289.3 281.1 | 69.2 69.5 | 68.6 69.3 | 62.5 (70.6) 62.3(70.6) | 38.2 37.7 | 33.0 33.2 | 79.8 79.9 | 62.5 62.6 | 70.9 70.6 | 86.8 85.9 |\n| Bunny-695K + LLaVA-665K | Frozen Trainable | 1501.3 1521.5 | 272.5 291.8 | 72.4 71.9 | 71.1 71.3 | 63.0(71.1) 63.1 (71.1) | 37.8 38.0 | 33.4 33.1 | 81.0 81.1 | 63.6 63.7 | 69.5 70.8 | 87.2 87.0 |\n\nFine-tuning Data. Tab. 3 shows the ablation study on data strategy. Jointly leveraging Bunny695K and LLaVA-665K for one epoch generally yields performance improvements across multiple benchmarks. A comparison between training on Bunny-695K for two epochs and training on Bunny-695K and LLaVA-665K for one epoch demonstrates that increasing data diversity enhances performance under the same training cost. To further enhance diversity, we incorporate ALLaVAInstruct-4V [39]. Given the detailed answers provided in ALLaVA-Instruct-4V, we append 'Please answer the question in detail.' to the end of questions to avoid overfitting the model to long-form answers. As shown in Tab. 4, adding ALLaVA-Instruct-4V brings performance improvements. Based on above results, we set the fine-tuning stage to be 1 epoch in the rest experiments in this paper.\n\nTrainable vs. Frozen Vision Encoder. By default, we freeze the vision encoder in both stages of our primary experiments. We also investigate the influence of enabling the vision encoder to be trainable during the fine-tuning stage. As shown in Tab. 5, the results vary depending on the fine-tuning data used. When employing Bunny-695K + LLaVA-665K, unfreezing the vision encoder leads to better results. Tab. 6 shows that the performance can be further enhanced with additional finetuning data when the vision encoder is trainable.\n\nLearning Rate. As shown in Tab. 7, increasing the pre-training learning rate does not consistently lead to improvements. In this context, both 5 × 10 -4 and 8 × 10 -4 are relatively good choices, while 1 × 10 -3 is sub-optimal. As shown in Tab. 8, the original learning rates set in LLaV A are the most appropriate. A learning rate that is too high causes extremely inferior results, including several sudden spikes in loss during training.\n\nHigher Resolution. The ability of understanding high-resolution images and capturing the finegrained details is crucial for a helpful MLLM. Given that the default image size of SigLIP-SO is 384 × 384, we try two approaches to scale up image resolution: positional embedding interpolation and sub-image decomposition. Through positional embedding interpolation, the vision encoder is adapted to the new resolution during fine-tuning, i.e. 448 × 448, and an image is represented with 1024 tokens, resulting a 20% training time increase. Tab. 9 shows that the improvement is marginal. For sub-image decomposition, we leverage S 2 -Wrapper [65], a simple mechanism that extends pretrained vision model to multiple image scales, which processes images at multiple scales by interpolating, splitting, encoding, merging, and concatenating features, resulting in a multi-scale representation with 3 times channel dimension while maintaining the token length. It would increase the training time\n\nTable 6: Ablation study on fine-tuning data with vision encoder tuned. The experiments are conducted with SigLIP-SO [14] as the vision encoder, and S 2 -Wrapper [65] is enabled. B-695K, L-665K and A-Ins represents for Bunny-695K, LLaVA-665K and ALLaVA-Instruct-4V, respectively.\n\n| LLM        | Fine-tuning Data                  |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I | POPE      |\n|------------|-----------------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|-----------|\n| Phi-3-Mini | B-695K + L-665K                   |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   | 87.0      |\n| Phi-3-Mini | B-695K + L-665K + A-Ins           |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4 ( 71.9 ) |     42.4 |     38.1 |     82   |  62.9 |    77.8 | 86.9      |\n| Llama-3-8B | B-695K + L-665K B-695K + L-665K + |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 | 86.8 87.3 |\n| Llama-3-8B | A-Ins                             |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5 ( 73.1 ) |     40.4 |     38.8 |     82.8 |  64.1 |    79   |           |\n\nTable 7: Ablation study on pre-training learning rate. The model consists of SigLIP-SO [14] and Phi-2 [13], where Bunny-695K is utilized for fine-tuning and the vision encoder is frozen.\n\n| Pre-training LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 5 × 10 - 4        |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| 8 × 10 - 4        |  1494.3 |   277.5 |    69.6 |    69   | 62.1(70.5)    |     36.4 |     33.5 |     80   |  62.3 |    72.9 |   86.9 |\n| 1 × 10 - 3        |  1448.3 |   261.1 |    70.5 |    68.8 | 62.4(70.4)    |     36.8 |     33.3 |     80   |  62.3 |    71.1 |   86.4 |\n\nTable 8: Ablation study on fine-tuning learning rate. The model consists of SigLIP-SO [14] and Phi-2 [13], where Bunny-695K + LLaVA-665K is utilized for fine-tuning and the vision encoder is trainable.\n\n| LoRA LR    | Vision &Projector LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 2 × 10 - 4 | 2 × 10 - 5             |  1521.5 |   291.8 |    71.9 |    71.3 | 63.1 ( 71.1 ) |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| 1 × 10 - 4 | 1 × 10 - 5             |  1492.8 |   283.9 |    71.2 |    69.7 | 63.1 (70.9)   |     38.4 |     33.3 |     80.4 |  62.8 |    71.5 |   86.8 |\n| 4 × 10 - 4 | 4 × 10 - 5             |   987.7 |   240.4 |     0   |     0   | 25.9(26.3)    |     27.9 |     25.3 |     69   |  53.7 |    36.7 |   83.3 |\n\nfor 23%. As shown in Tab. 10, S 2 -Wrapper leads to a huge performance improvement while using Bunny-695K + LLaVA-665K. Considering the performance and high resolution, we take S 2 -Wrapper as the final solution.\n\nWeight Merging. During ablation studies, we obtain numerous models with the same architecture under different settings. We explore the potential benefits of merging two models by weighted averaging. As shown in Tab. 11, simple linear merging greatly enhances performance, with the best results forming our Bunny-4B/8B models. This method is more efficient and effective than hyper-parameter searching for improving model performance. We also observe that the results on some benchmarks (e.g. MME) are very sensitive to the weight factor.\n\nBackbone Combination. We also investigate Bunny's performance with various combinations of vision encoders and language models. As shown in Tab. 12, all combinations show exceptional performance, with SigLIP-SO [14] and Llama-3-8B [21] achieving the best results. This highlights the flexibility of the Bunny framework and the high quality of the curated training data. Note that in the fine-tuning stage, Phi-1.5, StableLM-2 and Phi-2-based Bunny are trained on Bunny-695K for 1 epoch with the vision encoder frozen. In contrast, Phi-3-Mini and Llama-3-8B-based Bunny are trained using additional fine-tuning data, S 2 -Wrapper, trainable vision encoder, weight merging, etc., as illustrated before.\n\n## 4.4 Qualitative Demonstrations\n\nWe also present several qualitative test cases for Bunny. As illustrated in Tab. 13, Bunny-8B with SigLIP-SO [14] and Llama-3-8B [21] demonstrates exceptional capabilities in visual understanding, common sense reasoning, external knowledge referring, optical character recognizing, and mathematical problem-solving. These results feature Bunny-8B as a powerful and versatile visual assistant. Additionally, Tab. 14 presents several test cases in Chinese using Bunny-8B. Although we do not intentionally include Chinese in the multimodal training data, Bunny still exhibits impressive Chinese instruction-following capabilities.\n\n## 5 Conclusion\n\nWe present Bunny, a family of lightweight but powerful multimodal models. It offers multiple plugand-play vision encoders, including EVA-CLIP and SigLIP, and language backbones, including Phi1.5, Qwen1.5-1.8B, StableLM-2, MiniCPM-2B, Phi-2, Phi-3-Mini, and Llama-3-8B. To compensate for the decrease in model size, we construct more informative training data by selecting data from a broader source. Remarkably, our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple benchmarks.\n\nTable 9: Results on scaling up image resolution by positional embedding interpolation. The model consists of SigLIP-SO [14] and Phi-2 [13].\n\n| Fine-tuning Data        | Vision Encoder Res   | MME P      |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|----------------------|------------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K + LLaVA-665K | Trainable            | 384 1521.5 |   291.8 |    71.9 |    71.3 | 63.1(71.1)    |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| Bunny-695K + LLaVA-665K | Trainable            | 448 1489.2 |   309.6 |    71.6 |    71.5 | 63.3 ( 71.4 ) |     37.2 |     32.5 |     81.4 |  63.2 |    71   |   87.2 |\n\nTable 10: Ablation study on scaling up image resolution by sub-image decomposition (S 2 -Wrapper [65]). The model consists of SigLIP-SO [14] and Phi-3-Mini [34].\n\n| Fine-tuning Data        | Vision Encoder   | Res MME P       | MME C       | MMB T     | MMB D     | SEED(-I)                 |   MMMU V | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|-----------------|-------------|-----------|-----------|--------------------------|----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen           | 384 1402.0      | 286.1       | 73.1      | 72.3      | 64.6 ( 71.9 )            |     40.7 | 39.3      | 80.6      | 62.4      | 75.3      | 85.8 86.6 |\n|                         |                  | 1152 1473.1     | 332.1       | 72.8      | 72.0      | 64.1(71.3)               |     41.8 | 38.3      | 80.6      | 62.4 63.0 | 75.6 74.4 | 86.4      |\n| Bunny-695K + LLaVA-665K | Trainable        | 384 1490.5 1152 | 316.4 336.4 | 72.4 74.4 | 72.0 74.3 | 63.7(71.3) 64.2 ( 71.4 ) |     40   | 38.3 38.6 | 81.3 81.6 |           |           |           |\n| Bunny-695K + LLaVA-665K |                  | 1503.5          |             |           |           |                          |     38.8 |           |           | 63.1      | 75.0      | 87.0      |\n\nTable 11: Ablation study on weight merging. The experiments are conducted with SigLIP-SO [14] as the vision encoder. The weight factor is selected empirically. F / T represents whether the V ision E ncoder is F rozen or T rainable. B-695K, L-665K and A-Ins represents for Bunny-695K, LLaVA-665K and ALLaVA-Instruct-4V, respectively.\n\n| LLM        | S 2   | Fine-tuning Data             | VE   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|-------|------------------------------|------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n|            |       | B-695K                       | F    |  1402   |   286.1 |    73.1 |    72.3 | 64.6 (71.9)   |     40.7 |     39.3 |     80.6 |  62.4 |    75.3 |   85.8 |\n| Phi-3-Mini | w \\   | o B-695K + L-665K            | T    |  1490.5 |   316.4 |    72.4 |    72   | 63.7(71.3)    |     40   |     38.3 |     81.3 |  63   |    74.4 |   86.4 |\n|            |       | 3:7 Averaging                |      |  1495.2 |   338.9 |    74   |    73.5 | 64.5( 72.1 )  |     40.1 |     39.1 |     81.5 |  63.5 |    75.2 |   86.7 |\n|            |       | B-695K                       | F    |  1473.1 |   332.1 |    72.8 |    72   | 64.1(71.3)    |     41.8 |     38.3 |     80.6 |  62.4 |    75.6 |   86.6 |\n| Phi-3-Mini | w \\   | B-695K + L-665K              | T    |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   |   87   |\n|            |       | 3:7 Averaging                |      |  1503.9 |   362.9 |    74.1 |    74.1 | 64.6 ( 71.7 ) |     40.2 |     38.8 |     81.7 |  63.4 |    76.3 |   87   |\n|            |       | B-695K + A-Ins               | F    |  1552.4 |   306.8 |    74   |    73.8 | 62.8(71.0)    |     40.6 |     38.1 |     81   |  62.3 |    75   |   86.7 |\n| Phi-3-Mini | w \\   | B-695K + L-665K + A-Ins      | T    |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4(71.9)    |     42.4 |     38.1 |     82   |  62.9 |    77.8 |   86.9 |\n|            |       | 3:7 Averaging ( Bunny-4B     | )    |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9 ( 72.5 ) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n|            |       | B-695K                       | F    |  1542.7 |   344.6 |    75.7 |    74.8 | 65.2(73.1)    |     42.4 |     37.1 |     82   |  64.4 |    78.7 |   85.9 |\n| Llama-3-8B | w \\ o | B-695K + L-665K              | T    |  1562.2 |   319.6 |    75.3 |    74.4 | 65.2(72.5)    |     43.4 |     38   |     82.4 |  64.1 |    77.4 |   86.4 |\n|            |       | 5:5 Averaging                |      |  1588.9 |   321.1 |    77.2 |    76.7 | 65.9 ( 73.3 ) |     42.8 |     39   |     82.6 |  64.8 |    80.4 |   86.9 |\n|            |       | B-695K                       | F    |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Llama-3-8B | w \\   | B-695K + L-665K              | T    |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 |   86.8 |\n|            |       | 6:4 Averaging                |      |  1607.8 |   324.6 |    76.5 |    76.3 | 65.9 ( 73.6 ) |     43.7 |     38.8 |     82.3 |  64.5 |    80.4 |   87.4 |\n|            |       | B-695K + A-Ins               | F    |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 (73.4)   |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |\n| Llama-3-8B | w \\   | B-695K + L-665K + A-Ins      | T    |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5(73.1)    |     40.4 |     38.8 |     82.8 |  64.1 |    79   |   87.3 |\n|            |       | 4.2:5.8 Averaging ( Bunny-8B | )    |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2 ( 73.5 ) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |\n\nTable 12: More results of Bunny with various language and vision backbones. The best performances are achieved by integrating SigLIP-SO [14] and Llama-3-8B [21].\n\n| Vision Encoder      | LLM               |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)   |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------|-------------------|---------|---------|---------|---------|------------|----------|----------|----------|-------|---------|--------|\n| EVA02-CLIP-L (0.4B) | Phi-1.5 (1.3B)    |  1213.7 |   278.9 |    60.9 |    56.8 | 56.4(64.1) |     30   |     28.4 |     76.5 |  60.4 |    58.2 |   86.1 |\n| EVA02-CLIP-L (0.4B) | StableLM-2 (1.6B) |  1301   |   235   |    58.4 |    56.4 | 55.3(62.8) |     29.8 |     29.4 |     74.6 |  56.7 |    60   |   84.8 |\n| EVA02-CLIP-L (0.4B) | Phi-2 (2.7B)      |  1421   |   285.4 |    68.6 |    67.4 | 62.2(70.2) |     35.9 |     32.6 |     78.9 |  62.3 |    69.1 |   87.1 |\n| SigLIP-SO (0.4B)    | Phi-1.5 (1.3B)    |  1230   |   237.5 |    61.2 |    59.7 | 57.7(65.3) |     30   |     29.1 |     78   |  61.1 |    61.3 |   85.8 |\n| SigLIP-SO (0.4B)    | StableLM-2 (1.6B) |  1366.8 |   236.1 |    65.1 |    62.8 | 58.8(67.5) |     29.9 |     29.8 |     78.9 |  60.9 |    61.1 |   85.9 |\n| SigLIP-SO (0.4B)    | Phi-2 (2.7B)      |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.7) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| SigLIP-SO (0.4B)    | Phi-3-Mini (3.8B) |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9(72.5) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n| SigLIP-SO (0.4B)    | Llama-3-8B        |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2(73.5) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Do you know who are they?\n\nThey are members of the Beatles, one of the most famous rock bands in history.\n\n## Do you recognize where does this picture come from?\n\nYes, this is a famous photo taken by Iain Macmillan on Abbey Road in London, England. It was used as the cover art for the Beatles' album 'Abbey Road' released in 1969.\n\n## How do you know?\n\nThe image has been widely recognized due to its iconic status and the distinctive white lines that cross the street, which are part of the pedestrian crossing at Abbey Road.\n\n## What kind of sport can I play here?\n\nGiven the snowy environment, you could engage in winter sports such as skiing or snowboarding.\n\n## Is it safe for me to go skiing here?\n\nIt appears to be a suitable location for skiing due to the presence of snow-covered slopes. However, it would be best to check with local authorities for any safety guidelines or restrictions before engaging in this activity.\n\n## What kind of art is this?\n\nThis is a painting by Vincent van Gogh, specifically a still life featuring sunflowers in a vase.\n\n## Do you find the painting beautiful?\n\nAs an AI, I don't have personal opinions or emotions. However, many people consider Van Gogh's paintings to be beautiful due to their vibrant colors and unique style.\n\n## What is the astronaut holding in his hand?\n\nThe astronaut is holding a green bottle of beer in his hand.\n\n## Can this happen in real world?\n\nIt's highly unlikely that an astronaut would be sitting on the moon with a beer bottle. This image seems to be a creative representation or a humorous depiction rather than a realistic scenario.\n\n## Why is that?\n\nAstronauts are trained for space missions and their primary focus is on scientific research and exploration. They follow strict protocols and guidelines to ensure safety and efficiency during their time in space. Drinking beer on the moon would not align with these objectives.\n\n## Why is the image funny?\n\nThe image is funny because it features a cat sitting in front of a wall with yellow letters spelling out 'Huh?' on it. This creates a humorous juxtaposition between the cat's expression and the question mark, suggesting that the cat might be confused or puzzled by something.\n\n## When should I use the meme?\n\nYou can use this meme when you want to express confusion or puzzlement about something. It could also be used humorously to ask for clarification or to poke fun at someone who seems perplexed.\n\n## What is the height and base of the triangle?\n\nThe height of the triangle is 8 meters and its base is 20 meters.\n\n## Can you calculate the area of triangle?\n\nYes, I can calculate the area of the triangle. The formula for the area of a triangle is (base * height) / 2. In this case, it would be (20m * 8m) / 2 = 80 square meters.\n\n## Students of what grade should be able to solve this problem?\n\nThis problem should be solvable by students in middle school or high school who have learned basic geometry concepts such as calculating the area of triangles using the formula (base * height) / 2.\n\nTable 13: Qualitative performances of Bunny-8B : The bold text represents user input, while the non-bold text is the model response. The results demonstrate Bunny's versatile capabilities in visual understanding, common sense reasoning, external knowledge referring, optical character recognizing, and mathematical problem solving.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- 请 用 中 文 鉴 赏 一下 图 片 ？\n\n这 是 一 幅 奇 幻 的 绘 画 ， 描 绘 了一 位 女 性 和 一 只 红 色 的 龙 。 女 性 的 脸 部 被 细 腻 地 描 绘 ， 包 括 她 的 眼睛 、 鼻 子 、 嘴 巴 和 耳 朵 。 她 长 着 一 头 卷 曲 的 发 型 ， 发 色 是 粉 红 色 的 。 同 时 ， 她 的皮 肤 也 呈 现 出 一 种 粉 红 色 的 光 泽 。\n\n在 女 性 面 前 飞 舞 着 一 只 红 色 的 龙 ， 龙 的 身 体 部 分 可 以 看 到 它 的 翅 膀 和 尾 巴 。 龙 的 颜 色 非 常 鲜 艳 ， 体 现 了 绘 画 中 奇 幻 元 素 的 特 点 。\n\n整 体 来 说 ， 这 幅 画 充 满 了 奇 幻 和 梦 幻 的 氛 围 ， 女 性 和 龙 之 间 的 互 动 关 系 也 很 有 趣 。\n\n- 请 用 中 文 详 细 描 述这 张 图 片\n\n这 是 一 幅 描 绘 了一个 美 丽 的 自 然 景 观 的 画 面 。 画 面 中 ， 两个人 在 一 艘 小 船 上 ， 正 在 穿 过 一 片 水 面 。 这 艘船 位 于 画 面 的 中 心 ， 人们 似 乎 是 在 享 受 着 这 个 宁 静 的 湖 泊 的 美 景 。\n\n水 面 上 还 有 一些 反 射 ， 可 能 是 来 自 周 围 的 山 脉 和 树 林 的 光 芒 。 这 些 反 射 使 得 水 面 更 加 迷 人 ， 增 添 了一 种神秘 感 。\n\n在 画 面 背 景 中 ， 有 一 座 山 脉 ， 它 们 高 耸 入 云 ， 形 成 了了一个 壮 丽 的 天 际 线 。 山 脉 的 山峰 部 分 被 阳 光 照 亮 ， 反 映 在 水 面 上 ， 给 整 个 画 面 带 来 了 温 暖 的 氛 围 。\n\n在 山 脉 的 脚 下 ， 是 一 片 茂 密 的 树 林 。 这 些 树 林 的 叶 子 呈 现 出 不 同 的 颜 色 ， 可 能 是 由 于 季 节 的 变化 或 日 落 的 光 芒 。 这 些 树 林 的 反 影 也 可 以 在 水 面 上 看 到 ， 这 使 得 整 个 画 面 更 加 生 动 。\n\n总 体 来 说 ， 这 幅 画 面 展 示 了一个 宁 静 、 美 丽 的 自 然 景 观 ， 两个人 在 其 中 度 过 了一 段 愉 快 的 时 光 。\n\n路 牌 上 的 医 院 名 称 是 什么 ？ 路 牌 上 的 医 院 名 称 是 'Kwong Wah Hospital' 。\n\n- 巴 士 的 运 营 时 间 是 几 点 ？\n\n巴 士 的 运 营 时 间 是 从 早 上 7 点 到 晚 上 12 点 。\n\n- 这 是 哪 座 城 市 ？\n\n这 是 香 港 的 市 区 。\n\n- 请 用 中 文 回 答 为什么 这 张 图 片 如 此 有 趣 ？\n\n这 张 图 片 非 常 有 趣 ， 因 为 它 展 示 了一个 小 狗 的 可 爱 和 天 真 。 小 狗 的 双 手 抱 着 自 己 的 头 ， 表 明 它 在 思 考 或 感 到 困 惑 。 这 场 景 让 人 联 想 到 人 类 的 行 为 ， 特 别 是 孩子 们 经 常 这 样 做 。 同 时 ， 小 狗 的 颜 色 和 表 情 也 很 吸 引 人 ， 让 人 无 法 不 被 其 可 爱 所 吸 引 。\n\n- 这 张 图 的 创 意 体 现 在 什么 地 方 ？\n\n这 张 图 的 创 意 体 现 在 将 一个 船 和 一 只 鸟 安 置 在 一个 蛋 中 ， 创 造 出 一个 奇 怪 、 有 趣 的 场 景 。\n\n- 这 张 图 片 可 能 具 有 什么 用 途 ？\n\n这 张 图 可 能 是 用 于 表 达 创 意 或 设计 的 概 念 ， 或 者 作 为一 种 艺 术 作 品 展 现 。\n\nTable 14: Qualitative performances of Bunny-8B : The results demonstrate its good Chinese instruction-following ability.\n\n## References\n\n- [1] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. 1, 2\n- [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35: 23716-23736, 2022. 1, 2\n- [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1, 2\n- [4] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=lLmqxkfSIw . 2\n- [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023. 1, 2, 4, 5\n- [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017. 1, 2\n- [7] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 1, 2\n- [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240): 1-113, 2023. 1, 2\n- [9] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. arXiv preprint arXiv:2401.02330 , 2024. 1, 2\n- [10] Zhenwei Shao, Zhou Yu, Jun Yu, Xuecheng Ouyang, Lihao Zheng, Zhenbiao Gai, Mingyang Wang, and Jiajun Ding. Imp: Highly capable large multimodal models for mobile devices. arXiv preprint arXiv:2405.12107 , 2024. 1, 2, 4, 5\n- [11] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, En Yu, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Small language model meets with reinforced vision vocabulary. arXiv preprint arXiv:2401.12503 , 2024. 1, 2\n- [12] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 , 2023. 1, 2, 3\n- [13] Microsoft. Phi-2: The surprising power of small language models, 2023. URL https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models . 1, 2, 3, 5, 6, 7, 8\n- [14] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 11975-11986, October 2023. 1, 3, 5, 6, 7, 8\n\n- [15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886 , 2023. 1, 2\n- [16] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024. 1, 2, 3\n- [17] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087 , 2023. 1, 2, 4, 5\n- [18] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. 2\n- [19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020. 2\n- [20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 2\n- [21] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL\\_CARD.md . 2, 3, 6, 7, 8\n- [22] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023. 2\n- [23] Stability AI. Introducing stable lm 2, 2024. URL https://stability.ai/news/ introducing-stable-lm-2 . 2, 3\n- [24] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385 , 2024. 2\n- [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning , volume 202 of Proceedings of Machine Learning Research , pages 19730-19742. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/li23q.html . 2, 4, 5\n- [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https: //openreview.net/forum?id=w0H2xGHlkw . 2\n- [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 26296-26306, June 2024. 3, 4, 5\n- [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/ . 4, 5\n- [29] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. URL https://llava-vl.github.io/blog/ 2024-05-10-llava-next-stronger-llms/ . 2, 4, 5\n- [30] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview. net/forum?id=1tZbq88f27 . 2\n\n- [31] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023. 2, 4, 5\n- [32] Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones. arXiv preprint arXiv:2312.16862 , 2023. 2\n- [33] Qwen Team. Introducing qwen1.5, February 2024. URL https://qwenlm.github.io/ blog/qwen1.5/ . 3\n- [34] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 , 2024. 3, 8\n- [35] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389 , 2023. 3\n- [36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022. URL https://openreview.net/forum? id=M3Y74vmsMcY . 3\n- [37] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540 , 2023. 3\n- [38] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=CfXh93NDgH . 4\n- [39] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for a lite vision-language model. arXiv preprint arXiv:2402.11684 , 2024. 4, 6\n- [40] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9 . 4\n- [41] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 , 2023. 4, 5\n- [42] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281 , 2023. 4, 5\n- [43] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 , 2023. 4, 5\n- [44] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning\n\n- benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9556-9567, June 2024. 4, 5\n- [45] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904-6913, 2017. 4, 5\n- [46] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709, 2019. 4, 5\n- [47] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS) , 2022. 4, 5\n- [48] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 292-305, Singapore, December 2023. Association for Computational Linguistics. 4, 5\n- [49] OpenAI. Gpt-4 technical report, 2023. 4, 5\n- [50] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL https://openreview.net/forum?id=vvoWPYqZJA . 4, 5\n- [51] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574 , 2023. 4, 5\n- [52] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793 , 2023. 4, 5\n- [53] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 26689-26699, June 2024. 4, 5\n- [54] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814 , 2023. 4, 5\n- [55] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 13040-13051, June 2024. 4, 5\n- [56] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et al. Sphinx-x: Scaling data and parameters for a family of multi-modal large language models. arXiv preprint arXiv:2402.05935 , 2024. 4, 5\n- [57] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2024. 4, 5\n- [58] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. 4, 5\n\n- [59] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis &amp; insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611 , 2024. 4, 5\n- [60] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246 , 2024. 4, 5\n- [61] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766 , 2024. 4, 5\n- [62] Junlong Jia, Ying Hu, Xi Weng, Yiming Shi, Miao Li, Xingjian Zhang, Baichuan Zhou, Ziyu Liu, Jie Luo, Lei Huang, et al. Tinyllava factory: A modularized codebase for small-scale large multimodal models. arXiv preprint arXiv:2405.11788 , 2024. 4, 5\n- [63] Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, and Jian Tang. A comprehensive overhaul of multimodal assistant with small language models. arXiv preprint arXiv:2403.06199 , 2024. 4, 5\n- [64] Minicpm-v 2.0: An efficient end-side mllm with strong ocr and understanding capabilities, 2024. URL https://openbmb.vercel.app/minicpm-v-2-en . 4, 5\n- [65] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? arXiv preprint arXiv:2403.13043 , 2024. 6, 8",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model                            | MME P   | MME C   | MMB T   | MMB D   | SEED(-I)    | MMMU V   | MMMU T   | VQA v2   | GQA   | SQA I   | POPE   |\n|----------------------------------|---------|---------|---------|---------|-------------|----------|----------|----------|-------|---------|--------|\n| Size > 8B                        |         |         |         |         |             |          |          |          |       |         |        |\n| GPT-4V-1106 [49]                 | 1334.0  | 437.5   | 77.0    | 75.1    | -/71.6      | 53.8     | -        | -        | -     | 82.1    | 75.4   |\n| BLIP-2-Flan-T5-XXL [25]          | 1293.8  | 290.0   | -       | -       | -/-         | 35.4     | 34.0     | 65.0     | 44.6  | 64.5    | -      |\n| InstructBLIP-Flan-T5-XXL [50]    | 1212.8  | 291.8   | -       | -       | -/-         | 35.7     | 33.8     | -        | 47.9  | 70.6    | -      |\n| BLIP-2-Vicuna-13B [25]           | -       | -       | -       | -       | -/-         | -        | -        | -        | 41.0  | 61.0    | -      |\n| InstructBLIP-Vicuna-13B [50]     | -       | -       | -       | -       | -/-         | -        | -        | -        | 49.5  | 63.1    | 83.7   |\n| Shikra-13B [5]                   | -       | -       | -       | -       | -/-         | -        | -        | 77.4     | -     | -       | -      |\n| SVIT-v1.5-13B [17]               | 1565.8  | 323.2   | 69.1    | -       | 61.9/-      | -        | 33.3     | 80.3     | 64.1  | 70.0    | -      |\n| LVIS-INSTRUCT4V-13B [51]         | 1574.9  | 286.8   | -       | 68.0    | 61.6/-      | -        | -        | 80.7     | 63.6  | 70.6    | 86.0   |\n| ShareGPT4V-13B [52]              | 1618.7  | 303.2   | -       | 68.5    | -/70.8      | -        | -        | 81.0     | 64.8  | 71.2    | -      |\n| VILA1.5-13B [53]                 | 1569.6  | -       | -       | 74.9    | 65.1/72.6   | 37.9     | 33.6     | 82.8     | 64.3  | 80.1    | 86.3   |\n| Mini-Gemini-HD-13B [54]          | 1597.0  | 320.0   | -       | 68.6    | -/-         | 37.3     | 35.1     | -        | -     | -       | -      |\n| LLaVA-v1.5-13B (LoRA) [27]       | 1541.7  | 300.4 § | 68.4 §  | 68.5    | 61.3/-      | 40.0 §   | 33.2 §   | 80.0     | 63.3  | 71.2    | 86.7   |\n| LLaVA-NeXT-13B [28]              | 1575.0  | 326.0   | -       | 70.0    | -/71.9      | 36.2     | -        | 82.8     | 65.4  | 73.6    | 86.2   |\n| 4B < Size ≤ 8B                   |         |         |         |         |             |          |          |          |       |         |        |\n| InstructBLIP-Vicuna-7B [50]      | -       | -       | 33.9    | 36.0    | 53.4/-      | -        | -        | -        | 49.2  | 60.5    | -      |\n| MiniGPT-v2 [31]                  | -       | -       | -       | -       | -/-         | -        | -        | -        | 60.3  | -       | -      |\n| Shikra-7B [5]                    | -       | -       | 60.2    | 58.8    | -/-         | -        | -        | -        | -     | -       | -      |\n| mPLUG-Owl2 [55]                  | 1450.2  | 313.2   | 66.0    | 66.5    | 57.8/-      | 32.7     | 32.1     | 79.4     | 56.1  | 68.7    | 85.8   |\n| SPHINX-Intern2 [56]              | 1260.4  | 294.6   | -       | 57.9    | -/68.8      | -        | -        | 75.5     | 56.2  | 70.4    | 86.9   |\n| Yi-VL-6B [57]                    | -       | -       | 68.4    | 68.6    | -/67.6      | 39.1     | 37.8     | -        | -     | -       | -      |\n| DeepSeek-VL-7B [58]              | -       | -       | -       | 73.2    | -/70.4      | 36.6     | -        | -        | -     | -       | 88.1   |\n| LLaVA-v1.5-7B (LoRA) [27]        | 1476.9  | 267.9 § | 66.1 §  | 66.1    | 60.1/-      | 34.4 §   | 31.7 §   | 79.1     | 63.0  | 68.4    | 86.4   |\n| LLaVA-NeXT-7B [28]               | 1519.0  | 332.0   | -       | 67.4    | -/70.2      | 35.8     | -        | 81.8     | 64.2  | 70.1    | 86.5   |\n| MM1-7B-Chat [59]                 | 1529.3  | 328.9   | -       | 72.3    | 64.0/69.9   | 37.0     | 35.6     | 82.8     | -     | 72.6    | 86.6   |\n| Idefics2 [60]                    | -       | -       | 76.7    | -       | -/-         | 43.0     | 37.7     | 81.2     | -     | -       | -      |\n| LLaVA-NeXT-Llama-3-8B [29]       | 1603.7  | 367.8   | -       | 72.1    | -/-         | 41.7     | -        | -        | -     | -       | -      |\n| Bunny-8B                         | 1644.1  | 367.5   | 78.1    | 77.2    | 66.2 / 73.5 | 43.3     | 39.0     | 82.9     | 64.0  | 79.9    | 87.2   |\n| Size ≤ 4B                        |         |         |         |         |             |          |          |          |       |         |        |\n| MobileVLM-V2-3B [61]             | 1440.5  | -       | -       | 63.2    | -/-         | -        | -        | -        | 61.1  | 70.0    | 84.7   |\n| TinyLLaVA-Phi-2-SigLIP-3.1B [62] | 1466.4  | -       | -       | -       | -/-         | 38.4     | -        | 80.1     | 62.1  | 73.0    | 87.2   |\n| VILA1.5-3B [53]                  | 1442.4  | -       | -       | 63.4    | 60.9/67.9   | 33.3     | 30.8     | 80.4     | 61.5  | 69.0    | 85.9   |\n| Mipha-3B [63]                    | 1488.9  | 295.0   | -       | 69.7    | -/68.9      | -        | -        | 81.3     | 63.9  | 70.9    | 86.7   |\n| MiniCPM-V 2.0 [64]               | 1411.4  | 396.8   | 69.1    | 69.6    | -/67.1      | 38.2     | -        | -        | -     | 80.7    | 86.3   |\n| MM1-3B-Chat [59]                 | 1482.5  | 279.3   | -       | 67.8    | 63.0/68.8   | 33.9     | 33.7     | 82.0     | -     | 69.4    | 87.4   |\n| Imp-v1.5-4B-Phi3 [10]            | 1507.7  | -       | -       | 73.3    | -/-         | -        | -        | 81.5     | 63.5  | 78.3    | 86.9   |\n| Bunny-4B                         | 1581.5  | 361.1   | 75.7    | 74.2    | 64.9 / 72.5 | 41.4     | 38.4     | 82.1     | 63.2  | 78.3    | 87.2   |"
    },
    {
      "index": 1,
      "markdown": "| LLM Tuning        |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Fully Tuning LoRA |  1465.2 |   277.5 |    67.9 |    67.2 | 62.1(69.5)    |     36.8 |     33.1 |     79.1 |  61.7 |    72.8 |   85.9 |\n| Fully Tuning LoRA |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |"
    },
    {
      "index": 2,
      "markdown": "| Fine-tuning Data        |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K              |       1 |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.6)    |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| Bunny-695K              |       2 |  1480.6 |   272.5 |    71.2 |    69   | 63.0 ( 71.3 ) |     37.6 |     32.5 |     81   |  64   |    69.7 |   86.3 |\n| Bunny-695K + LLaVA-665K |       1 |  1501.3 |   272.5 |    72.4 |    71.1 | 63.0 (71.1)   |     37.8 |     33.4 |     81   |  63.6 |    69.5 |   87.2 |"
    },
    {
      "index": 3,
      "markdown": "| Fine-tuning Data                |   Epoch |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------------------|---------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K                      |       1 |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Bunny-695K + ALLaVA-Instruct-4V |       1 |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 ( 73.4 ) |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |"
    },
    {
      "index": 4,
      "markdown": "| Fine-tuning Data        | Vision Encoder   | MME P         | MME C       | MMB T     | MMB D     | SEED(-I)               | MMMU V    | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|---------------|-------------|-----------|-----------|------------------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen Trainable | 1488.8 1486.0 | 289.3 281.1 | 69.2 69.5 | 68.6 69.3 | 62.5 (70.6) 62.3(70.6) | 38.2 37.7 | 33.0 33.2 | 79.8 79.9 | 62.5 62.6 | 70.9 70.6 | 86.8 85.9 |\n| Bunny-695K + LLaVA-665K | Frozen Trainable | 1501.3 1521.5 | 272.5 291.8 | 72.4 71.9 | 71.1 71.3 | 63.0(71.1) 63.1 (71.1) | 37.8 38.0 | 33.4 33.1 | 81.0 81.1 | 63.6 63.7 | 69.5 70.8 | 87.2 87.0 |"
    },
    {
      "index": 5,
      "markdown": "| LLM        | Fine-tuning Data                  |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I | POPE      |\n|------------|-----------------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|-----------|\n| Phi-3-Mini | B-695K + L-665K                   |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   | 87.0      |\n| Phi-3-Mini | B-695K + L-665K + A-Ins           |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4 ( 71.9 ) |     42.4 |     38.1 |     82   |  62.9 |    77.8 | 86.9      |\n| Llama-3-8B | B-695K + L-665K B-695K + L-665K + |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 | 86.8 87.3 |\n| Llama-3-8B | A-Ins                             |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5 ( 73.1 ) |     40.4 |     38.8 |     82.8 |  64.1 |    79   |           |"
    },
    {
      "index": 6,
      "markdown": "| Pre-training LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 5 × 10 - 4        |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5 ( 70.6 ) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| 8 × 10 - 4        |  1494.3 |   277.5 |    69.6 |    69   | 62.1(70.5)    |     36.4 |     33.5 |     80   |  62.3 |    72.9 |   86.9 |\n| 1 × 10 - 3        |  1448.3 |   261.1 |    70.5 |    68.8 | 62.4(70.4)    |     36.8 |     33.3 |     80   |  62.3 |    71.1 |   86.4 |"
    },
    {
      "index": 7,
      "markdown": "| LoRA LR    | Vision &Projector LR   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|------------------------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| 2 × 10 - 4 | 2 × 10 - 5             |  1521.5 |   291.8 |    71.9 |    71.3 | 63.1 ( 71.1 ) |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| 1 × 10 - 4 | 1 × 10 - 5             |  1492.8 |   283.9 |    71.2 |    69.7 | 63.1 (70.9)   |     38.4 |     33.3 |     80.4 |  62.8 |    71.5 |   86.8 |\n| 4 × 10 - 4 | 4 × 10 - 5             |   987.7 |   240.4 |     0   |     0   | 25.9(26.3)    |     27.9 |     25.3 |     69   |  53.7 |    36.7 |   83.3 |"
    },
    {
      "index": 8,
      "markdown": "| Fine-tuning Data        | Vision Encoder Res   | MME P      |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|-------------------------|----------------------|------------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n| Bunny-695K + LLaVA-665K | Trainable            | 384 1521.5 |   291.8 |    71.9 |    71.3 | 63.1(71.1)    |     38   |     33.1 |     81.1 |  63.7 |    70.8 |   87   |\n| Bunny-695K + LLaVA-665K | Trainable            | 448 1489.2 |   309.6 |    71.6 |    71.5 | 63.3 ( 71.4 ) |     37.2 |     32.5 |     81.4 |  63.2 |    71   |   87.2 |"
    },
    {
      "index": 9,
      "markdown": "| Fine-tuning Data        | Vision Encoder   | Res MME P       | MME C       | MMB T     | MMB D     | SEED(-I)                 |   MMMU V | MMMU T    | VQA v2    | GQA       | SQA I     | POPE      |\n|-------------------------|------------------|-----------------|-------------|-----------|-----------|--------------------------|----------|-----------|-----------|-----------|-----------|-----------|\n| Bunny-695K              | Frozen           | 384 1402.0      | 286.1       | 73.1      | 72.3      | 64.6 ( 71.9 )            |     40.7 | 39.3      | 80.6      | 62.4      | 75.3      | 85.8 86.6 |\n|                         |                  | 1152 1473.1     | 332.1       | 72.8      | 72.0      | 64.1(71.3)               |     41.8 | 38.3      | 80.6      | 62.4 63.0 | 75.6 74.4 | 86.4      |\n| Bunny-695K + LLaVA-665K | Trainable        | 384 1490.5 1152 | 316.4 336.4 | 72.4 74.4 | 72.0 74.3 | 63.7(71.3) 64.2 ( 71.4 ) |     40   | 38.3 38.6 | 81.3 81.6 |           |           |           |\n| Bunny-695K + LLaVA-665K |                  | 1503.5          |             |           |           |                          |     38.8 |           |           | 63.1      | 75.0      | 87.0      |"
    },
    {
      "index": 10,
      "markdown": "| LLM        | S 2   | Fine-tuning Data             | VE   |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)      |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|------------|-------|------------------------------|------|---------|---------|---------|---------|---------------|----------|----------|----------|-------|---------|--------|\n|            |       | B-695K                       | F    |  1402   |   286.1 |    73.1 |    72.3 | 64.6 (71.9)   |     40.7 |     39.3 |     80.6 |  62.4 |    75.3 |   85.8 |\n| Phi-3-Mini | w \\   | o B-695K + L-665K            | T    |  1490.5 |   316.4 |    72.4 |    72   | 63.7(71.3)    |     40   |     38.3 |     81.3 |  63   |    74.4 |   86.4 |\n|            |       | 3:7 Averaging                |      |  1495.2 |   338.9 |    74   |    73.5 | 64.5( 72.1 )  |     40.1 |     39.1 |     81.5 |  63.5 |    75.2 |   86.7 |\n|            |       | B-695K                       | F    |  1473.1 |   332.1 |    72.8 |    72   | 64.1(71.3)    |     41.8 |     38.3 |     80.6 |  62.4 |    75.6 |   86.6 |\n| Phi-3-Mini | w \\   | B-695K + L-665K              | T    |  1503.5 |   336.4 |    74.4 |    74.3 | 64.2(71.4)    |     38.8 |     38.6 |     81.6 |  63.1 |    75   |   87   |\n|            |       | 3:7 Averaging                |      |  1503.9 |   362.9 |    74.1 |    74.1 | 64.6 ( 71.7 ) |     40.2 |     38.8 |     81.7 |  63.4 |    76.3 |   87   |\n|            |       | B-695K + A-Ins               | F    |  1552.4 |   306.8 |    74   |    73.8 | 62.8(71.0)    |     40.6 |     38.1 |     81   |  62.3 |    75   |   86.7 |\n| Phi-3-Mini | w \\   | B-695K + L-665K + A-Ins      | T    |  1590.1 |   343.9 |    75.6 |    74.4 | 64.4(71.9)    |     42.4 |     38.1 |     82   |  62.9 |    77.8 |   86.9 |\n|            |       | 3:7 Averaging ( Bunny-4B     | )    |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9 ( 72.5 ) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n|            |       | B-695K                       | F    |  1542.7 |   344.6 |    75.7 |    74.8 | 65.2(73.1)    |     42.4 |     37.1 |     82   |  64.4 |    78.7 |   85.9 |\n| Llama-3-8B | w \\ o | B-695K + L-665K              | T    |  1562.2 |   319.6 |    75.3 |    74.4 | 65.2(72.5)    |     43.4 |     38   |     82.4 |  64.1 |    77.4 |   86.4 |\n|            |       | 5:5 Averaging                |      |  1588.9 |   321.1 |    77.2 |    76.7 | 65.9 ( 73.3 ) |     42.8 |     39   |     82.6 |  64.8 |    80.4 |   86.9 |\n|            |       | B-695K                       | F    |  1614.7 |   310   |    75.6 |    75.4 | 65.2(72.6)    |     42.3 |     38.2 |     81.5 |  63.4 |    80.4 |   87.3 |\n| Llama-3-8B | w \\   | B-695K + L-665K              | T    |  1571.2 |   314.3 |    75.9 |    75.5 | 64.8(72.6)    |     42.2 |     37.6 |     82.4 |  64.8 |    79.2 |   86.8 |\n|            |       | 6:4 Averaging                |      |  1607.8 |   324.6 |    76.5 |    76.3 | 65.9 ( 73.6 ) |     43.7 |     38.8 |     82.3 |  64.5 |    80.4 |   87.4 |\n|            |       | B-695K + A-Ins               | F    |  1599.6 |   348.9 |    76.3 |    75.6 | 66.2 (73.4)   |     41.6 |     38.3 |     81.9 |  64.1 |    79.5 |   86.4 |\n| Llama-3-8B | w \\   | B-695K + L-665K + A-Ins      | T    |  1649.7 |   341.1 |    77.2 |    76.5 | 65.5(73.1)    |     40.4 |     38.8 |     82.8 |  64.1 |    79   |   87.3 |\n|            |       | 4.2:5.8 Averaging ( Bunny-8B | )    |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2 ( 73.5 ) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |"
    },
    {
      "index": 11,
      "markdown": "| Vision Encoder      | LLM               |   MME P |   MME C |   MMB T |   MMB D | SEED(-I)   |   MMMU V |   MMMU T |   VQA v2 |   GQA |   SQA I |   POPE |\n|---------------------|-------------------|---------|---------|---------|---------|------------|----------|----------|----------|-------|---------|--------|\n| EVA02-CLIP-L (0.4B) | Phi-1.5 (1.3B)    |  1213.7 |   278.9 |    60.9 |    56.8 | 56.4(64.1) |     30   |     28.4 |     76.5 |  60.4 |    58.2 |   86.1 |\n| EVA02-CLIP-L (0.4B) | StableLM-2 (1.6B) |  1301   |   235   |    58.4 |    56.4 | 55.3(62.8) |     29.8 |     29.4 |     74.6 |  56.7 |    60   |   84.8 |\n| EVA02-CLIP-L (0.4B) | Phi-2 (2.7B)      |  1421   |   285.4 |    68.6 |    67.4 | 62.2(70.2) |     35.9 |     32.6 |     78.9 |  62.3 |    69.1 |   87.1 |\n| SigLIP-SO (0.4B)    | Phi-1.5 (1.3B)    |  1230   |   237.5 |    61.2 |    59.7 | 57.7(65.3) |     30   |     29.1 |     78   |  61.1 |    61.3 |   85.8 |\n| SigLIP-SO (0.4B)    | StableLM-2 (1.6B) |  1366.8 |   236.1 |    65.1 |    62.8 | 58.8(67.5) |     29.9 |     29.8 |     78.9 |  60.9 |    61.1 |   85.9 |\n| SigLIP-SO (0.4B)    | Phi-2 (2.7B)      |  1488.8 |   289.3 |    69.2 |    68.6 | 62.5(70.7) |     38.2 |     33   |     79.8 |  62.5 |    70.9 |   86.8 |\n| SigLIP-SO (0.4B)    | Phi-3-Mini (3.8B) |  1581.5 |   361.1 |    75.7 |    74.2 | 64.9(72.5) |     41.4 |     38.4 |     82.1 |  63.2 |    78.3 |   87.2 |\n| SigLIP-SO (0.4B)    | Llama-3-8B        |  1644.1 |   367.5 |    78.1 |    77.2 | 66.2(73.5) |     43.3 |     39   |     82.9 |  64   |    79.9 |   87.2 |"
    }
  ],
  "stats": {
    "pages": 15,
    "chunksCreated": 97,
    "totalCharacters": 62183,
    "totalWords": 10131,
    "numTables": 12,
    "processingTimeMs": 50005
  }
}