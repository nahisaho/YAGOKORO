{
  "paper": {
    "id": "2310.10631v3",
    "title": "Llemma: An Open Language Model For Mathematics",
    "abstract": "We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.",
    "authors": [
      "Zhangir Azerbayev",
      "Hailey Schoelkopf",
      "Keiran Paster",
      "Marco Dos Santos",
      "Stephen McAleer",
      "Albert Q. Jiang",
      "Jia Deng",
      "Stella Biderman",
      "Sean Welleck"
    ],
    "published": "2023-10-16T17:54:07.000Z",
    "updated": "2024-03-15T19:14:39.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.10631v3",
    "absUrl": "https://arxiv.org/abs/2310.10631v3"
  },
  "chunks": [
    {
      "id": "2310.10631v3-chunk-0",
      "content": "Zhangir Azerbayev 1 , 2 Hailey Schoelkopf 2 Keiran Paster 3 , 4 Marco Dos Santos 5 Stephen McAleer 6 Albert Q. Jiang 5 Jia Deng 1 Stella Biderman 2 Sean Welleck 6 , 7\n\n1 Princeton University 2 EleutherAI 3 University of Toronto 4 Vector Institute 5 University of Cambridge 6 Carnegie Mellon University 7 University of Washington",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS",
        "chunkIndex": 0,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-1",
      "content": "We present LLEMMA, a large language model for mathematics. We continue pretraining Code Llama on Proof -Pile -2 , a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding LLEMMA. On the MATH benchmark LLEMMA outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof -Pile -2 , and code to replicate our experiments. 1",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-2",
      "content": "Language models trained on diverse mixtures of text display remarkably general language understanding and generation capabilities (Brown et al., 2020; Chowdhery et al., 2022), serving as base models that are adapted to a wide range of applications (Raffel et al., 2023). Applications such as open-ended dialogue (Thoppilan et al., 2022; Touvron et al., 2023) or instruction following (Ouyang et al., 2022; Wei et al., 2022) require balanced performance across the entire distribution of natural text, thus favoring generalist models . However, if we seek to maximize performance within one domain, such as medicine (Singhal et al., 2022; 2023), finance (Wu et al., 2023), or science (Taylor et al., 2022), a domain-specific language model may offer superior capabilities for a given computational cost, or lower computational cost for a given level of capability.\n\nIn this work, we train a domain-specific language model for mathematics. We have several motivations",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 2,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-3",
      "content": "ities for a given computational cost, or lower computational cost for a given level of capability.\n\nIn this work, we train a domain-specific language model for mathematics. We have several motivations\n\nFigure 1: Continued pretraining on Proof -Pile -2 yields LLEMMA, a base model with improved mathematical capabilities.\n\n<!-- image -->\n\nfor doing so. First, solving mathematical problems requires pattern matching against a large body of specialized prior knowledge, thus serving as an ideal setting for domain adaptation. Second, mathematical reasoning is in itself a central AI task, its study dating back to at least Gelernter (1959) and Wang (1960) and continuing to today (Lu et al., 2023). Third, language models capable of strong mathematical reasoning are upstream of a number of research topics, such as reward modeling (Uesato et al., 2022; Lightman et al., 2023), reinforcement learning for reasoning (Polu et al., 2022; Lample et al., 2022), and algorithmic reasoning (Zhou et al., 2022;",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-4",
      "content": "pics, such as reward modeling (Uesato et al., 2022; Lightman et al., 2023), reinforcement learning for reasoning (Polu et al., 2022; Lample et al., 2022), and algorithmic reasoning (Zhou et al., 2022; Zhang et al., 2023).\n\n1 https://github.com/EleutherAI/math-lm\n\nAlthough domain-specific models for mathematics have been trained in the past, they have either been closed access (Lewkowycz et al., 2022), limiting their ability to become a platform for further research, or have lagged far behind the closed access state-of-the-art (Azerbayev et al., 2023).\n\nWe present a recipe for adapting a language model to mathematics through continued pretraining (Lewkowycz et al., 2022; Rozière et al., 2023) on Proof -Pile -2 , a diverse mixture of math-related text and code. Applying the recipe to Code Llama (Rozière et al., 2023) yields LLEMMA: 7 billion and 34 billion parameter base language models with substantially improved mathematical capabilities.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-5",
      "content": "ed text and code. Applying the recipe to Code Llama (Rozière et al., 2023) yields LLEMMA: 7 billion and 34 billion parameter base language models with substantially improved mathematical capabilities.\n\nSpecifically, our contributions are as follows:\n\n1. We train and release the LLEMMA models: 7B and 34B parameter language models specialized for mathematics. The LLEMMA models are a new state-of-the-art for publicly released base models on MATH (Lewkowycz et al., 2022).\n2. We release the AlgebraicStack , a dataset of 11B tokens of code specifically related to mathematics.\n3. We demonstrate that LLEMMA is capable of using computational tools to solve mathematical problems, namely, the Python interpreter and formal theorem provers.\n4. Unlike prior mathematics language models such as Minerva (Lewkowycz et al., 2022), the LLEMMA models are open access and we open source our training data and code. This allows LLEMMA to serve as a platform for future research in mathematical reasoning.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-6",
      "content": "erva (Lewkowycz et al., 2022), the LLEMMA models are open access and we open source our training data and code. This allows LLEMMA to serve as a platform for future research in mathematical reasoning.\n\nOur work builds on findings in Minerva (Lewkowycz et al., 2022), but differs in several ways: (1) LLEMMA's training and evaluation covers a wider range of data and tasks, notably code data (e.g., the AlgebraicStack ), tool use, and formal mathematics; (2) our work only depends on publicly accessible tools and data; (3) we provide new analyses related to the continued training data mixture, memorization, and additional supervised finetuning; (4) we make all artifacts publicly available.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-7",
      "content": "LLEMMA models are 7 billion and 34 billion parameter language models specialized for mathematics. Our approach is to continue pretraining Code Llama (Rozière et al., 2023) on the Proof -Pile -2 .\n\nFigure 2: Comparison of LLEMMA and Minerva training\n\n|                   |                        |                        | Dataset                            | Tokens   | Open   |\n|-------------------|------------------------|------------------------|------------------------------------|----------|--------|\n| Model             | Adaptation tokens Open | Adaptation tokens Open | Minerva Dataset                    | 38.5B    | ✗      |\n| Minerva-8b        | 164B                   | ✗                      | Proof - Pile - 2 (ours)            | 55B      | ✓      |\n| Minerva-62b       | 109B                   | ✗                      | Code ( AlgebraicStack )            | 11B      | ✓      |\n| LLEMMA-7b (ours)  | 200B                   | ✓                      | OpenWebMath (Paster et al., 2023)",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2 APPROACH",
        "chunkIndex": 7,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-8",
      "content": "| ✗                      | Code ( AlgebraicStack )            | 11B      | ✓      |\n| LLEMMA-7b (ours)  | 200B                   | ✓                      | OpenWebMath (Paster et al., 2023)) | 15B      | ✓      |\n| LLEMMA-34b (ours) | 50B                    | ✓                      | ArXiv (Computer, 2023))            | 29B      | ✓      |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2 APPROACH",
        "chunkIndex": 8,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-9",
      "content": "We form the Proof -Pile -2 , a 55B-token mixture of scientific papers, web data containing mathematics, and mathematical code. With the exception of the Lean proofsteps subset (see Appendix B), the Proof -Pile -2 has a knowledge cutoff of April 2023.\n\nCode. Computational tools such as numerical simulations, computer algebra systems, and formal theorem provers are of ever increasing importance to mathematicians (Avigad, 2018). Motivated by this fact, we create AlgebraicStack , an 11B-token dataset of source code from 17 languages, spanning numerical, symbolic, and formal math. The dataset consists of filtered code from the Stack (Kocetkov et al., 2022), public GitHub repositories, and formal proofstep data. Table 9 shows the number of tokens by language in AlgebraicStack . See Appendix B.1 for further details on AlgebraicStack .",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.1 DATA: Proof -Pile -2",
        "chunkIndex": 9,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-10",
      "content": "cetkov et al., 2022), public GitHub repositories, and formal proofstep data. Table 9 shows the number of tokens by language in AlgebraicStack . See Appendix B.1 for further details on AlgebraicStack .\n\nWeb data. We use OpenWebMath (Paster et al., 2023), a 15B-token dataset of high-quality web pages filtered for mathematical content. OpenWebMath filters CommonCrawl web pages based\n\non math-related keywords and a classifier-based math score, preserves mathematical formatting (e.g., L A T E X, AsciiMath), and includes additional quality filters (e.g., perplexity, domain, length) and near-deduplication. Refer to Paster et al. (2023) for a full description of OpenWebMath.\n\nScientific papers. We use the ArXiv subset of RedPajama (Computer, 2023), an open-access reproduction of the LLaMA training dataset. The ArXiv subset contains 29B tokens.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.1 DATA: Proof -Pile -2",
        "chunkIndex": 10,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-11",
      "content": "description of OpenWebMath.\n\nScientific papers. We use the ArXiv subset of RedPajama (Computer, 2023), an open-access reproduction of the LLaMA training dataset. The ArXiv subset contains 29B tokens.\n\nGeneral natural language and code data. Following Lewkowycz et al. (2022), our training mixture consists of a small amount of general domain data, which functions as a form of regularization. Since the pretraining dataset for LLaMA 2 is undisclosed, we use the Pile (Gao et al., 2020; Biderman et al., 2022) as a surrogate training dataset. We set 95% of our training mixture to be the Proof -Pile -2 , 2% to be from the Pile (with ArXiv removed, as it is separately in Proof -Pile -2 ), and 3% to be the GitHub subset of RedPajama (Computer, 2023).\n\nFurther information on dataset composition and a datasheet are in Appendix B and Appendix E, respectively. We publicly release Proof -Pile -2 at hf.co/datasets/EleutherAI/proof-pile-2 .",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.1 DATA: Proof -Pile -2",
        "chunkIndex": 11,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-12",
      "content": "a datasheet are in Appendix B and Appendix E, respectively. We publicly release Proof -Pile -2 at hf.co/datasets/EleutherAI/proof-pile-2 .",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.1 DATA: Proof -Pile -2",
        "chunkIndex": 12,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-13",
      "content": "Each model is initialized from Code Llama (Rozière et al., 2023). Code Llama models are decoderonly transformer language models initialized from Llama 2 (Touvron et al., 2023) and further trained on 500B tokens of code. We continue training the Code Llama models on Proof -Pile -2 using a standard autoregressive language modeling objective. We train the 7B model for 200B tokens, and the 34B model for 50B tokens.\n\nWe train all models in bfloat16 mixed precision using the GPT-NeoX library (Andonian et al., 2023) across 256 A100 40GB GPUs. We use Tensor Parallelism (Shoeybi et al., 2019) with a world size of 2 for LLEMMA-7B , and a world size of 8 for LLEMMA-34B, alongside ZeRO Stage 1 sharded optimizer states (Rajbhandari et al., 2020) across Data Parallel (Goyal et al., 2017) replicas. We use Flash Attention 2 (Dao, 2023) to improve throughput and further reduce memory requirements.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.2 MODEL AND TRAINING",
        "chunkIndex": 13,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-14",
      "content": "arded optimizer states (Rajbhandari et al., 2020) across Data Parallel (Goyal et al., 2017) replicas. We use Flash Attention 2 (Dao, 2023) to improve throughput and further reduce memory requirements.\n\nLLEMMA 7B is trained for 42 , 000 steps with a global batch size of 4 million tokens and a 4096 token context length. This corresponds to roughly 23 , 000 A100-hours. The learning rate is warmed up to 1 · 10 -4 over 500 steps, then set to cosine decay to 1 / 30 th of the maximum learning rate over 48 , 000 steps. The reason for the discrepancy between the number of training steps and the scheduler length is that we planned to train for 48 , 000 steps, but encountered NaN losses after step 42 , 000 , likely caused by unstable optimization or hardware failures (Elsen et al., 2023).\n\nLLEMMA 34B is trained for 12 , 000 steps with a global batch size of 4 million tokens and a 4096 context length. This corresponds to roughly 47 , 000 A100-hours.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.2 MODEL AND TRAINING",
        "chunkIndex": 14,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-15",
      "content": "rdware failures (Elsen et al., 2023).\n\nLLEMMA 34B is trained for 12 , 000 steps with a global batch size of 4 million tokens and a 4096 context length. This corresponds to roughly 47 , 000 A100-hours. The learning rate is warmed up to 5 · 10 -5 over 500 steps, then decayed to 1 / 30 th the peak learning rate.\n\nBefore training LLEMMA 7B, we contract the RoPE (Su et al., 2022) base period of the Code Llama 7B initialization from θ = 1 , 000 , 000 to θ = 10 , 000 . This is so that the long context finetuning procedure described in Peng et al. (2023)and Rozière et al. (2023) can be repeated on the trained LLEMMA 7B (we leave actually doing so to future work). Due to compute constraints, we were unable to verify that training LLEMMA 34B with a contracted RoPE base period did not come with a performance penalty, therefore for that model we preserved θ = 1 , 000 , 000 .",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "2.2 MODEL AND TRAINING",
        "chunkIndex": 15,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-16",
      "content": "Our goal is to evaluate LLEMMA as a base model for mathematical text. To this end, we compare LLEMMA models using few-shot evaluation (Brown et al., 2020), and primarily focus on state-of-theart models that have not been finetuned on supervised examples for the task. First, we evaluate the model's ability to solve mathematics problems using chain of thought reasoning (Wei et al., 2023) and majority voting (Wang et al., 2023). Our evaluations include MATH (Hendrycks et al., 2021b) and GSM8k (Cobbe et al., 2021), the de-facto standard benchmarks for evaluating quantitative reasoning in language models (Lewkowycz et al., 2022). Second, we explore few-shot tool use and formal theorem proving. Third, we study the effects of memorization and the data mixture. Appendix G contains a preliminary study of supervised finetuning with LLEMMA.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3 EVALUATION",
        "chunkIndex": 16,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-17",
      "content": "These tasks involve generating self-contained text solutions to problems expressed in L A T E X or natural language, without using external tools (Lewkowycz et al., 2022). We use the following evaluation:\n\n- MATH (Hendrycks et al., 2021b), a dataset with 12.5k problems (5k evaluation) from high-school math competitions. Given a problem statement, the model generates a L A T E Xsolution and an answer that must match a reference answer. We follow a similar task implementation to Lewkowycz et al. (2022), using their four-example prompt and evaluating answers for exact string match or SymPy equivalence.\n- GSM8k (Cobbe et al., 2021), a dataset of middle-school level math word problems. We use the 8-shot prompt from Wei et al. (2023), as Lewkowycz et al. (2022) do not specify their evaluation prompt or number of few-shot examples.\n- OCWCourses (Lewkowycz et al., 2022), a collection of undergraduate-level STEM problems harvested from MIT's OpenCourseWare.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 17,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-18",
      "content": "22) do not specify their evaluation prompt or number of few-shot examples.\n- OCWCourses (Lewkowycz et al., 2022), a collection of undergraduate-level STEM problems harvested from MIT's OpenCourseWare. We use the four-example prompt provided by (Lewkowycz et al., 2022).\n- MMLU-STEM (Hendrycks et al., 2021a), a subset of 18 out of 57 subjects in the MMLU benchmark. We follow Lewkowycz et al. (2022) and use their provided four-example chain-ofthought prompt.\n- SAT , we create a dataset consisting of the 32 math questions that do not contain figures from the May 2023 College Board SAT examination, which is after our model's knowledge cutoff.\n\nFigure 3: Example of a LLEMMA 34B solution to a MATH (Hendrycks et al., 2021a) problem. This problem is tagged with difficulty level 5, the highest in MATH. The model was conditioned on the 4-shot prompt described in subsection 3.1, and the solution was produced by greedy decoding.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 18,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-19",
      "content": "blem. This problem is tagged with difficulty level 5, the highest in MATH. The model was conditioned on the 4-shot prompt described in subsection 3.1, and the solution was produced by greedy decoding. The model had to apply two nontrivial steps to solve this problem: (1) noticing that swapping the order of summation simplifies the problem, and (2) noticing that the resulting sum telescopes.\n\n<!-- image -->\n\nWe compare with Minerva (Lewkowycz et al., 2022), which continued pretraining the PaLM language model on a dataset of technical content; Code Llama, the initialization of LLEMMA's continued pretraining; and Llama 2, the initialization of Code Llama's continued pretraining on code. For open access models, we report scores computed using our evaluation suite, which is implemented as a fork of the Language Model Evaluation Harness (Gao et al., 2021). For Minerva models, we report benchmark scores from Lewkowycz et al. (2022).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 19,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-20",
      "content": "ted using our evaluation suite, which is implemented as a fork of the Language Model Evaluation Harness (Gao et al., 2021). For Minerva models, we report benchmark scores from Lewkowycz et al. (2022).\n\nResults. LLEMMA's continued pretraining on Proof -Pile -2 improves few-shot performance on the five mathematical benchmarks. LLEMMA 34B improves over Code Llama by 20 percentage points on GSM8k and 13 points on MATH, and LLEMMA 7B outperforms the proprietary Minerva model. Our approach also outperforms all open-weight language models at the time of writing. We conclude that continued pretraining on Proof -Pile -2 is effective for improving a pretrained model's ability to perform mathematical problem solving.\n\nLLEMMA is pretrained on a diverse distribution of mathematics-related data, and is not tuned for a particular task. Therefore, we expect that LLEMMA can adapt to many other tasks via task-specific finetuning and few-shot prompting.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 20,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-21",
      "content": "se distribution of mathematics-related data, and is not tuned for a particular task. Therefore, we expect that LLEMMA can adapt to many other tasks via task-specific finetuning and few-shot prompting.\n\nTable 1: Results on our five chain-of-thought reasoning tasks with samples generated via greedy decoding. Minerva results are quoted from Lewkowycz et al. (2022). Note that CodeLlama 7B performs worse than random guessing (25%) on MMLU and SAT, largely due to failing to conclude its chain of thought with a valid answer.\n\n|            |      | GSM8k   | OCW   | MMLU-STEM   | SAT    | MATH   |\n|------------|------|---------|-------|-------------|--------|--------|\n| Llama 2    | 7B   | 11.8%   | 3.7%  | 29.9%       | 25.0%  | 3.2%   |\n| Code Llama | 7B   | 10.5%   | 4.4%  | 25.1%       | 9.4%   | 4.5%   |\n| Minerva    | 8B   | 16.2%   | 7.7%  | 35.6%       | -      | 14.1%  |\n| LLEMMA     | 7B   | 36.4%   | 7.7%  | 37.7%       | 53.1 % | 18.0 % |\n| Code Llama | 34B  | 29.6%   | 7.0%  | 40.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 21,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-22",
      "content": "| 4.5%   |\n| Minerva    | 8B   | 16.2%   | 7.7%  | 35.6%       | -      | 14.1%  |\n| LLEMMA     | 7B   | 36.4%   | 7.7%  | 37.7%       | 53.1 % | 18.0 % |\n| Code Llama | 34B  | 29.6%   | 7.0%  | 40.5%       | 40.6%  | 12.2%  |\n| LLEMMA     | 34B  | 51.5%   | 11.8% | 49.0%       | 71.9 % | 25.0%  |\n| Minerva    | 62B  | 52.4%   | 12.0% | 53.9%       | -      | 27.6%  |\n| Minerva    | 540B | 58.8%   | 17.6% | 63.9%       | -      | 33.6%  |\n\nTable 2: Majority voting results for LLEMMA and Minerva. Minerva results are quoted from Lewkowycz et al. (2022). Voting is done with k = 256 for MATH, k = 100 for GSM8k and OCW, and k = 16 for MMLU-STEM and SAT. We sample with temperature T = 0 . 6 for k = 256 and k = 100 and T = 0 . 3 for k = 16 , and use nucleus sampling with p = 0 . 95 (Holtzman et al., 2020). Due to compute constraints, we do not calculate majority voting scores for Llama 2 and Code Llama.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 22,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-23",
      "content": "k = 100 and T = 0 . 3 for k = 16 , and use nucleus sampling with p = 0 . 95 (Holtzman et al., 2020). Due to compute constraints, we do not calculate majority voting scores for Llama 2 and Code Llama.\n\n|         |      | GSM8k maj@ k   | OCW maj@ k   | MMLU-STEM maj@ k   | SAT maj@ k   | MATH maj@ k   |\n|---------|------|----------------|--------------|--------------------|--------------|---------------|\n| Minerva | 8B   | 28.4%          | 12.5%        | 43.4%              | -            | 25.4%         |\n| LLEMMA  | 7B   | 54.0%          | 14.3%        | 49.9%              | 78.1 %       | 33.5%         |\n| LLEMMA  | 34B  | 69.3 %         | 18.4 %       | 59.7 %             | 81.3 %       | 43.1 %        |\n| Minerva | 62B  | 68.5%          | 23.5%        | 63.5%              | -            | 43.4%         |\n| Minerva | 540B | 78.5%          | 30.8%        | 75.0%              | -            | 50.3%         |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 23,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-24",
      "content": "| 43.4%         |\n| Minerva | 540B | 78.5%          | 30.8%        | 75.0%              | -            | 50.3%         |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING",
        "chunkIndex": 24,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-25",
      "content": "These tasks involve solving problems with access to computational tools. We evaluate the following:\n\n- MATH+Python , the model is prompted to alternately describe a solution step in natural language, then execute that step with code. The final answer is a program that executes to a numeric type or a SymPy object. Our few-shot prompt includes examples that use built-in numeric operations, the math module, and SymPy .\n- GSM8k+Python , solving a GSM8k word problem by writing a Python program that executes to an integer answer. We use the prompt from Gao et al. (2023).\n\nResults. As seen in Table 3, LLEMMA improves over Code Llama on both tasks. Its performance on MATH and GSM8k with tools is also higher than its performance on these datasets without tools.\n\nTable 3: Mathematical problem solving with tool use.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.2 MATHEMATICAL PROBLEM SOLVING WITH TOOL USE",
        "chunkIndex": 25,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-26",
      "content": "s over Code Llama on both tasks. Its performance on MATH and GSM8k with tools is also higher than its performance on these datasets without tools.\n\nTable 3: Mathematical problem solving with tool use.\n\n|            |     | GSM8k+Python pass@1   | MATH+Python pass@1   |\n|------------|-----|-----------------------|----------------------|\n| Code Llama | 7B  | 27.1%                 | 17.2%                |\n| LLEMMA     | 7B  | 40.1%                 | 21.5%                |\n| Code Llama | 34B | 52.7%                 | 23.5%                |\n| LLEMMA     | 34B | 62.6%                 | 27.1%                |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.2 MATHEMATICAL PROBLEM SOLVING WITH TOOL USE",
        "chunkIndex": 26,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-27",
      "content": "Interactive proof assistants such as Lean (de Moura et al., 2015), Isabelle (Wenzel et al., 2008), and Coq (Paulin-Mohring, 1989a;b) express mathematics in programming languages that allow for verification. These languages are data scarce compared to mainstream languages, especially in the context of pretraining. For instance, the Stack dataset used to pretrain language models in the BigCode project (Allal et al., 2023) has over 700 gigabytes of Python, compared to 322 megabytes of Lean. Proof assistants also require models to leverage information that is not present in raw source code, such as goal states that contain information about each step of a proof.\n\nProblem (MATH Number theory 185): When a number is divided by 5, the remainder is 3. What is the remainder when twice the number is divided by 5? Show that it is 1.\n\nHuman-written informal proof: If our number is n , then n ≡ 3 (mod 5)\n\n<!-- formula-not-decoded -->\n\nThe remainder is 1 when the number is divided by 5.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 27,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-28",
      "content": "the number is divided by 5? Show that it is 1.\n\nHuman-written informal proof: If our number is n , then n ≡ 3 (mod 5)\n\n<!-- formula-not-decoded -->\n\nThe remainder is 1 when the number is divided by 5.\n\n```\nInformal-to-formal (Isabelle): {Problem, human-written informal proof} theorem mathd_numbertheory_185: fixes n ::nat assumes \"n mod 5 = 3\" shows \"(2 * n) mod 5 = 1\" proof -have \"2 * n = n + n\" < ATP > also have \" . . . mod 5 = (n mod 5 + n mod 5) mod 5\" < ATP > also have \" . . . = (3 + 3) mod 5\" using assms < ATP > also have \" . . . = 1\" < ATP > finally show ?thesis < ATP > qed\n```\n\nFigure 4: Example formal proofs from LLEMMA-7b. Left: The model is given a problem, informal proof, and formal statement, following Jiang et al. (2023). It generates a formal proof (starting with proof -) containing Isabelle code and calls to automation (shown as &lt;ATP&gt; ). Right: The model is given a proof state, visualized as a grey comment, and generates the subsequent step (e.g. rw [.. ).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 28,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-29",
      "content": "of -) containing Isabelle code and calls to automation (shown as &lt;ATP&gt; ). Right: The model is given a proof state, visualized as a grey comment, and generates the subsequent step (e.g. rw [.. ).\n\n```\nFormal-to-formal (Lean 4): theorem mathd_numbertheory_185 (n : N ) (h 0 : n % 5 = 3) : 2 * n % 5 = 1 := by --INPUT (step 1): --n: N --h 0 : n % 5 = 3 --⊢ 2 * n % 5 = 1 rw [mul_mod, h 0 ] --INPUT (step 2): --n: N --h 0 : n % 5 = 3 --⊢ 2 % 5 * 3 % 5 = 1 simp only [h 0 , mul_one]\n```\n\nProof -Pile -2 's AlgebraicStack contains over 1.5 billion tokens of formal mathematics data, including proof states extracted from Lean and Isabelle formalizations. While a full investigation of formal math is outside the scope of this paper, we evaluate LLEMMA few-shot on two tasks:\n\n. This tells us that .\n\n- Informal-to-formal proving (Jiang et al., 2023), the task of generating a formal proof, given a formal statement, an informal L A T E X statement, and an informal L A T E X proof.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 29,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-30",
      "content": "s tells us that .\n\n- Informal-to-formal proving (Jiang et al., 2023), the task of generating a formal proof, given a formal statement, an informal L A T E X statement, and an informal L A T E X proof. The formal proof is checked by the proof assistant. We use the Isabelle proof assistant and evaluate on miniF2F (Zheng et al., 2021), a benchmark consisting of problem statements from Olympiads and undergraduate coursework. For the prompt, we use 11 (formal statement, informal statement, informal proof, formal proof) examples from Jiang et al. (2023), selecting 7 examples for number theory problems, and 6 examples for all others. We generate a single proof with greedy decoding.\n- Formal-to-formal proving (e.g., Polu &amp; Sutskever (2020)), the task of proving a formal statement by generating a sequence of proof steps (tactics). At each step, the input is a state x t given by the proof assistant, and the language model's task is to generate a proof step y t (a sequence of code).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 30,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-31",
      "content": "generating a sequence of proof steps (tactics). At each step, the input is a state x t given by the proof assistant, and the language model's task is to generate a proof step y t (a sequence of code). The proof step is checked by the proof assistant, yielding a new state x t +1 or an error message. The process continues, stopping if a proof is completed or a timeout is reached. We prompt the model using three ( x t , y t ) examples. We evaluate on miniF2F (Zheng et al., 2021) using the Lean 4 proof assistant, and use a standard best first search. See Appendix D for more details.\n\nResults. As seen in Table 4, LLEMMA's continued pretraining on Proof -Pile -2 improved few-shot performance on the two formal theorem proving tasks.\n\n| Method         | Informal-to-formal   | Informal-to-formal   | Method                | Formal-to-formal   | Formal-to-formal   |\n|----------------|----------------------|----------------------|-----------------------|--------------------|--------------------|\n|",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 31,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-32",
      "content": "| Method                | Formal-to-formal   | Formal-to-formal   |\n|----------------|----------------------|----------------------|-----------------------|--------------------|--------------------|\n|                | miniF2F-valid        | miniF2F-test         |                       | Search             | miniF2F-test       |\n| Sledgehammer   | 14.72%               | 20.49%               | ReProver (fine-tuned) | 1 × 64             | 26.50%             |\n| Code Llama 7b  | 16.31%               | 17.62%               | Code Llama 7b         | 1 × 32             | 20.49%             |\n| Code Llama 34b | 18.45%               | 18.03%               | Code Llama 34b        | 1 × 32             | 22.13%             |\n| LLEMMA-7b      | 20.60%               | 22.13%               | COPRA (GPT-4)         | - †                | 23.36%             |\n| LLEMMA-34b     | 21.03%               | 21.31%               | LLEMMA-7b             | 1 × 32             | 26.23%             |\n|",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 32,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-33",
      "content": ")         | - †                | 23.36%             |\n| LLEMMA-34b     | 21.03%               | 21.31%               | LLEMMA-7b             | 1 × 32             | 26.23%             |\n|                |                      |                      | LLEMMA-34b            | 1 × 32             | 25.82%             |\n\nTable 4: Formal theorem proving tasks. Left : Informal-to-formal proving in Isabelle, showing the percentage of proven theorems with greedy decoding. Right : Formal-to-formal proving in Lean, showing the percentage of proven theorems with the given number of attempts × generations-periteration of best first search, and a 10-minute timeout. Sledgehammer (Paulson &amp; Nipkow, 2023) is built-in Isabelle automation. ReProver (Yang et al., 2023) is a supervised and retrieval-augmented model. COPRA (Thakur et al., 2023) is a retrieval-augmented GPT-4 based method. † COPRA does not use best first search, but instead samples from GPT-4 (OpenAI, 2023) a maximum of 60 times.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 33,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-34",
      "content": "l-augmented model. COPRA (Thakur et al., 2023) is a retrieval-augmented GPT-4 based method. † COPRA does not use best first search, but instead samples from GPT-4 (OpenAI, 2023) a maximum of 60 times.\n\nOn informal-to-formal proving, LLEMMA-7b closes 22.1% of the theorems, improving upon its Code Llama initialization and the Sledgehammer prover. The theorems that LLEMMA proves are often complementary to those proved with Sledgehammer: taking the union of Sledgehammer and LLEMMA proofs results in 26 new validation proofs (an 11 percentage-point increase), and 17 new test proofs (a 7 point increase); see Appendix Table 11. Prior to our work, the only demonstration of few-shot proof autoformalization used the proprietary Codex model (Jiang et al., 2023).\n\nOn Lean 4 formal-to-formal proving, LLEMMA-7b improves upon its Code Llama initialization, and performs similar to ReProver (Yang et al., 2023), a retrieval-augmented language model finetuned for tactic prediction.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 34,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-35",
      "content": "al-to-formal proving, LLEMMA-7b improves upon its Code Llama initialization, and performs similar to ReProver (Yang et al., 2023), a retrieval-augmented language model finetuned for tactic prediction. LLEMMA adapts to the task using a 3 example prompt, which to our knowledge is the first demonstration of few-shot tactic prediction for theorem proving by an open model.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.3 FORMAL MATHEMATICS",
        "chunkIndex": 35,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-36",
      "content": "When training a language model, it is common to upsample high-quality subsets of the training data according to mixture weights (Brown et al., 2020; Gao et al., 2020; Xie et al., 2023). We select mixture weights by doing short training runs on several hand-picked mixture weights, then choosing the one which minimizes perplexity on a set of high-quality held-out text (we use the MATH training set). Table 5 shows the MATH training set perplexity of models trained using different mixtures of arXiv to web to code. Based on these results, we trained LLEMMA with a ratio of 2 : 4 : 1 . Note that our methodology uses the MATH training set to determine a training hyperparameter, though we expect that the effect is similar to that of related high-quality texts.\n\nTable 5: MATH training set perplexity of Code Llama 7B models trained using different data mixtures for a reduced number of steps. Each mixture is represented by its arXiv:Web:Code ratio.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.4 IMPACT OF DATA MIXTURE",
        "chunkIndex": 36,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-37",
      "content": "lity texts.\n\nTable 5: MATH training set perplexity of Code Llama 7B models trained using different data mixtures for a reduced number of steps. Each mixture is represented by its arXiv:Web:Code ratio.\n\n| Mixture   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   |\n|-----------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n|           | Overall                        | Prealgebra                     | Algebra                        | Number Theory                  | Counting& Probability          | Geometry                       | Intermediate Algebra",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.4 IMPACT OF DATA MIXTURE",
        "chunkIndex": 37,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-38",
      "content": "| Prealgebra                     | Algebra                        | Number Theory                  | Counting& Probability          | Geometry                       | Intermediate Algebra           | Precalculus                    |\n| 2:4:1     | 1.478                          | 1.495                          | 1.515                          | 1.552                          | 1.475                          | 1.519                          | 1.439                          | 1.331                          |\n| 2:4:2     | 1.482                          | 1.500                          | 1.519                          | 1.556                          | 1.477                          | 1.524                          | 1.443                          | 1.334                          |\n| 4:2:1     | 1.487                          | 1.505                          | 1.524                          | 1.561                          | 1.481                          | 1.534",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.4 IMPACT OF DATA MIXTURE",
        "chunkIndex": 38,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-39",
      "content": "| 1.487                          | 1.505                          | 1.524                          | 1.561                          | 1.481                          | 1.534                          | 1.447                          | 1.338                          |\n| 4:2:2     | 1.489                          | 1.508                          | 1.527                          | 1.562                          | 1.483                          | 1.538                          | 1.447                          | 1.339                          |\n| 4:4:1     | 1.487                          | 1.506                          | 1.525                          | 1.561                          | 1.482                          | 1.529                          | 1.446                          | 1.335                          |\n| 4:4:2     | 1.485                          | 1.503                          | 1.523                          | 1.559                          | 1.480",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.4 IMPACT OF DATA MIXTURE",
        "chunkIndex": 39,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-40",
      "content": "|\n| 4:4:2     | 1.485                          | 1.503                          | 1.523                          | 1.559                          | 1.480                          | 1.529                          | 1.444                          | 1.334                          |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.4 IMPACT OF DATA MIXTURE",
        "chunkIndex": 40,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-41",
      "content": "Do test problems or solutions appear in the corpus? We check whether any 30-gram in a test sequence (either an input problem or an output solution) occurs in any OpenWebMath or AlgebraicStack document. If so, we say that a hit occurred between the sequence and the document. Table 6 shows hits between sequences from MATH and documents from Proof -Pile -2 . Using our methodology, around 7% of MATH test problem statements and 0.6% of MATH test solutions have hits. Note that our methodology gives a lower bound on the number of semantically equivalent sequences (e.g., it does not account for alternative phrasing).\n\nWe manually inspected 100 uniformly sampled hits between a test problem statement and an OpenWebMath document. 41 of the cases had no solution, which included websites with a list of problems, discussions, or hints. 49 had an alternative solution to the MATH ground-truth solution, but with the same answer.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.5 DATASET OVERLAP AND MEMORIZATION",
        "chunkIndex": 41,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-42",
      "content": "nt. 41 of the cases had no solution, which included websites with a list of problems, discussions, or hints. 49 had an alternative solution to the MATH ground-truth solution, but with the same answer. These include solutions that solve the problem differently than the ground-truth, solutions with missing details, and discussions that include the answer. 9 cases had a missing or incorrect answer, and 1 had the same solution as in the ground-truth. In summary, we find that solutions can appear in a corpus derived from web documents, particularly alternative solutions to those in the evaluation set. We repeated our analysis with 20-gram hits and our findings were similar, though with false positives; see Appendix Figure 6 for examples.\n\nTable 6: Left: 30-gram hits between MATH test problems or solutions and Proof -Pile -2 documents. Example and Docs are the numbers of unique test examples and Proof -Pile -2 documents with a hit.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.5 DATASET OVERLAP AND MEMORIZATION",
        "chunkIndex": 42,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-43",
      "content": "es.\n\nTable 6: Left: 30-gram hits between MATH test problems or solutions and Proof -Pile -2 documents. Example and Docs are the numbers of unique test examples and Proof -Pile -2 documents with a hit. Right: manual inspection of 100 hits between a problem statement and a Proof -Pile -2 document.\n\n| Proof - Pile - 2   | Test   |   Problem Example Docs |   Problem Example Docs |   Solution Example Docs |   Solution Example Docs | Same solution                                                        | 1    |\n|--------------------|--------|------------------------|------------------------|-------------------------|-------------------------|----------------------------------------------------------------------|------|\n| OpenWebMath        | MATH   |                    348 |                    717 |                      34 |                      46 | Different solution, same answer Different solution, different answer | 49 9 |\n| AlgebraicStack     | MATH   |                      3 |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.5 DATASET OVERLAP AND MEMORIZATION",
        "chunkIndex": 43,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-44",
      "content": "7 |                      34 |                      46 | Different solution, same answer Different solution, different answer | 49 9 |\n| AlgebraicStack     | MATH   |                      3 |                      3 |                       1 |                       1 | No solution                                                          | 41   |\n| OpenWebMath        | GSM8k  |                      2 |                      3 |                       0 |                       0 | Different problem                                                    | 0    |\n| AlgebraicStack     | GSM8k  |                      0 |                      0 |                       0 |                       0 |                                                                      |      |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "3.5 DATASET OVERLAP AND MEMORIZATION",
        "chunkIndex": 44,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-45",
      "content": "Next, we evaluate LLEMMA-34b on the test examples with a 30-gram hit, and the test examples without a 30gram hit. Table 7 shows the accuracy partitioned by MATH difficulty level. The model's accuracy remains low on difficult problems (e.g., 6.08% on Level 5 problems with a hit, versus 6.39% on problems without a hit), and we observe no clear relationship between 30-gram hits and accuracy across difficulty levels. We conclude that a nontrivial match between a test example and a training document did not imply that the model generated a memorized correct answer. We repeated the analysis with 20-grams and with the 7b model, and our findings were analogous. Figure 7 shows an example.\n\nTable 7: LLEMMA-34b's accuracy on hits (a 30-gram overlap between a problem or solution and a training sequence) and nonhits by MATH difficulty level.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "How do problems in the corpus impact performance?",
        "chunkIndex": 45,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-46",
      "content": "dings were analogous. Figure 7 shows an example.\n\nTable 7: LLEMMA-34b's accuracy on hits (a 30-gram overlap between a problem or solution and a training sequence) and nonhits by MATH difficulty level.\n\n| MATH Level   |   Hit Accuracy |   Nonhit Accuracy |   # Hits |\n|--------------|----------------|-------------------|----------|\n| Level 1      |          72.73 |             61.5  |       11 |\n| Level 2      |          35.71 |             40.18 |       28 |\n| Level 3      |          30.36 |             26.88 |       56 |\n| Level 4      |          14.89 |             16.61 |       94 |\n| Level 5      |           6.08 |              6.39 |      181 |\n\nFinally, we check 30-gram hits between LLEMMA's MATH generations and OpenWebMath. There were 13 hits, which occurred when the model generated a common sequence of numbers (e.g., a list of Fibonacci numbers), plus one instance of factoring a polynomial. Appendix Figure 6 shows an example.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "How do problems in the corpus impact performance?",
        "chunkIndex": 46,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-47",
      "content": "were 13 hits, which occurred when the model generated a common sequence of numbers (e.g., a list of Fibonacci numbers), plus one instance of factoring a polynomial. Appendix Figure 6 shows an example. We find all of these observations worthy of further study. Using LLEMMA and Proof -Pile -2 to better understand data, memorization, and performance is an interesting future direction. We include the code for our analysis in the LLEMMA repository.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "How do problems in the corpus impact performance?",
        "chunkIndex": 47,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-48",
      "content": "Large-scale language modeling. Recent progress in large language models involves two connected threads: the increasing scale of models and data (Hoffmann et al., 2022; Kaplan et al., 2020; Chowdhery et al., 2022), and a progression toward more generalist models (Radford et al., 2019; Brown et al., 2020) which are capable of solving diverse problems and adapting quickly to novel tasks. A third thread relates to enabling open access to language models with these capabilities (Black et al., 2022; Biderman et al., 2023; Touvron et al., 2023; Rozière et al., 2023). Our work provides a recipe for specializing these language models to the domain of mathematics, providing a platform for further research and applications.\n\nDomain adaptation. Language model applications typically require a general-domain pretraining step, followed by a shorter fine-tuning step.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "4 RELATED WORK",
        "chunkIndex": 48,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-49",
      "content": "providing a platform for further research and applications.\n\nDomain adaptation. Language model applications typically require a general-domain pretraining step, followed by a shorter fine-tuning step. The finetuning step is often aimed at imbuing instructionfollowing ability (Sanh et al., 2022; Wei et al., 2022) or aligning a model's outputs with human preferences (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). Other work explores adapting pretrained models to novel domains by continued training (Rozière et al., 2023; Beltagy et al., 2019), parameter-efficient finetuning methods (Yong et al., 2023), retrieval augmentation (Min et al., 2023; Asai et al., 2023), and other techniques. We provide an adaptation recipe involving continued training and targeted data collection.\n\nLanguage models for mathematics.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "4 RELATED WORK",
        "chunkIndex": 49,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-50",
      "content": "l augmentation (Min et al., 2023; Asai et al., 2023), and other techniques. We provide an adaptation recipe involving continued training and targeted data collection.\n\nLanguage models for mathematics. Applying large language models to problems in mathematics is an active subfield of machine learning, including benchmarking mathematical knowledge and reasoning at varying levels (Hendrycks et al., 2021b; Zheng et al., 2021; Welleck et al., 2022; Azerbayev et al., 2023). Although achieving strong mathematical reasoning is an important target, it is difficult to assess the correctness of models' answers and processes, especially as models become more capable (Bowman et al., 2022; Uesato et al., 2022; Lightman et al., 2023; Cobbe et al., 2021).\n\nA number of recent works focus on supervised finetuning on task-relevant (input, output) pairs (e.g.,Yu et al. (2023); Yue et al. (2023)).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "4 RELATED WORK",
        "chunkIndex": 50,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-51",
      "content": "to et al., 2022; Lightman et al., 2023; Cobbe et al., 2021).\n\nA number of recent works focus on supervised finetuning on task-relevant (input, output) pairs (e.g.,Yu et al. (2023); Yue et al. (2023)). Doing so boosts performance on some common mathematical language modeling benchmarks, but trains the model for these specific tasks. In contrast, Lewkowycz et al. (2022) and our work seek to train a base language model as a platform for further development.\n\nLanguage models for formal mathematics. An ongoing line of work explores integrating language models with interactive proof assistants in the context of mathematics. This includes synthesizing proofs via tactic prediction (Polu &amp; Sutskever, 2020; Han et al., 2022; Lample et al., 2022; Jiang et al., 2022), autoformalization (Wu et al., 2022; Jiang et al., 2023), and integrated tools (Welleck &amp;Saha, 2023).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "4 RELATED WORK",
        "chunkIndex": 51,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-52",
      "content": "iction (Polu &amp; Sutskever, 2020; Han et al., 2022; Lample et al., 2022; Jiang et al., 2022), autoformalization (Wu et al., 2022; Jiang et al., 2023), and integrated tools (Welleck &amp;Saha, 2023). Due to high computational costs of search, language models applied to this domain have traditionally been small, but recent work has demonstrated promise in the use of larger models (First et al., 2023; Jiang et al., 2023). Our work provides a demonstration of few-shot proof autoformalization and tactic prediction, a large collection of formal mathematics data, along with an open access model for further exploring these directions.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "4 RELATED WORK",
        "chunkIndex": 52,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-53",
      "content": "We introduce LLEMMA and Proof -Pile -2 , a novel base model and corpus for language modeling of mathematics. Our models, dataset, and code are openly available. We have shown that LLEMMA achieves state-of-the-art results for open-weights models on mathematical problem solving benchmarks, shown capabilities of using external tools via Python code, and demonstrated few-shot tactic prediction for theorem proving. We hope that LLEMMA and Proof -Pile -2 will be a useful base for future work on understanding language model generalization and dataset composition, investigating the limits of domain-specific language models, using language models as tools for mathematicians, and improving the mathematical capabilities of language models.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "5 CONCLUSION",
        "chunkIndex": 53,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-54",
      "content": "We would like to thank Dragomir Radev, Arman Cohan, Jesse Michael Han, and the Deepmind Blueshift team for valuable guidance. We thank Jonah Philion for the model name. We thank Aviya Skowron for advising us on ethical considerations in the development and release of our models. We thank Jonathan Laurent and Leo Du for contributions to our open-source code.\n\nWe would also like to thank several parties for donating computing resources for this project: Stability AI (training the LLEMMA models), CoreWeave (evaluations and finetuning), the Province of Ontario and companies sponsoring the Vector Institute for Artificial Intelligence ( www.vectorinstitute.ai/partners ), and Brigham Young University (finetuning). KP is supported by an NSERC PGS-D award.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "ACKNOWLEDGEMENTS",
        "chunkIndex": 54,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-55",
      "content": "- Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. Santacoder: don't reach for the stars! In Deep Learning for Code (DL4C) Workshop , 2023.\n- Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 55,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-56",
      "content": ", Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Thérien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in PyTorch. GitHub Repo, 9 2023. URL https://www.github.com/eleutherai/ gpt-neox .\n- Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts) , pp. 41-46, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-tutorials.6. URL https: //aclanthology.org/2023.acl-tutorials.6 .\n- Jeremy Avigad. The mechanization of mathematics. Notices of the AMS , 65(6):681-90, 2018.\n- Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 56,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-57",
      "content": "3.acl-tutorials.6 .\n- Jeremy Avigad. The mechanization of mathematics. Notices of the AMS , 65(6):681-90, 2018.\n- Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R. Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. ArXiv , abs/2302.12433, 2023.\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n- Iz Beltagy, Kyle Lo, and Arman Cohan.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 57,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-58",
      "content": ", Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n- Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 3615-3620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371 .\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning , pp. 2397-2430. PMLR, 2023.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 58,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-59",
      "content": ", USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning , pp. 2397-2430. PMLR, 2023.\n\n- Stella Rose Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. ArXiv , abs/2201.07311, 2022.\n- Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode# 5-Workshop on Challenges &amp;Perspectives in Creating Large Language Models , pp. 95-136, 2022.\n- Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil˙ e Lukoši¯ ut˙ e, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli TranJohnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladis",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 59,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-60",
      "content": "hen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli TranJohnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540 , 2022.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 60,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-61",
      "content": "Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv , abs/2005.14165, 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 61,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-62",
      "content": "d Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- Katherine M. Collins, Albert Q. Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B. Tenenbaum, William Hart, Timothy Gowers, Wenda Li, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694 , 2023.\n- Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 , 2023.\n- Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 62,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-63",
      "content": "sm and work partitioning. arXiv preprint arXiv:2307.08691 , 2023.\n- Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25 , pp. 378-388. Springer, 2015.\n- Erich Elsen, Curtis Hawthorne, and Arushi Somani. The adventure of the errant hardware, 2023. URL https://www.adept.ai/blog/sherlock-sdc .\n- Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. arXiv preprint arXiv:2303.04910 , 2023.\n- Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 63,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-64",
      "content": ", Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv , abs/2101.00027, 2020.\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Jason Ociepa, Chris Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\n\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .\n\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2022.\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 64,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-65",
      "content": "nd Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2022.\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2023.\n- Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. Datasheets for datasets, 2021.\n- Herbert L. Gelernter. Realization of a geometry theorem proving machine. In IFIP Congress , 1959. URL https://api.semanticscholar.org/CorpusID:18484295 .\n- Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR , abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677 .\n- Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 65,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-66",
      "content": "rate, large minibatch SGD: training imagenet in 1 hour. CoRR , abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677 .\n- Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu. Proof artifact cotraining for theorem proving with language models. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=rpxJc9j04U .\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2021a.\n- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS , 2021b.\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 66,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-67",
      "content": "astian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020.\n- Albert Q. Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle proofs. 6th Conference on Artificial Intelligence and Theorem Proving , 2021.\n- Albert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygó´ zd´ z, Piotr Miło´ s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 67,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-68",
      "content": ", Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygó´ zd´ z, Piotr Miło´ s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. arXiv preprint arXiv:2205.10893 , 2022.\n- Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SMa9EAovKMC .\n- Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 68,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-69",
      "content": ".08361 , 2020.\n- Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint , 2022.\n- Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, and Timothée Lacroix. Hypertree proof search for neural theorem proving. arXiv preprint arXiv:2205.11491 , 2022.\n\n- Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 69,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-70",
      "content": "amasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022.\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023.\n- Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 14605-14631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 70,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-71",
      "content": "g of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 14605-14631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long.817 .\n- Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 , 2023.\n- The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs , CPP 2020, pp. 367-381, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450370974. doi: 10.1145/ 3372885.3373824. URL https://doi.org/10.1145/3372885.3373824 .\n- Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 71,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-72",
      "content": "ISBN 9781450370974. doi: 10.1145/ 3372885.3373824. URL https://doi.org/10.1145/3372885.3373824 .\n- Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore, 2023.\n- Scott Morrison. lean-training-data. https://github.com/semorrison/ lean-training-data , 2023.\n- OpenAI. Gpt-4 technical report, 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.\n- Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR , abs/2310.06786, 2023.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 72,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-73",
      "content": "v preprint arXiv:2203.02155 , 2022.\n- Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR , abs/2310.06786, 2023. doi: 10.48550/ ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786 .\n- Christine Paulin-Mohring. Extracting ω 's programs from proofs in the calculus of constructions. In Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming languages , pp. 89-104, 1989a.\n- Christine Paulin-Mohring. Extraction de programmes dans le Calcul des Constructions . PhD thesis, Université Paris-Diderot-Paris VII, 1989b.\n- Larry Paulson and Tobias Nipkow. The sledgehammer: Let automatic theorem provers write your isabelle scripts!, 2023. URL https://isabelle.in.tum.de/ website-Isabelle2009-1/sledgehammer.html .\n- Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 73,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-74",
      "content": "ps://isabelle.in.tum.de/ website-Isabelle2009-1/sledgehammer.html .\n- Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.\n- Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393 , 2020.\n- Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344 , 2022.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog , 2019.\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 74,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-75",
      "content": "Language models are unsupervised multitask learners. OpenAI Blog , 2019.\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC '20. IEEE Press, 2020. ISBN 9781728199986. doi: 10.5555/3433701.3433727. URL https://dl.acm.org/doi/10. 5555/3433701.3433727 .\n- Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 75,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-76",
      "content": "Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 , 2023.\n- Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 76,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-77",
      "content": "wden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 , 2022.\n- Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. Computing Research Repository , 2019. doi: 10.48550/arXiv.1909.08053. URL https://arxiv.org/abs/1909.08053v4 . Version 4.\n- Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 77,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-78",
      "content": "ni, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge, 2022.\n- Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 78,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-79",
      "content": "rcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023.\n- Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2022.\n- Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022.\n- Amitayush Thakur, Yeming Wen, and Swarat Chaudhuri. A language-agent approach to formal theorem-proving, 2023.\n- Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menega",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 79,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-80",
      "content": "iel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\n\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 80,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-81",
      "content": "Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoq",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 81,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-82",
      "content": "inet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n- Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback, 2022.\n- H. Wang. Toward mechanical mathematics. IBM Journal of Research and Development , 4(1):2-22, 1960. doi: 10.1147/rd.41.0002.\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 82,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-83",
      "content": "IBM Journal of Research and Development , 4(1):2-22, 1960. doi: 10.1147/rd.41.0002.\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=1PL1NIMMrw .\n- Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2022.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n- Sean Welleck. Neural theorem proving tutorial. https://github.com/wellecks/ ntptutorial , 2023.\n- Sean Welleck and Rahul Saha. llmstep: Llm proofstep suggestions in lean. https://github.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 83,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-84",
      "content": "dels, 2023.\n- Sean Welleck. Neural theorem proving tutorial. https://github.com/wellecks/ ntptutorial , 2023.\n- Sean Welleck and Rahul Saha. llmstep: Llm proofstep suggestions in lean. https://github. com/wellecks/llmstep , 2023.\n- Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=rhdfTOiXBng .\n- Makarius Wenzel, Lawrence C Paulson, and Tobias Nipkow. The isabelle framework. In Theorem Proving in Higher Order Logics: 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings 21 , pp. 33-38. Springer, 2008.\n- Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 84,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-85",
      "content": "18-21, 2008. Proceedings 21 , pp. 33-38. Springer, 2008.\n- Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance, 2023.\n\n- Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=IUikebJ1Bf0 .\n- Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429 , 2023.\n- Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 85,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-86",
      "content": "ds up language model pretraining. arXiv preprint arXiv:2305.10429 , 2023.\n- Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. In Neural Information Processing Systems (NeurIPS) , 2023.\n- Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina. BLOOM+1: Adding language support to BLOOM for zero-shot prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 11682-11703, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.653.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 86,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-87",
      "content": "of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 11682-11703, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.653. URL https://aclanthology.org/2023.acl-long.653 .\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284 , 2023.\n- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR , abs/2309.05653, 2023. doi: 10.48550/arXiv.2309.05653. URL https://doi.org/10. 48550/arXiv.2309.05653 .\n- Shizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. Can transformers learn to solve problems recursively?, 2023.\n- Kunhao Zheng, Jesse Michael Han, and Stanislas Polu.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 87,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-88",
      "content": "Shizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. Can transformers learn to solve problems recursively?, 2023.\n- Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110 , 2021.\n- Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning, 2022.\n- Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "REFERENCES",
        "chunkIndex": 88,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-89",
      "content": "Training Data. Zhangir Azerbayev, Keiran Paster, Marco Dos Santos, Sean Welleck.\n\nModel training. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster.\n\nEvaluations. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Sean Welleck.\n\nFormal math evaluations. Sean Welleck.\n\nMemorization analysis. Sean Welleck, Keiran Paster.\n\nSenior Authorship and Advising. Jia Deng, Stella Biderman, Sean Welleck.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "A AUTHOR CONTRIBUTIONS",
        "chunkIndex": 89,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-90",
      "content": "Table 8: Proof -Pile -2 data sources (top), general language and code data included during training (bottom), and the mixture weights of each component during training.\n\n| Data source              | Tokens   | Weight   |\n|--------------------------|----------|----------|\n| Proof - Pile - 2         | 55B      | -        |\n| Code ( AlgebraicStack )  | 11B      | 1.00     |\n| Web (OpenWebMath)        | 15B      | 4.00     |\n| Papers (ArXiv)           | 29B      | 2.00     |\n| General code (RedPajama) | 59B      | 0.22     |\n| General language (Pile)  | 300B     | 0.15     |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B DATA: Proof -Pile -2",
        "chunkIndex": 90,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-91",
      "content": "AlgebraicStack contains roughly 11B tokens of code related to mathematics. We describe its sources, filtering, and content below. Table 9 shows the number of tokens per language in AlgebraicStack .\n\nTable 9: Tokens in AlgebraicStack , computed with the Llama tokenizer.\n\n| Language   | AlgebraicStack tokens   | Language   | AlgebraicStack tokens   |\n|------------|-------------------------|------------|-------------------------|\n| Agda       | 35.2M                   | Julia      | 531.0M                  |\n| C          | 25.1M                   | Jupyter    | 199.1M                  |\n| C++        | 954.1M                  | Lean       | 285.6M                  |\n| Coq        | 281.9M                  | Maple      | 2.0M                    |\n| Fortran    | 724.9M                  | Matlab     | 65.8M                   |\n| GAP        | 3.6M                    | Python     | 6,098.8M                |\n| Haskell    | 9.1M                    | R          | 71.3M                   |\n| Idris",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1 MATHEMATICAL CODE: AlgebraicStack",
        "chunkIndex": 91,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-92",
      "content": "| 65.8M                   |\n| GAP        | 3.6M                    | Python     | 6,098.8M                |\n| Haskell    | 9.1M                    | R          | 71.3M                   |\n| Idris      | 10.9M                   | Tex        | 567.7M                  |\n| Isabelle   | 1,089.7M                | Total      | 10,955.7M               |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1 MATHEMATICAL CODE: AlgebraicStack",
        "chunkIndex": 92,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-93",
      "content": "The following programming languages were either barely present in the Stack or consisted of largely incorrect filetypes, so we downloaded data for these languages directly via the Github Python API.\n\n- Coq : We filter for files with the .v extension, and include Coq via including files that match a heuristic filter for the keywords \"Theorem\" , \"Proof\" , \"Qed\" , \"Inductive\" , \"Definition\" , \"Fixpoint\" and exclude Verilog files via the keyword blacklist \"pragma\" , \"endmodule\" , \"posedge\" , \"negedge\" , \"wire\" . We additionally exclude files noted as automatically generated.\n\n- Isabelle : We filter for files with the .thy extension and include files matching the keyword whitelist \"theorem \" , \"lemma \" . We keep only isabelle-prover/mirror-afp-devel and discard all other older copies of the Archive of Formal Proofs.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.1 GITHUB CODE",
        "chunkIndex": 93,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-94",
      "content": "thy extension and include files matching the keyword whitelist \"theorem \" , \"lemma \" . We keep only isabelle-prover/mirror-afp-devel and discard all other older copies of the Archive of Formal Proofs. We further remove theorem statements and proofs that have a theorem name in the PISA (Jiang et al., 2021) test set.\n- Lean : We filter for files with the .lean extension, using the keyword whitelist \"theorem \" , \"lemma \" , \"example \" . We remove all dependency files, and in order to avoid known benchmark contamination, we blacklist the ProofNet and MiniF2F repositories. We further remove theorems or lemmas that share a theorem name with the LeanDojo (Yang et al., 2023) val or test sets.\n- MATLAB : We filter for files with the .m extension, using the keyword whitelist \"#import\" , \"interface\" , \"implementation\" , \"property\" , and blacklist C files via the keywords \"#include\" and the regex r' main \\ (.*{$'",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.1 GITHUB CODE",
        "chunkIndex": 94,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-95",
      "content": "for files with the .m extension, using the keyword whitelist \"#import\" , \"interface\" , \"implementation\" , \"property\" , and blacklist C files via the keywords \"#include\" and the regex r' main \\ (.*{$'\n\nWe implemented a cutoff date for our Github API downloads, and used a cutoff date of April 1, 2023.\n\nFor all languages, unless otherwise stated, we additionally filtered out files with a filesize greater than 1048575 bytes or with a numerical density (ratio of digit characters to non-digit characters) of 0 . 5 . We additionally perform document-level exact deduplication by removing documents which contain an overlapping 2048-character chunk as another document.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.1 GITHUB CODE",
        "chunkIndex": 95,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-96",
      "content": "We extract a dataset of (tactic state, next tactic) pairs from Mathlib 4 (mathlib Community, 2020) using the lean-training-data (Morrison, 2023) tool. We use Mathlib 4 commit c779bd5 , which was created on August 20th 2023.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.2 LEAN PROOFSTEPS",
        "chunkIndex": 96,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-97",
      "content": "We construct a dataset of Isabelle proofs, building upon the PISA dataset Jiang et al. (2021). Isabelle Proofsteps comprises proofs from the Archive of Formal Proofs and Isabelle Standard Library, scraped with PISA Jiang et al. (2021). Each entry in the dataset includes the theorem statement, the proof states and the proof steps, separated by specific tags. To maintain the integrity of evaluations using the PISA test set, we decontaminate Isabelle Proofsteps by removing theorems whose names overlap with those in the PISA test set. Although this approach results in a strict filtering - removing more than 10,000 theorems although there are only 3600 in the PISA test set - we consider it acceptable in order to mitigate data contamination. After filtering, Isabelle Proofsteps contains 251,000 theorems.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.3 ISABELLE PROOFSTEPS",
        "chunkIndex": 97,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-98",
      "content": "We source the following programming languages from the Stack (Kocetkov et al., 2022) dataset, and describe our filtering process and quality issues we chose to mitigate beyond our default quality heuristics:\n\n- Agda : Only standard filters applied.\n- C : We include documents based on a keyword whitelist, namely: \"#include &lt;fftw.h&gt;\" , \"#include &lt;fftw3.h&gt;\" , \"#include &lt;rfftw.h&gt;\" , \"#include &lt;gsl\" , \"#include &lt;cblas.h&gt;\" , \"#include &lt;blas.h&gt;\" , \"#include &lt;lapacke.h&gt;\" , \"#include &lt;nlopt.h&gt;\" , \"#include &lt;petsc.h&gt;\" .\n- C++ : We include documents based on a keyword whitelist, namely: \"#include &lt;adept\\_arrays.h&gt;\" , \"#include &lt;adept.h&gt;\" , \"#include &lt;alglib&gt; , \"#include &lt;boost\" , \"#include &lt;armadillo\" , \"#include &lt;blitz\" , \"#include &lt;Eigen\" , \"#include &lt;deal.II\" , \"#include &lt;dlib\" , \"#include &lt;NTL\" , \"#include &lt;mtl\" .\n- Fortran : Only standard filters applied.\n- GAP : Only standard filters applied.\n- Hask",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.4 STACK FILTERING",
        "chunkIndex": 98,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-99",
      "content": "\"#include &lt;Eigen\" , \"#include &lt;deal.II\" , \"#include &lt;dlib\" , \"#include &lt;NTL\" , \"#include &lt;mtl\" .\n- Fortran : Only standard filters applied.\n- GAP : Only standard filters applied.\n- Haskell : We filtered the data to only contain files with the following imports: Numeric.LinearAlgebra , Numeric.SpecFunctions , Numeric.Vector , Statistics , Data.Complex .\n\n- Idris : Only standard filters applied.\n- Julia : We filtered out mislabeled JSON lines files. We removed files larger than 10,000 characters long which both were not files containing tests and which had a lower numerical density than 0 . 5 , and otherwise ignored numerical density. We additionally only accepted files within a specific keyword whitelist, to attempt to control relevance to scientific computing, namely: \"LinearAlgebra\" , \"DifferentialEquations\" , \"Symbolics\" , \"Distributions\" , \"DataFrames\" , \"DynamicalSystems\" , \"Turing\" , \"Gen\" , \"JuMP\" , \"sqrt\" , \"abs\" , \"zeros\" , \"ones\" , \"sin\" , \"cos\" , \"tan\" , \"log\"",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.4 STACK FILTERING",
        "chunkIndex": 99,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-100",
      "content": "rAlgebra\" , \"DifferentialEquations\" , \"Symbolics\" , \"Distributions\" , \"DataFrames\" , \"DynamicalSystems\" , \"Turing\" , \"Gen\" , \"JuMP\" , \"sqrt\" , \"abs\" , \"zeros\" , \"ones\" , \"sin\" , \"cos\" , \"tan\" , \"log\" , \"exp\" , \"integrate\" , \"likelihood\" , \"Matrix\" , π , \"pi\" , \"rand\" , \"grad\" .\n- Jupyter : We found that many Jupyter notebook files were large due to containing long cell outputs, such as base64 images, long tracebacks, or other extra JSON cell metadata. We use nbconvert to convert notebooks to a markdown format, removing metadata.\n- Maple : We filtered out files with a size greater than 100 , 000 bytes, and found that some files were XML. We filtered all files beginning with an XML declaration.\n- Python : We filtered notebooks and JSON files out by excluding documents with beginning \"{\" characters, and included only files importing from a fixed list of libraries.\n- R : We excluded all files beginning with an XML declaration.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.4 STACK FILTERING",
        "chunkIndex": 100,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-101",
      "content": "nd JSON files out by excluding documents with beginning \"{\" characters, and included only files importing from a fixed list of libraries.\n- R : We excluded all files beginning with an XML declaration. We additionally filtered out all notebooks, and filtered all files containing MacOS \"Resource Fork\" files.\n- Tex : We used a max file size of 10,000,000 bytes. We excluded tex files found in directories named \"latex/\" because these were often auto-generated files, and excluded documents using gnuplot . We included only documents containing one of the keywords \" \\ chapter{\" , \" \\ chapter*{\" , \" \\ section{\" , \" \\ section*{\" , \" \\ subsection{\" , \" \\ subsection*{\" , \" \\ subsubsection{\" , \" \\ subsubsection*{\" , \" \\ paragraph{\" , \" \\ subparagraph{\" , and additionally only included documents identified as English by a classifier from the langid package.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.4 STACK FILTERING",
        "chunkIndex": 101,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-102",
      "content": "section*{\" , \" \\ subsubsection{\" , \" \\ subsubsection*{\" , \" \\ paragraph{\" , \" \\ subparagraph{\" , and additionally only included documents identified as English by a classifier from the langid package.\n\nFor all languages we used within the Stack, unless otherwise stated, we additionally filtered out files with a filesize greater than 1048575 bytes or with a numerical density (ratio of digit characters to non-digit characters) of 0 . 5 .\n\nWe used v1.2 of the near-deduplicated Stack as a base for processing.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.1.4 STACK FILTERING",
        "chunkIndex": 102,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-103",
      "content": "We use the entirety of ArXiv, as accessed by Computer (2023) in April 2023. For further information on preprocessing applied to ArXiv, see Computer (2023).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.2 PAPERS: ARXIV",
        "chunkIndex": 103,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-104",
      "content": "For the web portion of our training dataset, we use OpenWebMath (Paster et al., 2023).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "B.3 WEB: OPENWEBMATH",
        "chunkIndex": 104,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-105",
      "content": "We implement a variety of math-related tasks and evaluation protocols into a public fork of the Language Model Evaluation Harness (Gao et al., 2021). The Harness provides a model-agnostic framework for standardized, reproducible evaluation of language models.\n\nWe add the following tasks for the evaluations in this paper:\n\n- hendrycks\\_math\\_ppl : Perplexity evaluation on MATH (Hendrycks et al., 2021a) sub-tasks.\n- minif2f\\_isabelle : Proof autoformalization in Isabelle on the miniF2F benchmark based on Jiang et al. (2023), with a Portal-to-Isabelle (Jiang et al., 2021) proof checker.\n- minerva\\_math : The MATH benchmark with the prompt and Sympy evaluation from Minerva (Lewkowycz et al., 2022).\n- minerva-hendrycksTest : MMLU-STEM tasks following Lewkowycz et al. (2022).\n\n- ocw\\_courses : The OCW Courses task from Lewkowycz et al. (2022).\n- python\\_gsm8k : GSM8k with Python, based on Gao et al. (2022).\n- sympy\\_math : MATH with Sympy evaluation.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "C EVALUATION HARNESS",
        "chunkIndex": 105,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-106",
      "content": "wkowycz et al. (2022).\n\n- ocw\\_courses : The OCW Courses task from Lewkowycz et al. (2022).\n- python\\_gsm8k : GSM8k with Python, based on Gao et al. (2022).\n- sympy\\_math : MATH with Sympy evaluation.\n\nWe include a link to the implementations for these tasks, including full prompts, in our public codebase.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "C EVALUATION HARNESS",
        "chunkIndex": 106,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-107",
      "content": "We follow Jiang et al. (2023), allowing the model to issue a call to built-in Isabelle automation in the output proof by generating sledgehammer . This calls Sledgehammer (Paulson &amp; Nipkow, 2023) and the list of heuristics listed in Jiang et al. (2023). Following Jiang et al. (2023), as a baseline we use Sledgehammer and the heuristics executed at the beginning of the proof (referred to as Sledgehammer in the main text for brevity). We use a 30-second timeout for Sledgehammer and implement proof checking via Portal-to-Isabelle (Jiang et al., 2021). Refer to the implementation in the Evaluation Harness for further details.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "D.1 ISABELLE INFORMAL-TO-FORMAL THEOREM PROVING",
        "chunkIndex": 107,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-108",
      "content": "Theorem proving via tactic prediction involves interacting with a proof assistant after each step of a proof. Implementing these interactions within the evaluation harness is outside the scope of this work. Therefore, for the Lean theorem proving task we use a separate evaluation setup based on an open-source implementation (Welleck, 2023). We include our evaluation code in our public codebase.\n\nSetup. We evaluate on miniF2F (Zheng et al., 2021), which consists of 488 formalized statements from math competitions and undergraduate coursework. Given a formalized statement, the task is to generate a formal proof that is checked by Lean.\n\nWe use best first search, commonly used for neural tactic prediction models (e.g., Polu &amp; Sutskever (2020)). Best first search is parameterized by the number of attempts (N), generated tactics per iteration (S), and maximum iterations (T). We define the search budget to be the maximum number of generated tactics, N × S × T .",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "D.2 LEAN THEOREM PROVING",
        "chunkIndex": 108,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-109",
      "content": "is parameterized by the number of attempts (N), generated tactics per iteration (S), and maximum iterations (T). We define the search budget to be the maximum number of generated tactics, N × S × T . We set our search budget to N = 1 , S = 32 , and T = 100 , less than that of the baseline model. Following Yang et al. (2023), we generate tactics with beam search and use a 10 minute timeout. We adapt the proof search implementation from Welleck (2023), which uses LeanDojo v.1.1.2 (Yang et al., 2023) for interaction. We use Lean 4 miniF2F, using https://github.com/rah4927/lean-dojo-mew commit d00c776260c77de7e70125ef0cd119de6c0ff1de . Note that the ReProver baseline from (Yang et al., 2023) reports performance with Lean 3.\n\nPrompt. We prompt the model with three (state, tactic) examples, shown in Figure 5.\n\n̸",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "D.2 LEAN THEOREM PROVING",
        "chunkIndex": 109,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-110",
      "content": "25ef0cd119de6c0ff1de . Note that the ReProver baseline from (Yang et al., 2023) reports performance with Lean 3.\n\nPrompt. We prompt the model with three (state, tactic) examples, shown in Figure 5.\n\n̸\n\n```\n\"\"\"Given the Lean 4 tactic state, suggest a next tactic. Here are some examples: Tactic state: ---α : Type u_1 r : α → α → Prop inst 1 : DecidableEq α inst : IsIrrefl α r ⊢ CutExpand r ≤ InvImage (Finsupp.Lex ( c r Π fun x x_1 => x = x_1) fun x x_1 => x < x_1) ↑ toFinsupp ---Next tactic: ---rintro s t ⟨ u, a, hr, he ⟩ ---Tactic state: ---ι : Type u_1 I J : Box ι x y : ι → R I J : WithBot (Box ι ) ⊢ ↑ I = ↑ J ↔ I = J ---Next tactic: ---simp only [Subset.antisymm_iff, ← le_antisymm_iff, withBotCoe_subset_iff] ---Tactic state: ---m n : N h : Nat.coprime m n ⊢ Nat.gcd m n = 1 ---Next tactic: ---rw [ ← h.gcd_eq_one] ---Tactic state: ---%s ---Next tactic: ---\"\"\"\n```\n\nFigure 5: Prompt for the Lean theorem proving experiments.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "D.2 LEAN THEOREM PROVING",
        "chunkIndex": 110,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-111",
      "content": "---rw [ ← h.gcd_eq_one] ---Tactic state: ---%s ---Next tactic: ---\"\"\"\n```\n\nFigure 5: Prompt for the Lean theorem proving experiments.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "D.2 LEAN THEOREM PROVING",
        "chunkIndex": 111,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-112",
      "content": "We provide a datasheet for Proof -Pile -2 , following the framework in Gebru et al. (2021).\n\n| MOTIVATION                                                                                                                   | MOTIVATION                                                                                                                                                                        |\n|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| For what purpose was the dataset cre- ated?                                                                                  | Proof - Pile - 2 was created for the training or finetuning of domain-specific large lan- guage models for general mathematics tasks.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 112,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-113",
      "content": "| Proof - Pile - 2 was created for the training or finetuning of domain-specific large lan- guage models for general mathematics tasks.                                             |\n| Who created the dataset and on behalf of which entity?                                                                       | The dataset was created by the authors of this paper for the purposes of this research project.                                                                                   |\n| Who funded the creation of the dataset?                                                                                      | The creation of the dataset was funded by the coauthors' grants and employers, as fur- ther described in section 5.                                                               |\n| Any other comment?                                                                                                           | Any ot",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 113,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-114",
      "content": "|\n| Any other comment?                                                                                                           | Any other comment?                                                                                                                                                                |\n| COMPOSITION                                                                                                                  | COMPOSITION                                                                                                                                                                       |\n| What do the instances that comprise the dataset represent?                                                                   | Instances are text-only documents.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 114,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-115",
      "content": "|\n| What do the instances that comprise the dataset represent?                                                                   | Instances are text-only documents.                                                                                                                                                |\n| How many instances are there in total?                                                                                       | We detail fine-grained token counts else- where in this paper.                                                                                                                    |\n| Does the dataset contain all possible in- stances or is it a sample (not necessarily random) of instances from a larger set? | Our dataset is filtered based on our assess- ments of quality for the language modeling task. More detail on methodology can be found in Appendix B.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 115,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-116",
      "content": "cessarily random) of instances from a larger set? | Our dataset is filtered based on our assess- ments of quality for the language modeling task. More detail on methodology can be found in Appendix B.                              |\n| What data does each instance consist of?                                                                                     | Each instance is a text-only document, alongside metadata about its originating split and filename or location.                                                                   |\n| Is there a label or target associated with each instance?                                                                    | No.                                                                                                                                                                               |\n| Is any information missing from individ- ual instances?                                                                      | Yes, we filter undesi",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 116,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-117",
      "content": "|\n| Is any information missing from individ- ual instances?                                                                      | Yes, we filter undesired noise, such as base64-encoded images, from some doc- uments.                                                                                             |\n| Are relationships between individual in- stances made explicit?                                                              | No.                                                                                                                                                                               |\n| Are there recommended data splits?                                                                                           | Yes, we release a canonical train, validation, and test split of the dataset, which we follow in this work.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 117,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-118",
      "content": "| Yes, we release a canonical train, validation, and test split of the dataset, which we follow in this work.                                                                       |\n| Are there any errors, sources of noise, or redundancies in the dataset?                                                      | We make our best efforts to remove errors or sources of noise, but our dataset will naturally contain documents with errors or noise, and may contain near-duplicate doc- uments. |\n| Is the dataset self-contained, or does it link to or otherwise rely on external re- sources?                                 | The dataset is self-contained, but can also be reconstructed based on external publicly available data sources and datasets follow- ing our instructions.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 118,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-119",
      "content": "re- sources?                                 | The dataset is self-contained, but can also be reconstructed based on external publicly available data sources and datasets follow- ing our instructions.                         |\n| Does the dataset contain data that might be considered confidential?                                                         | All documents in Proof - Pile - 2 are publicly available online.                                                                                                                  |\n\n| Does the dataset contain data that, if viewed directly, might be offensive, in- sulting, threatening, or might otherwise cause anxiety?   | We estimate toxic content to be less preva- lent in our dataset than other more general web-based datasets, due to its technical fo- cus. However, it is likely to contain such content.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 119,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-120",
      "content": "se anxiety?   | We estimate toxic content to be less preva- lent in our dataset than other more general web-based datasets, due to its technical fo- cus. However, it is likely to contain such content.                                                               |\n|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| COLLECTION                                                                                                                                | COLLECTION",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 120,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-121",
      "content": "LLECTION                                                                                                                                                                                                                                             |\n| How was the data associated with each instance acquired?                                                                                  | Data was largely sourced from existing pub- lic subsets, such as the RedPajama dataset (Computer, 2023), OpenWebMath dataset (Paster et al., 2023), and via filtering the Stack (Kocetkov et al., 2022). Some data was collected using the Github API. |\n| What mechanisms or procedures were used to collect the data?                                                                              | See above.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 121,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-122",
      "content": "|\n| If the dataset is a sample from a larger set, what was the sampling strategy?                                                             | We release the entirety of the dataset fol- lowing the application of our quality filters. We randomly held out validation and test splits from the dataset.                                                                                           |\n| Who was involved in the data collec- tion process and how were they compen- sated?                                                        | The authors of this paper participated in lo- cating, retrieving, and filtering the dataset.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 122,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-123",
      "content": "lec- tion process and how were they compen- sated?                                                        | The authors of this paper participated in lo- cating, retrieving, and filtering the dataset.                                                                                                                                                           |\n| Over what timeframe was the data col- lected?                                                                                             | This data was collected in 2023, with a cut- off date of April 2023 for all subsets with the exception of our Lean proofstep data.                                                                                                                     |\n| Were any ethical review processes con- ducted?                                                                                            | Yes, the authors conducted an informal eth- ical review internally.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 123,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-124",
      "content": "y ethical review processes con- ducted?                                                                                            | Yes, the authors conducted an informal eth- ical review internally.                                                                                                                                                                                    |\n| PREPROCESSING                                                                                                                             | PREPROCESSING                                                                                                                                                                                                                                          |\n| Was any preprocessing/cleaning/labeling of the data done?                                                                                 | Yes, the authors extensively filtered the dataset subsets in keeping with our expec-",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 124,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-125",
      "content": "aning/labeling of the data done?                                                                                 | Yes, the authors extensively filtered the dataset subsets in keeping with our expec- tations for high-quality language modeling data in our domain. See Appendix B for further detail on filtering steps taken.                                        |\n| Was the 'raw' data saved in addition to the preprocessed/cleaned/labeled data?                                                            | Raw data can be accessed via reuse of our provided codebase.                                                                                                                                                                                           |\n| Is the software that was used to prepro- cess/clean/label the data available?                                                             | Yes.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 125,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-126",
      "content": "|\n| Is the software that was used to prepro- cess/clean/label the data available?                                                             | Yes. We release our codebase, which can be used to reproduce our dataset and its con- struction process, at https://github. com/EleutherAI/math-lm .                                                                                                   |\n| USES                                                                                                                                      | USES                                                                                                                                                                                                                                                   |\n| Has the dataset been used for any tasks already?                                                                                          | Yes, this dataset has",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 126,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-127",
      "content": "|\n| Has the dataset been used for any tasks already?                                                                                          | Yes, this dataset has been used to train the LLEMMA language models as a domain adaptation and continued pretraining cor- pus.                                                                                                                         |\n| Is there a repository that links to any or all papers or systems that use the dataset?                                                    | No.                                                                                                                                                                                                                                                    |\n| What (other) tasks could the dataset be used for?                                                                                         | The dataset was specifically targeted as",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 127,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-128",
      "content": "|\n| What (other) tasks could the dataset be used for?                                                                                         | The dataset was specifically targeted as a high quality language modeling corpus for the mathematics domain, but may be useful for general-purpose language modeling or unforeseen other downstream uses.                                              |\n\nTable 10: Datasheet for Proof -Pile -2 , following the framework introduced by Gebru et al. (2021).\n\n| Is there anything about the composition of the dataset or the way it was col- lected and preprocessed/cleaned/labeled that might impact future uses?   | We filtered the dataset with the intent of creating a model useful for mathematical tasks with solely English text.                                                             |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------|--",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 128,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-129",
      "content": "|\n|--------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Are there tasks for which the dataset should not be used?                                                                                              | The dataset should not be used with the intent to cause harm or for models intended for the purposes of harm.                                                                   |\n| DISTRIBUTION                                                                                                                                           | DISTRIBUTION",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 129,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-130",
      "content": "| DISTRIBUTION                                                                                                                                                                    |\n| Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?                                     | We make the dataset publicly available for reproducibility, analysis, and other further downstream uses.                                                                        |\n| How will the dataset will be distributed?                                                                                                              | We provide code to replicate the dataset, and release it via the Huggingface Hub.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 130,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-131",
      "content": "ibuted?                                                                                                              | We provide code to replicate the dataset, and release it via the Huggingface Hub.                                                                                               |\n| When will the dataset be distributed?                                                                                                                  | The dataset is available immediately.                                                                                                                                           |\n| Will the dataset be distributed under a copyright or other intellectual prop- erty (IP) license, and/or under applicable terms of use (ToU)?           | We do not relicense the dataset's compo- nents, and do not impose our own use re- strictions.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 131,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-132",
      "content": "ght or other intellectual prop- erty (IP) license, and/or under applicable terms of use (ToU)?           | We do not relicense the dataset's compo- nents, and do not impose our own use re- strictions.                                                                                   |\n| Have any third parties imposed IP-based or other restrictions on the data associ- ated with the instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| Do any export controls or other regula- tory restrictions apply to the dataset or to individual instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| MAINTENANCE",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 132,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-133",
      "content": "|\n| MAINTENANCE                                                                                                                                            | MAINTENANCE                                                                                                                                                                     |\n| Who will be supporting/hosting/main- taining the dataset?                                                                                              | The dataset will be hosted on the Hug- gingFace Hub and able to be recreated via code at https://github.com/ EleutherAI/math-lm . The dataset will not be updated post-release.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 133,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-134",
      "content": "| The dataset will be hosted on the Hug- gingFace Hub and able to be recreated via code at https://github.com/ EleutherAI/math-lm . The dataset will not be updated post-release. |\n| How can the owner/curator/manager of the dataset be contacted?                                                                                         | Via email at za2514@princeton.edu                                                                                                                                               |\n| Is there an erratum?                                                                                                                                   | No.                                                                                                                                                                             |\n| Will the dataset be updated?",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 134,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-135",
      "content": "|\n| Will the dataset be updated?                                                                                                                           | No.                                                                                                                                                                             |\n| If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?                                           | No.                                                                                                                                                                             |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "E DATASHEET",
        "chunkIndex": 135,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-136",
      "content": "Table 11 shows additional results on Isabelle proof autoformalization, including the union of theorems closed by Sledgehammer and the given language model.\n\nTable 11: Isabelle autoformalization . ∗ We exclude the 11 examples used in the few-shot prompts. Pass@1 with greedy decoding.\n\n| Method                       | Autoformalization pass@1   | Autoformalization pass@1   |\n|------------------------------|----------------------------|----------------------------|\n|                              | miniF2F-valid ∗            | miniF2F-test               |\n| Sledgehammer                 | 14.72%                     | 20.49%                     |\n| Code Llama 7b                | 16.31%                     | 17.62%                     |\n| LLEMMA-7b                    | 20.60%                     | 22.13%                     |\n| Code Llama 7b ∪ Sledgehammer | 20.17%                     | 25.00%                     |\n| LLEMMA-7b ∪ Sledgehammer     | 25.97%                     | 27.46%",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "F.1 PROOF AUTOFORMALIZATION",
        "chunkIndex": 136,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-137",
      "content": "| 22.13%                     |\n| Code Llama 7b ∪ Sledgehammer | 20.17%                     | 25.00%                     |\n| LLEMMA-7b ∪ Sledgehammer     | 25.97%                     | 27.46%                     |",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "F.1 PROOF AUTOFORMALIZATION",
        "chunkIndex": 137,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-138",
      "content": "A full exploration of finetuning applications for LLEMMA, such as instruction following (Ouyang et al., 2022; Wei et al., 2022), dialogue modeling (Thoppilan et al., 2022; Touvron et al., 2023; Collins et al., 2023), and reward modeling (Cobbe et al., 2021; Lightman et al., 2023) are outside the scope of this work. However, to establish that LLEMMA retains its advantage over other open models when finetuned, we conduct preliminary experiments finetuning LLEMMA-7B on MetaMathQA (Yu et al., 2023), a supervised dataset targeted at the MATH and GSM8k benchmarks. Results are shown in Table 12.\n\nTable 12: Finetuning of various 7B base models on supervised mathematics datasets. All results with a Llama 2 initialization are copied from the literature (Luo et al., 2023; Yu et al., 2023). The LLEMMA 7B finetune is trained with identical hyperparameters to the models in Yu et al. (2023)",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "G SUPERVISED FINETUNING",
        "chunkIndex": 138,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-139",
      "content": "lts with a Llama 2 initialization are copied from the literature (Luo et al., 2023; Yu et al., 2023). The LLEMMA 7B finetune is trained with identical hyperparameters to the models in Yu et al. (2023)\n\n| Initialization   | Finetune Dataset         | MATH   | GSM8k   |\n|------------------|--------------------------|--------|---------|\n| Llama 2 7B       | WizardMath (Proprietary) | 10.7%  | 54.9%   |\n| Llama 2 7B       | MetaMathQA               | 19.4%  | 66.4%   |\n| LLEMMA 7B        | MetaMathQA               | 25.2%  | 66.5%   |\n| Llama 2 70B      | WizardMath (Proprietary) | 22.7%  | 81.6%   |\n| Llama 2 70B      | MetaMathQA               | 26.6 % | 82.3 %  |\n\n.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "G SUPERVISED FINETUNING",
        "chunkIndex": 139,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-140",
      "content": "Dataset overlap. Figure 6 shows example false positives when checking n -gram overlap with OpenWebMath documents for various n . Figure 7 shows an example OpenWebMath document that has 30-gram overlap with a MATH problem, and LLEMMA-7b's generated solution.\n\nTask outputs. Figure 8 shows a generated proof in the informal2formal theorem proving task.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "H QUALITATIVE EXAMPLES",
        "chunkIndex": 140,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-141",
      "content": "2D affine transformations can be better represented using 2 by 2 matrices, since they are simply linear combinations of 2 variables. The advantage of this is that the matrices are associative under multiplication Also, GPUs and modern toolkits are optimised to work with this representation. As a result, a scale matrix is \\begin{bmatrix} s\\_x &amp; 0 \\\\ 0 &amp; s\\_y \\end{bmatrix}, and a rotation matrix is \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}.\n\nA translation matrix is simply \\begin{bmatrix} 1 &amp; \\frac{t\\_x}{y} \\\\ \\frac{t\\_y}{x} &amp; 1 ...",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "OpenWebMath document",
        "chunkIndex": 141,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-142",
      "content": "<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "MATH problem",
        "chunkIndex": 142,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-143",
      "content": "\\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Hit",
        "chunkIndex": 143,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-144",
      "content": "A number is selected at random from 1 through 100, inclusive. What is the probability that the number is a divisor of 50? Express your answer as a common fraction.\n\n<!-- formula-not-decoded -->\n\nThere are a 100 integers between 1-100, inclusive. Since 50 is $$2*5^2$$, it has $$(1+1)(1+2)=(2)(3)=6$$ factors. Thus, the answer is $$\\frac{6}{100}=\\boxed{\\frac{3}{50}}.$$",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Basic Probability",
        "chunkIndex": 144,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-145",
      "content": "A number is selected at random from 1 through 100, inclusive. What is the probability that the number is a perfect square?",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "MATH problem",
        "chunkIndex": 145,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-146",
      "content": "A number is selected at random from 1 through 100, inclusive. What is the probability that the number is a",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Hit",
        "chunkIndex": 146,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-147",
      "content": "Fig. 2.\n\nUse values of the most used medicinal plants in the Safi Province (Morocco).\n\nIt is also important to note that for the abovementioned medicinal plants, many other folk uses have been reported in different regions of Morocco. Furthermore, literature-based proof revealed that these species have proven a wide variety of biological and pharmacological activities (Table 4, Ref. [14, 17, 19, 20, 21, 23, 24, 26, 28, 30, 31, 34, 35, 36, 38, 39, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]), which may confirm the different popular applications of ...",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "OpenWebMath document",
        "chunkIndex": 147,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-148",
      "content": "The first 100 positive integers are 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20, 21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49, 50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78, 79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100. We can see that...\n\nFigure 6: Data overlap: Example false positives using 10-gram match between MATH solutions and OpenWebMath documents ( top ), 20-gram match between MATH problems and OpenWebMath documents ( middle ), and 30-gram match between LLEMMA-7b's generated solutions and OpenWebMath documents ( bottom ).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Generated solution (LLEMMA 7b):",
        "chunkIndex": 148,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-149",
      "content": "```\nA triangle is formed with edges along the line $y=\\frac{2}{3}x+5$, the $x$-axis, and the line $x=k$. If the area of the triangle is less than $20$, find the sum of all possible integral values of $k$. Feb 28, 2018 Look at the graph, here...two triangles are possible : https://www.desmos.com/calculator/m6wnjpgldq The height of the triangles at any point will be formed by [ (2/3)x + 5 ] And the bases will be [ x -(- 7.5)] = [ x + 7.5] So....we want to solve this (1/2) [ (2/3)x + 5 ] [ x + 7.5 ] = 20 [ (2/3)x + 5 ] [ x + 7.5] = 40 (2/3)x^2 + 5x + 5x + 37.5 = 0 (2/3)x^2 + 10x -2.5 = 0 Using a little technology.....the max x value for the triangle formed above the x axis will be = .246 And the min x value for the triangle formed below the x axis will be = -15.246 With the given boundaries, the integer sums of all possible x values of k giving triangles with an area < 20 units^2 = [ (-15) + (-14 ) + (-13) + ...+ ( -2) + ( -1 ) + 0 ] = -(15) (16) / 2 = -120\n```\n\nFeb 28, 2018",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "OpenWebMath document",
        "chunkIndex": 149,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-150",
      "content": "n boundaries, the integer sums of all possible x values of k giving triangles with an area < 20 units^2 = [ (-15) + (-14 ) + (-13) + ...+ ( -2) + ( -1 ) + 0 ] = -(15) (16) / 2 = -120\n```\n\nFeb 28, 2018\n\nMATH Problem: A triangle is formed with edges along the line y = 2 3 x +5 , the x -axis, and the line x = k . If the area of the triangle is less than 20 , find the sum of all possible integral values of k . Answer: -120",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "OpenWebMath document",
        "chunkIndex": 150,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-151",
      "content": "The triangle has vertices ( k, 0) , (0 , 5) , and ( k, 2 3 k +5) . The base of the triangle has length k , and the height of the triangle has length 2 3 k +5 . The area of the triangle is therefore\n\n<!-- formula-not-decoded -->\n\nSince the area of the triangle is less than 20 , we have\n\n<!-- formula-not-decoded -->\n\nThis inequality is true if and only if 1 6 k 2 + 5 4 k -10 is positive. Since 1 6 k 2 + 5 4 k -10 is a quadratic with positive leading coefficient, it is positive if and only if its vertex is less than 0 . The vertex of 1 6 k 2 + 5 4 k -10 is ( -10 3 , -125 18 ) . Since -10 3 &lt; 0 , we have that 1 6 k 2 + 5 4 k -10 &gt; 0 if and only if k &lt; -10 3 . Therefore, the only possible integral values of k are k = 0 , 1 , 2 , 3 , 4 . The sum of these values is 10 . Final Answer: The final answer is 10 .\n\nFigure 7: Data overlap: Example OpenWebMath document that has a 30-gram overlap with the given MATH problem, and LLEMMA-7b's generated solution.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Generated solution (LLEMMA 7b):",
        "chunkIndex": 151,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-152",
      "content": "values is 10 . Final Answer: The final answer is 10 .\n\nFigure 7: Data overlap: Example OpenWebMath document that has a 30-gram overlap with the given MATH problem, and LLEMMA-7b's generated solution.\n\n```\nProblem: If 3 a + b + c = -3 , a +3 b + c = 9 , a + b +3 c = 19 , then find abc . Show that it is -56. Informal Proof (Human-written): Summing all three equations yields that 5 a +5 b +5 c = -3 + 9 + 19 = 25 . Thus, a + b + c = 5 . Subtracting this from each of the given equations, we obtain that 2 a = -8 , 2 b = 4 , 2 c = 14 . Thus, a = -4 , b = 2 , c = 7 , and their product is abc = -4 × 2 × 7 = -56 . Formal Statement and Proof: theorem mathd_algebra_338: fixes a b c :: real assumes \"3 * a + b + c = -3\" and \"a + 3 * b + c = 9\" and \"a + b + 3 * c = 19\" shows \"a * b * c = -56\" proof -(* Summing all three equations yields that 5a + 5b + 5c = -3 + 9 + 19 = 25. Thus, a + b + c = 5.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Generated solution (LLEMMA 7b):",
        "chunkIndex": 152,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-153",
      "content": "es \"3 * a + b + c = -3\" and \"a + 3 * b + c = 9\" and \"a + b + 3 * c = 19\" shows \"a * b * c = -56\" proof -(* Summing all three equations yields that 5a + 5b + 5c = -3 + 9 + 19 = 25. Thus, a + b + c = 5. * ) have \"5 * a + 5 * b + 5 * c = -3 + 9 + 19\" using assms < ATP > then have \"5 * (a + b + c) = 25\" < ATP > then have \"a + b + c = 5\" < ATP > (* Subtracting this from each of the given equations , we obtain that 2a = -8, 2b = 4, 2c = 14. Thus, a = -4, b = 2, c =7, and their product is abc = -4 \\times 2 \\times 7 = -56. * ) then have \"2 * a = -8\" \"2 * b = 4\" \"2 * c = 14\" using assms < ATP > then have \"a = -4\" \"b = 2\" \"c = 7\" < ATP > then show ?thesis < ATP > qed\n```\n\nFigure 8: Informal-to-formal proving . The model is given the problem, informal proof, and formal statement, following Jiang et al. (2023).",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Generated solution (LLEMMA 7b):",
        "chunkIndex": 153,
        "totalChunks": 155
      }
    },
    {
      "id": "2310.10631v3-chunk-154",
      "content": "\" \"b = 2\" \"c = 7\" < ATP > then show ?thesis < ATP > qed\n```\n\nFigure 8: Informal-to-formal proving . The model is given the problem, informal proof, and formal statement, following Jiang et al. (2023). It generates a formal proof (starting with proof -) containing Isabelle code, comments ( (*...*) ) that align the informal and formal proofs, and calls to an automated prover (shown as &lt;ATP&gt; ). The proof is from LLEMMA-7b with greedy decoding.",
      "metadata": {
        "source": "arxiv:2310.10631v3",
        "title": "Llemma: An Open Language Model For Mathematics",
        "authors": [
          "Zhangir Azerbayev",
          "Hailey Schoelkopf",
          "Keiran Paster",
          "Marco Dos Santos",
          "Stephen McAleer",
          "Albert Q. Jiang",
          "Jia Deng",
          "Stella Biderman",
          "Sean Welleck"
        ],
        "section": "Generated solution (LLEMMA 7b):",
        "chunkIndex": 154,
        "totalChunks": 155
      }
    }
  ],
  "fullText": "## LLEMMA: AN OPEN LANGUAGE MODEL FOR MATHEMATICS\n\nZhangir Azerbayev 1 , 2 Hailey Schoelkopf 2 Keiran Paster 3 , 4 Marco Dos Santos 5 Stephen McAleer 6 Albert Q. Jiang 5 Jia Deng 1 Stella Biderman 2 Sean Welleck 6 , 7\n\n1 Princeton University 2 EleutherAI 3 University of Toronto 4 Vector Institute 5 University of Cambridge 6 Carnegie Mellon University 7 University of Washington\n\n## ABSTRACT\n\nWe present LLEMMA, a large language model for mathematics. We continue pretraining Code Llama on Proof -Pile -2 , a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding LLEMMA. On the MATH benchmark LLEMMA outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, LLEMMA is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof -Pile -2 , and code to replicate our experiments. 1\n\n## 1 INTRODUCTION\n\nLanguage models trained on diverse mixtures of text display remarkably general language understanding and generation capabilities (Brown et al., 2020; Chowdhery et al., 2022), serving as base models that are adapted to a wide range of applications (Raffel et al., 2023). Applications such as open-ended dialogue (Thoppilan et al., 2022; Touvron et al., 2023) or instruction following (Ouyang et al., 2022; Wei et al., 2022) require balanced performance across the entire distribution of natural text, thus favoring generalist models . However, if we seek to maximize performance within one domain, such as medicine (Singhal et al., 2022; 2023), finance (Wu et al., 2023), or science (Taylor et al., 2022), a domain-specific language model may offer superior capabilities for a given computational cost, or lower computational cost for a given level of capability.\n\nIn this work, we train a domain-specific language model for mathematics. We have several motivations\n\nFigure 1: Continued pretraining on Proof -Pile -2 yields LLEMMA, a base model with improved mathematical capabilities.\n\n<!-- image -->\n\nfor doing so. First, solving mathematical problems requires pattern matching against a large body of specialized prior knowledge, thus serving as an ideal setting for domain adaptation. Second, mathematical reasoning is in itself a central AI task, its study dating back to at least Gelernter (1959) and Wang (1960) and continuing to today (Lu et al., 2023). Third, language models capable of strong mathematical reasoning are upstream of a number of research topics, such as reward modeling (Uesato et al., 2022; Lightman et al., 2023), reinforcement learning for reasoning (Polu et al., 2022; Lample et al., 2022), and algorithmic reasoning (Zhou et al., 2022; Zhang et al., 2023).\n\n1 https://github.com/EleutherAI/math-lm\n\nAlthough domain-specific models for mathematics have been trained in the past, they have either been closed access (Lewkowycz et al., 2022), limiting their ability to become a platform for further research, or have lagged far behind the closed access state-of-the-art (Azerbayev et al., 2023).\n\nWe present a recipe for adapting a language model to mathematics through continued pretraining (Lewkowycz et al., 2022; Rozière et al., 2023) on Proof -Pile -2 , a diverse mixture of math-related text and code. Applying the recipe to Code Llama (Rozière et al., 2023) yields LLEMMA: 7 billion and 34 billion parameter base language models with substantially improved mathematical capabilities.\n\nSpecifically, our contributions are as follows:\n\n1. We train and release the LLEMMA models: 7B and 34B parameter language models specialized for mathematics. The LLEMMA models are a new state-of-the-art for publicly released base models on MATH (Lewkowycz et al., 2022).\n2. We release the AlgebraicStack , a dataset of 11B tokens of code specifically related to mathematics.\n3. We demonstrate that LLEMMA is capable of using computational tools to solve mathematical problems, namely, the Python interpreter and formal theorem provers.\n4. Unlike prior mathematics language models such as Minerva (Lewkowycz et al., 2022), the LLEMMA models are open access and we open source our training data and code. This allows LLEMMA to serve as a platform for future research in mathematical reasoning.\n\nOur work builds on findings in Minerva (Lewkowycz et al., 2022), but differs in several ways: (1) LLEMMA's training and evaluation covers a wider range of data and tasks, notably code data (e.g., the AlgebraicStack ), tool use, and formal mathematics; (2) our work only depends on publicly accessible tools and data; (3) we provide new analyses related to the continued training data mixture, memorization, and additional supervised finetuning; (4) we make all artifacts publicly available.\n\n## 2 APPROACH\n\nLLEMMA models are 7 billion and 34 billion parameter language models specialized for mathematics. Our approach is to continue pretraining Code Llama (Rozière et al., 2023) on the Proof -Pile -2 .\n\nFigure 2: Comparison of LLEMMA and Minerva training\n\n|                   |                        |                        | Dataset                            | Tokens   | Open   |\n|-------------------|------------------------|------------------------|------------------------------------|----------|--------|\n| Model             | Adaptation tokens Open | Adaptation tokens Open | Minerva Dataset                    | 38.5B    | ✗      |\n| Minerva-8b        | 164B                   | ✗                      | Proof - Pile - 2 (ours)            | 55B      | ✓      |\n| Minerva-62b       | 109B                   | ✗                      | Code ( AlgebraicStack )            | 11B      | ✓      |\n| LLEMMA-7b (ours)  | 200B                   | ✓                      | OpenWebMath (Paster et al., 2023)) | 15B      | ✓      |\n| LLEMMA-34b (ours) | 50B                    | ✓                      | ArXiv (Computer, 2023))            | 29B      | ✓      |\n\n## 2.1 DATA: Proof -Pile -2\n\nWe form the Proof -Pile -2 , a 55B-token mixture of scientific papers, web data containing mathematics, and mathematical code. With the exception of the Lean proofsteps subset (see Appendix B), the Proof -Pile -2 has a knowledge cutoff of April 2023.\n\nCode. Computational tools such as numerical simulations, computer algebra systems, and formal theorem provers are of ever increasing importance to mathematicians (Avigad, 2018). Motivated by this fact, we create AlgebraicStack , an 11B-token dataset of source code from 17 languages, spanning numerical, symbolic, and formal math. The dataset consists of filtered code from the Stack (Kocetkov et al., 2022), public GitHub repositories, and formal proofstep data. Table 9 shows the number of tokens by language in AlgebraicStack . See Appendix B.1 for further details on AlgebraicStack .\n\nWeb data. We use OpenWebMath (Paster et al., 2023), a 15B-token dataset of high-quality web pages filtered for mathematical content. OpenWebMath filters CommonCrawl web pages based\n\non math-related keywords and a classifier-based math score, preserves mathematical formatting (e.g., L A T E X, AsciiMath), and includes additional quality filters (e.g., perplexity, domain, length) and near-deduplication. Refer to Paster et al. (2023) for a full description of OpenWebMath.\n\nScientific papers. We use the ArXiv subset of RedPajama (Computer, 2023), an open-access reproduction of the LLaMA training dataset. The ArXiv subset contains 29B tokens.\n\nGeneral natural language and code data. Following Lewkowycz et al. (2022), our training mixture consists of a small amount of general domain data, which functions as a form of regularization. Since the pretraining dataset for LLaMA 2 is undisclosed, we use the Pile (Gao et al., 2020; Biderman et al., 2022) as a surrogate training dataset. We set 95% of our training mixture to be the Proof -Pile -2 , 2% to be from the Pile (with ArXiv removed, as it is separately in Proof -Pile -2 ), and 3% to be the GitHub subset of RedPajama (Computer, 2023).\n\nFurther information on dataset composition and a datasheet are in Appendix B and Appendix E, respectively. We publicly release Proof -Pile -2 at hf.co/datasets/EleutherAI/proof-pile-2 .\n\n## 2.2 MODEL AND TRAINING\n\nEach model is initialized from Code Llama (Rozière et al., 2023). Code Llama models are decoderonly transformer language models initialized from Llama 2 (Touvron et al., 2023) and further trained on 500B tokens of code. We continue training the Code Llama models on Proof -Pile -2 using a standard autoregressive language modeling objective. We train the 7B model for 200B tokens, and the 34B model for 50B tokens.\n\nWe train all models in bfloat16 mixed precision using the GPT-NeoX library (Andonian et al., 2023) across 256 A100 40GB GPUs. We use Tensor Parallelism (Shoeybi et al., 2019) with a world size of 2 for LLEMMA-7B , and a world size of 8 for LLEMMA-34B, alongside ZeRO Stage 1 sharded optimizer states (Rajbhandari et al., 2020) across Data Parallel (Goyal et al., 2017) replicas. We use Flash Attention 2 (Dao, 2023) to improve throughput and further reduce memory requirements.\n\nLLEMMA 7B is trained for 42 , 000 steps with a global batch size of 4 million tokens and a 4096 token context length. This corresponds to roughly 23 , 000 A100-hours. The learning rate is warmed up to 1 · 10 -4 over 500 steps, then set to cosine decay to 1 / 30 th of the maximum learning rate over 48 , 000 steps. The reason for the discrepancy between the number of training steps and the scheduler length is that we planned to train for 48 , 000 steps, but encountered NaN losses after step 42 , 000 , likely caused by unstable optimization or hardware failures (Elsen et al., 2023).\n\nLLEMMA 34B is trained for 12 , 000 steps with a global batch size of 4 million tokens and a 4096 context length. This corresponds to roughly 47 , 000 A100-hours. The learning rate is warmed up to 5 · 10 -5 over 500 steps, then decayed to 1 / 30 th the peak learning rate.\n\nBefore training LLEMMA 7B, we contract the RoPE (Su et al., 2022) base period of the Code Llama 7B initialization from θ = 1 , 000 , 000 to θ = 10 , 000 . This is so that the long context finetuning procedure described in Peng et al. (2023)and Rozière et al. (2023) can be repeated on the trained LLEMMA 7B (we leave actually doing so to future work). Due to compute constraints, we were unable to verify that training LLEMMA 34B with a contracted RoPE base period did not come with a performance penalty, therefore for that model we preserved θ = 1 , 000 , 000 .\n\n## 3 EVALUATION\n\nOur goal is to evaluate LLEMMA as a base model for mathematical text. To this end, we compare LLEMMA models using few-shot evaluation (Brown et al., 2020), and primarily focus on state-of-theart models that have not been finetuned on supervised examples for the task. First, we evaluate the model's ability to solve mathematics problems using chain of thought reasoning (Wei et al., 2023) and majority voting (Wang et al., 2023). Our evaluations include MATH (Hendrycks et al., 2021b) and GSM8k (Cobbe et al., 2021), the de-facto standard benchmarks for evaluating quantitative reasoning in language models (Lewkowycz et al., 2022). Second, we explore few-shot tool use and formal theorem proving. Third, we study the effects of memorization and the data mixture. Appendix G contains a preliminary study of supervised finetuning with LLEMMA.\n\n## 3.1 CHAIN-OF-THOUGHT MATHEMATICAL PROBLEM SOLVING\n\nThese tasks involve generating self-contained text solutions to problems expressed in L A T E X or natural language, without using external tools (Lewkowycz et al., 2022). We use the following evaluation:\n\n- MATH (Hendrycks et al., 2021b), a dataset with 12.5k problems (5k evaluation) from high-school math competitions. Given a problem statement, the model generates a L A T E Xsolution and an answer that must match a reference answer. We follow a similar task implementation to Lewkowycz et al. (2022), using their four-example prompt and evaluating answers for exact string match or SymPy equivalence.\n- GSM8k (Cobbe et al., 2021), a dataset of middle-school level math word problems. We use the 8-shot prompt from Wei et al. (2023), as Lewkowycz et al. (2022) do not specify their evaluation prompt or number of few-shot examples.\n- OCWCourses (Lewkowycz et al., 2022), a collection of undergraduate-level STEM problems harvested from MIT's OpenCourseWare. We use the four-example prompt provided by (Lewkowycz et al., 2022).\n- MMLU-STEM (Hendrycks et al., 2021a), a subset of 18 out of 57 subjects in the MMLU benchmark. We follow Lewkowycz et al. (2022) and use their provided four-example chain-ofthought prompt.\n- SAT , we create a dataset consisting of the 32 math questions that do not contain figures from the May 2023 College Board SAT examination, which is after our model's knowledge cutoff.\n\nFigure 3: Example of a LLEMMA 34B solution to a MATH (Hendrycks et al., 2021a) problem. This problem is tagged with difficulty level 5, the highest in MATH. The model was conditioned on the 4-shot prompt described in subsection 3.1, and the solution was produced by greedy decoding. The model had to apply two nontrivial steps to solve this problem: (1) noticing that swapping the order of summation simplifies the problem, and (2) noticing that the resulting sum telescopes.\n\n<!-- image -->\n\nWe compare with Minerva (Lewkowycz et al., 2022), which continued pretraining the PaLM language model on a dataset of technical content; Code Llama, the initialization of LLEMMA's continued pretraining; and Llama 2, the initialization of Code Llama's continued pretraining on code. For open access models, we report scores computed using our evaluation suite, which is implemented as a fork of the Language Model Evaluation Harness (Gao et al., 2021). For Minerva models, we report benchmark scores from Lewkowycz et al. (2022).\n\nResults. LLEMMA's continued pretraining on Proof -Pile -2 improves few-shot performance on the five mathematical benchmarks. LLEMMA 34B improves over Code Llama by 20 percentage points on GSM8k and 13 points on MATH, and LLEMMA 7B outperforms the proprietary Minerva model. Our approach also outperforms all open-weight language models at the time of writing. We conclude that continued pretraining on Proof -Pile -2 is effective for improving a pretrained model's ability to perform mathematical problem solving.\n\nLLEMMA is pretrained on a diverse distribution of mathematics-related data, and is not tuned for a particular task. Therefore, we expect that LLEMMA can adapt to many other tasks via task-specific finetuning and few-shot prompting.\n\nTable 1: Results on our five chain-of-thought reasoning tasks with samples generated via greedy decoding. Minerva results are quoted from Lewkowycz et al. (2022). Note that CodeLlama 7B performs worse than random guessing (25%) on MMLU and SAT, largely due to failing to conclude its chain of thought with a valid answer.\n\n|            |      | GSM8k   | OCW   | MMLU-STEM   | SAT    | MATH   |\n|------------|------|---------|-------|-------------|--------|--------|\n| Llama 2    | 7B   | 11.8%   | 3.7%  | 29.9%       | 25.0%  | 3.2%   |\n| Code Llama | 7B   | 10.5%   | 4.4%  | 25.1%       | 9.4%   | 4.5%   |\n| Minerva    | 8B   | 16.2%   | 7.7%  | 35.6%       | -      | 14.1%  |\n| LLEMMA     | 7B   | 36.4%   | 7.7%  | 37.7%       | 53.1 % | 18.0 % |\n| Code Llama | 34B  | 29.6%   | 7.0%  | 40.5%       | 40.6%  | 12.2%  |\n| LLEMMA     | 34B  | 51.5%   | 11.8% | 49.0%       | 71.9 % | 25.0%  |\n| Minerva    | 62B  | 52.4%   | 12.0% | 53.9%       | -      | 27.6%  |\n| Minerva    | 540B | 58.8%   | 17.6% | 63.9%       | -      | 33.6%  |\n\nTable 2: Majority voting results for LLEMMA and Minerva. Minerva results are quoted from Lewkowycz et al. (2022). Voting is done with k = 256 for MATH, k = 100 for GSM8k and OCW, and k = 16 for MMLU-STEM and SAT. We sample with temperature T = 0 . 6 for k = 256 and k = 100 and T = 0 . 3 for k = 16 , and use nucleus sampling with p = 0 . 95 (Holtzman et al., 2020). Due to compute constraints, we do not calculate majority voting scores for Llama 2 and Code Llama.\n\n|         |      | GSM8k maj@ k   | OCW maj@ k   | MMLU-STEM maj@ k   | SAT maj@ k   | MATH maj@ k   |\n|---------|------|----------------|--------------|--------------------|--------------|---------------|\n| Minerva | 8B   | 28.4%          | 12.5%        | 43.4%              | -            | 25.4%         |\n| LLEMMA  | 7B   | 54.0%          | 14.3%        | 49.9%              | 78.1 %       | 33.5%         |\n| LLEMMA  | 34B  | 69.3 %         | 18.4 %       | 59.7 %             | 81.3 %       | 43.1 %        |\n| Minerva | 62B  | 68.5%          | 23.5%        | 63.5%              | -            | 43.4%         |\n| Minerva | 540B | 78.5%          | 30.8%        | 75.0%              | -            | 50.3%         |\n\n## 3.2 MATHEMATICAL PROBLEM SOLVING WITH TOOL USE\n\nThese tasks involve solving problems with access to computational tools. We evaluate the following:\n\n- MATH+Python , the model is prompted to alternately describe a solution step in natural language, then execute that step with code. The final answer is a program that executes to a numeric type or a SymPy object. Our few-shot prompt includes examples that use built-in numeric operations, the math module, and SymPy .\n- GSM8k+Python , solving a GSM8k word problem by writing a Python program that executes to an integer answer. We use the prompt from Gao et al. (2023).\n\nResults. As seen in Table 3, LLEMMA improves over Code Llama on both tasks. Its performance on MATH and GSM8k with tools is also higher than its performance on these datasets without tools.\n\nTable 3: Mathematical problem solving with tool use.\n\n|            |     | GSM8k+Python pass@1   | MATH+Python pass@1   |\n|------------|-----|-----------------------|----------------------|\n| Code Llama | 7B  | 27.1%                 | 17.2%                |\n| LLEMMA     | 7B  | 40.1%                 | 21.5%                |\n| Code Llama | 34B | 52.7%                 | 23.5%                |\n| LLEMMA     | 34B | 62.6%                 | 27.1%                |\n\n## 3.3 FORMAL MATHEMATICS\n\nInteractive proof assistants such as Lean (de Moura et al., 2015), Isabelle (Wenzel et al., 2008), and Coq (Paulin-Mohring, 1989a;b) express mathematics in programming languages that allow for verification. These languages are data scarce compared to mainstream languages, especially in the context of pretraining. For instance, the Stack dataset used to pretrain language models in the BigCode project (Allal et al., 2023) has over 700 gigabytes of Python, compared to 322 megabytes of Lean. Proof assistants also require models to leverage information that is not present in raw source code, such as goal states that contain information about each step of a proof.\n\nProblem (MATH Number theory 185): When a number is divided by 5, the remainder is 3. What is the remainder when twice the number is divided by 5? Show that it is 1.\n\nHuman-written informal proof: If our number is n , then n ≡ 3 (mod 5)\n\n<!-- formula-not-decoded -->\n\nThe remainder is 1 when the number is divided by 5.\n\n```\nInformal-to-formal (Isabelle): {Problem, human-written informal proof} theorem mathd_numbertheory_185: fixes n ::nat assumes \"n mod 5 = 3\" shows \"(2 * n) mod 5 = 1\" proof -have \"2 * n = n + n\" < ATP > also have \" . . . mod 5 = (n mod 5 + n mod 5) mod 5\" < ATP > also have \" . . . = (3 + 3) mod 5\" using assms < ATP > also have \" . . . = 1\" < ATP > finally show ?thesis < ATP > qed\n```\n\nFigure 4: Example formal proofs from LLEMMA-7b. Left: The model is given a problem, informal proof, and formal statement, following Jiang et al. (2023). It generates a formal proof (starting with proof -) containing Isabelle code and calls to automation (shown as &lt;ATP&gt; ). Right: The model is given a proof state, visualized as a grey comment, and generates the subsequent step (e.g. rw [.. ).\n\n```\nFormal-to-formal (Lean 4): theorem mathd_numbertheory_185 (n : N ) (h 0 : n % 5 = 3) : 2 * n % 5 = 1 := by --INPUT (step 1): --n: N --h 0 : n % 5 = 3 --⊢ 2 * n % 5 = 1 rw [mul_mod, h 0 ] --INPUT (step 2): --n: N --h 0 : n % 5 = 3 --⊢ 2 % 5 * 3 % 5 = 1 simp only [h 0 , mul_one]\n```\n\nProof -Pile -2 's AlgebraicStack contains over 1.5 billion tokens of formal mathematics data, including proof states extracted from Lean and Isabelle formalizations. While a full investigation of formal math is outside the scope of this paper, we evaluate LLEMMA few-shot on two tasks:\n\n. This tells us that .\n\n- Informal-to-formal proving (Jiang et al., 2023), the task of generating a formal proof, given a formal statement, an informal L A T E X statement, and an informal L A T E X proof. The formal proof is checked by the proof assistant. We use the Isabelle proof assistant and evaluate on miniF2F (Zheng et al., 2021), a benchmark consisting of problem statements from Olympiads and undergraduate coursework. For the prompt, we use 11 (formal statement, informal statement, informal proof, formal proof) examples from Jiang et al. (2023), selecting 7 examples for number theory problems, and 6 examples for all others. We generate a single proof with greedy decoding.\n- Formal-to-formal proving (e.g., Polu &amp; Sutskever (2020)), the task of proving a formal statement by generating a sequence of proof steps (tactics). At each step, the input is a state x t given by the proof assistant, and the language model's task is to generate a proof step y t (a sequence of code). The proof step is checked by the proof assistant, yielding a new state x t +1 or an error message. The process continues, stopping if a proof is completed or a timeout is reached. We prompt the model using three ( x t , y t ) examples. We evaluate on miniF2F (Zheng et al., 2021) using the Lean 4 proof assistant, and use a standard best first search. See Appendix D for more details.\n\nResults. As seen in Table 4, LLEMMA's continued pretraining on Proof -Pile -2 improved few-shot performance on the two formal theorem proving tasks.\n\n| Method         | Informal-to-formal   | Informal-to-formal   | Method                | Formal-to-formal   | Formal-to-formal   |\n|----------------|----------------------|----------------------|-----------------------|--------------------|--------------------|\n|                | miniF2F-valid        | miniF2F-test         |                       | Search             | miniF2F-test       |\n| Sledgehammer   | 14.72%               | 20.49%               | ReProver (fine-tuned) | 1 × 64             | 26.50%             |\n| Code Llama 7b  | 16.31%               | 17.62%               | Code Llama 7b         | 1 × 32             | 20.49%             |\n| Code Llama 34b | 18.45%               | 18.03%               | Code Llama 34b        | 1 × 32             | 22.13%             |\n| LLEMMA-7b      | 20.60%               | 22.13%               | COPRA (GPT-4)         | - †                | 23.36%             |\n| LLEMMA-34b     | 21.03%               | 21.31%               | LLEMMA-7b             | 1 × 32             | 26.23%             |\n|                |                      |                      | LLEMMA-34b            | 1 × 32             | 25.82%             |\n\nTable 4: Formal theorem proving tasks. Left : Informal-to-formal proving in Isabelle, showing the percentage of proven theorems with greedy decoding. Right : Formal-to-formal proving in Lean, showing the percentage of proven theorems with the given number of attempts × generations-periteration of best first search, and a 10-minute timeout. Sledgehammer (Paulson &amp; Nipkow, 2023) is built-in Isabelle automation. ReProver (Yang et al., 2023) is a supervised and retrieval-augmented model. COPRA (Thakur et al., 2023) is a retrieval-augmented GPT-4 based method. † COPRA does not use best first search, but instead samples from GPT-4 (OpenAI, 2023) a maximum of 60 times.\n\nOn informal-to-formal proving, LLEMMA-7b closes 22.1% of the theorems, improving upon its Code Llama initialization and the Sledgehammer prover. The theorems that LLEMMA proves are often complementary to those proved with Sledgehammer: taking the union of Sledgehammer and LLEMMA proofs results in 26 new validation proofs (an 11 percentage-point increase), and 17 new test proofs (a 7 point increase); see Appendix Table 11. Prior to our work, the only demonstration of few-shot proof autoformalization used the proprietary Codex model (Jiang et al., 2023).\n\nOn Lean 4 formal-to-formal proving, LLEMMA-7b improves upon its Code Llama initialization, and performs similar to ReProver (Yang et al., 2023), a retrieval-augmented language model finetuned for tactic prediction. LLEMMA adapts to the task using a 3 example prompt, which to our knowledge is the first demonstration of few-shot tactic prediction for theorem proving by an open model.\n\n## 3.4 IMPACT OF DATA MIXTURE\n\nWhen training a language model, it is common to upsample high-quality subsets of the training data according to mixture weights (Brown et al., 2020; Gao et al., 2020; Xie et al., 2023). We select mixture weights by doing short training runs on several hand-picked mixture weights, then choosing the one which minimizes perplexity on a set of high-quality held-out text (we use the MATH training set). Table 5 shows the MATH training set perplexity of models trained using different mixtures of arXiv to web to code. Based on these results, we trained LLEMMA with a ratio of 2 : 4 : 1 . Note that our methodology uses the MATH training set to determine a training hyperparameter, though we expect that the effect is similar to that of related high-quality texts.\n\nTable 5: MATH training set perplexity of Code Llama 7B models trained using different data mixtures for a reduced number of steps. Each mixture is represented by its arXiv:Web:Code ratio.\n\n| Mixture   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   |\n|-----------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n|           | Overall                        | Prealgebra                     | Algebra                        | Number Theory                  | Counting& Probability          | Geometry                       | Intermediate Algebra           | Precalculus                    |\n| 2:4:1     | 1.478                          | 1.495                          | 1.515                          | 1.552                          | 1.475                          | 1.519                          | 1.439                          | 1.331                          |\n| 2:4:2     | 1.482                          | 1.500                          | 1.519                          | 1.556                          | 1.477                          | 1.524                          | 1.443                          | 1.334                          |\n| 4:2:1     | 1.487                          | 1.505                          | 1.524                          | 1.561                          | 1.481                          | 1.534                          | 1.447                          | 1.338                          |\n| 4:2:2     | 1.489                          | 1.508                          | 1.527                          | 1.562                          | 1.483                          | 1.538                          | 1.447                          | 1.339                          |\n| 4:4:1     | 1.487                          | 1.506                          | 1.525                          | 1.561                          | 1.482                          | 1.529                          | 1.446                          | 1.335                          |\n| 4:4:2     | 1.485                          | 1.503                          | 1.523                          | 1.559                          | 1.480                          | 1.529                          | 1.444                          | 1.334                          |\n\n## 3.5 DATASET OVERLAP AND MEMORIZATION\n\nDo test problems or solutions appear in the corpus? We check whether any 30-gram in a test sequence (either an input problem or an output solution) occurs in any OpenWebMath or AlgebraicStack document. If so, we say that a hit occurred between the sequence and the document. Table 6 shows hits between sequences from MATH and documents from Proof -Pile -2 . Using our methodology, around 7% of MATH test problem statements and 0.6% of MATH test solutions have hits. Note that our methodology gives a lower bound on the number of semantically equivalent sequences (e.g., it does not account for alternative phrasing).\n\nWe manually inspected 100 uniformly sampled hits between a test problem statement and an OpenWebMath document. 41 of the cases had no solution, which included websites with a list of problems, discussions, or hints. 49 had an alternative solution to the MATH ground-truth solution, but with the same answer. These include solutions that solve the problem differently than the ground-truth, solutions with missing details, and discussions that include the answer. 9 cases had a missing or incorrect answer, and 1 had the same solution as in the ground-truth. In summary, we find that solutions can appear in a corpus derived from web documents, particularly alternative solutions to those in the evaluation set. We repeated our analysis with 20-gram hits and our findings were similar, though with false positives; see Appendix Figure 6 for examples.\n\nTable 6: Left: 30-gram hits between MATH test problems or solutions and Proof -Pile -2 documents. Example and Docs are the numbers of unique test examples and Proof -Pile -2 documents with a hit. Right: manual inspection of 100 hits between a problem statement and a Proof -Pile -2 document.\n\n| Proof - Pile - 2   | Test   |   Problem Example Docs |   Problem Example Docs |   Solution Example Docs |   Solution Example Docs | Same solution                                                        | 1    |\n|--------------------|--------|------------------------|------------------------|-------------------------|-------------------------|----------------------------------------------------------------------|------|\n| OpenWebMath        | MATH   |                    348 |                    717 |                      34 |                      46 | Different solution, same answer Different solution, different answer | 49 9 |\n| AlgebraicStack     | MATH   |                      3 |                      3 |                       1 |                       1 | No solution                                                          | 41   |\n| OpenWebMath        | GSM8k  |                      2 |                      3 |                       0 |                       0 | Different problem                                                    | 0    |\n| AlgebraicStack     | GSM8k  |                      0 |                      0 |                       0 |                       0 |                                                                      |      |\n\n## How do problems in the corpus impact performance?\n\nNext, we evaluate LLEMMA-34b on the test examples with a 30-gram hit, and the test examples without a 30gram hit. Table 7 shows the accuracy partitioned by MATH difficulty level. The model's accuracy remains low on difficult problems (e.g., 6.08% on Level 5 problems with a hit, versus 6.39% on problems without a hit), and we observe no clear relationship between 30-gram hits and accuracy across difficulty levels. We conclude that a nontrivial match between a test example and a training document did not imply that the model generated a memorized correct answer. We repeated the analysis with 20-grams and with the 7b model, and our findings were analogous. Figure 7 shows an example.\n\nTable 7: LLEMMA-34b's accuracy on hits (a 30-gram overlap between a problem or solution and a training sequence) and nonhits by MATH difficulty level.\n\n| MATH Level   |   Hit Accuracy |   Nonhit Accuracy |   # Hits |\n|--------------|----------------|-------------------|----------|\n| Level 1      |          72.73 |             61.5  |       11 |\n| Level 2      |          35.71 |             40.18 |       28 |\n| Level 3      |          30.36 |             26.88 |       56 |\n| Level 4      |          14.89 |             16.61 |       94 |\n| Level 5      |           6.08 |              6.39 |      181 |\n\nFinally, we check 30-gram hits between LLEMMA's MATH generations and OpenWebMath. There were 13 hits, which occurred when the model generated a common sequence of numbers (e.g., a list of Fibonacci numbers), plus one instance of factoring a polynomial. Appendix Figure 6 shows an example. We find all of these observations worthy of further study. Using LLEMMA and Proof -Pile -2 to better understand data, memorization, and performance is an interesting future direction. We include the code for our analysis in the LLEMMA repository.\n\n## 4 RELATED WORK\n\nLarge-scale language modeling. Recent progress in large language models involves two connected threads: the increasing scale of models and data (Hoffmann et al., 2022; Kaplan et al., 2020; Chowdhery et al., 2022), and a progression toward more generalist models (Radford et al., 2019; Brown et al., 2020) which are capable of solving diverse problems and adapting quickly to novel tasks. A third thread relates to enabling open access to language models with these capabilities (Black et al., 2022; Biderman et al., 2023; Touvron et al., 2023; Rozière et al., 2023). Our work provides a recipe for specializing these language models to the domain of mathematics, providing a platform for further research and applications.\n\nDomain adaptation. Language model applications typically require a general-domain pretraining step, followed by a shorter fine-tuning step. The finetuning step is often aimed at imbuing instructionfollowing ability (Sanh et al., 2022; Wei et al., 2022) or aligning a model's outputs with human preferences (Ziegler et al., 2019; Ouyang et al., 2022; Bai et al., 2022). Other work explores adapting pretrained models to novel domains by continued training (Rozière et al., 2023; Beltagy et al., 2019), parameter-efficient finetuning methods (Yong et al., 2023), retrieval augmentation (Min et al., 2023; Asai et al., 2023), and other techniques. We provide an adaptation recipe involving continued training and targeted data collection.\n\nLanguage models for mathematics. Applying large language models to problems in mathematics is an active subfield of machine learning, including benchmarking mathematical knowledge and reasoning at varying levels (Hendrycks et al., 2021b; Zheng et al., 2021; Welleck et al., 2022; Azerbayev et al., 2023). Although achieving strong mathematical reasoning is an important target, it is difficult to assess the correctness of models' answers and processes, especially as models become more capable (Bowman et al., 2022; Uesato et al., 2022; Lightman et al., 2023; Cobbe et al., 2021).\n\nA number of recent works focus on supervised finetuning on task-relevant (input, output) pairs (e.g.,Yu et al. (2023); Yue et al. (2023)). Doing so boosts performance on some common mathematical language modeling benchmarks, but trains the model for these specific tasks. In contrast, Lewkowycz et al. (2022) and our work seek to train a base language model as a platform for further development.\n\nLanguage models for formal mathematics. An ongoing line of work explores integrating language models with interactive proof assistants in the context of mathematics. This includes synthesizing proofs via tactic prediction (Polu &amp; Sutskever, 2020; Han et al., 2022; Lample et al., 2022; Jiang et al., 2022), autoformalization (Wu et al., 2022; Jiang et al., 2023), and integrated tools (Welleck &amp;Saha, 2023). Due to high computational costs of search, language models applied to this domain have traditionally been small, but recent work has demonstrated promise in the use of larger models (First et al., 2023; Jiang et al., 2023). Our work provides a demonstration of few-shot proof autoformalization and tactic prediction, a large collection of formal mathematics data, along with an open access model for further exploring these directions.\n\n## 5 CONCLUSION\n\nWe introduce LLEMMA and Proof -Pile -2 , a novel base model and corpus for language modeling of mathematics. Our models, dataset, and code are openly available. We have shown that LLEMMA achieves state-of-the-art results for open-weights models on mathematical problem solving benchmarks, shown capabilities of using external tools via Python code, and demonstrated few-shot tactic prediction for theorem proving. We hope that LLEMMA and Proof -Pile -2 will be a useful base for future work on understanding language model generalization and dataset composition, investigating the limits of domain-specific language models, using language models as tools for mathematicians, and improving the mathematical capabilities of language models.\n\n## ACKNOWLEDGEMENTS\n\nWe would like to thank Dragomir Radev, Arman Cohan, Jesse Michael Han, and the Deepmind Blueshift team for valuable guidance. We thank Jonah Philion for the model name. We thank Aviya Skowron for advising us on ethical considerations in the development and release of our models. We thank Jonathan Laurent and Leo Du for contributions to our open-source code.\n\nWe would also like to thank several parties for donating computing resources for this project: Stability AI (training the LLEMMA models), CoreWeave (evaluations and finetuning), the Province of Ontario and companies sponsoring the Vector Institute for Artificial Intelligence ( www.vectorinstitute.ai/partners ), and Brigham Young University (finetuning). KP is supported by an NSERC PGS-D award.\n\n## REFERENCES\n\n- Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. Santacoder: don't reach for the stars! In Deep Learning for Code (DL4C) Workshop , 2023.\n- Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Thérien, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in PyTorch. GitHub Repo, 9 2023. URL https://www.github.com/eleutherai/ gpt-neox .\n- Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts) , pp. 41-46, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-tutorials.6. URL https: //aclanthology.org/2023.acl-tutorials.6 .\n- Jeremy Avigad. The mechanization of mathematics. Notices of the AMS , 65(6):681-90, 2018.\n- Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir R. Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. ArXiv , abs/2302.12433, 2023.\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n- Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 3615-3620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1371. URL https://aclanthology.org/D19-1371 .\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning , pp. 2397-2430. PMLR, 2023.\n\n- Stella Rose Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. ArXiv , abs/2201.07311, 2022.\n- Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In Proceedings of BigScience Episode# 5-Workshop on Challenges &amp;Perspectives in Creating Large Language Models , pp. 95-136, 2022.\n- Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil˙ e Lukoši¯ ut˙ e, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli TranJohnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and Jared Kaplan. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540 , 2022.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv , abs/2005.14165, 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- Katherine M. Collins, Albert Q. Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B. Tenenbaum, William Hart, Timothy Gowers, Wenda Li, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through interactions. arXiv preprint arXiv:2306.01694 , 2023.\n- Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 , 2023.\n- Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25 , pp. 378-388. Springer, 2015.\n- Erich Elsen, Curtis Hawthorne, and Arushi Somani. The adventure of the errant hardware, 2023. URL https://www.adept.ai/blog/sherlock-sdc .\n- Emily First, Markus N. Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. arXiv preprint arXiv:2303.04910 , 2023.\n- Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv , abs/2101.00027, 2020.\n- Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Jason Ociepa, Chris Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\n\nEric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628 .\n\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2022.\n- Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435 , 2023.\n- Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III au2, and Kate Crawford. Datasheets for datasets, 2021.\n- Herbert L. Gelernter. Realization of a geometry theorem proving machine. In IFIP Congress , 1959. URL https://api.semanticscholar.org/CorpusID:18484295 .\n- Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR , abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677 .\n- Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu. Proof artifact cotraining for theorem proving with language models. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=rpxJc9j04U .\n- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2021a.\n- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS , 2021b.\n- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020.\n- Albert Q. Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle proofs. 6th Conference on Artificial Intelligence and Theorem Proving , 2021.\n- Albert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygó´ zd´ z, Piotr Miło´ s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. arXiv preprint arXiv:2205.10893 , 2022.\n- Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SMa9EAovKMC .\n- Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code. Preprint , 2022.\n- Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, and Timothée Lacroix. Hypertree proof search for neural theorem proving. arXiv preprint arXiv:2205.11491 , 2022.\n\n- Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022.\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023.\n- Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 14605-14631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long.817 .\n- Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 , 2023.\n- The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs , CPP 2020, pp. 367-381, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450370974. doi: 10.1145/ 3372885.3373824. URL https://doi.org/10.1145/3372885.3373824 .\n- Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore, 2023.\n- Scott Morrison. lean-training-data. https://github.com/semorrison/ lean-training-data , 2023.\n- OpenAI. Gpt-4 technical report, 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 , 2022.\n- Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR , abs/2310.06786, 2023. doi: 10.48550/ ARXIV.2310.06786. URL https://doi.org/10.48550/arXiv.2310.06786 .\n- Christine Paulin-Mohring. Extracting ω 's programs from proofs in the calculus of constructions. In Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming languages , pp. 89-104, 1989a.\n- Christine Paulin-Mohring. Extraction de programmes dans le Calcul des Constructions . PhD thesis, Université Paris-Diderot-Paris VII, 1989b.\n- Larry Paulson and Tobias Nipkow. The sledgehammer: Let automatic theorem provers write your isabelle scripts!, 2023. URL https://isabelle.in.tum.de/ website-Isabelle2009-1/sledgehammer.html .\n- Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.\n- Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393 , 2020.\n- Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344 , 2022.\n\n- Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog , 2019.\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023.\n- Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC '20. IEEE Press, 2020. ISBN 9781728199986. doi: 10.5555/3433701.3433727. URL https://dl.acm.org/doi/10. 5555/3433701.3433727 .\n- Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 , 2023.\n- Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207 , 2022.\n- Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. Computing Research Repository , 2019. doi: 10.48550/arXiv.1909.08053. URL https://arxiv.org/abs/1909.08053v4 . Version 4.\n- Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge, 2022.\n- Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023.\n- Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864 , 2022.\n- Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022.\n- Amitayush Thakur, Yeming Wen, and Swarat Chaudhuri. A language-agent approach to formal theorem-proving, 2023.\n- Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\n\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.\n\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n- Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and outcome-based feedback, 2022.\n- H. Wang. Toward mechanical mathematics. IBM Journal of Research and Development , 4(1):2-22, 1960. doi: 10.1147/rd.41.0002.\n- Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=1PL1NIMMrw .\n- Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2022.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n- Sean Welleck. Neural theorem proving tutorial. https://github.com/wellecks/ ntptutorial , 2023.\n- Sean Welleck and Rahul Saha. llmstep: Llm proofstep suggestions in lean. https://github. com/wellecks/llmstep , 2023.\n- Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=rhdfTOiXBng .\n- Makarius Wenzel, Lawrence C Paulson, and Tobias Nipkow. The isabelle framework. In Theorem Proving in Higher Order Logics: 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings 21 , pp. 33-38. Springer, 2008.\n- Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance, 2023.\n\n- Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=IUikebJ1Bf0 .\n- Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. arXiv preprint arXiv:2305.10429 , 2023.\n- Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. In Neural Information Processing Systems (NeurIPS) , 2023.\n- Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina. BLOOM+1: Adding language support to BLOOM for zero-shot prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 11682-11703, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. acl-long.653. URL https://aclanthology.org/2023.acl-long.653 .\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284 , 2023.\n- Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. CoRR , abs/2309.05653, 2023. doi: 10.48550/arXiv.2309.05653. URL https://doi.org/10. 48550/arXiv.2309.05653 .\n- Shizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. Can transformers learn to solve problems recursively?, 2023.\n- Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110 , 2021.\n- Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning, 2022.\n- Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.\n\n## A AUTHOR CONTRIBUTIONS\n\nTraining Data. Zhangir Azerbayev, Keiran Paster, Marco Dos Santos, Sean Welleck.\n\nModel training. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster.\n\nEvaluations. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Sean Welleck.\n\nFormal math evaluations. Sean Welleck.\n\nMemorization analysis. Sean Welleck, Keiran Paster.\n\nSenior Authorship and Advising. Jia Deng, Stella Biderman, Sean Welleck.\n\n## B DATA: Proof -Pile -2\n\nTable 8: Proof -Pile -2 data sources (top), general language and code data included during training (bottom), and the mixture weights of each component during training.\n\n| Data source              | Tokens   | Weight   |\n|--------------------------|----------|----------|\n| Proof - Pile - 2         | 55B      | -        |\n| Code ( AlgebraicStack )  | 11B      | 1.00     |\n| Web (OpenWebMath)        | 15B      | 4.00     |\n| Papers (ArXiv)           | 29B      | 2.00     |\n| General code (RedPajama) | 59B      | 0.22     |\n| General language (Pile)  | 300B     | 0.15     |\n\n## B.1 MATHEMATICAL CODE: AlgebraicStack\n\nAlgebraicStack contains roughly 11B tokens of code related to mathematics. We describe its sources, filtering, and content below. Table 9 shows the number of tokens per language in AlgebraicStack .\n\nTable 9: Tokens in AlgebraicStack , computed with the Llama tokenizer.\n\n| Language   | AlgebraicStack tokens   | Language   | AlgebraicStack tokens   |\n|------------|-------------------------|------------|-------------------------|\n| Agda       | 35.2M                   | Julia      | 531.0M                  |\n| C          | 25.1M                   | Jupyter    | 199.1M                  |\n| C++        | 954.1M                  | Lean       | 285.6M                  |\n| Coq        | 281.9M                  | Maple      | 2.0M                    |\n| Fortran    | 724.9M                  | Matlab     | 65.8M                   |\n| GAP        | 3.6M                    | Python     | 6,098.8M                |\n| Haskell    | 9.1M                    | R          | 71.3M                   |\n| Idris      | 10.9M                   | Tex        | 567.7M                  |\n| Isabelle   | 1,089.7M                | Total      | 10,955.7M               |\n\n## B.1.1 GITHUB CODE\n\nThe following programming languages were either barely present in the Stack or consisted of largely incorrect filetypes, so we downloaded data for these languages directly via the Github Python API.\n\n- Coq : We filter for files with the .v extension, and include Coq via including files that match a heuristic filter for the keywords \"Theorem\" , \"Proof\" , \"Qed\" , \"Inductive\" , \"Definition\" , \"Fixpoint\" and exclude Verilog files via the keyword blacklist \"pragma\" , \"endmodule\" , \"posedge\" , \"negedge\" , \"wire\" . We additionally exclude files noted as automatically generated.\n\n- Isabelle : We filter for files with the .thy extension and include files matching the keyword whitelist \"theorem \" , \"lemma \" . We keep only isabelle-prover/mirror-afp-devel and discard all other older copies of the Archive of Formal Proofs. We further remove theorem statements and proofs that have a theorem name in the PISA (Jiang et al., 2021) test set.\n- Lean : We filter for files with the .lean extension, using the keyword whitelist \"theorem \" , \"lemma \" , \"example \" . We remove all dependency files, and in order to avoid known benchmark contamination, we blacklist the ProofNet and MiniF2F repositories. We further remove theorems or lemmas that share a theorem name with the LeanDojo (Yang et al., 2023) val or test sets.\n- MATLAB : We filter for files with the .m extension, using the keyword whitelist \"#import\" , \"interface\" , \"implementation\" , \"property\" , and blacklist C files via the keywords \"#include\" and the regex r' main \\ (.*{$'\n\nWe implemented a cutoff date for our Github API downloads, and used a cutoff date of April 1, 2023.\n\nFor all languages, unless otherwise stated, we additionally filtered out files with a filesize greater than 1048575 bytes or with a numerical density (ratio of digit characters to non-digit characters) of 0 . 5 . We additionally perform document-level exact deduplication by removing documents which contain an overlapping 2048-character chunk as another document.\n\n## B.1.2 LEAN PROOFSTEPS\n\nWe extract a dataset of (tactic state, next tactic) pairs from Mathlib 4 (mathlib Community, 2020) using the lean-training-data (Morrison, 2023) tool. We use Mathlib 4 commit c779bd5 , which was created on August 20th 2023.\n\n## B.1.3 ISABELLE PROOFSTEPS\n\nWe construct a dataset of Isabelle proofs, building upon the PISA dataset Jiang et al. (2021). Isabelle Proofsteps comprises proofs from the Archive of Formal Proofs and Isabelle Standard Library, scraped with PISA Jiang et al. (2021). Each entry in the dataset includes the theorem statement, the proof states and the proof steps, separated by specific tags. To maintain the integrity of evaluations using the PISA test set, we decontaminate Isabelle Proofsteps by removing theorems whose names overlap with those in the PISA test set. Although this approach results in a strict filtering - removing more than 10,000 theorems although there are only 3600 in the PISA test set - we consider it acceptable in order to mitigate data contamination. After filtering, Isabelle Proofsteps contains 251,000 theorems.\n\n## B.1.4 STACK FILTERING\n\nWe source the following programming languages from the Stack (Kocetkov et al., 2022) dataset, and describe our filtering process and quality issues we chose to mitigate beyond our default quality heuristics:\n\n- Agda : Only standard filters applied.\n- C : We include documents based on a keyword whitelist, namely: \"#include &lt;fftw.h&gt;\" , \"#include &lt;fftw3.h&gt;\" , \"#include &lt;rfftw.h&gt;\" , \"#include &lt;gsl\" , \"#include &lt;cblas.h&gt;\" , \"#include &lt;blas.h&gt;\" , \"#include &lt;lapacke.h&gt;\" , \"#include &lt;nlopt.h&gt;\" , \"#include &lt;petsc.h&gt;\" .\n- C++ : We include documents based on a keyword whitelist, namely: \"#include &lt;adept\\_arrays.h&gt;\" , \"#include &lt;adept.h&gt;\" , \"#include &lt;alglib&gt; , \"#include &lt;boost\" , \"#include &lt;armadillo\" , \"#include &lt;blitz\" , \"#include &lt;Eigen\" , \"#include &lt;deal.II\" , \"#include &lt;dlib\" , \"#include &lt;NTL\" , \"#include &lt;mtl\" .\n- Fortran : Only standard filters applied.\n- GAP : Only standard filters applied.\n- Haskell : We filtered the data to only contain files with the following imports: Numeric.LinearAlgebra , Numeric.SpecFunctions , Numeric.Vector , Statistics , Data.Complex .\n\n- Idris : Only standard filters applied.\n- Julia : We filtered out mislabeled JSON lines files. We removed files larger than 10,000 characters long which both were not files containing tests and which had a lower numerical density than 0 . 5 , and otherwise ignored numerical density. We additionally only accepted files within a specific keyword whitelist, to attempt to control relevance to scientific computing, namely: \"LinearAlgebra\" , \"DifferentialEquations\" , \"Symbolics\" , \"Distributions\" , \"DataFrames\" , \"DynamicalSystems\" , \"Turing\" , \"Gen\" , \"JuMP\" , \"sqrt\" , \"abs\" , \"zeros\" , \"ones\" , \"sin\" , \"cos\" , \"tan\" , \"log\" , \"exp\" , \"integrate\" , \"likelihood\" , \"Matrix\" , π , \"pi\" , \"rand\" , \"grad\" .\n- Jupyter : We found that many Jupyter notebook files were large due to containing long cell outputs, such as base64 images, long tracebacks, or other extra JSON cell metadata. We use nbconvert to convert notebooks to a markdown format, removing metadata.\n- Maple : We filtered out files with a size greater than 100 , 000 bytes, and found that some files were XML. We filtered all files beginning with an XML declaration.\n- Python : We filtered notebooks and JSON files out by excluding documents with beginning \"{\" characters, and included only files importing from a fixed list of libraries.\n- R : We excluded all files beginning with an XML declaration. We additionally filtered out all notebooks, and filtered all files containing MacOS \"Resource Fork\" files.\n- Tex : We used a max file size of 10,000,000 bytes. We excluded tex files found in directories named \"latex/\" because these were often auto-generated files, and excluded documents using gnuplot . We included only documents containing one of the keywords \" \\ chapter{\" , \" \\ chapter*{\" , \" \\ section{\" , \" \\ section*{\" , \" \\ subsection{\" , \" \\ subsection*{\" , \" \\ subsubsection{\" , \" \\ subsubsection*{\" , \" \\ paragraph{\" , \" \\ subparagraph{\" , and additionally only included documents identified as English by a classifier from the langid package.\n\nFor all languages we used within the Stack, unless otherwise stated, we additionally filtered out files with a filesize greater than 1048575 bytes or with a numerical density (ratio of digit characters to non-digit characters) of 0 . 5 .\n\nWe used v1.2 of the near-deduplicated Stack as a base for processing.\n\n## B.2 PAPERS: ARXIV\n\nWe use the entirety of ArXiv, as accessed by Computer (2023) in April 2023. For further information on preprocessing applied to ArXiv, see Computer (2023).\n\n## B.3 WEB: OPENWEBMATH\n\nFor the web portion of our training dataset, we use OpenWebMath (Paster et al., 2023).\n\n## C EVALUATION HARNESS\n\nWe implement a variety of math-related tasks and evaluation protocols into a public fork of the Language Model Evaluation Harness (Gao et al., 2021). The Harness provides a model-agnostic framework for standardized, reproducible evaluation of language models.\n\nWe add the following tasks for the evaluations in this paper:\n\n- hendrycks\\_math\\_ppl : Perplexity evaluation on MATH (Hendrycks et al., 2021a) sub-tasks.\n- minif2f\\_isabelle : Proof autoformalization in Isabelle on the miniF2F benchmark based on Jiang et al. (2023), with a Portal-to-Isabelle (Jiang et al., 2021) proof checker.\n- minerva\\_math : The MATH benchmark with the prompt and Sympy evaluation from Minerva (Lewkowycz et al., 2022).\n- minerva-hendrycksTest : MMLU-STEM tasks following Lewkowycz et al. (2022).\n\n- ocw\\_courses : The OCW Courses task from Lewkowycz et al. (2022).\n- python\\_gsm8k : GSM8k with Python, based on Gao et al. (2022).\n- sympy\\_math : MATH with Sympy evaluation.\n\nWe include a link to the implementations for these tasks, including full prompts, in our public codebase.\n\n## D EVALUATION: EXPERIMENT DETAILS\n\n## D.1 ISABELLE INFORMAL-TO-FORMAL THEOREM PROVING\n\nWe follow Jiang et al. (2023), allowing the model to issue a call to built-in Isabelle automation in the output proof by generating sledgehammer . This calls Sledgehammer (Paulson &amp; Nipkow, 2023) and the list of heuristics listed in Jiang et al. (2023). Following Jiang et al. (2023), as a baseline we use Sledgehammer and the heuristics executed at the beginning of the proof (referred to as Sledgehammer in the main text for brevity). We use a 30-second timeout for Sledgehammer and implement proof checking via Portal-to-Isabelle (Jiang et al., 2021). Refer to the implementation in the Evaluation Harness for further details.\n\n## D.2 LEAN THEOREM PROVING\n\nTheorem proving via tactic prediction involves interacting with a proof assistant after each step of a proof. Implementing these interactions within the evaluation harness is outside the scope of this work. Therefore, for the Lean theorem proving task we use a separate evaluation setup based on an open-source implementation (Welleck, 2023). We include our evaluation code in our public codebase.\n\nSetup. We evaluate on miniF2F (Zheng et al., 2021), which consists of 488 formalized statements from math competitions and undergraduate coursework. Given a formalized statement, the task is to generate a formal proof that is checked by Lean.\n\nWe use best first search, commonly used for neural tactic prediction models (e.g., Polu &amp; Sutskever (2020)). Best first search is parameterized by the number of attempts (N), generated tactics per iteration (S), and maximum iterations (T). We define the search budget to be the maximum number of generated tactics, N × S × T . We set our search budget to N = 1 , S = 32 , and T = 100 , less than that of the baseline model. Following Yang et al. (2023), we generate tactics with beam search and use a 10 minute timeout. We adapt the proof search implementation from Welleck (2023), which uses LeanDojo v.1.1.2 (Yang et al., 2023) for interaction. We use Lean 4 miniF2F, using https://github.com/rah4927/lean-dojo-mew commit d00c776260c77de7e70125ef0cd119de6c0ff1de . Note that the ReProver baseline from (Yang et al., 2023) reports performance with Lean 3.\n\nPrompt. We prompt the model with three (state, tactic) examples, shown in Figure 5.\n\n̸\n\n```\n\"\"\"Given the Lean 4 tactic state, suggest a next tactic. Here are some examples: Tactic state: ---α : Type u_1 r : α → α → Prop inst 1 : DecidableEq α inst : IsIrrefl α r ⊢ CutExpand r ≤ InvImage (Finsupp.Lex ( c r Π fun x x_1 => x = x_1) fun x x_1 => x < x_1) ↑ toFinsupp ---Next tactic: ---rintro s t ⟨ u, a, hr, he ⟩ ---Tactic state: ---ι : Type u_1 I J : Box ι x y : ι → R I J : WithBot (Box ι ) ⊢ ↑ I = ↑ J ↔ I = J ---Next tactic: ---simp only [Subset.antisymm_iff, ← le_antisymm_iff, withBotCoe_subset_iff] ---Tactic state: ---m n : N h : Nat.coprime m n ⊢ Nat.gcd m n = 1 ---Next tactic: ---rw [ ← h.gcd_eq_one] ---Tactic state: ---%s ---Next tactic: ---\"\"\"\n```\n\nFigure 5: Prompt for the Lean theorem proving experiments.\n\n## E DATASHEET\n\nWe provide a datasheet for Proof -Pile -2 , following the framework in Gebru et al. (2021).\n\n| MOTIVATION                                                                                                                   | MOTIVATION                                                                                                                                                                        |\n|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| For what purpose was the dataset cre- ated?                                                                                  | Proof - Pile - 2 was created for the training or finetuning of domain-specific large lan- guage models for general mathematics tasks.                                             |\n| Who created the dataset and on behalf of which entity?                                                                       | The dataset was created by the authors of this paper for the purposes of this research project.                                                                                   |\n| Who funded the creation of the dataset?                                                                                      | The creation of the dataset was funded by the coauthors' grants and employers, as fur- ther described in section 5.                                                               |\n| Any other comment?                                                                                                           | Any other comment?                                                                                                                                                                |\n| COMPOSITION                                                                                                                  | COMPOSITION                                                                                                                                                                       |\n| What do the instances that comprise the dataset represent?                                                                   | Instances are text-only documents.                                                                                                                                                |\n| How many instances are there in total?                                                                                       | We detail fine-grained token counts else- where in this paper.                                                                                                                    |\n| Does the dataset contain all possible in- stances or is it a sample (not necessarily random) of instances from a larger set? | Our dataset is filtered based on our assess- ments of quality for the language modeling task. More detail on methodology can be found in Appendix B.                              |\n| What data does each instance consist of?                                                                                     | Each instance is a text-only document, alongside metadata about its originating split and filename or location.                                                                   |\n| Is there a label or target associated with each instance?                                                                    | No.                                                                                                                                                                               |\n| Is any information missing from individ- ual instances?                                                                      | Yes, we filter undesired noise, such as base64-encoded images, from some doc- uments.                                                                                             |\n| Are relationships between individual in- stances made explicit?                                                              | No.                                                                                                                                                                               |\n| Are there recommended data splits?                                                                                           | Yes, we release a canonical train, validation, and test split of the dataset, which we follow in this work.                                                                       |\n| Are there any errors, sources of noise, or redundancies in the dataset?                                                      | We make our best efforts to remove errors or sources of noise, but our dataset will naturally contain documents with errors or noise, and may contain near-duplicate doc- uments. |\n| Is the dataset self-contained, or does it link to or otherwise rely on external re- sources?                                 | The dataset is self-contained, but can also be reconstructed based on external publicly available data sources and datasets follow- ing our instructions.                         |\n| Does the dataset contain data that might be considered confidential?                                                         | All documents in Proof - Pile - 2 are publicly available online.                                                                                                                  |\n\n| Does the dataset contain data that, if viewed directly, might be offensive, in- sulting, threatening, or might otherwise cause anxiety?   | We estimate toxic content to be less preva- lent in our dataset than other more general web-based datasets, due to its technical fo- cus. However, it is likely to contain such content.                                                               |\n|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| COLLECTION                                                                                                                                | COLLECTION                                                                                                                                                                                                                                             |\n| How was the data associated with each instance acquired?                                                                                  | Data was largely sourced from existing pub- lic subsets, such as the RedPajama dataset (Computer, 2023), OpenWebMath dataset (Paster et al., 2023), and via filtering the Stack (Kocetkov et al., 2022). Some data was collected using the Github API. |\n| What mechanisms or procedures were used to collect the data?                                                                              | See above.                                                                                                                                                                                                                                             |\n| If the dataset is a sample from a larger set, what was the sampling strategy?                                                             | We release the entirety of the dataset fol- lowing the application of our quality filters. We randomly held out validation and test splits from the dataset.                                                                                           |\n| Who was involved in the data collec- tion process and how were they compen- sated?                                                        | The authors of this paper participated in lo- cating, retrieving, and filtering the dataset.                                                                                                                                                           |\n| Over what timeframe was the data col- lected?                                                                                             | This data was collected in 2023, with a cut- off date of April 2023 for all subsets with the exception of our Lean proofstep data.                                                                                                                     |\n| Were any ethical review processes con- ducted?                                                                                            | Yes, the authors conducted an informal eth- ical review internally.                                                                                                                                                                                    |\n| PREPROCESSING                                                                                                                             | PREPROCESSING                                                                                                                                                                                                                                          |\n| Was any preprocessing/cleaning/labeling of the data done?                                                                                 | Yes, the authors extensively filtered the dataset subsets in keeping with our expec- tations for high-quality language modeling data in our domain. See Appendix B for further detail on filtering steps taken.                                        |\n| Was the 'raw' data saved in addition to the preprocessed/cleaned/labeled data?                                                            | Raw data can be accessed via reuse of our provided codebase.                                                                                                                                                                                           |\n| Is the software that was used to prepro- cess/clean/label the data available?                                                             | Yes. We release our codebase, which can be used to reproduce our dataset and its con- struction process, at https://github. com/EleutherAI/math-lm .                                                                                                   |\n| USES                                                                                                                                      | USES                                                                                                                                                                                                                                                   |\n| Has the dataset been used for any tasks already?                                                                                          | Yes, this dataset has been used to train the LLEMMA language models as a domain adaptation and continued pretraining cor- pus.                                                                                                                         |\n| Is there a repository that links to any or all papers or systems that use the dataset?                                                    | No.                                                                                                                                                                                                                                                    |\n| What (other) tasks could the dataset be used for?                                                                                         | The dataset was specifically targeted as a high quality language modeling corpus for the mathematics domain, but may be useful for general-purpose language modeling or unforeseen other downstream uses.                                              |\n\nTable 10: Datasheet for Proof -Pile -2 , following the framework introduced by Gebru et al. (2021).\n\n| Is there anything about the composition of the dataset or the way it was col- lected and preprocessed/cleaned/labeled that might impact future uses?   | We filtered the dataset with the intent of creating a model useful for mathematical tasks with solely English text.                                                             |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Are there tasks for which the dataset should not be used?                                                                                              | The dataset should not be used with the intent to cause harm or for models intended for the purposes of harm.                                                                   |\n| DISTRIBUTION                                                                                                                                           | DISTRIBUTION                                                                                                                                                                    |\n| Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?                                     | We make the dataset publicly available for reproducibility, analysis, and other further downstream uses.                                                                        |\n| How will the dataset will be distributed?                                                                                                              | We provide code to replicate the dataset, and release it via the Huggingface Hub.                                                                                               |\n| When will the dataset be distributed?                                                                                                                  | The dataset is available immediately.                                                                                                                                           |\n| Will the dataset be distributed under a copyright or other intellectual prop- erty (IP) license, and/or under applicable terms of use (ToU)?           | We do not relicense the dataset's compo- nents, and do not impose our own use re- strictions.                                                                                   |\n| Have any third parties imposed IP-based or other restrictions on the data associ- ated with the instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| Do any export controls or other regula- tory restrictions apply to the dataset or to individual instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| MAINTENANCE                                                                                                                                            | MAINTENANCE                                                                                                                                                                     |\n| Who will be supporting/hosting/main- taining the dataset?                                                                                              | The dataset will be hosted on the Hug- gingFace Hub and able to be recreated via code at https://github.com/ EleutherAI/math-lm . The dataset will not be updated post-release. |\n| How can the owner/curator/manager of the dataset be contacted?                                                                                         | Via email at za2514@princeton.edu                                                                                                                                               |\n| Is there an erratum?                                                                                                                                   | No.                                                                                                                                                                             |\n| Will the dataset be updated?                                                                                                                           | No.                                                                                                                                                                             |\n| If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?                                           | No.                                                                                                                                                                             |\n\n## F ADDITIONAL RESULTS\n\n## F.1 PROOF AUTOFORMALIZATION\n\nTable 11 shows additional results on Isabelle proof autoformalization, including the union of theorems closed by Sledgehammer and the given language model.\n\nTable 11: Isabelle autoformalization . ∗ We exclude the 11 examples used in the few-shot prompts. Pass@1 with greedy decoding.\n\n| Method                       | Autoformalization pass@1   | Autoformalization pass@1   |\n|------------------------------|----------------------------|----------------------------|\n|                              | miniF2F-valid ∗            | miniF2F-test               |\n| Sledgehammer                 | 14.72%                     | 20.49%                     |\n| Code Llama 7b                | 16.31%                     | 17.62%                     |\n| LLEMMA-7b                    | 20.60%                     | 22.13%                     |\n| Code Llama 7b ∪ Sledgehammer | 20.17%                     | 25.00%                     |\n| LLEMMA-7b ∪ Sledgehammer     | 25.97%                     | 27.46%                     |\n\n## G SUPERVISED FINETUNING\n\nA full exploration of finetuning applications for LLEMMA, such as instruction following (Ouyang et al., 2022; Wei et al., 2022), dialogue modeling (Thoppilan et al., 2022; Touvron et al., 2023; Collins et al., 2023), and reward modeling (Cobbe et al., 2021; Lightman et al., 2023) are outside the scope of this work. However, to establish that LLEMMA retains its advantage over other open models when finetuned, we conduct preliminary experiments finetuning LLEMMA-7B on MetaMathQA (Yu et al., 2023), a supervised dataset targeted at the MATH and GSM8k benchmarks. Results are shown in Table 12.\n\nTable 12: Finetuning of various 7B base models on supervised mathematics datasets. All results with a Llama 2 initialization are copied from the literature (Luo et al., 2023; Yu et al., 2023). The LLEMMA 7B finetune is trained with identical hyperparameters to the models in Yu et al. (2023)\n\n| Initialization   | Finetune Dataset         | MATH   | GSM8k   |\n|------------------|--------------------------|--------|---------|\n| Llama 2 7B       | WizardMath (Proprietary) | 10.7%  | 54.9%   |\n| Llama 2 7B       | MetaMathQA               | 19.4%  | 66.4%   |\n| LLEMMA 7B        | MetaMathQA               | 25.2%  | 66.5%   |\n| Llama 2 70B      | WizardMath (Proprietary) | 22.7%  | 81.6%   |\n| Llama 2 70B      | MetaMathQA               | 26.6 % | 82.3 %  |\n\n.\n\n## H QUALITATIVE EXAMPLES\n\nDataset overlap. Figure 6 shows example false positives when checking n -gram overlap with OpenWebMath documents for various n . Figure 7 shows an example OpenWebMath document that has 30-gram overlap with a MATH problem, and LLEMMA-7b's generated solution.\n\nTask outputs. Figure 8 shows a generated proof in the informal2formal theorem proving task.\n\n## OpenWebMath document\n\n2D affine transformations can be better represented using 2 by 2 matrices, since they are simply linear combinations of 2 variables. The advantage of this is that the matrices are associative under multiplication Also, GPUs and modern toolkits are optimised to work with this representation. As a result, a scale matrix is \\begin{bmatrix} s\\_x &amp; 0 \\\\ 0 &amp; s\\_y \\end{bmatrix}, and a rotation matrix is \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}.\n\nA translation matrix is simply \\begin{bmatrix} 1 &amp; \\frac{t\\_x}{y} \\\\ \\frac{t\\_y}{x} &amp; 1 ...\n\n## MATH problem\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n## Hit\n\n\\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos\n\n## OpenWebMath document\n\n# Basic Probability\n\nA number is selected at random from 1 through 100, inclusive. What is the probability that the number is a divisor of 50? Express your answer as a common fraction.\n\n<!-- formula-not-decoded -->\n\nThere are a 100 integers between 1-100, inclusive. Since 50 is $$2*5^2$$, it has $$(1+1)(1+2)=(2)(3)=6$$ factors. Thus, the answer is $$\\frac{6}{100}=\\boxed{\\frac{3}{50}}.$$\n\n## MATH problem\n\nA number is selected at random from 1 through 100, inclusive. What is the probability that the number is a perfect square?\n\n## Hit\n\nA number is selected at random from 1 through 100, inclusive. What is the probability that the number is a\n\n## OpenWebMath document\n\nFig. 2.\n\nUse values of the most used medicinal plants in the Safi Province (Morocco).\n\nIt is also important to note that for the abovementioned medicinal plants, many other folk uses have been reported in different regions of Morocco. Furthermore, literature-based proof revealed that these species have proven a wide variety of biological and pharmacological activities (Table 4, Ref. [14, 17, 19, 20, 21, 23, 24, 26, 28, 30, 31, 34, 35, 36, 38, 39, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116]), which may confirm the different popular applications of ...\n\n## Generated solution (LLEMMA 7b):\n\nThe first 100 positive integers are 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20, 21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49, 50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78, 79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100. We can see that...\n\nFigure 6: Data overlap: Example false positives using 10-gram match between MATH solutions and OpenWebMath documents ( top ), 20-gram match between MATH problems and OpenWebMath documents ( middle ), and 30-gram match between LLEMMA-7b's generated solutions and OpenWebMath documents ( bottom ).\n\n## OpenWebMath document\n\n```\nA triangle is formed with edges along the line $y=\\frac{2}{3}x+5$, the $x$-axis, and the line $x=k$. If the area of the triangle is less than $20$, find the sum of all possible integral values of $k$. Feb 28, 2018 Look at the graph, here...two triangles are possible : https://www.desmos.com/calculator/m6wnjpgldq The height of the triangles at any point will be formed by [ (2/3)x + 5 ] And the bases will be [ x -(- 7.5)] = [ x + 7.5] So....we want to solve this (1/2) [ (2/3)x + 5 ] [ x + 7.5 ] = 20 [ (2/3)x + 5 ] [ x + 7.5] = 40 (2/3)x^2 + 5x + 5x + 37.5 = 0 (2/3)x^2 + 10x -2.5 = 0 Using a little technology.....the max x value for the triangle formed above the x axis will be = .246 And the min x value for the triangle formed below the x axis will be = -15.246 With the given boundaries, the integer sums of all possible x values of k giving triangles with an area < 20 units^2 = [ (-15) + (-14 ) + (-13) + ...+ ( -2) + ( -1 ) + 0 ] = -(15) (16) / 2 = -120\n```\n\nFeb 28, 2018\n\nMATH Problem: A triangle is formed with edges along the line y = 2 3 x +5 , the x -axis, and the line x = k . If the area of the triangle is less than 20 , find the sum of all possible integral values of k . Answer: -120\n\n## Generated solution (LLEMMA 7b):\n\nThe triangle has vertices ( k, 0) , (0 , 5) , and ( k, 2 3 k +5) . The base of the triangle has length k , and the height of the triangle has length 2 3 k +5 . The area of the triangle is therefore\n\n<!-- formula-not-decoded -->\n\nSince the area of the triangle is less than 20 , we have\n\n<!-- formula-not-decoded -->\n\nThis inequality is true if and only if 1 6 k 2 + 5 4 k -10 is positive. Since 1 6 k 2 + 5 4 k -10 is a quadratic with positive leading coefficient, it is positive if and only if its vertex is less than 0 . The vertex of 1 6 k 2 + 5 4 k -10 is ( -10 3 , -125 18 ) . Since -10 3 &lt; 0 , we have that 1 6 k 2 + 5 4 k -10 &gt; 0 if and only if k &lt; -10 3 . Therefore, the only possible integral values of k are k = 0 , 1 , 2 , 3 , 4 . The sum of these values is 10 . Final Answer: The final answer is 10 .\n\nFigure 7: Data overlap: Example OpenWebMath document that has a 30-gram overlap with the given MATH problem, and LLEMMA-7b's generated solution.\n\n```\nProblem: If 3 a + b + c = -3 , a +3 b + c = 9 , a + b +3 c = 19 , then find abc . Show that it is -56. Informal Proof (Human-written): Summing all three equations yields that 5 a +5 b +5 c = -3 + 9 + 19 = 25 . Thus, a + b + c = 5 . Subtracting this from each of the given equations, we obtain that 2 a = -8 , 2 b = 4 , 2 c = 14 . Thus, a = -4 , b = 2 , c = 7 , and their product is abc = -4 × 2 × 7 = -56 . Formal Statement and Proof: theorem mathd_algebra_338: fixes a b c :: real assumes \"3 * a + b + c = -3\" and \"a + 3 * b + c = 9\" and \"a + b + 3 * c = 19\" shows \"a * b * c = -56\" proof -(* Summing all three equations yields that 5a + 5b + 5c = -3 + 9 + 19 = 25. Thus, a + b + c = 5. * ) have \"5 * a + 5 * b + 5 * c = -3 + 9 + 19\" using assms < ATP > then have \"5 * (a + b + c) = 25\" < ATP > then have \"a + b + c = 5\" < ATP > (* Subtracting this from each of the given equations , we obtain that 2a = -8, 2b = 4, 2c = 14. Thus, a = -4, b = 2, c =7, and their product is abc = -4 \\times 2 \\times 7 = -56. * ) then have \"2 * a = -8\" \"2 * b = 4\" \"2 * c = 14\" using assms < ATP > then have \"a = -4\" \"b = 2\" \"c = 7\" < ATP > then show ?thesis < ATP > qed\n```\n\nFigure 8: Informal-to-formal proving . The model is given the problem, informal proof, and formal statement, following Jiang et al. (2023). It generates a formal proof (starting with proof -) containing Isabelle code, comments ( (*...*) ) that align the informal and formal proofs, and calls to an automated prover (shown as &lt;ATP&gt; ). The proof is from LLEMMA-7b with greedy decoding.",
  "tables": [
    {
      "index": 0,
      "markdown": "|                   |                        |                        | Dataset                            | Tokens   | Open   |\n|-------------------|------------------------|------------------------|------------------------------------|----------|--------|\n| Model             | Adaptation tokens Open | Adaptation tokens Open | Minerva Dataset                    | 38.5B    | ✗      |\n| Minerva-8b        | 164B                   | ✗                      | Proof - Pile - 2 (ours)            | 55B      | ✓      |\n| Minerva-62b       | 109B                   | ✗                      | Code ( AlgebraicStack )            | 11B      | ✓      |\n| LLEMMA-7b (ours)  | 200B                   | ✓                      | OpenWebMath (Paster et al., 2023)) | 15B      | ✓      |\n| LLEMMA-34b (ours) | 50B                    | ✓                      | ArXiv (Computer, 2023))            | 29B      | ✓      |"
    },
    {
      "index": 1,
      "markdown": "|            |      | GSM8k   | OCW   | MMLU-STEM   | SAT    | MATH   |\n|------------|------|---------|-------|-------------|--------|--------|\n| Llama 2    | 7B   | 11.8%   | 3.7%  | 29.9%       | 25.0%  | 3.2%   |\n| Code Llama | 7B   | 10.5%   | 4.4%  | 25.1%       | 9.4%   | 4.5%   |\n| Minerva    | 8B   | 16.2%   | 7.7%  | 35.6%       | -      | 14.1%  |\n| LLEMMA     | 7B   | 36.4%   | 7.7%  | 37.7%       | 53.1 % | 18.0 % |\n| Code Llama | 34B  | 29.6%   | 7.0%  | 40.5%       | 40.6%  | 12.2%  |\n| LLEMMA     | 34B  | 51.5%   | 11.8% | 49.0%       | 71.9 % | 25.0%  |\n| Minerva    | 62B  | 52.4%   | 12.0% | 53.9%       | -      | 27.6%  |\n| Minerva    | 540B | 58.8%   | 17.6% | 63.9%       | -      | 33.6%  |"
    },
    {
      "index": 2,
      "markdown": "|         |      | GSM8k maj@ k   | OCW maj@ k   | MMLU-STEM maj@ k   | SAT maj@ k   | MATH maj@ k   |\n|---------|------|----------------|--------------|--------------------|--------------|---------------|\n| Minerva | 8B   | 28.4%          | 12.5%        | 43.4%              | -            | 25.4%         |\n| LLEMMA  | 7B   | 54.0%          | 14.3%        | 49.9%              | 78.1 %       | 33.5%         |\n| LLEMMA  | 34B  | 69.3 %         | 18.4 %       | 59.7 %             | 81.3 %       | 43.1 %        |\n| Minerva | 62B  | 68.5%          | 23.5%        | 63.5%              | -            | 43.4%         |\n| Minerva | 540B | 78.5%          | 30.8%        | 75.0%              | -            | 50.3%         |"
    },
    {
      "index": 3,
      "markdown": "|            |     | GSM8k+Python pass@1   | MATH+Python pass@1   |\n|------------|-----|-----------------------|----------------------|\n| Code Llama | 7B  | 27.1%                 | 17.2%                |\n| LLEMMA     | 7B  | 40.1%                 | 21.5%                |\n| Code Llama | 34B | 52.7%                 | 23.5%                |\n| LLEMMA     | 34B | 62.6%                 | 27.1%                |"
    },
    {
      "index": 4,
      "markdown": "| Method         | Informal-to-formal   | Informal-to-formal   | Method                | Formal-to-formal   | Formal-to-formal   |\n|----------------|----------------------|----------------------|-----------------------|--------------------|--------------------|\n|                | miniF2F-valid        | miniF2F-test         |                       | Search             | miniF2F-test       |\n| Sledgehammer   | 14.72%               | 20.49%               | ReProver (fine-tuned) | 1 × 64             | 26.50%             |\n| Code Llama 7b  | 16.31%               | 17.62%               | Code Llama 7b         | 1 × 32             | 20.49%             |\n| Code Llama 34b | 18.45%               | 18.03%               | Code Llama 34b        | 1 × 32             | 22.13%             |\n| LLEMMA-7b      | 20.60%               | 22.13%               | COPRA (GPT-4)         | - †                | 23.36%             |\n| LLEMMA-34b     | 21.03%               | 21.31%               | LLEMMA-7b             | 1 × 32             | 26.23%             |\n|                |                      |                      | LLEMMA-34b            | 1 × 32             | 25.82%             |"
    },
    {
      "index": 5,
      "markdown": "| Mixture   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   | MATH training set perplexity   |\n|-----------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n|           | Overall                        | Prealgebra                     | Algebra                        | Number Theory                  | Counting& Probability          | Geometry                       | Intermediate Algebra           | Precalculus                    |\n| 2:4:1     | 1.478                          | 1.495                          | 1.515                          | 1.552                          | 1.475                          | 1.519                          | 1.439                          | 1.331                          |\n| 2:4:2     | 1.482                          | 1.500                          | 1.519                          | 1.556                          | 1.477                          | 1.524                          | 1.443                          | 1.334                          |\n| 4:2:1     | 1.487                          | 1.505                          | 1.524                          | 1.561                          | 1.481                          | 1.534                          | 1.447                          | 1.338                          |\n| 4:2:2     | 1.489                          | 1.508                          | 1.527                          | 1.562                          | 1.483                          | 1.538                          | 1.447                          | 1.339                          |\n| 4:4:1     | 1.487                          | 1.506                          | 1.525                          | 1.561                          | 1.482                          | 1.529                          | 1.446                          | 1.335                          |\n| 4:4:2     | 1.485                          | 1.503                          | 1.523                          | 1.559                          | 1.480                          | 1.529                          | 1.444                          | 1.334                          |"
    },
    {
      "index": 6,
      "markdown": "| Proof - Pile - 2   | Test   |   Problem Example Docs |   Problem Example Docs |   Solution Example Docs |   Solution Example Docs | Same solution                                                        | 1    |\n|--------------------|--------|------------------------|------------------------|-------------------------|-------------------------|----------------------------------------------------------------------|------|\n| OpenWebMath        | MATH   |                    348 |                    717 |                      34 |                      46 | Different solution, same answer Different solution, different answer | 49 9 |\n| AlgebraicStack     | MATH   |                      3 |                      3 |                       1 |                       1 | No solution                                                          | 41   |\n| OpenWebMath        | GSM8k  |                      2 |                      3 |                       0 |                       0 | Different problem                                                    | 0    |\n| AlgebraicStack     | GSM8k  |                      0 |                      0 |                       0 |                       0 |                                                                      |      |"
    },
    {
      "index": 7,
      "markdown": "| MATH Level   |   Hit Accuracy |   Nonhit Accuracy |   # Hits |\n|--------------|----------------|-------------------|----------|\n| Level 1      |          72.73 |             61.5  |       11 |\n| Level 2      |          35.71 |             40.18 |       28 |\n| Level 3      |          30.36 |             26.88 |       56 |\n| Level 4      |          14.89 |             16.61 |       94 |\n| Level 5      |           6.08 |              6.39 |      181 |"
    },
    {
      "index": 8,
      "markdown": "| Data source              | Tokens   | Weight   |\n|--------------------------|----------|----------|\n| Proof - Pile - 2         | 55B      | -        |\n| Code ( AlgebraicStack )  | 11B      | 1.00     |\n| Web (OpenWebMath)        | 15B      | 4.00     |\n| Papers (ArXiv)           | 29B      | 2.00     |\n| General code (RedPajama) | 59B      | 0.22     |\n| General language (Pile)  | 300B     | 0.15     |"
    },
    {
      "index": 9,
      "markdown": "| Language   | AlgebraicStack tokens   | Language   | AlgebraicStack tokens   |\n|------------|-------------------------|------------|-------------------------|\n| Agda       | 35.2M                   | Julia      | 531.0M                  |\n| C          | 25.1M                   | Jupyter    | 199.1M                  |\n| C++        | 954.1M                  | Lean       | 285.6M                  |\n| Coq        | 281.9M                  | Maple      | 2.0M                    |\n| Fortran    | 724.9M                  | Matlab     | 65.8M                   |\n| GAP        | 3.6M                    | Python     | 6,098.8M                |\n| Haskell    | 9.1M                    | R          | 71.3M                   |\n| Idris      | 10.9M                   | Tex        | 567.7M                  |\n| Isabelle   | 1,089.7M                | Total      | 10,955.7M               |"
    },
    {
      "index": 10,
      "markdown": "| MOTIVATION                                                                                                                   | MOTIVATION                                                                                                                                                                        |\n|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| For what purpose was the dataset cre- ated?                                                                                  | Proof - Pile - 2 was created for the training or finetuning of domain-specific large lan- guage models for general mathematics tasks.                                             |\n| Who created the dataset and on behalf of which entity?                                                                       | The dataset was created by the authors of this paper for the purposes of this research project.                                                                                   |\n| Who funded the creation of the dataset?                                                                                      | The creation of the dataset was funded by the coauthors' grants and employers, as fur- ther described in section 5.                                                               |\n| Any other comment?                                                                                                           | Any other comment?                                                                                                                                                                |\n| COMPOSITION                                                                                                                  | COMPOSITION                                                                                                                                                                       |\n| What do the instances that comprise the dataset represent?                                                                   | Instances are text-only documents.                                                                                                                                                |\n| How many instances are there in total?                                                                                       | We detail fine-grained token counts else- where in this paper.                                                                                                                    |\n| Does the dataset contain all possible in- stances or is it a sample (not necessarily random) of instances from a larger set? | Our dataset is filtered based on our assess- ments of quality for the language modeling task. More detail on methodology can be found in Appendix B.                              |\n| What data does each instance consist of?                                                                                     | Each instance is a text-only document, alongside metadata about its originating split and filename or location.                                                                   |\n| Is there a label or target associated with each instance?                                                                    | No.                                                                                                                                                                               |\n| Is any information missing from individ- ual instances?                                                                      | Yes, we filter undesired noise, such as base64-encoded images, from some doc- uments.                                                                                             |\n| Are relationships between individual in- stances made explicit?                                                              | No.                                                                                                                                                                               |\n| Are there recommended data splits?                                                                                           | Yes, we release a canonical train, validation, and test split of the dataset, which we follow in this work.                                                                       |\n| Are there any errors, sources of noise, or redundancies in the dataset?                                                      | We make our best efforts to remove errors or sources of noise, but our dataset will naturally contain documents with errors or noise, and may contain near-duplicate doc- uments. |\n| Is the dataset self-contained, or does it link to or otherwise rely on external re- sources?                                 | The dataset is self-contained, but can also be reconstructed based on external publicly available data sources and datasets follow- ing our instructions.                         |\n| Does the dataset contain data that might be considered confidential?                                                         | All documents in Proof - Pile - 2 are publicly available online.                                                                                                                  |"
    },
    {
      "index": 11,
      "markdown": "| Does the dataset contain data that, if viewed directly, might be offensive, in- sulting, threatening, or might otherwise cause anxiety?   | We estimate toxic content to be less preva- lent in our dataset than other more general web-based datasets, due to its technical fo- cus. However, it is likely to contain such content.                                                               |\n|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| COLLECTION                                                                                                                                | COLLECTION                                                                                                                                                                                                                                             |\n| How was the data associated with each instance acquired?                                                                                  | Data was largely sourced from existing pub- lic subsets, such as the RedPajama dataset (Computer, 2023), OpenWebMath dataset (Paster et al., 2023), and via filtering the Stack (Kocetkov et al., 2022). Some data was collected using the Github API. |\n| What mechanisms or procedures were used to collect the data?                                                                              | See above.                                                                                                                                                                                                                                             |\n| If the dataset is a sample from a larger set, what was the sampling strategy?                                                             | We release the entirety of the dataset fol- lowing the application of our quality filters. We randomly held out validation and test splits from the dataset.                                                                                           |\n| Who was involved in the data collec- tion process and how were they compen- sated?                                                        | The authors of this paper participated in lo- cating, retrieving, and filtering the dataset.                                                                                                                                                           |\n| Over what timeframe was the data col- lected?                                                                                             | This data was collected in 2023, with a cut- off date of April 2023 for all subsets with the exception of our Lean proofstep data.                                                                                                                     |\n| Were any ethical review processes con- ducted?                                                                                            | Yes, the authors conducted an informal eth- ical review internally.                                                                                                                                                                                    |\n| PREPROCESSING                                                                                                                             | PREPROCESSING                                                                                                                                                                                                                                          |\n| Was any preprocessing/cleaning/labeling of the data done?                                                                                 | Yes, the authors extensively filtered the dataset subsets in keeping with our expec- tations for high-quality language modeling data in our domain. See Appendix B for further detail on filtering steps taken.                                        |\n| Was the 'raw' data saved in addition to the preprocessed/cleaned/labeled data?                                                            | Raw data can be accessed via reuse of our provided codebase.                                                                                                                                                                                           |\n| Is the software that was used to prepro- cess/clean/label the data available?                                                             | Yes. We release our codebase, which can be used to reproduce our dataset and its con- struction process, at https://github. com/EleutherAI/math-lm .                                                                                                   |\n| USES                                                                                                                                      | USES                                                                                                                                                                                                                                                   |\n| Has the dataset been used for any tasks already?                                                                                          | Yes, this dataset has been used to train the LLEMMA language models as a domain adaptation and continued pretraining cor- pus.                                                                                                                         |\n| Is there a repository that links to any or all papers or systems that use the dataset?                                                    | No.                                                                                                                                                                                                                                                    |\n| What (other) tasks could the dataset be used for?                                                                                         | The dataset was specifically targeted as a high quality language modeling corpus for the mathematics domain, but may be useful for general-purpose language modeling or unforeseen other downstream uses.                                              |"
    },
    {
      "index": 12,
      "markdown": "| Is there anything about the composition of the dataset or the way it was col- lected and preprocessed/cleaned/labeled that might impact future uses?   | We filtered the dataset with the intent of creating a model useful for mathematical tasks with solely English text.                                                             |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Are there tasks for which the dataset should not be used?                                                                                              | The dataset should not be used with the intent to cause harm or for models intended for the purposes of harm.                                                                   |\n| DISTRIBUTION                                                                                                                                           | DISTRIBUTION                                                                                                                                                                    |\n| Will the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created?                                     | We make the dataset publicly available for reproducibility, analysis, and other further downstream uses.                                                                        |\n| How will the dataset will be distributed?                                                                                                              | We provide code to replicate the dataset, and release it via the Huggingface Hub.                                                                                               |\n| When will the dataset be distributed?                                                                                                                  | The dataset is available immediately.                                                                                                                                           |\n| Will the dataset be distributed under a copyright or other intellectual prop- erty (IP) license, and/or under applicable terms of use (ToU)?           | We do not relicense the dataset's compo- nents, and do not impose our own use re- strictions.                                                                                   |\n| Have any third parties imposed IP-based or other restrictions on the data associ- ated with the instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| Do any export controls or other regula- tory restrictions apply to the dataset or to individual instances?                                             | Not to our knowledge.                                                                                                                                                           |\n| MAINTENANCE                                                                                                                                            | MAINTENANCE                                                                                                                                                                     |\n| Who will be supporting/hosting/main- taining the dataset?                                                                                              | The dataset will be hosted on the Hug- gingFace Hub and able to be recreated via code at https://github.com/ EleutherAI/math-lm . The dataset will not be updated post-release. |\n| How can the owner/curator/manager of the dataset be contacted?                                                                                         | Via email at za2514@princeton.edu                                                                                                                                               |\n| Is there an erratum?                                                                                                                                   | No.                                                                                                                                                                             |\n| Will the dataset be updated?                                                                                                                           | No.                                                                                                                                                                             |\n| If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?                                           | No.                                                                                                                                                                             |"
    },
    {
      "index": 13,
      "markdown": "| Method                       | Autoformalization pass@1   | Autoformalization pass@1   |\n|------------------------------|----------------------------|----------------------------|\n|                              | miniF2F-valid ∗            | miniF2F-test               |\n| Sledgehammer                 | 14.72%                     | 20.49%                     |\n| Code Llama 7b                | 16.31%                     | 17.62%                     |\n| LLEMMA-7b                    | 20.60%                     | 22.13%                     |\n| Code Llama 7b ∪ Sledgehammer | 20.17%                     | 25.00%                     |\n| LLEMMA-7b ∪ Sledgehammer     | 25.97%                     | 27.46%                     |"
    },
    {
      "index": 14,
      "markdown": "| Initialization   | Finetune Dataset         | MATH   | GSM8k   |\n|------------------|--------------------------|--------|---------|\n| Llama 2 7B       | WizardMath (Proprietary) | 10.7%  | 54.9%   |\n| Llama 2 7B       | MetaMathQA               | 19.4%  | 66.4%   |\n| LLEMMA 7B        | MetaMathQA               | 25.2%  | 66.5%   |\n| Llama 2 70B      | WizardMath (Proprietary) | 22.7%  | 81.6%   |\n| Llama 2 70B      | MetaMathQA               | 26.6 % | 82.3 %  |"
    }
  ],
  "stats": {
    "pages": 28,
    "chunksCreated": 155,
    "totalCharacters": 104667,
    "totalWords": 14385,
    "numTables": 15,
    "processingTimeMs": 35690
  }
}