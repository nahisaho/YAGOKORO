{
  "paper": {
    "id": "2311.10122v3",
    "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
    "abstract": "The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM. Code address: \\href{https://github.com/PKU-YuanGroup/Video-LLaVA}",
    "authors": [
      "Bin Lin",
      "Yang Ye",
      "Bin Zhu",
      "Jiaxi Cui",
      "Munan Ning",
      "Peng Jin",
      "Li Yuan"
    ],
    "published": "2023-11-16T10:59:44.000Z",
    "updated": "2024-10-01T12:07:31.000Z",
    "primaryCategory": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2311.10122v3",
    "absUrl": "https://arxiv.org/abs/2311.10122v3"
  },
  "chunks": [
    {
      "id": "2311.10122v3-chunk-0",
      "content": "Bin Lin 1 , Yang Ye 1 , Bin Zhu 1 , Jiaxi Cui 4 , Munang Ning 1,2,3 , Peng Jin 1,2,3 , Li Yuan 1,2,3\n\n1 2 Peng Cheng Laboratory,\n\nAI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School,\n\nPeking University Shenzhen Graduate School, 3 4 PandaVilla Tech Limited\n\nCorrespondence:\n\nyuanli-ece@pku.edu.cn\n\nGitHub:\n\nhttps://github.com/PKU-YuanGroup/Video-LLaVA",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "chunkIndex": 0,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-1",
      "content": "Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA , which learns from a mixed dataset of images and videos, mutually enhancing each other. As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-2",
      "content": "ed dataset of images and videos, mutually enhancing each other. As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-3",
      "content": "Recently, LLMs have gained rapid popularity in the AI community, such as GPT-3.5, GPT-4 (OpenAI, 2023), PaLM (Bi et al., 2020; Anil et al., 2023), and BLOOM (Scao et al., 2022). They rely on their powerful language comprehension abilities to follow human-provided instructions and provide corresponding responses. Typically, LLMs can only respond within the text input provided by the user, which is insufficient because human interaction with the world involves multiple channels, such as visual and textual. To this end, recent\n\nFigure 1: Comparing Different LVLM Paradigms. Video-LLaVA aligns images and videos before projection, allowing LLM to learn from a unified visual representation and endowing LLM with the ability to comprehend both images and videos simultaneously.\n\n<!-- image -->\n\nworks (Ye et al., 2023; Zhu et al., 2023b; Alayrac et al., 2022) have mapped images into text-like tokens, enabling LLMs to emerge with the ability to comprehend images.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-4",
      "content": "multaneously.\n\n<!-- image -->\n\nworks (Ye et al., 2023; Zhu et al., 2023b; Alayrac et al., 2022) have mapped images into text-like tokens, enabling LLMs to emerge with the ability to comprehend images. Despite their effectiveness, empowering LLMs to understand videos is more challenging than image-only comprehension tasks. Nevertheless, recent work (Maaz et al., 2023; Li et al., 2023c; Zhang et al., 2023a) has made initial strides in enabling interactions between video and language.\n\nHowever, most current LVLMs (Li et al., 2023b; Dai et al., 2023; Luo et al., 2023; Li et al., 2023a; Yin et al., 2023; Fu et al., 2023) can primarily handle a single visual modality, either imagelanguage or video-language. We compare different LVLM paradigms as shown in Figure 1, where VideoChat (Li et al., 2023c) and VideoLLaMA (Zhang et al., 2023a) utilize a share visual encoder to handle both images and videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-5",
      "content": "uage. We compare different LVLM paradigms as shown in Figure 1, where VideoChat (Li et al., 2023c) and VideoLLaMA (Zhang et al., 2023a) utilize a share visual encoder to handle both images and videos. However, due to the inherent differences in the media types of images and videos, it is challenging to learn a unified representation, and the performance falls significantly behind that of the specialized video expert model, Video-ChatGPT. Therefore,\n\nX-LLM (Chen et al., 2023) and Macaw-LLM (Lyu et al., 2023) allocate a modality-specific encoder for each modality, attempting to enable a LLM to comprehend images or videos through several projection layers. But their performances are inferior to dedicated video expert models such as VideoChatGPT (Maaz et al., 2023). We attribute this phenomenon to the lack of alignment before projection .",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-6",
      "content": "jection layers. But their performances are inferior to dedicated video expert models such as VideoChatGPT (Maaz et al., 2023). We attribute this phenomenon to the lack of alignment before projection . Because image features and video features reside in their own spaces, this poses a challenge for a LLM to learn their interactions from several poor projection layers. Some similar phenomenon such as alignment before fusion has been discussed by ALBEF(Li et al., 2021) and ViLT (Kim et al., 2021) in multi-model models. More recently, ImageBindLLM (Han et al., 2023) focuses on enabling the LLM to simultaneously process multiple modal inputs by pre-aligning each modality to a common feature space (Girdhar et al., 2023). Based on a large image-language model, ImageBind-LLM converts other modalities into the most similar image features by retrieving from a training-free image cached database.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-7",
      "content": "ce (Girdhar et al., 2023). Based on a large image-language model, ImageBind-LLM converts other modalities into the most similar image features by retrieving from a training-free image cached database. However, the indirect alignment approach of ImageBind-LLM may lead to performance degradation, and the LLM has no knowledge of actual video data.\n\nIn this work, we introduce Video-LLaVA , a simple but powerful baseline for the LVLM simultaneously handling both images and videos. Specifically, As shown in Figure 1, Video-LLaVA initially aligns the representations of images and videos to a unified visual feature space. Since the visual representations are already aligned prior to projection, we employ a shared projection layer to map the unified visual representation for the LLM. To enhance computational efficiency, Video-LLaVA undergoes joint training of images and videos, achieving remarkable results with 1 training epoch.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-8",
      "content": "o map the unified visual representation for the LLM. To enhance computational efficiency, Video-LLaVA undergoes joint training of images and videos, achieving remarkable results with 1 training epoch.\n\nAs a result, The proposed Video-LLaVA greatly enhances the ability of the LLM to simultaneously understand both images and videos. For image understanding, Video-LLaVA surpasses advanced LVLMs such as mPLUG-owl-7B and InstructBLIP7B in 5 image benchmarks. Additionally, utilizing 4 benchmark toolkits for a more comprehensive evaluation, Video-LLaVA-7B even outperforms IDEFICS-80B by 6.4% in MMBench. Moreover, similar trends can be observed in video understanding, where Video-LLaVA surpasses VideoChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% respectively on the MSVD, MSRVTT, TGIF, and ActivityNet video question-answering datasets. Ex- tensive ablation experiments demonstrate that alignment before projection yields greater benefits.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-9",
      "content": "10.1% respectively on the MSVD, MSRVTT, TGIF, and ActivityNet video question-answering datasets. Ex- tensive ablation experiments demonstrate that alignment before projection yields greater benefits. Additionally, joint training of images and videos can facilitate a unified visual representation in LLM comprehension.\n\nWe summarize our primary contributions as follows:\n\n- We introduce Video-LLaVA , a powerful LVLM baseline. During the training process, Video-LLaVA binds visual signals to the language feature space, unifying visual representations, and proposes a solution to align before projection. We enable an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n- Extensive experiments demonstrate that a unified visual representation benefits LLMs in learning to simultaneously handle both images and videos, validating the complementarity of modalities, showcasing significant superiority when compared to models specifically designed for either images o",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-10",
      "content": "ning to simultaneously handle both images and videos, validating the complementarity of modalities, showcasing significant superiority when compared to models specifically designed for either images or videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-11",
      "content": "When the well-known commercial model ChatGPT (OpenAI, 2023) was introduced, the The AI community released open-source Large Language Models (LLMs) by instruction tuning and increasing model sizes. These include LLaMA (Touvron et al., 2023a), Vicuna (Chiang et al., 2023), Alpaca (Taori et al., 2023), and more recently, LLaMA 2 (Touvron et al., 2023b). These models are tuned with instruction sets to emulate conversations between humans and AI assistants. Furthermore, InstructGPT (Ouyang et al., 2022) is trained based on GPT-3 (Brown et al., 2020) with 175 billion parameters through aligning with human preferences. However, LLMs can only interact within text. In this work, we introduce Video-LLaVA, which builds upon the powerful reasoning capabilities of LLM to extend modality interactions to images and videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.1 Large Language Models",
        "chunkIndex": 11,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-12",
      "content": "When extending LLMs to multi-modal, especially involving images and videos, the main approaches can be categorized into two types in Table 1: i) treating LLM as a scheduler, ii) treating LLM as a decoder.\n\nTable 1: Comparison between different Large Vision-Language Models. For methods that treat LLMs as scheduler, they do not require pre-alignment and joint training.\n\n| Methods                           | Image   | Video   | Pre-aligned   | Joint training   |\n|-----------------------------------|---------|---------|---------------|------------------|\n| LLMs as scheduler                 |         |         |               |                  |\n| VisualChatGPT (Wu et al., 2023)   | ✔       | ✗       | -             | -                |\n| HuggingGPT (Shen et al., 2023)    | ✔       | ✗       | -             | -                |\n| MM-REACT (Yang et al., 2023)      | ✔       | ✔       | -             | -                |\n| ViperGPT (Surís et al., 2023)     | ✔       | ✔       | -",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2 Large Vision-Language Models",
        "chunkIndex": 12,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-13",
      "content": "-             | -                |\n| MM-REACT (Yang et al., 2023)      | ✔       | ✔       | -             | -                |\n| ViperGPT (Surís et al., 2023)     | ✔       | ✔       | -             | -                |\n| LLMs as decoder                   |         |         |               |                  |\n| Mini-GPT4 (Zhu et al., 2023b)     | ✔       | ✗       | -             | ✗                |\n| LLaVA (Liu et al., 2023b)         | ✔       | ✗       | -             | ✗                |\n| Video-ChatGPT (Maaz et al., 2023) | ✗       | ✔       | -             | ✗                |\n| VideoChat (Li et al., 2023c)      | ✔       | ✔       | ✗             | ✔                |\n| Video-LLaMA (Zhang et al., 2023a) | ✔       | ✔       | ✗             | ✔                |\n| ImageBind-LLM (Han et al., 2023)  | ✔       | ✔       | ✔             | ✗                |\n| Video-LLaVA (Ours)                | ✔       | ✔       | ✔             | ✔                |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2 Large Vision-Language Models",
        "chunkIndex": 13,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-14",
      "content": "et al., 2023)  | ✔       | ✔       | ✔             | ✗                |\n| Video-LLaVA (Ours)                | ✔       | ✔       | ✔             | ✔                |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2 Large Vision-Language Models",
        "chunkIndex": 14,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-15",
      "content": "In the scheduler-based methods, various visual models are treated as plug-and-play modules. LLM schedules them according to the specific visual task requirements, like the assembly of building blocks. Some of these methods focus on images, such as VisualChatGPT (Wu et al., 2023) and HuggingGPT (Shen et al., 2023), while MM-REACT (Yang et al., 2023) and ViperGPT (Surís et al., 2023) can also handle videos. A key characteristic of these scheduler-based LVLMs is that they do not require end-to-end training, hence eliminating the need for pre-alignment and joint training of each modality.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2.1 LLMs as scheduler",
        "chunkIndex": 15,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-16",
      "content": "Regarding the approach of treating LLM as a decoder, this is our primary focus. MiniGPT-4 (Zhu et al., 2023b) aligns image tokens to the input of the large language model through several linear projection layers. However, this alignment is weak and lacks feedback from human instructions. Subsequently, mPLUG-Owl (Ye et al., 2023) adopts a two-stage training approach. In the first stage, images are aligned with language using an autoregressive pretraining style, and the second stage involves instruction tuning through using a human instruction dataset. With the increasing scale of large language model backends, approaches such as InstructBLIP (Dai et al., 2023) and LLaVA series (Liu et al., 2023b,a; Lin et al., 2024) collecte the larger human instruction datasets to train a larger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given instructions.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2.2 LLMs as decoder",
        "chunkIndex": 16,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-17",
      "content": "al., 2023b,a; Lin et al., 2024) collecte the larger human instruction datasets to train a larger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given instructions. Then they undergo end-to-end training using human instruction datasets, enabling the LLM with visual reasoning capabilities. Moreover, Video-ChatGPT (Maaz et al., 2023) design a 100k video instruction dataset, successfully empowering LLMs to comprehend videos. VideoChat (Li et al., 2023c) and Video-LLaMA (Zhang et al., 2023a) achieve this by conducting joint training, allowing LLMs to simultaneously handle images and videos. Expanding LLMs to additional visual modalities typically requires pre-alignment, as seen in LLaMA-Adapter (Zhang et al., 2023b; Gao et al., 2023) and ImageBind-LLM (Han et al., 2023). They bind other modalities to the image space through ImageBind's (Girdhar et al., 2023) modality encoder.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2.2 LLMs as decoder",
        "chunkIndex": 17,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-18",
      "content": "n LLaMA-Adapter (Zhang et al., 2023b; Gao et al., 2023) and ImageBind-LLM (Han et al., 2023). They bind other modalities to the image space through ImageBind's (Girdhar et al., 2023) modality encoder. These models have demonstrated that a unified feature space is advantageous for enhancing LLM's multi-modal reasoning capabilities. Distinguished from prior work, Video-LLaVA not only pre-aligns image and video features but also conducts joint training of images and videos, facilitating LLMs in learning multi-modal reasoning capabilities from a unified visual representation.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "2.2.2 LLMs as decoder",
        "chunkIndex": 18,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-19",
      "content": "As shown in Figure 2, Video-LLaVA consists of LanguageBind encoders f V (Zhu et al., 2023a) to extract features from the raw visual signal (im- ages or videos), a large language model f L such as Vicuna, visual projection layers f P and a word embedding layer f T . We initially obtain visual features using LanguageBind encoders. LanguageBind encoders are capable of mapping different modalities into the textual feature space, thereby providing us with a unified visual representation. Subsequently, the unified visual representation is encoded by shared projection layers, which is then combined with tokenized textual queries and fed into a large language model to generate corresponding responses.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.1.1 Framework Overview",
        "chunkIndex": 19,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-20",
      "content": "Our goal is to map images and videos into a shared feature space to enable the large language model to learn from a unified visual representation. We assume that the same information can be conveyed through multiple media. For example, a running dog can be expressed through language, a image or a video simultaneously. Therefore, we can compress information from different modalities into a common feature space, allowing the model to extract information from a dense feature space, facilitating modality interactions and complementarity. Hence, we chose the modality encoders from LanguageBind (Zhu et al., 2023a), which align images and videos with the textual feature space.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.1.2 United Visual Representation",
        "chunkIndex": 20,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-21",
      "content": "Specifically, LanguageBind initializes from OpenCLIP (Ilharco et al., 2021), naturally aligning images and language in a shared feature space. Subsequently, it aligns video representations to the language space using 3 million video-text pairs from VIDAL-10M (Zhu et al., 2023a). By sharing a language feature space, the image and video representations ultimately converge into a unified visual feature space, which we refer to as emergent alignment of images and videos. Therefore, our video encoder and image encoder are initialized from the LanguageBind encoders zoo, pre-aligning the inputs for LLM and reducing the gap between representations of different visual signals. The unified visual representation is fed into LLM after passing through a shared projection layer.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.1.3 Alignment Before Projection",
        "chunkIndex": 21,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-22",
      "content": "Overall, the process of generating responses by Video-LLaVA is similar to that of a large language model (GPT series). Given a textual input X T and visual signals X V, the input signals are encoded into a sequence of tokens according to Equation 1. By maximizing the likelihood probability in Equation 2, the model ultimately achieves multi-modal understanding capabilities.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere L is the length of the generated sequence X A, and θ is a trainable parameter. We dynamically conduct joint training on images and videos, wherein a single batch contains both image and video samples simultaneously.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.2 Training Pipeline",
        "chunkIndex": 22,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-23",
      "content": "At this stage, the model is required to acquire the ability to interpret visual signals within an extensive image/video-text pair dataset. Each visual signal corresponds to a single round of conversation data ( X q , X a ) , where X T = X q and X a is the ground truth. The training objective of this stage is the original auto-regressive loss, where the model learns the basic ability to view the vision. We freeze the other parameters of the model during this process.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.2.1 Understanding Training",
        "chunkIndex": 23,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-24",
      "content": "In this stage, the model is required to provide responses corresponding to different instructions. These instructions often involve more complex visual comprehension tasks, rather than just describing visual signals. Note that the conversation data ( X 1 q , X 1 a , · · · , X N q , X N a ) consists of multiple rounds.\n\n<!-- formula-not-decoded -->\n\nwhere r represents the round number. As shown in Equation 3, when r &gt; 1 we concatenate the conversations from all previous rounds with the current instruction as the input for this round. The training objective remains the same as in the previous stage. After this stage, the model learns to generate corresponding responses based on different instructions and requests. The LLM are also involved in training at this stage.\n\nFigure 2: Training framework and performance. Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.2.2 Instruction Tuning",
        "chunkIndex": 24,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-25",
      "content": "this stage.\n\nFigure 2: Training framework and performance. Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaVA framework demonstrates a data flow that generates corresponding responses based on input instructions. (b) VideoLLaVA achieves superior performances on a broad range of 15 datasets across image and video.\n\n<!-- image -->\n\nFigure 3: Data composition for training VideoLLaVA. The dataset for stage 1 consists of single-turn conversation, focusing on concise visual descriptions. In stage 2, the dataset comprises multi-turn conversations, emphasizing complex visual reasoning abilities.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "3.2.2 Instruction Tuning",
        "chunkIndex": 25,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-26",
      "content": "In 3, for the first stage of understanding pretraining, we use a subset of 558K LAION-CC-SBU image-text pairs with BLIP (Li et al., 2022) captions, which is sourced from CC3M (Sharma et al., 2018) and filtered by LLaVA (Liu et al., 2023b). The video-text pairs are derived from a subset provided by Valley (Luo et al., 2023), and we have access to 702k out of a total of 703k pairs, originating from WebVid (Bain et al., 2021). For the stage of instruction tuning, We gathered instructional datasets from two sources, including a 665k imagetext instruction dataset from LLaV A 1.5 (Liu et al., 2023a) and a 100k video-text instruction dataset from Video-ChatGPT (Maaz et al., 2023).",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.1.1 Data Details",
        "chunkIndex": 26,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-27",
      "content": "We employ Vicuna-7B v1.5 as the large language model. The visual encoders are derived from LanguageBind, initialized from OpenCLIP-L/14. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. The share projection layers consist of 2 fully connected layers with a GeLU (Hendrycks and Gimpel, 2016) activated function.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.1.2 Model Settings",
        "chunkIndex": 27,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-28",
      "content": "In the training process, we resize and crop each image, resulting in a size of 224×224 for each processed image. We uniformly sample 8 frames from each video, and each frame undergoes image preprocessing. The data in each batch is a random combination of images and videos. In the first stage, we train for one epoch with a batch size of 256, using the AdamW optimizer with a cosine learning rate schedule. In the second stage, we reduce the batch size to 128. The initial learning rate for both stages is set to 1e-3, with a warmup ratio of 0.03. Additional hyper-parameter settings can be found in the appendix.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.1.3 Training Details",
        "chunkIndex": 28,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-29",
      "content": "As shown in Table 2, we conduct a quantitative assessment of the video question-answering ca-\n\nTable 2: Comparison between different LVLMs on video reasoning benchmarks . We employ ChatGPTAssistant to evaluate the performance following Video-ChatGPT (Maaz et al., 2023). The version of ChatGPT is 'gpt-3.5-turbo'.\n\n| Methods       | LLM size   | MSVD-QA   | MSVD-QA   | MSRVTT-QA   | MSRVTT-QA   | TGIF-QA   | TGIF-QA   | ActivityNet-QA   | ActivityNet-QA   |\n|---------------|------------|-----------|-----------|-------------|-------------|-----------|-----------|------------------|------------------|\n|               |            | Accuracy  | Score     | Accuracy    | Score       | Accuracy  | Score     | Accuracy         | Score            |\n| FrozenBiLM    | 1B         | 32.2      | -         | 16.8        | -           | 41.0      | -         | 24.7             | -                |\n| VideoChat     | 7B         | 56.3      | 2.8       | 45.0        | 2.5         | 34.4      | 2.3",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 29,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-30",
      "content": "| 16.8        | -           | 41.0      | -         | 24.7             | -                |\n| VideoChat     | 7B         | 56.3      | 2.8       | 45.0        | 2.5         | 34.4      | 2.3       | -                | 2.2              |\n| LLaMA-Adapter | 7B         | 54.9      | 3.1       | 43.8        | 2.7         | -         | -         | 34.2             | 2.7              |\n| Video-LLaMA   | 7B         | 51.6      | 2.5       | 29.6        | 1.8         | -         | -         | 12.4             | 1.1              |\n| Video-ChatGPT | 7B         | 64.9      | 3.3       | 49.3        | 2.8         | 51.4      | 3.0       | 35.2             | 2.7              |\n| Chat-UniVi    | 7B         | 65.0      | 3.6       | 54.6        | 3.1         | 60.3      | 3.4       | 45.8             | 3.2              |\n| Video-LLaVA   | 7B         | 70.7      | 3.9       | 59.2        | 3.5         | 70.0      | 4.0       | 45.3             | 3.3              |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 30,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-31",
      "content": "| 3.4       | 45.8             | 3.2              |\n| Video-LLaVA   | 7B         | 70.7      | 3.9       | 59.2        | 3.5         | 70.0      | 4.0       | 45.3             | 3.3              |\n\nTable 3: Comparison between different LVLMs on image understanding benchmarks. 'Res.', 'L', 'V' respectively represent the input image resolution, LLaMA (Touvron et al., 2023a) and Vicuna (Chiang et al., 2023). Benchmark names are abbreviated due to page limitations. VQA-v2 (Goyal et al., 2017); GQA (Hudson and Manning, 2019); VisWiz (Gurari et al., 2018); SQA I : ScienceQA-IMG (Lu et al., 2022); VQA T : TextVQA (Singh et al., 2019); POPE (Li et al., 2023d); MMB: MMBench (Liu et al., 2023c); LLaVA W : LLaVA-Bench (In-theWild) (Liu et al., 2023b); MM-Vet (Yu et al., 2023). † donates that we reproduce LLaVA-1.5 with LanguageBindImage encoder to compare fairly. ∗ donates that there is some overlap in the training data.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 31,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-32",
      "content": "eWild) (Liu et al., 2023b); MM-Vet (Yu et al., 2023). † donates that we reproduce LLaVA-1.5 with LanguageBindImage encoder to compare fairly. ∗ donates that there is some overlap in the training data.\n\n| Methods      | LLM   | Res.   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   |\n|--------------|-------|--------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------------------|---------------------|---------------------|---------------------|\n|              |       |        | VQA v2                     | GQA                        | VisWiz                     | SQA I                      | VQA T                      | POPE                | MMB                 | LLaVA W             | MM-Vet",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 32,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-33",
      "content": "QA                        | VisWiz                     | SQA I                      | VQA T                      | POPE                | MMB                 | LLaVA W             | MM-Vet              |\n| LLaVA-1.5    | V-7B  | 336    | -                          | 62.0 ∗                     | -                          | -                          | -                          | -                   | -                   | -                   | 30.5                |\n| BLIP-2       | V-13B | 224    | 41.0                       | 41.0                       | 19.6                       | 61.0                       | 42.5                       | 85.3                | -                   | 38.1                | 22.4                |\n| InstructBLIP | V-13B | 224    | -                          | 49.5                       | 33.4                       | 63.1                       | 50.7                       | 78.9                | -                   | 58.2                | 25.6",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 33,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-34",
      "content": "49.5                       | 33.4                       | 63.1                       | 50.7                       | 78.9                | -                   | 58.2                | 25.6                |\n| IDEFICS-80B  | L-65B | 224    | 60.0                       | 45.2                       | 36.0                       | -                          | 30.9                       | -                   | 54.5                | -                   | -                   |\n| MiniGPT-4    | L-7B  | 224    | -                          | 30.8                       | 47.5                       | 25.4                       | 19.4                       | -                   | 23.0                | -                   | 22.1                |\n| IDEFICS-9B   | L-7B  | 224    | 50.9                       | 38.4                       | 35.5                       | -                          | 25.9                       | -                   | 48.2                | -                   | -",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 34,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-35",
      "content": "38.4                       | 35.5                       | -                          | 25.9                       | -                   | 48.2                | -                   | -                   |\n| mPLUG-Owl    | L-7B  | 224    | -                          | 14.0                       | 39.0                       | 2.8                        | 38.8                       | -                   | 46.6                | -                   | -                   |\n| Otter        | L-7B  | 224    | -                          | 38.1                       | 50.0                       | 27.2                       | 21.2                       | -                   | 32.6                | -                   | 24.6                |\n| InstructBLIP | V-7B  | 224    | -                          | 49.2                       | 34.5                       | 60.5                       | 50.1                       | -                   | 36.0                | 60.9                | 26.2",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 35,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-36",
      "content": "| 49.2                       | 34.5                       | 60.5                       | 50.1                       | -                   | 36.0                | 60.9                | 26.2                |\n| LLaVA-1.5 †  | V-7B  | 224    | 72.3 ∗                     | 56.9 ∗                     | 47.8                       | 67.9                       | 49.2                       | 83.3                | 59.5                | 63.3                | 25.7                |\n| Video-LLaVA  | V-7B  | 224    | 74.7 ∗                     | 60.3 ∗                     | 48.1                       | 66.4                       | 51.8                       | 84.4                | 60.9                | 73.1                | 32.0                |\n\npabilities of large video-language models on four datasets, including MSVD-QA (Chen and Dolan, 2011), MSRVTT-QA (Xu et al., 2016), TGIFQA (Jang et al., 2017) and ActivityNet-QA (Yu et al., 2019).",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 36,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-37",
      "content": "|\n\npabilities of large video-language models on four datasets, including MSVD-QA (Chen and Dolan, 2011), MSRVTT-QA (Xu et al., 2016), TGIFQA (Jang et al., 2017) and ActivityNet-QA (Yu et al., 2019). The evaluation pipeline for video understanding follows Video-ChatGPT. We report the accuracy and score, which is assessed using GPT-Assistant. Video-LLaVA consistently outperforms Video-ChatGPT in terms of questionanswering accuracy, which is an advanced large video-language model. Moreover, Video-LLaVA surpasses the powerful baseline of Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, we conduct comparisons with the recent\n\nSOTA model, Chat-UniVi (Jin et al., 2023). Despite Chat-UniVi utilizing more datasets such as MIMIC-IT (Li et al., 2023a), Video-LLaVA still demonstrate competitive results, surpassing ChatUniVi on MSVD, MSRVTT, and TGIF datasets.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 37,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-38",
      "content": "al., 2023). Despite Chat-UniVi utilizing more datasets such as MIMIC-IT (Li et al., 2023a), Video-LLaVA still demonstrate competitive results, surpassing ChatUniVi on MSVD, MSRVTT, and TGIF datasets. In summary, these results validate Video-LLaVA's ability to comprehend videos and provide contextually appropriate responses based on instructions.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.1 Zero-shot Video Understanding",
        "chunkIndex": 38,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-39",
      "content": "As shown in Table 3, we evaluate our approach for image understanding on five academic image question-answering benchmarks. Compared to the state-of-the-art model InstructBLIP-7B, VideoLLaVA demonstrates powerful image understand-\n\nTable 4: Zero-shot object hallucination evaluation results are reported for three POPE evaluation settings. 'Yes' indicates the proportion of positive responses to the given question. † donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly.\n\n| Methods      | LLM   | Adersarial   | Adersarial   | Adersarial   | Popular   | Popular   | Popular   | Random   | Random   | Random   |\n|--------------|-------|--------------|--------------|--------------|-----------|-----------|-----------|----------|----------|----------|\n| Methods      | LLM   | Accuracy     | F1-Score     | Yes          | Accuracy  | F1-Score  | Yes       | Accuracy | F1-Score | Yes      |\n| MiniGPT-4    | V-13B | 66.6         | 71.4         | 66.7         | 68.3",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.2 Zero-shot Image Question-answering",
        "chunkIndex": 39,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-40",
      "content": "LLM   | Accuracy     | F1-Score     | Yes          | Accuracy  | F1-Score  | Yes       | Accuracy | F1-Score | Yes      |\n| MiniGPT-4    | V-13B | 66.6         | 71.4         | 66.7         | 68.3      | 72.2      | 64.1      | 77.8     | 78.9     | 54.8     |\n| InstructBLIP | V-13B | 74.4         | 78.5         | 69.0         | 81.4      | 83.5      | 62.6      | 88.7     | 89.3     | 55.2     |\n| MM-GPT       | L-7B  | 50.0         | 66.7         | 100.0        | 50.0      | 66.7      | 100.0     | 50.0     | 66.7     | 100.0    |\n| mPLUG-Owl    | L-7B  | 50.7         | 66.8         | 98.7         | 50.9      | 66.9      | 98.6      | 54.0     | 66.4     | 95.6     |\n| Chat-UniVi   | V-7B  | 55.6         | 68.7         | 91.6         | 56.4      | 69.0      | 90.8      | 73.9     | 79.3     | 74.6     |\n| LLaVA-1.5 †  | L-7B  | 84.3         | 83.2         | 43.5         | 79.8      | 79.4      | 48.0      | 85.7     | 84.8     | 43.0     |\n| Video-LLaVA  | V-7B  | 81.6         | 80.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.2 Zero-shot Image Question-answering",
        "chunkIndex": 40,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-41",
      "content": "| 74.6     |\n| LLaVA-1.5 †  | L-7B  | 84.3         | 83.2         | 43.5         | 79.8      | 79.4      | 48.0      | 85.7     | 84.8     | 43.0     |\n| Video-LLaVA  | V-7B  | 81.6         | 80.8         | 45.8         | 85.3      | 84.0      | 42.1      | 86.2     | 85.2     | 42.0     |\n\ning capabilities, outperforming across all five question-answering benchmarks. Additionally, Video-LLaVA exhibits competitive results compared to several more powerful LVLMs, which are tuned based on 13B or 65B LLM, such as surpassing InstructBLIP-13B by 14.7% on VisWiz, highlighting its strong understanding ability in natural visual environments. Furthermore, to ensure a fair comparison, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder, called LLaVA-1.5 † . This demonstrates that the performance improvement observed in Video-LLaVA is not solely attributed to a stronger image encoder. Additional details can be found in Section 4.3.6.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.2 Zero-shot Image Question-answering",
        "chunkIndex": 41,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-42",
      "content": "called LLaVA-1.5 † . This demonstrates that the performance improvement observed in Video-LLaVA is not solely attributed to a stronger image encoder. Additional details can be found in Section 4.3.6.\n\nEvaluation under Image Benchmark Toolkits Additionally, we evaluate LVLMs using several benchmark toolkits for visual instruction tuning. These benchmark toolkits provide a detailed assessment of the model's capabilities through robust evaluation metrics. Video-LLaVA outperform InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on MMBench, LLaVA-Bench, and MM-Vet, respectively. It is worth noting that Video-LLaVA-7B still demonstrates advanced performance compared to larger LLM models, surpassing InstructBLIP13B by 6.4% on MM-Vet and IDEFICS-80B (Laurençon et al., 2023) by 6.4% on MMBench. These results demonstrate that Video-LLaVA exhibits a strong understanding of semantic aspects of scenes, enabling it to answer open-ended and free-form natural language questions about images.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.2 Zero-shot Image Question-answering",
        "chunkIndex": 42,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-43",
      "content": "lts demonstrate that Video-LLaVA exhibits a strong understanding of semantic aspects of scenes, enabling it to answer open-ended and free-form natural language questions about images.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.2 Zero-shot Image Question-answering",
        "chunkIndex": 43,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-44",
      "content": "As shown in Table 4, we report evaluation results for zero-shot object hallucinations, utilizing a evaluation pipeline derived from a polling-based query method (Li et al., 2023d). Video-LLaVA demon- strates competitive performance across three subsets: random, popular, and adversarial. Specifically, when compared to the 7B foundation model, VideoLLaVA consistently outperforms MM-GPT (Gong et al., 2023) across all three POPE hallucination evaluation subsets. Furthermore, when benchmarked against the larger 13B LLM, Video-LLaVA even surpasses Mini-GPT4 comprehensively. The successful performance of Video-LLaVA in object hallucination detection validates the consistency between unified visual representations and the generation of textual descriptions.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.2.3 Object Hallucination Evaluation",
        "chunkIndex": 44,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-45",
      "content": "To validate the performance degradation caused by separated visual representation, we conduct experiments to to explore the performance of the LLM learning from different visual representations. We define the use of LanguageBind image encoder as unified visual representation while the MAE encoder (He et al., 2022) use separated visual representation, which is a well-known and effective image feature extractor. Additionally, since MAE do not interact with multi-modal inputs during the training process, we utilize CLIP-L/14, a model of the same size. While CLIP-L/14 exhibits strong multimodal understanding capabilities, it is not prealigned with the video encoder. Consequently, this results in a lack of uniformity in the visual features provided to LLM. We only replace the image encoder of the same scale and keep the LanguageBind video encoder.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.1 Alignment Before Projection",
        "chunkIndex": 45,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-46",
      "content": "Due to replacing the image encoder with the MAE encoder, the video features and image features are\n\nTable 5: Effect of alignment before projection on image. 'United' refers to the unified visual representation, while 'Separated' refers to the separated visual representation. Benchmark names are abbreviated due to page limitations.\n\n| Methods        |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |\n|----------------|----------------------------------------------------------|----------------------------------------------------------|--------------------------------",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 46,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-47",
      "content": "olkit POPE MMB LLaVA W MM-Vet |\n|----------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|\n| Separated-MAE  |                                                     66   |                                                     55.4 |                                                     42.5 |                                                     65   |                                                     44.2 |                                        80.8 |                                        45.7 |                                        35.9 |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 47,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-48",
      "content": "44.2 |                                        80.8 |                                        45.7 |                                        35.9 |                                        20   |\n| Separated-CLIP |                                                     74.6 |                                                     59.9 |                                                     47.8 |                                                     67.3 |                                                     51.5 |                                        84.4 |                                        60.2 |                                        68.9 |                                        30.6 |\n| United         |                                                     74.7 |                                                     60.3 |                                                     48.1 |                                                     66.4 |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 48,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-49",
      "content": "7 |                                                     60.3 |                                                     48.1 |                                                     66.4 |                                                     51.8 |                                        84.4 |                                        60.9 |                                        73.1 |                                        32   |\n| ∆ Acc.         |                                                      0.1 |                                                      0.4 |                                                      0.3 |                                                     -0.9 |                                                      0.3 |                                         0   |                                         0.7 |                                         4.2 |                                         1.4 |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 49,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-50",
      "content": "0.3 |                                         0   |                                         0.7 |                                         4.2 |                                         1.4 |\n\nTable 6: Effect of joint training on video. We evaluate on four video question-answering datasets. ∗ denotes that we utilized only video data in both the first and second stages.\n\nFigure 4: Effect of alignment before projection on video. We validate and report the accuracy and score on four video question-answering datasets.\n\n| Methods          | MSVD-QA Accuracy Score   | MSVD-QA Accuracy Score   | MSRVTT-QA Accuracy   | Score   | TGIF-QA Accuracy Score   | TGIF-QA Accuracy Score   | ActivityNet-QA Accuracy   | ActivityNet-QA Accuracy   |\n|------------------|--------------------------|--------------------------|----------------------|---------|--------------------------|--------------------------|---------------------------|---------------------------|\n| Methods          | 64.8",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 50,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-51",
      "content": "---------------------|----------------------|---------|--------------------------|--------------------------|---------------------------|---------------------------|\n| Methods          | 64.8                     | 3.2                      | MSRVTT-QA Accuracy   | Score   |                          | 3.4                      |                           | Score                     |\n| Video-LLaVA ∗    |                          |                          | 58.3                 | 3.4     | 67.8                     |                          | 40.7                      | 2.0                       |\n| Joint with Image | 70.7                     | 3.9                      | 59.2                 | 3.5     | 70.0                     | 4.0                      | 45.3                      | 3.3                       |\n| ∆ Acc.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 51,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-52",
      "content": "| 3.9                      | 59.2                 | 3.5     | 70.0                     | 4.0                      | 45.3                      | 3.3                       |\n| ∆ Acc.           | +5.9                     | +0.7                     | +0.9                 | +0.1    | +2.2                     | +0.6                     | +4.6                      | +1.3                      |\n\n<!-- image -->\n\n<!-- image -->\n\nno longer unified during LLM's initial learning of visual representations. In Figure 4, compared to separated visual representation, the united visual representation significantly improves performance across 4 video question-answering datasets. Separated visual representations not only exhibit lower accuracy in question-answering, but also demonstrate a similar trend in answer scores. These results demonstrate that the unified visual representation can help the LLM further learn and understand videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 52,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-53",
      "content": "imilar trend in answer scores. These results demonstrate that the unified visual representation can help the LLM further learn and understand videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.2 For Video Understanding",
        "chunkIndex": 53,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-54",
      "content": "The unified visual representation demonstrates strong performance, surpassing the separated visual representation comprehensively across 5 image question-answering datasets and 4 benchmark toolkits in Table 5. Additionally, we observe a significant margin of performance improvement in the unified visual representation on the MM-\n\nFigure 5: Effect of joint training on image. † donates that We reproduce the results of LLaVA-1.5 at a resolution of 224×224 with LanguageBind-Image encoder for a fair comparison.\n\n<!-- image -->\n\nBench, LLaVA-Bench, and MM-Vet benchmark toolkits. This highlights that the unified visual representation not only enhances performance in image question-answering but also provides benefits in other aspects of image understanding, such as reducing object hallucination and improving OCR capabilities.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.3 For Image Understanding",
        "chunkIndex": 54,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-55",
      "content": "This subsection aims to validate the complementarity of images and videos during joint training, which can mutually enhance the LLM's understanding of images and videos based on a unified visual representation.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.4 Joint Training",
        "chunkIndex": 55,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-56",
      "content": "For comparing performance on video benchmarks, we remove image data during the training of VideoLLaVA, which is called Video-LLaVA ∗ . We com- pare with Video-LLaVA ∗ to assess the performance gains from joint image training on video benchmarks. In Table 6, we evaluate our model on four video question-answering datasets. Compared to Video-LLaVA ∗ without image in training, the model trained with joint images and videos achieves comprehensive improvements across all four video datasets. These results demonstrate that joint training of images and videos facilitates LLM's understanding of visual representations.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.5 For Video Understanding",
        "chunkIndex": 56,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-57",
      "content": "When comparing performance on image benchmarks, it is challenging to find a image-based LVLM with the same configuration as VideoLLaVA. To address this, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder and reproduce the results at a resolution of 224×224 by using the same training configuration, called LLaVA-1.5 † . As shown in Figure 5, Compared to LLaVA-1.5 † , which utilizes the same image encoder configuration, we observe performance improvements in 8 out of 9 benchmarks, demonstrating mutual improvement in visual understanding. Video-LLaVA outperform LLaVA-1.5 † in POPE, indicating that joint training with videos alleviates the object hallucination in images. The similar trend is observed on some other benchmark toolkits, such as LLaVA-Bench and MMBench, where video data significantly improves LLM's performance in complex reasoning and image conversation tasks.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "4.3.6 For Image Understanding",
        "chunkIndex": 57,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-58",
      "content": "While Video-LLaVA exhibits strong competitiveness in both images and videos, we still observed some limitations of Video-LLaVA. To begin with, Video-LLaVA performs moderately in understanding long videos. In Table 2, Chat-UniVi surpasses 0.5 on ActivityNet-QA because Video-LLaVA only utilizes uniformly sampled 8 frames to comprehend the video, which results in the loss of detailed information from long videos. Additionally, training Video-LLaVA is computationally expensive, requiring 3-4 days to complete the training process on 8 A100-80G GPUs.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "5.1 Limitation",
        "chunkIndex": 58,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-59",
      "content": "In the future, We maybe can explore more efficient shared projection mode that can compress tokens while preserving data features. This would support Video-LLaVA in better understanding long videos. Besides, Video-LLaVA can serve as a baseline to extend to additional visual-related modalities, such as depth and infrared images. Additionally, we could explore how to incorporate timestamp embeddings effectively, enabling large visual-language models to answer questions related to temporal relationships.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "5.2 Future Directions",
        "chunkIndex": 59,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-60",
      "content": "In this work, we introduce Video-LLaVA, a simple but powerful large visual-language baseline model. We propose a novel framework to address the issue of misalignment before projection, utilizing a LanguageBind encoder to pre-bind visual signals into the language feature space. To enable a LLM to comprehend both images and videos simultaneously, we conduct joint training on images and videos, allowing the LLM to learn multi-modal interactions from a unified visual representation. Extensive experiments demonstrate that joint training on images and videos mutually benefits performance. Furthermore, we validate that aligning visual representations before projection aids LLM learning. Remarkably, LLM, after learning from a unified visual representation, exhibits the remarkable ability to simultaneously engage with both images and videos, showcasing a powerful comprehension of unified visual concepts.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 60,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-61",
      "content": "ter learning from a unified visual representation, exhibits the remarkable ability to simultaneously engage with both images and videos, showcasing a powerful comprehension of unified visual concepts. These results collectively demonstrate the effectiveness of the Video-LLaVA training framework. As a unified visual training framework, the performance of Video-LLaVA even surpasses that of expert models designed specifically for images or videos.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 61,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-62",
      "content": "This work was supported in part by the Natural Science Foundation of China (No. 62202014, 62332002, 62425101), Shenzhen Basic Research Program (No.JCYJ20220813151736001).",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "Acknowledgments",
        "chunkIndex": 62,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-63",
      "content": "- Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716-23736.\n- Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 .\n- Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1728-1738.\n- Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. Palm: Pre-training an autoencoding&amp;autoregressive language model for context-conditioned generation.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-64",
      "content": "Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. Palm: Pre-training an autoencoding&amp;autoregressive language model for context-conditioned generation. arXiv preprint arXiv:2004.07159 .\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n- David Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies , pages 190-200.\n- Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-65",
      "content": "0.\n- Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160 .\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) .\n- Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint , arXiv:2305.06500.\n- Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2023.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-66",
      "content": "truction tuning. Preprint , arXiv:2305.06500.\n- Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 .\n- Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. 2023. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010 .\n- Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15180-15190.\n- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-67",
      "content": "ence on Computer Vision and Pattern Recognition , pages 15180-15190.\n- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 .\n- Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904-6913.\n- Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3608-3617.\n- Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-68",
      "content": "IEEE conference on computer vision and pattern recognition , pages 3608-3617.\n- Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905 .\n- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 16000-16009.\n- Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 .\n- Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709.\n- Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-69",
      "content": "gs of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709.\n- Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. Openclip. If you use this software, please cite it as below.\n- Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2758-2766.\n- Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. 2023. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046 .\n- Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-70",
      "content": "s with image and video understanding. arXiv preprint arXiv:2311.08046 .\n- Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning , pages 5583-5594. PMLR.\n- Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Preprint , arXiv:2306.16527.\n- Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 .\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-71",
      "content": "arXiv preprint arXiv:2305.03726 .\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 .\n- Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning , pages 12888-12900. PMLR.\n- Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems , 34:9694-9705.\n- KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023c. Videochat: Chat-centric video understanding.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-72",
      "content": "formation processing systems , 34:9694-9705.\n- KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023c. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 .\n- Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 .\n- Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. 2024. Moe-llava: Mixture of experts for large visionlanguage models. arXiv preprint arXiv:2401.15947 .\n- Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 .\n- Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-73",
      "content": "d Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 .\n- Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485 .\n- Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023c. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 .\n- Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems , 35:2507-2521.\n- Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. 2023. Valley: Video assistant with large language model enhanced ability.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-74",
      "content": "Systems , 35:2507-2521.\n- Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. 2023. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207 .\n- Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. 2023. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093 .\n- Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 .\n- OpenAI. 2023. Gpt-4 technical report. Preprint , arXiv:2303.08774.\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-75",
      "content": "Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 .\n- Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556-2565.\n- Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-76",
      "content": "th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556-2565.\n- Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 .\n- Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8317-8326.\n- Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 .\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-77",
      "content": "python execution for reasoning. arXiv preprint arXiv:2303.08128 .\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .\n- Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-78",
      "content": "arXiv preprint arXiv:2307.09288 .\n- Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 .\n- Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msrvtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 5288-5296.\n- Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 .\n- Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-79",
      "content": "o Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 .\n- Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549 .\n- Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 .\n- Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 9127-9134.\n- Hang Zhang, Xin Li, and Lidong Bing. 2023a.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-80",
      "content": "or understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 9127-9134.\n- Hang Zhang, Xin Li, and Lidong Bing. 2023a. Videollama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 .\n- Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. 2023b. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 .\n- Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. 2023a. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852 .\n- Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023b. Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-81",
      "content": "arXiv preprint arXiv:2310.01852 .\n- Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023b. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 .",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-82",
      "content": "Weshow some training settings as shown in Table 7. video encoder and image encoder are not trained in both stages. The projection layer consists of 2 linear layers with a GeLU (Hendrycks and Gimpel, 2016) activation function between them. Image and video share the projection layer.\n\nTable 7: Training setting.\n\n| Config                                                                                                                                               | Pretraining                                                                                         | Instruction tuning                                                                                  |\n|------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.1 Training Setting",
        "chunkIndex": 82,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-83",
      "content": "--------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| Video encoder Image encoder Optimizer Deepspeed Epochs Vision select layer Weight decay Warmup ratio Learning rate schedule Learning rate Batch size | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 |",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.1 Training Setting",
        "chunkIndex": 83,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-84",
      "content": "We show some unselected samples here, and these videos are sourced from Video-ChatGPT (Maaz et al., 2023).\n\n<!-- image -->\n\nFigure 6: Samples of Video-LLaVA in video understanding.\n\n<!-- image -->\n\n[Input Instruction]\n\nWhat is strange about this video?\n\nThe video is strange because it shows a man sitting next to a lion and next to a man. The man is also seen petting a lion in the grass.\n\n[Input Instruction]\n\nWhat is unusual about this video?\n\nThe unusual aspect of this video is that the giraffe is seen swimming i not a common sight. The giraffe is also seen jumping off a diving boa\n\nunderwater, which adds to the uniqueness of the video.\n\n[Input Instruction]\n\nCan you write a short poem inspired from\n\nSure, here's a short poem inspired by the video:\n\nThe flowers bloom, a gentle grace,\n\nTheir petals soft, their colors trace.\n\nTheir beauty, a sight to behold,\n\nA moment of peace, a moment to hold.\n\nTheir colors, a symphony of hues,\n\nA celebration of life, and all it can do.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 84,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-85",
      "content": "tle grace,\n\nTheir petals soft, their colors trace.\n\nTheir beauty, a sight to behold,\n\nA moment of peace, a moment to hold.\n\nTheir colors, a symphony of hues,\n\nA celebration of life, and all it can do.\n\nTheir beauty, a reminder to cherish,\n\nThe simple things, that bring us happiness.\n\nSo let us take a moment to appreciate,\n\nThe beauty of flowers, and all they can create.\n\n[Input Instruction]\n\nDescribe the video in detail.\n\nThe video starts with a car covered in snow, and a person is seen usin remove the snow from the car's windows. The person is seen scraping\n\nwindshield and the side windows of the car. The person continues to s the car, making sure that all the snow is removed. The video then sho\n\nwiping the snow off the car's windshield and side windows. The perso cloth to wipe the snow off the car's windows, making sure that the car\n\nclean. The video ends with the person finishing the job and the car bei",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 85,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-86",
      "content": "now off the car's windshield and side windows. The perso cloth to wipe the snow off the car's windows, making sure that the car\n\nclean. The video ends with the person finishing the job and the car bei\n\nOverall, the video shows a person removing snow from a car's windo and then wiping the snow off the car's windows to make sure it is com\n\n[Input Instruction]\n\nDescribe the video in detail.\n\nIn the video, we see a man walking down a track while holding a javel down the track and throws the javelin. The video then shows the man\n\nthe starting point, and we see the javelin landing on the ground. The m away from the camera. From the video, we can see that the man is a ja\n\nhe is practicing his throws. The track appears to be well-maintained, a surroundings seem to be quiet and peaceful. The man's form and techn\n\nimpressive, and it is clear that he has experience in the sport. Overall, t a glimpse into the world of javelin throwing and the dedication and sk\n\nin the sport.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 86,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-87",
      "content": "nd peaceful. The man's form and techn\n\nimpressive, and it is clear that he has experience in the sport. Overall, t a glimpse into the world of javelin throwing and the dedication and sk\n\nin the sport.\n\nFigure 7: Samples of Video-LLaVA in video understanding.\n\n[Input Instruction]\n\nDescribe the acti\n\nThe video shows a man playing the dr his hands and sticks, and he continues\n\n[Input Instruction]\n\nBriefly describe t\n\nThe video features a woman grooming hair and cleans its hooves, and the hor\n\nThe video also includes shots of the w\n\n[Input Instruction]\n\nWhat is the main\n\nThe main challenge faced by the peopl seen struggling to stay upright and ma\n\nforth.\n\n[Input Instruction]\n\nWhere is this vi is shown in the video?\n\nThe video is taken from the top of the\n\nNew York City. The statue is shown fr from the top of the statue itself.\n\n[Input\n\nInstruction]\n\nCan you\n\ndesc window as demonstrated in the vide\n\nSure! The man in the video demonstra down with a rag. He then applies a cle",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 87,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-88",
      "content": "atue is shown fr from the top of the statue itself.\n\n[Input\n\nInstruction]\n\nCan you\n\ndesc window as demonstrated in the vide\n\nSure! The man in the video demonstra down with a rag. He then applies a cle\n\nagain. Finally, he shows how to dry th n]\n\nWhy is this video funny?\n\nfunny because the baby is playing with a Wii remote, which is not a toy ciated with babies. The baby's actions are also humorous, such as pointing\n\nand making faces, which adds to the overall amusement of the video.\n\nn]\n\nCan you identify any safety hazards in this video?\n\nis playing with a toy truck and a toy car, which could pose a safety risk if upervised. However, the baby is sitting on the floor, so there is less risk of\n\nred to if they were playing with the toys on a higher surface.\n\non]\n\nCreate a short fairy tale with a moral lesson inspired time, in a kingdom far away, there lived a beautiful princess named Rose.\n\nwn for her\n\nkindness and\n\nbeauty, and\n\neveryone loved\n\nher.\n\nOne day,\n\na",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 88,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-89",
      "content": "eate a short fairy tale with a moral lesson inspired time, in a kingdom far away, there lived a beautiful princess named Rose.\n\nwn for her\n\nkindness and\n\nbeauty, and\n\neveryone loved\n\nher.\n\nOne day,\n\na\n\nince named Leo came to visit the kingdom. He was fascinated by Rose's ecided to stay in the kingdom. However, Rose's jealous sister, Lily, did not\n\ntion Leo was giving her sister. She tried to harm Rose, but Leo saved her in love. The kingdom celebrated their love and they lived happily ever after.\n\nthe story is that true love conquers all and that jealousy can lead to harmful\n\n.\n\nDescribe the video.\n\nn]\n\nows a person preparing a dish with peppers and oranges. They slice the oranges and lay them out on a table. The person then proceeds to cut the\n\noranges into strips and lays them out on a piece of wax paper. They use a he peppers and oranges into strips and lay them out on a piece of wax paper.",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 89,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-90",
      "content": "on a table. The person then proceeds to cut the\n\noranges into strips and lays them out on a piece of wax paper. They use a he peppers and oranges into strips and lay them out on a piece of wax paper.\n\nen uses a spoon to mix the peppers and oranges together. The video ends on cutting the peppers and oranges into strips and laying them out on a piece\n\n.\n\nn]\n\nWhat is the main focus of the video.\n\nus of the video is the process of creating a painting of a tree using various d colors. The video shows the artist's hand holding a paintbrush and using it\n\nree, as well as the use of different colors and techniques to create the on]\n\nWhat does the woman use to split the logs and how does uses an ax to split the logs. She holds the ax in her hand and swings it down\n\nShe repeats this process several times, splitting the log into smaller pieces.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 90,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-91",
      "content": "plit the logs and how does uses an ax to split the logs. She holds the ax in her hand and swings it down\n\nShe repeats this process several times, splitting the log into smaller pieces.\n\n<!-- image -->\n\nThe video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.\n\nThe video is about a man who is seen spinning and throwing a discus in a field. He then\n\nWhat activities are the woman and the dog engaged in\n\nThe woman and the dog are playing frisbee in the yard. The woman throws the frisbee,\n\nThe video shows two men arm wrestling on a table while being watched by a group of\n\nStep-by-step describe the cooking recipe shown in the\n\nThe video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that,",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 91,
        "totalChunks": 93
      }
    },
    {
      "id": "2311.10122v3-chunk-92",
      "content": "recipe shown in the\n\nThe video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that,\n\nthey mix the noodles and the ingredients together. Finally, they serve the dish on a plate.\n\nFigure 8: Samples of Video-LLaVA in video understanding.\n\nFigure 9: Samples of Video-LLaVA in video understanding.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.10122v3",
        "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
        "authors": [
          "Bin Lin",
          "Yang Ye",
          "Bin Zhu",
          "Jiaxi Cui",
          "Munan Ning",
          "Peng Jin",
          "Li Yuan"
        ],
        "section": "A.2 Exhibition Board",
        "chunkIndex": 92,
        "totalChunks": 93
      }
    }
  ],
  "fullText": "## Video-LLaVA: Learning United Visual Representation by Alignment Before Projection\n\nBin Lin 1 , Yang Ye 1 , Bin Zhu 1 , Jiaxi Cui 4 , Munang Ning 1,2,3 , Peng Jin 1,2,3 , Li Yuan 1,2,3\n\n1 2 Peng Cheng Laboratory,\n\nAI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School,\n\nPeking University Shenzhen Graduate School, 3 4 PandaVilla Tech Limited\n\nCorrespondence:\n\nyuanli-ece@pku.edu.cn\n\nGitHub:\n\nhttps://github.com/PKU-YuanGroup/Video-LLaVA\n\n## Abstract\n\nLarge Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA , which learns from a mixed dataset of images and videos, mutually enhancing each other. As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.\n\n## 1 Introduction\n\nRecently, LLMs have gained rapid popularity in the AI community, such as GPT-3.5, GPT-4 (OpenAI, 2023), PaLM (Bi et al., 2020; Anil et al., 2023), and BLOOM (Scao et al., 2022). They rely on their powerful language comprehension abilities to follow human-provided instructions and provide corresponding responses. Typically, LLMs can only respond within the text input provided by the user, which is insufficient because human interaction with the world involves multiple channels, such as visual and textual. To this end, recent\n\nFigure 1: Comparing Different LVLM Paradigms. Video-LLaVA aligns images and videos before projection, allowing LLM to learn from a unified visual representation and endowing LLM with the ability to comprehend both images and videos simultaneously.\n\n<!-- image -->\n\nworks (Ye et al., 2023; Zhu et al., 2023b; Alayrac et al., 2022) have mapped images into text-like tokens, enabling LLMs to emerge with the ability to comprehend images. Despite their effectiveness, empowering LLMs to understand videos is more challenging than image-only comprehension tasks. Nevertheless, recent work (Maaz et al., 2023; Li et al., 2023c; Zhang et al., 2023a) has made initial strides in enabling interactions between video and language.\n\nHowever, most current LVLMs (Li et al., 2023b; Dai et al., 2023; Luo et al., 2023; Li et al., 2023a; Yin et al., 2023; Fu et al., 2023) can primarily handle a single visual modality, either imagelanguage or video-language. We compare different LVLM paradigms as shown in Figure 1, where VideoChat (Li et al., 2023c) and VideoLLaMA (Zhang et al., 2023a) utilize a share visual encoder to handle both images and videos. However, due to the inherent differences in the media types of images and videos, it is challenging to learn a unified representation, and the performance falls significantly behind that of the specialized video expert model, Video-ChatGPT. Therefore,\n\nX-LLM (Chen et al., 2023) and Macaw-LLM (Lyu et al., 2023) allocate a modality-specific encoder for each modality, attempting to enable a LLM to comprehend images or videos through several projection layers. But their performances are inferior to dedicated video expert models such as VideoChatGPT (Maaz et al., 2023). We attribute this phenomenon to the lack of alignment before projection . Because image features and video features reside in their own spaces, this poses a challenge for a LLM to learn their interactions from several poor projection layers. Some similar phenomenon such as alignment before fusion has been discussed by ALBEF(Li et al., 2021) and ViLT (Kim et al., 2021) in multi-model models. More recently, ImageBindLLM (Han et al., 2023) focuses on enabling the LLM to simultaneously process multiple modal inputs by pre-aligning each modality to a common feature space (Girdhar et al., 2023). Based on a large image-language model, ImageBind-LLM converts other modalities into the most similar image features by retrieving from a training-free image cached database. However, the indirect alignment approach of ImageBind-LLM may lead to performance degradation, and the LLM has no knowledge of actual video data.\n\nIn this work, we introduce Video-LLaVA , a simple but powerful baseline for the LVLM simultaneously handling both images and videos. Specifically, As shown in Figure 1, Video-LLaVA initially aligns the representations of images and videos to a unified visual feature space. Since the visual representations are already aligned prior to projection, we employ a shared projection layer to map the unified visual representation for the LLM. To enhance computational efficiency, Video-LLaVA undergoes joint training of images and videos, achieving remarkable results with 1 training epoch.\n\nAs a result, The proposed Video-LLaVA greatly enhances the ability of the LLM to simultaneously understand both images and videos. For image understanding, Video-LLaVA surpasses advanced LVLMs such as mPLUG-owl-7B and InstructBLIP7B in 5 image benchmarks. Additionally, utilizing 4 benchmark toolkits for a more comprehensive evaluation, Video-LLaVA-7B even outperforms IDEFICS-80B by 6.4% in MMBench. Moreover, similar trends can be observed in video understanding, where Video-LLaVA surpasses VideoChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% respectively on the MSVD, MSRVTT, TGIF, and ActivityNet video question-answering datasets. Ex- tensive ablation experiments demonstrate that alignment before projection yields greater benefits. Additionally, joint training of images and videos can facilitate a unified visual representation in LLM comprehension.\n\nWe summarize our primary contributions as follows:\n\n- We introduce Video-LLaVA , a powerful LVLM baseline. During the training process, Video-LLaVA binds visual signals to the language feature space, unifying visual representations, and proposes a solution to align before projection. We enable an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n- Extensive experiments demonstrate that a unified visual representation benefits LLMs in learning to simultaneously handle both images and videos, validating the complementarity of modalities, showcasing significant superiority when compared to models specifically designed for either images or videos.\n\n## 2 Related Work\n\n## 2.1 Large Language Models\n\nWhen the well-known commercial model ChatGPT (OpenAI, 2023) was introduced, the The AI community released open-source Large Language Models (LLMs) by instruction tuning and increasing model sizes. These include LLaMA (Touvron et al., 2023a), Vicuna (Chiang et al., 2023), Alpaca (Taori et al., 2023), and more recently, LLaMA 2 (Touvron et al., 2023b). These models are tuned with instruction sets to emulate conversations between humans and AI assistants. Furthermore, InstructGPT (Ouyang et al., 2022) is trained based on GPT-3 (Brown et al., 2020) with 175 billion parameters through aligning with human preferences. However, LLMs can only interact within text. In this work, we introduce Video-LLaVA, which builds upon the powerful reasoning capabilities of LLM to extend modality interactions to images and videos.\n\n## 2.2 Large Vision-Language Models\n\nWhen extending LLMs to multi-modal, especially involving images and videos, the main approaches can be categorized into two types in Table 1: i) treating LLM as a scheduler, ii) treating LLM as a decoder.\n\nTable 1: Comparison between different Large Vision-Language Models. For methods that treat LLMs as scheduler, they do not require pre-alignment and joint training.\n\n| Methods                           | Image   | Video   | Pre-aligned   | Joint training   |\n|-----------------------------------|---------|---------|---------------|------------------|\n| LLMs as scheduler                 |         |         |               |                  |\n| VisualChatGPT (Wu et al., 2023)   | ✔       | ✗       | -             | -                |\n| HuggingGPT (Shen et al., 2023)    | ✔       | ✗       | -             | -                |\n| MM-REACT (Yang et al., 2023)      | ✔       | ✔       | -             | -                |\n| ViperGPT (Surís et al., 2023)     | ✔       | ✔       | -             | -                |\n| LLMs as decoder                   |         |         |               |                  |\n| Mini-GPT4 (Zhu et al., 2023b)     | ✔       | ✗       | -             | ✗                |\n| LLaVA (Liu et al., 2023b)         | ✔       | ✗       | -             | ✗                |\n| Video-ChatGPT (Maaz et al., 2023) | ✗       | ✔       | -             | ✗                |\n| VideoChat (Li et al., 2023c)      | ✔       | ✔       | ✗             | ✔                |\n| Video-LLaMA (Zhang et al., 2023a) | ✔       | ✔       | ✗             | ✔                |\n| ImageBind-LLM (Han et al., 2023)  | ✔       | ✔       | ✔             | ✗                |\n| Video-LLaVA (Ours)                | ✔       | ✔       | ✔             | ✔                |\n\n## 2.2.1 LLMs as scheduler\n\nIn the scheduler-based methods, various visual models are treated as plug-and-play modules. LLM schedules them according to the specific visual task requirements, like the assembly of building blocks. Some of these methods focus on images, such as VisualChatGPT (Wu et al., 2023) and HuggingGPT (Shen et al., 2023), while MM-REACT (Yang et al., 2023) and ViperGPT (Surís et al., 2023) can also handle videos. A key characteristic of these scheduler-based LVLMs is that they do not require end-to-end training, hence eliminating the need for pre-alignment and joint training of each modality.\n\n## 2.2.2 LLMs as decoder\n\nRegarding the approach of treating LLM as a decoder, this is our primary focus. MiniGPT-4 (Zhu et al., 2023b) aligns image tokens to the input of the large language model through several linear projection layers. However, this alignment is weak and lacks feedback from human instructions. Subsequently, mPLUG-Owl (Ye et al., 2023) adopts a two-stage training approach. In the first stage, images are aligned with language using an autoregressive pretraining style, and the second stage involves instruction tuning through using a human instruction dataset. With the increasing scale of large language model backends, approaches such as InstructBLIP (Dai et al., 2023) and LLaVA series (Liu et al., 2023b,a; Lin et al., 2024) collecte the larger human instruction datasets to train a larger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given instructions. Then they undergo end-to-end training using human instruction datasets, enabling the LLM with visual reasoning capabilities. Moreover, Video-ChatGPT (Maaz et al., 2023) design a 100k video instruction dataset, successfully empowering LLMs to comprehend videos. VideoChat (Li et al., 2023c) and Video-LLaMA (Zhang et al., 2023a) achieve this by conducting joint training, allowing LLMs to simultaneously handle images and videos. Expanding LLMs to additional visual modalities typically requires pre-alignment, as seen in LLaMA-Adapter (Zhang et al., 2023b; Gao et al., 2023) and ImageBind-LLM (Han et al., 2023). They bind other modalities to the image space through ImageBind's (Girdhar et al., 2023) modality encoder. These models have demonstrated that a unified feature space is advantageous for enhancing LLM's multi-modal reasoning capabilities. Distinguished from prior work, Video-LLaVA not only pre-aligns image and video features but also conducts joint training of images and videos, facilitating LLMs in learning multi-modal reasoning capabilities from a unified visual representation.\n\n## 3 Video-LLaVA\n\n## 3.1 Model Structure\n\n## 3.1.1 Framework Overview\n\nAs shown in Figure 2, Video-LLaVA consists of LanguageBind encoders f V (Zhu et al., 2023a) to extract features from the raw visual signal (im- ages or videos), a large language model f L such as Vicuna, visual projection layers f P and a word embedding layer f T . We initially obtain visual features using LanguageBind encoders. LanguageBind encoders are capable of mapping different modalities into the textual feature space, thereby providing us with a unified visual representation. Subsequently, the unified visual representation is encoded by shared projection layers, which is then combined with tokenized textual queries and fed into a large language model to generate corresponding responses.\n\n## 3.1.2 United Visual Representation\n\nOur goal is to map images and videos into a shared feature space to enable the large language model to learn from a unified visual representation. We assume that the same information can be conveyed through multiple media. For example, a running dog can be expressed through language, a image or a video simultaneously. Therefore, we can compress information from different modalities into a common feature space, allowing the model to extract information from a dense feature space, facilitating modality interactions and complementarity. Hence, we chose the modality encoders from LanguageBind (Zhu et al., 2023a), which align images and videos with the textual feature space.\n\n## 3.1.3 Alignment Before Projection\n\nSpecifically, LanguageBind initializes from OpenCLIP (Ilharco et al., 2021), naturally aligning images and language in a shared feature space. Subsequently, it aligns video representations to the language space using 3 million video-text pairs from VIDAL-10M (Zhu et al., 2023a). By sharing a language feature space, the image and video representations ultimately converge into a unified visual feature space, which we refer to as emergent alignment of images and videos. Therefore, our video encoder and image encoder are initialized from the LanguageBind encoders zoo, pre-aligning the inputs for LLM and reducing the gap between representations of different visual signals. The unified visual representation is fed into LLM after passing through a shared projection layer.\n\n## 3.2 Training Pipeline\n\nOverall, the process of generating responses by Video-LLaVA is similar to that of a large language model (GPT series). Given a textual input X T and visual signals X V, the input signals are encoded into a sequence of tokens according to Equation 1. By maximizing the likelihood probability in Equation 2, the model ultimately achieves multi-modal understanding capabilities.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere L is the length of the generated sequence X A, and θ is a trainable parameter. We dynamically conduct joint training on images and videos, wherein a single batch contains both image and video samples simultaneously.\n\n## 3.2.1 Understanding Training\n\nAt this stage, the model is required to acquire the ability to interpret visual signals within an extensive image/video-text pair dataset. Each visual signal corresponds to a single round of conversation data ( X q , X a ) , where X T = X q and X a is the ground truth. The training objective of this stage is the original auto-regressive loss, where the model learns the basic ability to view the vision. We freeze the other parameters of the model during this process.\n\n## 3.2.2 Instruction Tuning\n\nIn this stage, the model is required to provide responses corresponding to different instructions. These instructions often involve more complex visual comprehension tasks, rather than just describing visual signals. Note that the conversation data ( X 1 q , X 1 a , · · · , X N q , X N a ) consists of multiple rounds.\n\n<!-- formula-not-decoded -->\n\nwhere r represents the round number. As shown in Equation 3, when r &gt; 1 we concatenate the conversations from all previous rounds with the current instruction as the input for this round. The training objective remains the same as in the previous stage. After this stage, the model learns to generate corresponding responses based on different instructions and requests. The LLM are also involved in training at this stage.\n\nFigure 2: Training framework and performance. Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaVA framework demonstrates a data flow that generates corresponding responses based on input instructions. (b) VideoLLaVA achieves superior performances on a broad range of 15 datasets across image and video.\n\n<!-- image -->\n\nFigure 3: Data composition for training VideoLLaVA. The dataset for stage 1 consists of single-turn conversation, focusing on concise visual descriptions. In stage 2, the dataset comprises multi-turn conversations, emphasizing complex visual reasoning abilities.\n\n<!-- image -->\n\n## 4 Experiments\n\n## 4.1 Experimental Setup\n\n## 4.1.1 Data Details\n\nIn 3, for the first stage of understanding pretraining, we use a subset of 558K LAION-CC-SBU image-text pairs with BLIP (Li et al., 2022) captions, which is sourced from CC3M (Sharma et al., 2018) and filtered by LLaVA (Liu et al., 2023b). The video-text pairs are derived from a subset provided by Valley (Luo et al., 2023), and we have access to 702k out of a total of 703k pairs, originating from WebVid (Bain et al., 2021). For the stage of instruction tuning, We gathered instructional datasets from two sources, including a 665k imagetext instruction dataset from LLaV A 1.5 (Liu et al., 2023a) and a 100k video-text instruction dataset from Video-ChatGPT (Maaz et al., 2023).\n\n## 4.1.2 Model Settings\n\nWe employ Vicuna-7B v1.5 as the large language model. The visual encoders are derived from LanguageBind, initialized from OpenCLIP-L/14. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. The share projection layers consist of 2 fully connected layers with a GeLU (Hendrycks and Gimpel, 2016) activated function.\n\n## 4.1.3 Training Details\n\nIn the training process, we resize and crop each image, resulting in a size of 224×224 for each processed image. We uniformly sample 8 frames from each video, and each frame undergoes image preprocessing. The data in each batch is a random combination of images and videos. In the first stage, we train for one epoch with a batch size of 256, using the AdamW optimizer with a cosine learning rate schedule. In the second stage, we reduce the batch size to 128. The initial learning rate for both stages is set to 1e-3, with a warmup ratio of 0.03. Additional hyper-parameter settings can be found in the appendix.\n\n## 4.2 Quantitative Evaluation\n\n## 4.2.1 Zero-shot Video Understanding\n\nAs shown in Table 2, we conduct a quantitative assessment of the video question-answering ca-\n\nTable 2: Comparison between different LVLMs on video reasoning benchmarks . We employ ChatGPTAssistant to evaluate the performance following Video-ChatGPT (Maaz et al., 2023). The version of ChatGPT is 'gpt-3.5-turbo'.\n\n| Methods       | LLM size   | MSVD-QA   | MSVD-QA   | MSRVTT-QA   | MSRVTT-QA   | TGIF-QA   | TGIF-QA   | ActivityNet-QA   | ActivityNet-QA   |\n|---------------|------------|-----------|-----------|-------------|-------------|-----------|-----------|------------------|------------------|\n|               |            | Accuracy  | Score     | Accuracy    | Score       | Accuracy  | Score     | Accuracy         | Score            |\n| FrozenBiLM    | 1B         | 32.2      | -         | 16.8        | -           | 41.0      | -         | 24.7             | -                |\n| VideoChat     | 7B         | 56.3      | 2.8       | 45.0        | 2.5         | 34.4      | 2.3       | -                | 2.2              |\n| LLaMA-Adapter | 7B         | 54.9      | 3.1       | 43.8        | 2.7         | -         | -         | 34.2             | 2.7              |\n| Video-LLaMA   | 7B         | 51.6      | 2.5       | 29.6        | 1.8         | -         | -         | 12.4             | 1.1              |\n| Video-ChatGPT | 7B         | 64.9      | 3.3       | 49.3        | 2.8         | 51.4      | 3.0       | 35.2             | 2.7              |\n| Chat-UniVi    | 7B         | 65.0      | 3.6       | 54.6        | 3.1         | 60.3      | 3.4       | 45.8             | 3.2              |\n| Video-LLaVA   | 7B         | 70.7      | 3.9       | 59.2        | 3.5         | 70.0      | 4.0       | 45.3             | 3.3              |\n\nTable 3: Comparison between different LVLMs on image understanding benchmarks. 'Res.', 'L', 'V' respectively represent the input image resolution, LLaMA (Touvron et al., 2023a) and Vicuna (Chiang et al., 2023). Benchmark names are abbreviated due to page limitations. VQA-v2 (Goyal et al., 2017); GQA (Hudson and Manning, 2019); VisWiz (Gurari et al., 2018); SQA I : ScienceQA-IMG (Lu et al., 2022); VQA T : TextVQA (Singh et al., 2019); POPE (Li et al., 2023d); MMB: MMBench (Liu et al., 2023c); LLaVA W : LLaVA-Bench (In-theWild) (Liu et al., 2023b); MM-Vet (Yu et al., 2023). † donates that we reproduce LLaVA-1.5 with LanguageBindImage encoder to compare fairly. ∗ donates that there is some overlap in the training data.\n\n| Methods      | LLM   | Res.   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   |\n|--------------|-------|--------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------------------|---------------------|---------------------|---------------------|\n|              |       |        | VQA v2                     | GQA                        | VisWiz                     | SQA I                      | VQA T                      | POPE                | MMB                 | LLaVA W             | MM-Vet              |\n| LLaVA-1.5    | V-7B  | 336    | -                          | 62.0 ∗                     | -                          | -                          | -                          | -                   | -                   | -                   | 30.5                |\n| BLIP-2       | V-13B | 224    | 41.0                       | 41.0                       | 19.6                       | 61.0                       | 42.5                       | 85.3                | -                   | 38.1                | 22.4                |\n| InstructBLIP | V-13B | 224    | -                          | 49.5                       | 33.4                       | 63.1                       | 50.7                       | 78.9                | -                   | 58.2                | 25.6                |\n| IDEFICS-80B  | L-65B | 224    | 60.0                       | 45.2                       | 36.0                       | -                          | 30.9                       | -                   | 54.5                | -                   | -                   |\n| MiniGPT-4    | L-7B  | 224    | -                          | 30.8                       | 47.5                       | 25.4                       | 19.4                       | -                   | 23.0                | -                   | 22.1                |\n| IDEFICS-9B   | L-7B  | 224    | 50.9                       | 38.4                       | 35.5                       | -                          | 25.9                       | -                   | 48.2                | -                   | -                   |\n| mPLUG-Owl    | L-7B  | 224    | -                          | 14.0                       | 39.0                       | 2.8                        | 38.8                       | -                   | 46.6                | -                   | -                   |\n| Otter        | L-7B  | 224    | -                          | 38.1                       | 50.0                       | 27.2                       | 21.2                       | -                   | 32.6                | -                   | 24.6                |\n| InstructBLIP | V-7B  | 224    | -                          | 49.2                       | 34.5                       | 60.5                       | 50.1                       | -                   | 36.0                | 60.9                | 26.2                |\n| LLaVA-1.5 †  | V-7B  | 224    | 72.3 ∗                     | 56.9 ∗                     | 47.8                       | 67.9                       | 49.2                       | 83.3                | 59.5                | 63.3                | 25.7                |\n| Video-LLaVA  | V-7B  | 224    | 74.7 ∗                     | 60.3 ∗                     | 48.1                       | 66.4                       | 51.8                       | 84.4                | 60.9                | 73.1                | 32.0                |\n\npabilities of large video-language models on four datasets, including MSVD-QA (Chen and Dolan, 2011), MSRVTT-QA (Xu et al., 2016), TGIFQA (Jang et al., 2017) and ActivityNet-QA (Yu et al., 2019). The evaluation pipeline for video understanding follows Video-ChatGPT. We report the accuracy and score, which is assessed using GPT-Assistant. Video-LLaVA consistently outperforms Video-ChatGPT in terms of questionanswering accuracy, which is an advanced large video-language model. Moreover, Video-LLaVA surpasses the powerful baseline of Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, we conduct comparisons with the recent\n\nSOTA model, Chat-UniVi (Jin et al., 2023). Despite Chat-UniVi utilizing more datasets such as MIMIC-IT (Li et al., 2023a), Video-LLaVA still demonstrate competitive results, surpassing ChatUniVi on MSVD, MSRVTT, and TGIF datasets. In summary, these results validate Video-LLaVA's ability to comprehend videos and provide contextually appropriate responses based on instructions.\n\n## 4.2.2 Zero-shot Image Question-answering\n\nAs shown in Table 3, we evaluate our approach for image understanding on five academic image question-answering benchmarks. Compared to the state-of-the-art model InstructBLIP-7B, VideoLLaVA demonstrates powerful image understand-\n\nTable 4: Zero-shot object hallucination evaluation results are reported for three POPE evaluation settings. 'Yes' indicates the proportion of positive responses to the given question. † donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly.\n\n| Methods      | LLM   | Adersarial   | Adersarial   | Adersarial   | Popular   | Popular   | Popular   | Random   | Random   | Random   |\n|--------------|-------|--------------|--------------|--------------|-----------|-----------|-----------|----------|----------|----------|\n| Methods      | LLM   | Accuracy     | F1-Score     | Yes          | Accuracy  | F1-Score  | Yes       | Accuracy | F1-Score | Yes      |\n| MiniGPT-4    | V-13B | 66.6         | 71.4         | 66.7         | 68.3      | 72.2      | 64.1      | 77.8     | 78.9     | 54.8     |\n| InstructBLIP | V-13B | 74.4         | 78.5         | 69.0         | 81.4      | 83.5      | 62.6      | 88.7     | 89.3     | 55.2     |\n| MM-GPT       | L-7B  | 50.0         | 66.7         | 100.0        | 50.0      | 66.7      | 100.0     | 50.0     | 66.7     | 100.0    |\n| mPLUG-Owl    | L-7B  | 50.7         | 66.8         | 98.7         | 50.9      | 66.9      | 98.6      | 54.0     | 66.4     | 95.6     |\n| Chat-UniVi   | V-7B  | 55.6         | 68.7         | 91.6         | 56.4      | 69.0      | 90.8      | 73.9     | 79.3     | 74.6     |\n| LLaVA-1.5 †  | L-7B  | 84.3         | 83.2         | 43.5         | 79.8      | 79.4      | 48.0      | 85.7     | 84.8     | 43.0     |\n| Video-LLaVA  | V-7B  | 81.6         | 80.8         | 45.8         | 85.3      | 84.0      | 42.1      | 86.2     | 85.2     | 42.0     |\n\ning capabilities, outperforming across all five question-answering benchmarks. Additionally, Video-LLaVA exhibits competitive results compared to several more powerful LVLMs, which are tuned based on 13B or 65B LLM, such as surpassing InstructBLIP-13B by 14.7% on VisWiz, highlighting its strong understanding ability in natural visual environments. Furthermore, to ensure a fair comparison, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder, called LLaVA-1.5 † . This demonstrates that the performance improvement observed in Video-LLaVA is not solely attributed to a stronger image encoder. Additional details can be found in Section 4.3.6.\n\nEvaluation under Image Benchmark Toolkits Additionally, we evaluate LVLMs using several benchmark toolkits for visual instruction tuning. These benchmark toolkits provide a detailed assessment of the model's capabilities through robust evaluation metrics. Video-LLaVA outperform InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on MMBench, LLaVA-Bench, and MM-Vet, respectively. It is worth noting that Video-LLaVA-7B still demonstrates advanced performance compared to larger LLM models, surpassing InstructBLIP13B by 6.4% on MM-Vet and IDEFICS-80B (Laurençon et al., 2023) by 6.4% on MMBench. These results demonstrate that Video-LLaVA exhibits a strong understanding of semantic aspects of scenes, enabling it to answer open-ended and free-form natural language questions about images.\n\n## 4.2.3 Object Hallucination Evaluation\n\nAs shown in Table 4, we report evaluation results for zero-shot object hallucinations, utilizing a evaluation pipeline derived from a polling-based query method (Li et al., 2023d). Video-LLaVA demon- strates competitive performance across three subsets: random, popular, and adversarial. Specifically, when compared to the 7B foundation model, VideoLLaVA consistently outperforms MM-GPT (Gong et al., 2023) across all three POPE hallucination evaluation subsets. Furthermore, when benchmarked against the larger 13B LLM, Video-LLaVA even surpasses Mini-GPT4 comprehensively. The successful performance of Video-LLaVA in object hallucination detection validates the consistency between unified visual representations and the generation of textual descriptions.\n\n## 4.3 Ablation Results\n\n## 4.3.1 Alignment Before Projection\n\nTo validate the performance degradation caused by separated visual representation, we conduct experiments to to explore the performance of the LLM learning from different visual representations. We define the use of LanguageBind image encoder as unified visual representation while the MAE encoder (He et al., 2022) use separated visual representation, which is a well-known and effective image feature extractor. Additionally, since MAE do not interact with multi-modal inputs during the training process, we utilize CLIP-L/14, a model of the same size. While CLIP-L/14 exhibits strong multimodal understanding capabilities, it is not prealigned with the video encoder. Consequently, this results in a lack of uniformity in the visual features provided to LLM. We only replace the image encoder of the same scale and keep the LanguageBind video encoder.\n\n## 4.3.2 For Video Understanding\n\nDue to replacing the image encoder with the MAE encoder, the video features and image features are\n\nTable 5: Effect of alignment before projection on image. 'United' refers to the unified visual representation, while 'Separated' refers to the separated visual representation. Benchmark names are abbreviated due to page limitations.\n\n| Methods        |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |\n|----------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|\n| Separated-MAE  |                                                     66   |                                                     55.4 |                                                     42.5 |                                                     65   |                                                     44.2 |                                        80.8 |                                        45.7 |                                        35.9 |                                        20   |\n| Separated-CLIP |                                                     74.6 |                                                     59.9 |                                                     47.8 |                                                     67.3 |                                                     51.5 |                                        84.4 |                                        60.2 |                                        68.9 |                                        30.6 |\n| United         |                                                     74.7 |                                                     60.3 |                                                     48.1 |                                                     66.4 |                                                     51.8 |                                        84.4 |                                        60.9 |                                        73.1 |                                        32   |\n| ∆ Acc.         |                                                      0.1 |                                                      0.4 |                                                      0.3 |                                                     -0.9 |                                                      0.3 |                                         0   |                                         0.7 |                                         4.2 |                                         1.4 |\n\nTable 6: Effect of joint training on video. We evaluate on four video question-answering datasets. ∗ denotes that we utilized only video data in both the first and second stages.\n\nFigure 4: Effect of alignment before projection on video. We validate and report the accuracy and score on four video question-answering datasets.\n\n| Methods          | MSVD-QA Accuracy Score   | MSVD-QA Accuracy Score   | MSRVTT-QA Accuracy   | Score   | TGIF-QA Accuracy Score   | TGIF-QA Accuracy Score   | ActivityNet-QA Accuracy   | ActivityNet-QA Accuracy   |\n|------------------|--------------------------|--------------------------|----------------------|---------|--------------------------|--------------------------|---------------------------|---------------------------|\n| Methods          | 64.8                     | 3.2                      | MSRVTT-QA Accuracy   | Score   |                          | 3.4                      |                           | Score                     |\n| Video-LLaVA ∗    |                          |                          | 58.3                 | 3.4     | 67.8                     |                          | 40.7                      | 2.0                       |\n| Joint with Image | 70.7                     | 3.9                      | 59.2                 | 3.5     | 70.0                     | 4.0                      | 45.3                      | 3.3                       |\n| ∆ Acc.           | +5.9                     | +0.7                     | +0.9                 | +0.1    | +2.2                     | +0.6                     | +4.6                      | +1.3                      |\n\n<!-- image -->\n\n<!-- image -->\n\nno longer unified during LLM's initial learning of visual representations. In Figure 4, compared to separated visual representation, the united visual representation significantly improves performance across 4 video question-answering datasets. Separated visual representations not only exhibit lower accuracy in question-answering, but also demonstrate a similar trend in answer scores. These results demonstrate that the unified visual representation can help the LLM further learn and understand videos.\n\n## 4.3.3 For Image Understanding\n\nThe unified visual representation demonstrates strong performance, surpassing the separated visual representation comprehensively across 5 image question-answering datasets and 4 benchmark toolkits in Table 5. Additionally, we observe a significant margin of performance improvement in the unified visual representation on the MM-\n\nFigure 5: Effect of joint training on image. † donates that We reproduce the results of LLaVA-1.5 at a resolution of 224×224 with LanguageBind-Image encoder for a fair comparison.\n\n<!-- image -->\n\nBench, LLaVA-Bench, and MM-Vet benchmark toolkits. This highlights that the unified visual representation not only enhances performance in image question-answering but also provides benefits in other aspects of image understanding, such as reducing object hallucination and improving OCR capabilities.\n\n## 4.3.4 Joint Training\n\nThis subsection aims to validate the complementarity of images and videos during joint training, which can mutually enhance the LLM's understanding of images and videos based on a unified visual representation.\n\n## 4.3.5 For Video Understanding\n\nFor comparing performance on video benchmarks, we remove image data during the training of VideoLLaVA, which is called Video-LLaVA ∗ . We com- pare with Video-LLaVA ∗ to assess the performance gains from joint image training on video benchmarks. In Table 6, we evaluate our model on four video question-answering datasets. Compared to Video-LLaVA ∗ without image in training, the model trained with joint images and videos achieves comprehensive improvements across all four video datasets. These results demonstrate that joint training of images and videos facilitates LLM's understanding of visual representations.\n\n## 4.3.6 For Image Understanding\n\nWhen comparing performance on image benchmarks, it is challenging to find a image-based LVLM with the same configuration as VideoLLaVA. To address this, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder and reproduce the results at a resolution of 224×224 by using the same training configuration, called LLaVA-1.5 † . As shown in Figure 5, Compared to LLaVA-1.5 † , which utilizes the same image encoder configuration, we observe performance improvements in 8 out of 9 benchmarks, demonstrating mutual improvement in visual understanding. Video-LLaVA outperform LLaVA-1.5 † in POPE, indicating that joint training with videos alleviates the object hallucination in images. The similar trend is observed on some other benchmark toolkits, such as LLaVA-Bench and MMBench, where video data significantly improves LLM's performance in complex reasoning and image conversation tasks.\n\n## 5 Limitation and Future Directions\n\n## 5.1 Limitation\n\nWhile Video-LLaVA exhibits strong competitiveness in both images and videos, we still observed some limitations of Video-LLaVA. To begin with, Video-LLaVA performs moderately in understanding long videos. In Table 2, Chat-UniVi surpasses 0.5 on ActivityNet-QA because Video-LLaVA only utilizes uniformly sampled 8 frames to comprehend the video, which results in the loss of detailed information from long videos. Additionally, training Video-LLaVA is computationally expensive, requiring 3-4 days to complete the training process on 8 A100-80G GPUs.\n\n## 5.2 Future Directions\n\nIn the future, We maybe can explore more efficient shared projection mode that can compress tokens while preserving data features. This would support Video-LLaVA in better understanding long videos. Besides, Video-LLaVA can serve as a baseline to extend to additional visual-related modalities, such as depth and infrared images. Additionally, we could explore how to incorporate timestamp embeddings effectively, enabling large visual-language models to answer questions related to temporal relationships.\n\n## 6 Conclusion\n\nIn this work, we introduce Video-LLaVA, a simple but powerful large visual-language baseline model. We propose a novel framework to address the issue of misalignment before projection, utilizing a LanguageBind encoder to pre-bind visual signals into the language feature space. To enable a LLM to comprehend both images and videos simultaneously, we conduct joint training on images and videos, allowing the LLM to learn multi-modal interactions from a unified visual representation. Extensive experiments demonstrate that joint training on images and videos mutually benefits performance. Furthermore, we validate that aligning visual representations before projection aids LLM learning. Remarkably, LLM, after learning from a unified visual representation, exhibits the remarkable ability to simultaneously engage with both images and videos, showcasing a powerful comprehension of unified visual concepts. These results collectively demonstrate the effectiveness of the Video-LLaVA training framework. As a unified visual training framework, the performance of Video-LLaVA even surpasses that of expert models designed specifically for images or videos.\n\n## Acknowledgments\n\nThis work was supported in part by the Natural Science Foundation of China (No. 62202014, 62332002, 62425101), Shenzhen Basic Research Program (No.JCYJ20220813151736001).\n\n## References\n\n- Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716-23736.\n- Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 .\n- Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1728-1738.\n- Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2020. Palm: Pre-training an autoencoding&amp;autoregressive language model for context-conditioned generation. arXiv preprint arXiv:2004.07159 .\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n- David Chen and William B Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies , pages 190-200.\n- Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. 2023. Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160 .\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) .\n- Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint , arXiv:2305.06500.\n- Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 .\n- Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. 2023. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010 .\n- Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 15180-15190.\n- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790 .\n- Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904-6913.\n- Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3608-3617.\n- Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905 .\n- Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 16000-16009.\n- Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 .\n- Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6700-6709.\n- Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. Openclip. If you use this software, please cite it as below.\n- Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatiotemporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2758-2766.\n- Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. 2023. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046 .\n- Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning , pages 5583-5594. PMLR.\n- Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. 2023. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Preprint , arXiv:2306.16527.\n- Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. 2023a. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 .\n- Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 .\n- Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning , pages 12888-12900. PMLR.\n- Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems , 34:9694-9705.\n- KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023c. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 .\n- Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023d. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 .\n- Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. 2024. Moe-llava: Mixture of experts for large visionlanguage models. arXiv preprint arXiv:2401.15947 .\n- Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 .\n- Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485 .\n- Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023c. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 .\n- Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems , 35:2507-2521.\n- Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. 2023. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207 .\n- Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. 2023. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093 .\n- Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 .\n- OpenAI. 2023. Gpt-4 technical report. Preprint , arXiv:2303.08774.\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 .\n- Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2556-2565.\n- Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 .\n- Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 8317-8326.\n- Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128 .\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An instruction-following llama model.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 .\n- Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 .\n- Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msrvtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 5288-5296.\n- Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 .\n- Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 .\n- Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549 .\n- Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 .\n- Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33, pages 9127-9134.\n- Hang Zhang, Xin Li, and Lidong Bing. 2023a. Videollama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 .\n- Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. 2023b. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 .\n- Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. 2023a. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. arXiv preprint arXiv:2310.01852 .\n- Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023b. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 .\n\n## A Example Appendix\n\n## A.1 Training Setting\n\nWeshow some training settings as shown in Table 7. video encoder and image encoder are not trained in both stages. The projection layer consists of 2 linear layers with a GeLU (Hendrycks and Gimpel, 2016) activation function between them. Image and video share the projection layer.\n\nTable 7: Training setting.\n\n| Config                                                                                                                                               | Pretraining                                                                                         | Instruction tuning                                                                                  |\n|------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| Video encoder Image encoder Optimizer Deepspeed Epochs Vision select layer Weight decay Warmup ratio Learning rate schedule Learning rate Batch size | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 |\n\n## A.2 Exhibition Board\n\nWe show some unselected samples here, and these videos are sourced from Video-ChatGPT (Maaz et al., 2023).\n\n<!-- image -->\n\nFigure 6: Samples of Video-LLaVA in video understanding.\n\n<!-- image -->\n\n[Input Instruction]\n\nWhat is strange about this video?\n\nThe video is strange because it shows a man sitting next to a lion and next to a man. The man is also seen petting a lion in the grass.\n\n[Input Instruction]\n\nWhat is unusual about this video?\n\nThe unusual aspect of this video is that the giraffe is seen swimming i not a common sight. The giraffe is also seen jumping off a diving boa\n\nunderwater, which adds to the uniqueness of the video.\n\n[Input Instruction]\n\nCan you write a short poem inspired from\n\nSure, here's a short poem inspired by the video:\n\nThe flowers bloom, a gentle grace,\n\nTheir petals soft, their colors trace.\n\nTheir beauty, a sight to behold,\n\nA moment of peace, a moment to hold.\n\nTheir colors, a symphony of hues,\n\nA celebration of life, and all it can do.\n\nTheir beauty, a reminder to cherish,\n\nThe simple things, that bring us happiness.\n\nSo let us take a moment to appreciate,\n\nThe beauty of flowers, and all they can create.\n\n[Input Instruction]\n\nDescribe the video in detail.\n\nThe video starts with a car covered in snow, and a person is seen usin remove the snow from the car's windows. The person is seen scraping\n\nwindshield and the side windows of the car. The person continues to s the car, making sure that all the snow is removed. The video then sho\n\nwiping the snow off the car's windshield and side windows. The perso cloth to wipe the snow off the car's windows, making sure that the car\n\nclean. The video ends with the person finishing the job and the car bei\n\nOverall, the video shows a person removing snow from a car's windo and then wiping the snow off the car's windows to make sure it is com\n\n[Input Instruction]\n\nDescribe the video in detail.\n\nIn the video, we see a man walking down a track while holding a javel down the track and throws the javelin. The video then shows the man\n\nthe starting point, and we see the javelin landing on the ground. The m away from the camera. From the video, we can see that the man is a ja\n\nhe is practicing his throws. The track appears to be well-maintained, a surroundings seem to be quiet and peaceful. The man's form and techn\n\nimpressive, and it is clear that he has experience in the sport. Overall, t a glimpse into the world of javelin throwing and the dedication and sk\n\nin the sport.\n\nFigure 7: Samples of Video-LLaVA in video understanding.\n\n[Input Instruction]\n\nDescribe the acti\n\nThe video shows a man playing the dr his hands and sticks, and he continues\n\n[Input Instruction]\n\nBriefly describe t\n\nThe video features a woman grooming hair and cleans its hooves, and the hor\n\nThe video also includes shots of the w\n\n[Input Instruction]\n\nWhat is the main\n\nThe main challenge faced by the peopl seen struggling to stay upright and ma\n\nforth.\n\n[Input Instruction]\n\nWhere is this vi is shown in the video?\n\nThe video is taken from the top of the\n\nNew York City. The statue is shown fr from the top of the statue itself.\n\n[Input\n\nInstruction]\n\nCan you\n\ndesc window as demonstrated in the vide\n\nSure! The man in the video demonstra down with a rag. He then applies a cle\n\nagain. Finally, he shows how to dry th n]\n\nWhy is this video funny?\n\nfunny because the baby is playing with a Wii remote, which is not a toy ciated with babies. The baby's actions are also humorous, such as pointing\n\nand making faces, which adds to the overall amusement of the video.\n\nn]\n\nCan you identify any safety hazards in this video?\n\nis playing with a toy truck and a toy car, which could pose a safety risk if upervised. However, the baby is sitting on the floor, so there is less risk of\n\nred to if they were playing with the toys on a higher surface.\n\non]\n\nCreate a short fairy tale with a moral lesson inspired time, in a kingdom far away, there lived a beautiful princess named Rose.\n\nwn for her\n\nkindness and\n\nbeauty, and\n\neveryone loved\n\nher.\n\nOne day,\n\na\n\nince named Leo came to visit the kingdom. He was fascinated by Rose's ecided to stay in the kingdom. However, Rose's jealous sister, Lily, did not\n\ntion Leo was giving her sister. She tried to harm Rose, but Leo saved her in love. The kingdom celebrated their love and they lived happily ever after.\n\nthe story is that true love conquers all and that jealousy can lead to harmful\n\n.\n\nDescribe the video.\n\nn]\n\nows a person preparing a dish with peppers and oranges. They slice the oranges and lay them out on a table. The person then proceeds to cut the\n\noranges into strips and lays them out on a piece of wax paper. They use a he peppers and oranges into strips and lay them out on a piece of wax paper.\n\nen uses a spoon to mix the peppers and oranges together. The video ends on cutting the peppers and oranges into strips and laying them out on a piece\n\n.\n\nn]\n\nWhat is the main focus of the video.\n\nus of the video is the process of creating a painting of a tree using various d colors. The video shows the artist's hand holding a paintbrush and using it\n\nree, as well as the use of different colors and techniques to create the on]\n\nWhat does the woman use to split the logs and how does uses an ax to split the logs. She holds the ax in her hand and swings it down\n\nShe repeats this process several times, splitting the log into smaller pieces.\n\n<!-- image -->\n\nThe video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.\n\nThe video is about a man who is seen spinning and throwing a discus in a field. He then\n\nWhat activities are the woman and the dog engaged in\n\nThe woman and the dog are playing frisbee in the yard. The woman throws the frisbee,\n\nThe video shows two men arm wrestling on a table while being watched by a group of\n\nStep-by-step describe the cooking recipe shown in the\n\nThe video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that,\n\nthey mix the noodles and the ingredients together. Finally, they serve the dish on a plate.\n\nFigure 8: Samples of Video-LLaVA in video understanding.\n\nFigure 9: Samples of Video-LLaVA in video understanding.\n\n<!-- image -->",
  "tables": [
    {
      "index": 0,
      "markdown": "| Methods                           | Image   | Video   | Pre-aligned   | Joint training   |\n|-----------------------------------|---------|---------|---------------|------------------|\n| LLMs as scheduler                 |         |         |               |                  |\n| VisualChatGPT (Wu et al., 2023)   | ✔       | ✗       | -             | -                |\n| HuggingGPT (Shen et al., 2023)    | ✔       | ✗       | -             | -                |\n| MM-REACT (Yang et al., 2023)      | ✔       | ✔       | -             | -                |\n| ViperGPT (Surís et al., 2023)     | ✔       | ✔       | -             | -                |\n| LLMs as decoder                   |         |         |               |                  |\n| Mini-GPT4 (Zhu et al., 2023b)     | ✔       | ✗       | -             | ✗                |\n| LLaVA (Liu et al., 2023b)         | ✔       | ✗       | -             | ✗                |\n| Video-ChatGPT (Maaz et al., 2023) | ✗       | ✔       | -             | ✗                |\n| VideoChat (Li et al., 2023c)      | ✔       | ✔       | ✗             | ✔                |\n| Video-LLaMA (Zhang et al., 2023a) | ✔       | ✔       | ✗             | ✔                |\n| ImageBind-LLM (Han et al., 2023)  | ✔       | ✔       | ✔             | ✗                |\n| Video-LLaVA (Ours)                | ✔       | ✔       | ✔             | ✔                |"
    },
    {
      "index": 1,
      "markdown": "| Methods       | LLM size   | MSVD-QA   | MSVD-QA   | MSRVTT-QA   | MSRVTT-QA   | TGIF-QA   | TGIF-QA   | ActivityNet-QA   | ActivityNet-QA   |\n|---------------|------------|-----------|-----------|-------------|-------------|-----------|-----------|------------------|------------------|\n|               |            | Accuracy  | Score     | Accuracy    | Score       | Accuracy  | Score     | Accuracy         | Score            |\n| FrozenBiLM    | 1B         | 32.2      | -         | 16.8        | -           | 41.0      | -         | 24.7             | -                |\n| VideoChat     | 7B         | 56.3      | 2.8       | 45.0        | 2.5         | 34.4      | 2.3       | -                | 2.2              |\n| LLaMA-Adapter | 7B         | 54.9      | 3.1       | 43.8        | 2.7         | -         | -         | 34.2             | 2.7              |\n| Video-LLaMA   | 7B         | 51.6      | 2.5       | 29.6        | 1.8         | -         | -         | 12.4             | 1.1              |\n| Video-ChatGPT | 7B         | 64.9      | 3.3       | 49.3        | 2.8         | 51.4      | 3.0       | 35.2             | 2.7              |\n| Chat-UniVi    | 7B         | 65.0      | 3.6       | 54.6        | 3.1         | 60.3      | 3.4       | 45.8             | 3.2              |\n| Video-LLaVA   | 7B         | 70.7      | 3.9       | 59.2        | 3.5         | 70.0      | 4.0       | 45.3             | 3.3              |"
    },
    {
      "index": 2,
      "markdown": "| Methods      | LLM   | Res.   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Image Question Answering   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   | Benchmark Toolkit   |\n|--------------|-------|--------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------------------|---------------------|---------------------|---------------------|\n|              |       |        | VQA v2                     | GQA                        | VisWiz                     | SQA I                      | VQA T                      | POPE                | MMB                 | LLaVA W             | MM-Vet              |\n| LLaVA-1.5    | V-7B  | 336    | -                          | 62.0 ∗                     | -                          | -                          | -                          | -                   | -                   | -                   | 30.5                |\n| BLIP-2       | V-13B | 224    | 41.0                       | 41.0                       | 19.6                       | 61.0                       | 42.5                       | 85.3                | -                   | 38.1                | 22.4                |\n| InstructBLIP | V-13B | 224    | -                          | 49.5                       | 33.4                       | 63.1                       | 50.7                       | 78.9                | -                   | 58.2                | 25.6                |\n| IDEFICS-80B  | L-65B | 224    | 60.0                       | 45.2                       | 36.0                       | -                          | 30.9                       | -                   | 54.5                | -                   | -                   |\n| MiniGPT-4    | L-7B  | 224    | -                          | 30.8                       | 47.5                       | 25.4                       | 19.4                       | -                   | 23.0                | -                   | 22.1                |\n| IDEFICS-9B   | L-7B  | 224    | 50.9                       | 38.4                       | 35.5                       | -                          | 25.9                       | -                   | 48.2                | -                   | -                   |\n| mPLUG-Owl    | L-7B  | 224    | -                          | 14.0                       | 39.0                       | 2.8                        | 38.8                       | -                   | 46.6                | -                   | -                   |\n| Otter        | L-7B  | 224    | -                          | 38.1                       | 50.0                       | 27.2                       | 21.2                       | -                   | 32.6                | -                   | 24.6                |\n| InstructBLIP | V-7B  | 224    | -                          | 49.2                       | 34.5                       | 60.5                       | 50.1                       | -                   | 36.0                | 60.9                | 26.2                |\n| LLaVA-1.5 †  | V-7B  | 224    | 72.3 ∗                     | 56.9 ∗                     | 47.8                       | 67.9                       | 49.2                       | 83.3                | 59.5                | 63.3                | 25.7                |\n| Video-LLaVA  | V-7B  | 224    | 74.7 ∗                     | 60.3 ∗                     | 48.1                       | 66.4                       | 51.8                       | 84.4                | 60.9                | 73.1                | 32.0                |"
    },
    {
      "index": 3,
      "markdown": "| Methods      | LLM   | Adersarial   | Adersarial   | Adersarial   | Popular   | Popular   | Popular   | Random   | Random   | Random   |\n|--------------|-------|--------------|--------------|--------------|-----------|-----------|-----------|----------|----------|----------|\n| Methods      | LLM   | Accuracy     | F1-Score     | Yes          | Accuracy  | F1-Score  | Yes       | Accuracy | F1-Score | Yes      |\n| MiniGPT-4    | V-13B | 66.6         | 71.4         | 66.7         | 68.3      | 72.2      | 64.1      | 77.8     | 78.9     | 54.8     |\n| InstructBLIP | V-13B | 74.4         | 78.5         | 69.0         | 81.4      | 83.5      | 62.6      | 88.7     | 89.3     | 55.2     |\n| MM-GPT       | L-7B  | 50.0         | 66.7         | 100.0        | 50.0      | 66.7      | 100.0     | 50.0     | 66.7     | 100.0    |\n| mPLUG-Owl    | L-7B  | 50.7         | 66.8         | 98.7         | 50.9      | 66.9      | 98.6      | 54.0     | 66.4     | 95.6     |\n| Chat-UniVi   | V-7B  | 55.6         | 68.7         | 91.6         | 56.4      | 69.0      | 90.8      | 73.9     | 79.3     | 74.6     |\n| LLaVA-1.5 †  | L-7B  | 84.3         | 83.2         | 43.5         | 79.8      | 79.4      | 48.0      | 85.7     | 84.8     | 43.0     |\n| Video-LLaVA  | V-7B  | 81.6         | 80.8         | 45.8         | 85.3      | 84.0      | 42.1      | 86.2     | 85.2     | 42.0     |"
    },
    {
      "index": 4,
      "markdown": "| Methods        |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Image Question Answering VQA v2 GQA VisWiz SQA I VQA T |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |   Benchmark Toolkit POPE MMB LLaVA W MM-Vet |\n|----------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|\n| Separated-MAE  |                                                     66   |                                                     55.4 |                                                     42.5 |                                                     65   |                                                     44.2 |                                        80.8 |                                        45.7 |                                        35.9 |                                        20   |\n| Separated-CLIP |                                                     74.6 |                                                     59.9 |                                                     47.8 |                                                     67.3 |                                                     51.5 |                                        84.4 |                                        60.2 |                                        68.9 |                                        30.6 |\n| United         |                                                     74.7 |                                                     60.3 |                                                     48.1 |                                                     66.4 |                                                     51.8 |                                        84.4 |                                        60.9 |                                        73.1 |                                        32   |\n| ∆ Acc.         |                                                      0.1 |                                                      0.4 |                                                      0.3 |                                                     -0.9 |                                                      0.3 |                                         0   |                                         0.7 |                                         4.2 |                                         1.4 |"
    },
    {
      "index": 5,
      "markdown": "| Methods          | MSVD-QA Accuracy Score   | MSVD-QA Accuracy Score   | MSRVTT-QA Accuracy   | Score   | TGIF-QA Accuracy Score   | TGIF-QA Accuracy Score   | ActivityNet-QA Accuracy   | ActivityNet-QA Accuracy   |\n|------------------|--------------------------|--------------------------|----------------------|---------|--------------------------|--------------------------|---------------------------|---------------------------|\n| Methods          | 64.8                     | 3.2                      | MSRVTT-QA Accuracy   | Score   |                          | 3.4                      |                           | Score                     |\n| Video-LLaVA ∗    |                          |                          | 58.3                 | 3.4     | 67.8                     |                          | 40.7                      | 2.0                       |\n| Joint with Image | 70.7                     | 3.9                      | 59.2                 | 3.5     | 70.0                     | 4.0                      | 45.3                      | 3.3                       |\n| ∆ Acc.           | +5.9                     | +0.7                     | +0.9                 | +0.1    | +2.2                     | +0.6                     | +4.6                      | +1.3                      |"
    },
    {
      "index": 6,
      "markdown": "| Config                                                                                                                                               | Pretraining                                                                                         | Instruction tuning                                                                                  |\n|------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| Video encoder Image encoder Optimizer Deepspeed Epochs Vision select layer Weight decay Warmup ratio Learning rate schedule Learning rate Batch size | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 | LanguageBind-Video-LoRA LanguageBind-Image AdamW Zero2 1 -2 0.0 0.03 cosine decay 1e-3 2e-5 256 128 |"
    }
  ],
  "stats": {
    "pages": 14,
    "chunksCreated": 93,
    "totalCharacters": 63812,
    "totalWords": 8894,
    "numTables": 7,
    "processingTimeMs": 27312
  }
}