{
  "paper": {
    "id": "2402.05468v3",
    "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
    "abstract": "We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness. We apply it to training energy-based models and finetuning denoising diffusions.",
    "authors": [
      "Pierre Marion",
      "Anna Korba",
      "Peter Bartlett",
      "Mathieu Blondel",
      "Valentin De Bortoli",
      "Arnaud Doucet",
      "Felipe Llinares-López",
      "Courtney Paquette",
      "Quentin Berthet"
    ],
    "published": "2024-02-08T08:00:11.000Z",
    "updated": "2025-03-05T19:22:24.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2402.05468v3",
    "absUrl": "https://arxiv.org/abs/2402.05468v3"
  },
  "chunks": [
    {
      "id": "2402.05468v3-chunk-0",
      "content": "Pierre Marion 12\n\nInstitute of Mathematics, EPFL Lausanne, Switzerland\n\nAnna Korba ENSAE CREST IP Paris, France",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Implicit Diffusion: Efficient optimization through stochastic sampling",
        "chunkIndex": 0,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-1",
      "content": "Google DeepMind",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-Lopez , Courtney Paquette, Quentin Berthet 1",
        "chunkIndex": 1,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-2",
      "content": "Sampling and automatic differentiation are both ubiquitous in modern machine learning. At its intersection, differentiating through a sampling operation, with respect to the parameters of the sampling process, is a problem that is both challenging and broadly applicable. We introduce a general framework and a new algorithm for first-order optimization of parameterized stochastic diffusions, performing jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical and experimental results showcasing the performance of our method.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-3",
      "content": "Sampling from a target distribution is a ubiquitous task at the heart of various methods in machine learning, optimization, and statistics. Increasingly, sampling algorithms rely on iteratively applying large-scale parameterized functions (e.g. neural networks), as in denoising diffusion models (Ho et al., 2020).\n\n1 Corresponding authors. Address correspondance at pierre.marion@epfl.ch and qberthet@google.com .\n\n2 Work mostly done while a student researcher at Google DeepMind.\n\nProceedings of the 28 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258. Copyright 2025 by the author(s).\n\nFigure 1: Optimizing through sampling with Implicit Diffusion to finetune denoising diffusion models. Reward is brightness for MNIST and red for CIFAR-10.\n\n<!-- image -->\n\nThis iterative sampling operation implicitly maps a parameter θ ∈ R p to some distribution π ⋆ ( θ ) in P .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-4",
      "content": "sing diffusion models. Reward is brightness for MNIST and red for CIFAR-10.\n\n<!-- image -->\n\nThis iterative sampling operation implicitly maps a parameter θ ∈ R p to some distribution π ⋆ ( θ ) in P .\n\nIn this work, we focus on optimization problems over these implicitly parameterized distributions. For a space of distributions P (e.g. over R d ), and a function F : P → R , our main problem is\n\n<!-- formula-not-decoded -->\n\nThis setting encompasses for instance learning parameterized Langevin diffusions, contrastive learning of energy-based models (Gutmann and Hyv¨ arinen, 2012) or finetuning denoising diffusion models (e.g., Dvijotham et al., 2023; Clark et al., 2024), as illustrated by Figure 1. Applying first-order optimizers to this problem raises the challenge of computing gradients of functions of the target distribution with respect to the parameter: we have to differentiate through a sampling operation , where the link between θ and π ⋆ ( θ ) can be implicit (see, e.g., Figure",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-5",
      "content": "f functions of the target distribution with respect to the parameter: we have to differentiate through a sampling operation , where the link between θ and π ⋆ ( θ ) can be implicit (see, e.g., Figure 2).\n\nTo this aim, we propose to exploit the perspective of sampling as optimization , where the task of sampling is seen as an optimization problem over the space of probability distributions P (see Korba and Salim, 2022, and references therein). Typically, approximating a target probability distribution π can be cast as the minimization of a dissimilarity functional between probability distributions w.r.t. π , that only vanishes at the target. For instance, Langevin diffusion dynamics follow a gradient flow of a Kullback-Leibler (KL) objective w.r.t. the Wasserstein-2 distance (Jordan et al., 1998).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-6",
      "content": ".t. π , that only vanishes at the target. For instance, Langevin diffusion dynamics follow a gradient flow of a Kullback-Leibler (KL) objective w.r.t. the Wasserstein-2 distance (Jordan et al., 1998).\n\nThis allows us to draw a link between optimization through stochastic sampling and bilevel optimization , which often involves computing derivatives of the solution of a parameterized optimization problem obtained after iterative steps of an algorithm. Bilevel optimization is an active area of research with many relevant applications in machine learning, such as hyperparameter optimization (Franceschi et al., 2018) or meta-learning (Liu et al., 2019). In particular, there is a significant effort in the literature for developing tractable and provably efficient algorithms in a large-scale setting (Pedregosa, 2016; Chen et al., 2021b; Arbel and Mairal, 2022; Blondel et al., 2022; Dagr´ eou et al., 2022)-see Appendix D for additional related work.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-7",
      "content": "ly efficient algorithms in a large-scale setting (Pedregosa, 2016; Chen et al., 2021b; Arbel and Mairal, 2022; Blondel et al., 2022; Dagr´ eou et al., 2022)-see Appendix D for additional related work. This literature focuses mostly on problems with finite-dimensional variables, in contrast with our work where the solution of the inner problem is a distribution in P .\n\nThese motivating similarities, while useful, are not limiting. We also consider settings where the sampling iterations are not readily interpretable as an optimization algorithm. Denoising diffusion cannot directly be formalized as descent dynamics of an objective functional over P , but its output is determined by a parameter θ (i.e. weights of the score matching neural networks).\n\nMain Contributions. In this work, we introduce the algorithm of Implicit Diffusion , an effective and principled technique for optimizing through a sampling operation. More precisely,",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-8",
      "content": "ural networks).\n\nMain Contributions. In this work, we introduce the algorithm of Implicit Diffusion , an effective and principled technique for optimizing through a sampling operation. More precisely,\n\n- We present a general framework describing parameterized sampling algorithms, and introduce Implicit Diffusion optimization, a single-loop optimization algorithm to optimize through sampling.\n- We provide theoretical guarantees in the continuous and discrete time settings in Section 4.\n- We showcase in Section 5 its performance and efficiency in experimental settings . Applications include finetuning denoising diffusions and training energy-based models.\n\nTo allow for reproducibility, we provide an implementa- tion 3 of our algorithm in JAX (Bradbury et al., 2018).\n\nNotation. For a set X (such as R d ), we write P for the set of probability distributions on X , omitting reference to X . For f a differentiable function on R d , we denote by ∇ f its gradient function.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 8,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-9",
      "content": "ation. For a set X (such as R d ), we write P for the set of probability distributions on X , omitting reference to X . For f a differentiable function on R d , we denote by ∇ f its gradient function. If f is a differentiable function of k variables, we let ∇ i f denote its gradient w.r.t. its i -th variable.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 9,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-10",
      "content": "The core operation that we consider is sampling by running a stochastic diffusion process that depends on a parameter θ ∈ R p . We consider iterative sampling operators that map from a parameter space to a space of probabilities . We denote by π ⋆ ( θ ) ∈ P the outcome distribution of this sampling operator. This parameterized distribution is defined in an implicit manner: there is not always an explicit way to write down its dependency on θ . More formally, it is defined as follows.\n\nDefinition 2.1 (Iterative sampling operators) . For a parameter θ ∈ R p , a sequence of parameterized functions Σ s ( · , θ ) from P to P defines a diffusion sampling process , from p 0 ∈ P iterating\n\n<!-- formula-not-decoded -->\n\nThe outcome π ⋆ ( θ ) ∈ P , when s → ∞ , or for some s = T defines a sampling operator π ⋆ : R p →P .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 10,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-11",
      "content": "to P defines a diffusion sampling process , from p 0 ∈ P iterating\n\n<!-- formula-not-decoded -->\n\nThe outcome π ⋆ ( θ ) ∈ P , when s → ∞ , or for some s = T defines a sampling operator π ⋆ : R p →P .\n\nWe embrace the formalism of stochastic processes as acting on probability distributions. This perspective focuses on the dynamics of the distribution ( p s ) s ≥ 0 , and allows us to more clearly present our optimization problem and algorithms. In practice, however, in all the examples that we consider, this is realized by an iterative process on some random variable X s such that X s ∼ p s .\n\nExample 2.2 . Consider the process defined by X s +1 = X s -2 δ ( X s -θ ) + √ 2 δB s , where the B s are i.i.d. standard Gaussian, X 0 ∼ p 0 := N ( µ 0 , σ 2 0 ), and δ ∈ (0 , 1). This is the discrete-time version of Langevin dynamics for V ( x, θ ) = 0 . 5( x -θ ) 2 (see Section 2.2).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 11,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-12",
      "content": "where the B s are i.i.d. standard Gaussian, X 0 ∼ p 0 := N ( µ 0 , σ 2 0 ), and δ ∈ (0 , 1). This is the discrete-time version of Langevin dynamics for V ( x, θ ) = 0 . 5( x -θ ) 2 (see Section 2.2). The dynamics induced on probabilities p s = N ( µ s , σ 2 s ) are µ s = θ +(1 -2 δ ) s ( µ 0 -θ ) and σ 2 s = 1+(1 -2 δ ) 2 s ( σ 2 0 -1). The sampling operator for s →∞ is therefore defined by π ⋆ : θ →N ( θ, 1).\n\nMore generally, we may consider the iterates X s of the process defined for noise variables ( B s ) s ≥ 0\n\n<!-- formula-not-decoded -->\n\n3 https://github.com/google-deepmind/implicit\\_ diffusion\n\nApplying f s ( · , θ ) to X s ∼ p s implicitly defines a dynamic Σ s ( · , θ ) on the distribution. The dynamics on the variables in (2) induce dynamics on the distributions described in (1). Note that, in the special case of normalizing flows (Kobyzev et al., 2019; Papamakarios et al., 2021), explicit formulas for p s can be derived and evaluated.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 12,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-13",
      "content": "cs on the distributions described in (1). Note that, in the special case of normalizing flows (Kobyzev et al., 2019; Papamakarios et al., 2021), explicit formulas for p s can be derived and evaluated.\n\nRemark 2.3 . i) We consider settings with discrete time steps, fitting our focus on algorithms to sample and optimize through sampling. This encompasses in particular the discretization of many continuous-time stochastic processes of interest. Most of our motivations are of this latter type, and we describe these distinctions in our examples (see Section 2.2).\n\nii) As noted above, these dynamics are often realized by an iterative process on variables X s , or even on an i.i.d. batch of samples ( X 1 s , . . . , X n s ). When the iterates Σ s ( p s , θ ) are written in our presentation (e.g. in optimization algorithms in Section 3), it is often a shorthand to mean that we have access to samples from p s , or equivalently to an empirical version ˆ p s, ( n ) of the population distribution",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 13,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-14",
      "content": "in optimization algorithms in Section 3), it is often a shorthand to mean that we have access to samples from p s , or equivalently to an empirical version ˆ p s, ( n ) of the population distribution p s . Sample versions of our algorithms are described in Appendix A.\n\niii) One of the special cases considered in our analysis are stationary processes with infinite time horizon, where the sampling operation can be interpreted as optimizing over the set of distributions\n\n<!-- formula-not-decoded -->\n\nIn this case, the iterative operations in (1) can often be directly interpreted as descent steps for the objective G ( · , θ ). However, our methodology is not limited to this setting: we also consider general sampling schemes with no stationarity and no inner G , but only a sampling process defined by Σ s .\n\nOptimization objective. We aim to optimize with respect to θ the output of the sampling operator, for a function F : P → R . In other words, we consider the optimization problem",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 14,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-15",
      "content": "cess defined by Σ s .\n\nOptimization objective. We aim to optimize with respect to θ the output of the sampling operator, for a function F : P → R . In other words, we consider the optimization problem\n\n<!-- formula-not-decoded -->\n\nThis formulation transforms a problem over distributions in P to a finite-dimensional problem over θ ∈ R p . Further, this allows for convenient post-optimization sampling: for some θ opt ∈ R p obtained by solving (4), one can sample from π ⋆ ( θ opt ). This is the common paradigm in model finetuning.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.1 Sampling and optimization perspectives",
        "chunkIndex": 15,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-16",
      "content": "Langevin dynamics. They are defined by the stochastic differential equation (SDE) (Roberts and\n\nTweedie, 1996)\n\n<!-- formula-not-decoded -->\n\nwhere V and θ ∈ R p are such that this SDE has a solution for t &gt; 0 that converges in distribution. Here π ⋆ ( θ ) is the limiting distribution of X t when t →∞ , which is the Gibbs distribution\n\n<!-- formula-not-decoded -->\n\nwhere Z θ = ∫ exp( -V ( x, θ ))d x is the normalization factor. To fit our setting of iterative sampling algorithms (2), one can consider the discretization for γ &gt; 0\n\n<!-- formula-not-decoded -->\n\nFor G ( p, θ ) = KL( p || π ⋆ ( θ )), the outcome of the sampling operator π ⋆ ( θ ) is a minimum of G ( · , θ ), and the SDE (5) implements a gradient flow for G in the space of measures, with respect to the Wasserstein-2 distance (Jordan et al., 1998). Two optimization objectives F are of particular interest in this case.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.2 Examples",
        "chunkIndex": 16,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-17",
      "content": "(5) implements a gradient flow for G in the space of measures, with respect to the Wasserstein-2 distance (Jordan et al., 1998). Two optimization objectives F are of particular interest in this case. First, we may want to maximize some reward R : R d → R over our samples, in which case the objective writes F ( p ) := -E x ∼ p [ R ( x )]. Second, to approximate a reference distribution p ref with sample access, it is possible to take F ( p ) := KL( p ref || p ). This case corresponds to training energy-based models (Gutmann and Hyv¨ arinen, 2012). It is also possible to consider a linear combination of these two objectives.\n\nDenoising diffusion. It consists in running the SDE for Y 0 ∼ N (0 , I ),\n\n<!-- formula-not-decoded -->\n\nwhere s θ : R d × [0 , T ] → R d is a parameterized score function (Hyv¨ arinen, 2005; Vincent, 2011; Ho et al., 2020). Its aim is to reverse a forward process d X t = -X t d t + √ 2d B t , where we have sample access to X 0 ∼ p data ∈ P .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.2 Examples",
        "chunkIndex": 17,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-18",
      "content": "arameterized score function (Hyv¨ arinen, 2005; Vincent, 2011; Ho et al., 2020). Its aim is to reverse a forward process d X t = -X t d t + √ 2d B t , where we have sample access to X 0 ∼ p data ∈ P . More precisely, denoting by p t the distribution of X t , if s θ ≈ ∇ log p t , then the distribution of Y T is close to p data for large T (Anderson, 1982), which allows approximate sampling from p data . Implementations of s θ include U-Nets (Ronneberger et al., 2015) or Vision Transformers that split the image into patches (Peebles and Xie, 2023). We present for simplicity an unconditioned model, but conditioning (on class, prompt, etc.) also falls in our framework.\n\nWe are interested in optimizing through diffusion sampling and consider π ⋆ ( θ ) as the distribution of Y T . A key example is when θ 0 represents the weights of a model s θ 0 that has been pretrained by score matching and one wants to finetune the target distribution π ⋆ ( θ ), e.g.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.2 Examples",
        "chunkIndex": 18,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-19",
      "content": "the distribution of Y T . A key example is when θ 0 represents the weights of a model s θ 0 that has been pretrained by score matching and one wants to finetune the target distribution π ⋆ ( θ ), e.g. in order to increase a reward R : R d → R . Figure 3 situates our contribution within the broader\n\nFigure 2: A step of optimization through sampling. For a given parameter θ 0 , the sampling process is defined by applying Σ s for s ∈ [ T ], producing π ⋆ ( θ 0 ). The goal of optimization through sampling is to update θ to minimize ℓ = F ◦ π ⋆ . Here the objective F corresponds to having lighter images (on average), which produces thicker digits.\n\n<!-- image -->\n\nliterature on this problem (see details in Appendix D). Note that this finetuning step does not require access to p data . As for Langevin dynamics, we consider in our algorithms discrete approximations of the process (7). However in this case, there exists no natural functional G minimized by the sampling process.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.2 Examples",
        "chunkIndex": 19,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-20",
      "content": "data . As for Langevin dynamics, we consider in our algorithms discrete approximations of the process (7). However in this case, there exists no natural functional G minimized by the sampling process. An alternative to (7) producing the same marginal distributions is the ordinary differential equation (ODE)\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "2.2 Examples",
        "chunkIndex": 20,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-21",
      "content": "The objective (4) presents several challenges, that we review here. We then introduce an overview of our approach, before detailing our algorithms.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3 METHODS",
        "chunkIndex": 21,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-22",
      "content": "Estimation of gradients through sampling. Even with samples from π ⋆ ( θ ), applying a first-order method to (4) requires evaluating gradients of ℓ = F ◦ π ⋆ . Since there is no closed form for ℓ and no explicit computational graph, we consider the following alternative setting to evaluate gradients.\n\nDefinition 3.1 (Implicit gradient estimation) . The gradient of ℓ can be implicitly estimated if Σ s , F are such that there exists Γ : P × R p → R p such that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ).\n\nIn practice we rarely reach exactly the distribution π ⋆ ( θ ), e.g. because a finite number of iterations of sampling is performed. Then, if ˆ π ≈ π ⋆ ( θ ), the gradient can be approximated by ˆ g = Γ(ˆ π, θ ). In particular, given access to approximate samples of π ⋆ ( θ ), it is possible to compute an estimate of ∇ ℓ ( θ ), and this is at the heart of our methods-see Appendix A.1 for more details.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.1 Overview",
        "chunkIndex": 22,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-23",
      "content": "π, θ ). In particular, given access to approximate samples of π ⋆ ( θ ), it is possible to compute an estimate of ∇ ℓ ( θ ), and this is at the heart of our methods-see Appendix A.1 for more details. Note that when Γ is linear in its first argument, sample access to π ⋆ ( θ ) yields unbiased estimates of the gradient. This case has been studied with various approaches (see Sutton et al., 1999; Fu and Hu, 2012; Pflug, 2012; De Bortoli et al., 2021, and Appendix D).\n\nFigure 3: Main approaches for reward tuning of denoising diffusions. References are given in Appendix D.\n\n<!-- image -->\n\nRemark 3.2 . Definition 3.1 is in fact always satisfied by the tautological definition Γ( p, θ ) := ∇ ℓ ( θ ). Rather than the existence of Γ, key questions are whether Γ is easily computable, can be stochastically approximated as explained above, or enjoys theoretical guarantees. We present several sampling problems in Section 3.2",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.1 Overview",
        "chunkIndex": 23,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-24",
      "content": "nce of Γ, key questions are whether Γ is easily computable, can be stochastically approximated as explained above, or enjoys theoretical guarantees. We present several sampling problems in Section 3.2\n\nwhere this is the case. This point of view should also extend beyond sampling to any setting that involves optimizing over a parameterized distribution, as for instance in Wasserstein Distributionally Robust Optimization (Mohajerin Esfahani and Kuhn, 2018), a method for robust learning. We leave these extensions to future investigations.\n\nBeyond nested-loop approaches. Sampling from π ⋆ ( θ ) is usually only feasible via iterations of the sampling process Σ s . The most straightforward method is then a nested loop: at each optimization step k , run an inner loop for a large number T of steps of Σ s to produce ˆ π k ≈ π ⋆ ( θ k ), and use it to evaluate a gradient. Algorithm 1 formalizes this baseline.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.1 Overview",
        "chunkIndex": 24,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-25",
      "content": "sted loop: at each optimization step k , run an inner loop for a large number T of steps of Σ s to produce ˆ π k ≈ π ⋆ ( θ k ), and use it to evaluate a gradient. Algorithm 1 formalizes this baseline. This approach can be inefficient for two reasons: first, it requires solving the inner sampling problem at each optimization step . Further, nested loops are typically impractical with accelerator-oriented hardware. These issues can be partially alleviated by techniques like gradient checkpointing (see references in Appendix D).\n\nAlgorithm 1 Vanilla nested-loop approach (Baseline) input θ 0 ∈ R p , p 0 ∈ P for k ∈ { 0 , . . . , K -1 } (outer optimization loop) do p (0) k ← p 0 for s ∈ { 0 , . . . , T -1 } (inner sampling loop) do p ( s +1) k ← Σ s ( p ( s ) k , θ k ) ˆ π k ← p ( T ) k θ k +1 ← θ k -η Γ(ˆ π k , θ k ) (or another optimizer) output θ K",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.1 Overview",
        "chunkIndex": 25,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-26",
      "content": "n loop) do p (0) k ← p 0 for s ∈ { 0 , . . . , T -1 } (inner sampling loop) do p ( s +1) k ← Σ s ( p ( s ) k , θ k ) ˆ π k ← p ( T ) k θ k +1 ← θ k -η Γ(ˆ π k , θ k ) (or another optimizer) output θ K\n\nWe rather follow an approach inspired by methods in bilevel optimization, aiming to jointly iterate on both the sampling problem (evaluation of π ⋆ -the inner problem), and the optimization problem over θ ∈ R p (the outer objective F ). We describe these methods in Section 3.3 and Algorithms 2 and 3. The connection with bilevel optimization is especially seamless when sampling can indeed be cast as an optimization problem over distributions in P , as in (3). However, as noted above, our approach generalizes beyond.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.1 Overview",
        "chunkIndex": 26,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-27",
      "content": "We explain how to perform implicit gradient estimation as in Definition 3.1, that is, how to derive expressions for the function Γ, in several cases of interest.\n\nDirect analytical derivation. For Langevin dynamics, it is possible to derive analytical expressions for Γ depending on the outer objective F . We illustrate this idea for the two objectives introduced in Section 2.2. First, in the case where F rew ( p ) = -E x ∼ p [ R ( x )], a computation detailed in Appendix A.2 and the use of Definition 3.1 show that, for ℓ rew ( θ ) = F rew ( π ⋆ ( θ )),\n\n<!-- formula-not-decoded -->\n\nNote that this formula does not involve gradients of R , hence our approach handles any non-differentiable reward. Second, in the case where F ref ( p ) = KL( p ref || p ), we then have (Gutmann and Hyv¨ arinen, 2012), for ℓ ref ( θ ) = F ref ( π ⋆ ( θ )),\n\n<!-- formula-not-decoded -->\n\nThis is known as contrastive learning when p ref is given by data, and suggests taking Γ as",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.2 Gradient estimation through sampling",
        "chunkIndex": 27,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-28",
      "content": "n have (Gutmann and Hyv¨ arinen, 2012), for ℓ ref ( θ ) = F ref ( π ⋆ ( θ )),\n\n<!-- formula-not-decoded -->\n\nThis is known as contrastive learning when p ref is given by data, and suggests taking Γ as\n\n<!-- formula-not-decoded -->\n\nThis extends to linear combinations of Γ rew and Γ ref .\n\nImplicit differentiation. When, as in (3), π ⋆ ( θ ) = argmin G ( · , θ ), under generic assumptions on G the implicit function theorem (see Krantz and Parks, 2002; Blondel et al., 2022 and Appendix A.4) shows that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ) with\n\n<!-- formula-not-decoded -->\n\nHere F ′ ( p ) : X → R denotes the first variation of F at p ∈ P (see Definition B.1) and γ ( p, θ ) is the solution of the linear system ∫ ∇ 1 , 1 G ( p, θ )[ x, x ′ ] γ ( p, θ )[ x ′ ]d x ′ = -∇ 1 , 2 G ( p, θ )[ x ]. Although this gives us a general way to define gradients of π ⋆ ( θ ) with respect to θ , solving this linear system is generally not feasible.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.2 Gradient estimation through sampling",
        "chunkIndex": 28,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-29",
      "content": "x ′ ] γ ( p, θ )[ x ′ ]d x ′ = -∇ 1 , 2 G ( p, θ )[ x ]. Although this gives us a general way to define gradients of π ⋆ ( θ ) with respect to θ , solving this linear system is generally not feasible. One exception is when sampling over a finite state space X , in which case P is finite-dimensional, and the integrals boil down to matrix-vector products.\n\nDifferential adjoint method. The adjoint method allows computing gradients through differential equation solvers (Pontryagin, 1987; Li et al., 2020), applying in particular for denoising diffusion. It can be connected to implicit differentiation, by defining G over a measure path instead of a single measure p (see, e.g., Kidger, 2022). To introduce this method, consider the ODE d Y t = µ ( t, Y t , θ )d t integrated between 0 and some T &gt; 0. This setting encompasses the denoising diffusion ODE (8). Assume that the outer objective F writes as the expectation of some differentiable reward R , namely F ( p ) = E x ∼ p [ R ( x )].",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.2 Gradient estimation through sampling",
        "chunkIndex": 29,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-30",
      "content": "e T &gt; 0. This setting encompasses the denoising diffusion ODE (8). Assume that the outer objective F writes as the expectation of some differentiable reward R , namely F ( p ) = E x ∼ p [ R ( x )]. Let Z 0 ∼ p, A 0 = ∇ R ( Z 0 ) , G 0 = 0, and consider the ODE system\n\n<!-- formula-not-decoded -->\n\nDefining Γ( p, θ ) := G T , the adjoint method shows that Γ( π ⋆ ( θ ) , θ ) is an unbiased estimate of ∇ ℓ ( θ ). We refer to Appendix A.3 for details and explanations on how to differentiate through the SDE sampler (7) and to incorporate a KL term in the reward by using Girsanov's theorem.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.2 Gradient estimation through sampling",
        "chunkIndex": 30,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-31",
      "content": "To circumvent solving the inner sampling problem in Algorithm 1, we propose in Algorithm 2 a joint singleloop approach that keeps track of a single dynamic of probabilities ( p k ) k ≥ 0 . At each optimization step, the probability p k is updated with one sampling step that depends on the current parameter θ k . As noted in Section 3.1, there are parallels with approaches in the literature when Γ is linear, but we go beyond in making no such assumption.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "3.3 Implicit Diffusion optimization algorithm",
        "chunkIndex": 31,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-32",
      "content": "<!-- formula-not-decoded -->\n\nThis point of view is well-suited for stationary processes with infinite-time horizon, but does not directly adapt to differentiation through diffusions with a finite-time horizon (and no stationary property). Indeed, it does not make sense in this case to run sampling for an arbitrary number of steps. Our approach can then be adapted as follows.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 2 Implicit Diff. optimization, infinite time",
        "chunkIndex": 32,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-33",
      "content": "<!-- formula-not-decoded -->\n\nFinite time-horizon: queuing trick. When π ⋆ ( θ ) is obtained by a large, but finite number T of iterations of the operator Σ s , we leverage hardware parallelism to evaluate in parallel several, say M , steps of the dynamics of the distribution p k , through a queue of length M . We present for simplicity in Figure 4 and in Algorithm 3 the case where M = T and discuss extensions in Appendix A.3. At each step, the last element of the queue p ( M ) k provides a distribution to update\n\nθ through evaluation of Γ. Note that its dynamics in the previous M steps, from p (0) k -M to p ( M ) k , used the M previous values of the parameter θ k -M , . . . , θ k -1 . In particular, it provides a biased estimate of ∇ ℓ ( θ k ). Importantly, leveraging parallelism, the runtime of our algorithm is O ( K ), gaining a factor of T compared to the nested-loop approach, but at a higher memory cost.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 3 Implicit Diff. optimization, finite time",
        "chunkIndex": 33,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-34",
      "content": "Our theoretical guarantees cover Langevin diffusions in the continuous and discrete settings, and a simple case of denoising diffusion. Proofs are given in Appendix B.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4 THEORETICAL ANALYSIS",
        "chunkIndex": 34,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-35",
      "content": "The continuous-time equivalent of Algorithm 2 in the case of Langevin dynamics is\n\n<!-- formula-not-decoded -->\n\nwhere p t denotes the distribution of X t and ε t &gt; 0 corresponds to the ratio of learning rates between the inner and the outer problems. In practice Γ( p t , θ t ) is approximated on a finite sample, making the dynamics in θ t stochastic. We leave the analysis of these stochastic dynamics for future work. A possible tool to do so is the propagation of chaos (Chaintron and Diez, 2022; Suzuki et al., 2023), a theory which aims at quantifying the deviation between the dynamics of a system of finitely many interacting particles and the limiting behavior described by a mean-field density.\n\nRecalling the definition (6) of π ⋆ ( θ ), we require the following assumptions.\n\nAssumption 4.1. π ⋆ ( θ t ) verifies the Log-Sobolev inequality with constant µ &gt; 0 for all t ≥ 0, i.e., for all p ∈ P , KL( p || π ⋆ ( θ t )) ≤ 1 2 µ ∥∇ log ( p π ⋆ ( θ t ) ) ∥ 2 L 2 ( p ) .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.1 Langevin with continuous flow",
        "chunkIndex": 35,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-36",
      "content": "ons.\n\nAssumption 4.1. π ⋆ ( θ t ) verifies the Log-Sobolev inequality with constant µ &gt; 0 for all t ≥ 0, i.e., for all p ∈ P , KL( p || π ⋆ ( θ t )) ≤ 1 2 µ ∥∇ log ( p π ⋆ ( θ t ) ) ∥ 2 L 2 ( p ) .\n\nAssumption 4.2. V is continuously differentiable and for θ ∈ R p , x ∈ R d , ∥∇ 2 V ( x, θ ) ∥ ≤ C , for some C &gt; 0.\n\nAssumption 4.1 generalizes µ -strong convexity of the potentials ( V ( · , θ t )) t ≥ 0 , including for instance distributions π whose potentials are bounded perturbations of a strongly convex potential (Bakry et al., 2014; Vempala and Wibisono, 2019). Assumptions 4.1 and 4.2 hold for example when the potential defines a mixture of Gaussians and the parameters θ determine the weights of the mixture (see Appendix B.1.2 for details). We also assume that the outer updates are bounded and Lipschitz continuous for the KL divergence.\n\nAssumption 4.3. For p, q ∈ P , θ ∈ R p , ∥ Γ( p, θ ) ∥ ≤ C and ∥ Γ( p, θ ) -Γ( q, θ ) ∥ ≤ K Γ √ KL( p || q ), for some C, K Γ &gt; 0.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.1 Langevin with continuous flow",
        "chunkIndex": 36,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-37",
      "content": "dates are bounded and Lipschitz continuous for the KL divergence.\n\nAssumption 4.3. For p, q ∈ P , θ ∈ R p , ∥ Γ( p, θ ) ∥ ≤ C and ∥ Γ( p, θ ) -Γ( q, θ ) ∥ ≤ K Γ √ KL( p || q ), for some C, K Γ &gt; 0.\n\nFigure 4: Illustration of the Implicit Diffusion algorithm, in the finite time setting. Left: Sampling - one step of the parameterized sampling scheme is applied in parallel to all distributions in the queue. Right: Optimization the last element of the queue is used to compute a gradient for the parameter.\n\n<!-- image -->\n\nThe next proposition shows that this assumption holds for the examples of interest given in Section 2.\n\nProposition 4.4. Consider a bounded function R : R d → R . Then, under Assumption 4.2, functions Γ rew and Γ ref defined by (9) -(10) satisfy Assumption 4.3.\n\nSince we make no strong convexity assumption, we cannot hope to prove convergence to a global minimizer, rather convergence of the average objective gradients.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.1 Langevin with continuous flow",
        "chunkIndex": 37,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-38",
      "content": "ed by (9) -(10) satisfy Assumption 4.3.\n\nSince we make no strong convexity assumption, we cannot hope to prove convergence to a global minimizer, rather convergence of the average objective gradients. Note that, with strong convexity, it is a rather standard extension to prove convergence to the global minimizer (see, e.g., in a similar context, Dagr´ eou et al., 2022).\n\nTheorem 4.5. Take ε t = min(1 , 1 / √ t ) in (11) . Then, under Assumptions 4.1, 4.2, and 4.3,\n\n<!-- formula-not-decoded -->\n\nThe proof starts by noting that updates in θ would follow the gradient flow for ℓ if p t = π ⋆ ( θ t ). The deviation to these ideal dynamics can be controlled by the KL divergence of p t from π ⋆ ( θ t ), which can itself be bounded since updates in X t are gradient steps for the KL (see Section 2.2). Finally, the decay of the ratio of learning rates ε t ensures that π ⋆ ( θ t ) is not moving away from p t too fast due to updates in θ t .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.1 Langevin with continuous flow",
        "chunkIndex": 38,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-39",
      "content": "ates in X t are gradient steps for the KL (see Section 2.2). Finally, the decay of the ratio of learning rates ε t ensures that π ⋆ ( θ t ) is not moving away from p t too fast due to updates in θ t . Taking ε t small amounts to a two-timescale approach , a tool commonly used to tackle non-convex optimization problems in machine learning (Heusel et al., 2017; Arbel and Mairal, 2022; Hong et al., 2023; Marion and Berthier, 2023).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.1 Langevin with continuous flow",
        "chunkIndex": 39,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-40",
      "content": "We now consider the discrete version of (11), namely\n\n<!-- formula-not-decoded -->\n\nwhere p k denotes the distribution of X k . This setting is more challenging due to the discretization bias (Dalalyan, 2017). We make classical smoothness assumptions to analyze discrete gradient descent (e.g., Cheng and Bartlett, 2018):\n\nAssumption 4.6. The functions ∇ 1 V ( · , θ ), ∇ 1 V ( x, · ) and ∇ ℓ are respectively L X -Lipschitz for all θ ∈ R p , L Θ -Lipschitz for all x ∈ R d , and L -Lipschitz.\n\nWe can then show the following convergence result.\n\nTheorem 4.7. Take γ k = c 1 / √ k and ε k = 1 / √ k in (12) . Under Assumptions 4.1, 4.2, 4.3, and 4.6,\n\n<!-- formula-not-decoded -->\n\nThe proof technique to bound the KL in discrete iterations is inspired by Cheng and Bartlett (2018). Comparing to the continuous case, this step incurs a discretization error proportional to γ k → 0, inducing a slower convergence rate. The result is similar to Dagr´ eou et al.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.2 Langevin with discrete flow",
        "chunkIndex": 40,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-41",
      "content": "ng and Bartlett (2018). Comparing to the continuous case, this step incurs a discretization error proportional to γ k → 0, inducing a slower convergence rate. The result is similar to Dagr´ eou et al. (2022) for finite-dimensional bilevel optimization, albeit our final convergence rate is slower.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.2 Langevin with discrete flow",
        "chunkIndex": 41,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-42",
      "content": "The case of denoising diffusion is more challenging since π ⋆ ( θ ) can not be readily characterized as the stationary point of an iterative process. We study a 1D Gaussian case and leave more general analysis for future work. Considering p data = N ( θ data , 1) and the forward process of Section 2.2, a straightforward computation shows that the score is given by ∇ log p t ( x ) = -( x -θ data e -t ). A natural parameterization of the score function is therefore s θ ( x, t ) := -( x -θe -t ). With this parameterization, the output of the sampling process (7) is π ⋆ ( θ ) = N ( θ (1 -e -2 T ) , 1). Remarkably, π ⋆ ( θ ) is Gaussian for all θ ∈ R , making the analytical study tractable.\n\nAssume that pretraining with samples of p data yields θ = θ 0 , and we want to finetune the model towards some other θ target ∈ R by optimizing the reward R ( x ) = -( x -θ target ) 2 over samples of π ⋆ ( θ ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.3 Denoising diffusion",
        "chunkIndex": 42,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-43",
      "content": "t pretraining with samples of p data yields θ = θ 0 , and we want to finetune the model towards some other θ target ∈ R by optimizing the reward R ( x ) = -( x -θ target ) 2 over samples of π ⋆ ( θ ). A short computation shows that ∇ ℓ ( θ ) = -E x ∼ π ⋆ ( θ ) R ′ ( x )(1 -e -2 T ), hence one can take Γ( p, θ ) = -E x ∼ p R ′ ( x )(1 -e -2 T ). In this setting, we study a continuous-time version of\n\nAlgorithm 3, where Σ is the denoising diffusion (7) and Γ is given above, and show convergence of θ to θ target . This shows that Algorithm 3 successfully finetunes the parameter to optimize the reward. We refer to Appendix B.3 for details.\n\nProposition 4.8. (informal) Let ( θ t ) t ≥ 0 be given by the continuous-time equivalent of Algorithm 3. Then ∥ θ 2 T -θ target ∥ = O ( e -T ) , and π ⋆ ( θ 2 T ) = N ( µ 2 T , 1) with µ 2 T = θ target + O ( e -T ) .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "4.3 Denoising diffusion",
        "chunkIndex": 43,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-44",
      "content": "We empirically illustrate the performance of Implicit Diffusion. Details are given in Appendix C.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5 EXPERIMENTS",
        "chunkIndex": 44,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-45",
      "content": "We consider the case where the potential V ( · , θ ) is a logsumexp of quadratics-so that the outcome distributions are mixtures of Gaussians. We optimize the reward R ( x ) = 1 ( x 1 &gt; 0) exp ( -∥ x -µ ∥ 2 ) , for µ ∈ R d , thereby illustrating the ability of our method to optimize rewards that are not differentiable. We run six sampling algorithms, including the infinite time-horizon version of Implicit Diffusion (Algorithm 2), all starting from p 0 = N (0 , I d ) and for K = 5 , 000 steps.\n\n- ■ Langevin diffusion (5) with potential V ( · , θ 0 ) for some fixed θ 0 ∈ R p , no reward.\n- ⋆ Implicit Diffusion with F ( p ) = -E X ∼ p [ R ( X )], yields both a sample ˆ p K and parameters θ opt .\n- ▼ Langevin (5) with potential V ( · , θ 0 ) -λR smooth , where R smooth is a smoothed version of R .\n- Langevin (5) with potential V ( · , θ opt ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.1 Reward training of Langevin processes",
        "chunkIndex": 45,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-46",
      "content": "yields both a sample ˆ p K and parameters θ opt .\n- ▼ Langevin (5) with potential V ( · , θ 0 ) -λR smooth , where R smooth is a smoothed version of R .\n- Langevin (5) with potential V ( · , θ opt ). This is inference post-training with Implicit Diffusion.\n- ♦ ♦ Nested loop (Algorithm 1) with T inner sampling steps for each gradient step.\n- ▲ ▲ Unrolling through the last step of sampling with T inner sampling steps for each outer step.\n\nFigure 5: Contour lines and samples for ( ): Langevin θ 0 - ( ) Unrolling with T = 100 inner sampling steps -( ) Implicit Diffusion.\n\n<!-- image -->\n\nBoth qualitatively (Figure 5) and quantitatively (Figure 6), we observe that our approach efficiently op- timizes through sampling. We analyze performance both in terms of steps (number of optimization stepsupdates in θ ) and gradient evaluations (number of sampling steps). After K optimization steps, our algorithm yields both θ opt := θ K and a sample ˆ p K approximately from π ⋆ ( θ opt ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.1 Reward training of Langevin processes",
        "chunkIndex": 46,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-47",
      "content": "ization stepsupdates in θ ) and gradient evaluations (number of sampling steps). After K optimization steps, our algorithm yields both θ opt := θ K and a sample ˆ p K approximately from π ⋆ ( θ opt ). Then, it is convenient and fast to sample post hoc, with a Langevin process using θ opt -as observed in Figure 6. This is similar in spirit to inference with a finetuned model, post-reward training. We compare our approach with several baselines. First, directly adding a reward term ( ▼ ) is less efficient: it tends to overfit on the reward, as the target distribution of this process is out of the family of π ⋆ ( θ )'s. Second, unrolling through the last step of sampling ( ▲ , ▲ ) leads to much slower optimization. Finally, using the nested loop approach ( ♦ , ♦ ) makes each optimization step T times more costly, while barely improving the performance after a given number of steps (due to less biased gradients).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.1 Reward training of Langevin processes",
        "chunkIndex": 47,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-48",
      "content": "Finally, using the nested loop approach ( ♦ , ♦ ) makes each optimization step T times more costly, while barely improving the performance after a given number of steps (due to less biased gradients). When comparing in terms of number of gradient evaluations, Implicit Diffusion strongly outperforms the nested loop approach. In other words, for a given computational budget, it is optimal to take T = 1 and perform more optimization steps, which corresponds to the Implicit Diffusion single-loop approach. Further comparison plots, as well as a variant of this experiment where we learn a reference distribution (i.e., train from scratch an energy-based model) are included in Appendix C. We also include an experiment where the potential V parameterizes the means and the covariances of the Gaussians in addition to the weights of the mixture.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.1 Reward training of Langevin processes",
        "chunkIndex": 48,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-49",
      "content": "We also apply Implicit Diffusion for reward finetuning of denoising diffusion models pretrained on image datasets. We denote by θ 0 the weights of a pretrained model, such that π ⋆ ( θ 0 ) ≈ p data . For various reward functions on the samples R : R d → R , we consider\n\n<!-- formula-not-decoded -->\n\ncommon in reward finetuning (see, e.g., Ziegler et al., 2019, and references therein), for positive and negative values of λ . We run Implicit Diffusion using the finite time-horizon variant (Algorithm 3), applying the adjoint method on SDEs for gradient estimation. We report selected samples of π ⋆ ( θ t ), as well as reward and KL divergence estimates (see Figures 1 and 7-9).\n\nWe report results on models pretrained on the image datasets MNIST (LeCun and Cortes, 1998), CIFAR-10 (Krizhevsky, 2009), and LSUN (bedrooms) (Yu et al., 2016). For MNIST, we use a 2 . 5M parameters model (no label conditioning). Our reward is the average brightness (i.e. average of all pixel values).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.2 Reward training of denoising diffusion",
        "chunkIndex": 49,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-50",
      "content": "(Krizhevsky, 2009), and LSUN (bedrooms) (Yu et al., 2016). For MNIST, we use a 2 . 5M parameters model (no label conditioning). Our reward is the average brightness (i.e. average of all pixel values). For CIFAR10 and LSUN, we pretrain a 53 . 2M parameters model,\n\n<!-- image -->\n\nFigure 6: Metrics for reward training of Langevin processes, 10 runs. The setting and color code are detailed in Section 5.1. Left: Reward on the sample distribution, at each outer objective step, averaged on a batch. Middle: Log-likelihood of π ⋆ ( θ opt ) on the sample distribution, at each outer step, averaged on a batch-higher is better. Right: Number of gradient evaluations needed to reach a given log-likelihood threshold (y-axis)-lower is better, for various sizes of inner loop T (x-axis) and various methods (same symbols and colors as in left plots). Implicit Diffusion is T = 1. White symbols with colored edges correspond to a log-likelihood threshold of -5 .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.2 Reward training of denoising diffusion",
        "chunkIndex": 50,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-51",
      "content": "of inner loop T (x-axis) and various methods (same symbols and colors as in left plots). Implicit Diffusion is T = 1. White symbols with colored edges correspond to a log-likelihood threshold of -5 . 8 (red dashed line in the middle plot) and fully-colored symbols to a threshold of -2 . 0 (black dashed line in the middle plot).\n\nFigure 7: Reward training with Implicit Diffusion for various learning rates η and reward strengths λ/β . For each dataset, we plot together the reward and the negative KL divergence w.r.t. π ⋆ ( θ 0 ).\n\n<!-- image -->\n\nFigure 8: Samples of reward training after pretraining on LSUN ( λ/β = 10). The reward incentives for redder images. Images are re-sampled with the same seed every five steps (see Appendix C.2).\n\n<!-- image -->\n\nwith label conditioning for CIFAR-10. Our reward is the average brightness of the red channel minus the average on the other channels.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.2 Reward training of denoising diffusion",
        "chunkIndex": 51,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-52",
      "content": "e same seed every five steps (see Appendix C.2).\n\n<!-- image -->\n\nwith label conditioning for CIFAR-10. Our reward is the average brightness of the red channel minus the average on the other channels. For pretraining, we follow the simple diffusion method (Hoogeboom et al., 2023) and use U-Net models (Ronneberger et al., 2015). We display visual examples in Figures 1, 8, 9 and in Appendix C, where we also report additional\n\nFigure 9: Samples of reward training after pretraining on MNIST. The reward favors darker images ( λ/β = -30). Images are re-sampled with the same seed every five steps (see Appendix C.2).\n\n<!-- image -->\n\nmetrics. While the finetuned models diverge from the original distribution, they retain overall semantic information (e.g. brighter digits are thicker, rather than on a gray background). We observe in Figure 7 the competition between reward and divergence to the pretrained distribution.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.2 Reward training of denoising diffusion",
        "chunkIndex": 52,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-53",
      "content": "overall semantic information (e.g. brighter digits are thicker, rather than on a gray background). We observe in Figure 7 the competition between reward and divergence to the pretrained distribution.\n\nPossible limitations of our approach include sensitivity to the choice of hyperparameters (learning rate η , reward strength λ/β , size of the queue M ), bias in the gradient estimation, practical applicability to larger scale problems or more complex rewards. We plan to investigate these questions in future research.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "5.2 Reward training of denoising diffusion",
        "chunkIndex": 53,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-54",
      "content": "The authors would like to thank Fabian Pedregosa for very fruitful discussions on implicit differentiation and bilevel optimization that led to this project, Vincent Roulet for very insightful notes and comments about early drafts of this work as well as help with experiment implementation, Emiel Hoogeboom for extensive help\n\non pretraining diffusion models, and Cl´ ement Cr´ epy for help with open-sourcing. PM and AK thank Google for their academic support in the form respectively of a Google PhD Fellowship and a gift in support of her academic research.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Acknowledgments",
        "chunkIndex": 54,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-55",
      "content": "- L. Ambrosio, N. Gigli, and G. Savar´ e. Gradient flows: in metric spaces and in the space of probability measures . Springer Science &amp; Business Media, 2005.\n- B. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications , 12(3):313-326, 1982.\n- M. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In International Conference on Learning Representations , 2022.\n- Y. F. Atchad´ e, G. Fort, and E. Moulines. On perturbed proximal gradient algorithms. The Journal of Machine Learning Research , 18(1):310-342, 2017.\n- D. Bakry, I. Gentil, M. Ledoux, et al. Analysis and geometry of Markov diffusion operators , volume 103. Springer, 2014.\n- K. Black, M. Janner, Y. Du, I. Kostrikov, and S. Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations , 2024.\n- M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-56",
      "content": "Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations , 2024.\n- M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-L´ opez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. Advances in neural information processing systems , 35:52305242, 2022.\n- J. Bolte, E. Pauwels, and S. Vaiter. One-step differentiation of iterative algorithms. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages 77089-77103. Curran Associates, Inc., 2023.\n- J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/ jax .\n10. L.-P. Chaintron and A. Diez.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-57",
      "content": "Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/ jax .\n10. L.-P. Chaintron and A. Diez. Propagation of chaos: A review of models, methods and applications. i. models and methods. Kinetic and Related Models , 15(6):895-1015, 2022.\n11. H.-B. Chen, S. Chewi, and J. Niles-Weed. Dimensionfree log-sobolev inequalities for mixture distributions. Journal of Functional Analysis , 281(11): 109236, 2021a.\n- T. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 25294-25307. Curran Associates, Inc., 2021b.\n- T. Chen, Y. Sun, Q. Xiao, and W. Yin. A singletimescale method for stochastic bilevel optimization.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-58",
      "content": "ral Information Processing Systems , volume 34, pages 25294-25307. Curran Associates, Inc., 2021b.\n- T. Chen, Y. Sun, Q. Xiao, and W. Yin. A singletimescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics , pages 2466-2488. PMLR, 2022.\n- X. Cheng and P. L. Bartlett. Convergence of Langevin MCMC in KL-divergence. In F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of ALT2018 , volume 83 of Proceedings of Machine Learning Research , pages 186-211. PMLR, 2018.\n- K. Clark, P. Vicol, K. Swersky, and D. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations , 2024.\n- M. Dagr´ eou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-59",
      "content": "blin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 26698-26710. Curran Associates, Inc., 2022.\n- A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology , 79(3):651-676, 2017.\n- V. De Bortoli, A. Durmus, M. Pereyra, and A. F. Vidal. Efficient stochastic optimisation by unadjusted langevin monte carlo: Application to maximum marginal likelihood and empirical bayesian estimation. Statistics and Computing , 31:1-18, 2021.\n- P. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-60",
      "content": "sian estimation. Statistics and Computing , 31:1-18, 2021.\n- P. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 8780-8794. Curran Associates, Inc., 2021.\n- H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. SHUM, and T. Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.\n- K. D. Dvijotham, S. Omidshafiei, K. Lee, K. M. Collins, D. Ramachandran, A. Weller, M. Ghavamzadeh, M. Nasr, Y. Fan, and J. Z. Liu. Algorithms for optimal adaptation of diffusion models to reward functions. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-61",
      "content": "mzadeh, M. Nasr, Y. Fan, and J. Z. Liu. Algorithms for optimal adaptation of diffusion models to reward functions. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023.\n\n- A. Eberle. Reflection couplings and contraction rates for diffusions. Probability theory and related fields , 166:851-886, 2016.\n- Y. Fan, O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee. Dpok: Reinforcement learning for finetuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381 , 2023.\n- L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 1568-1577. PMLR, 10-15 Jul 2018.\n- M. C. Fu and J.-Q. Hu.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-62",
      "content": "editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 1568-1577. PMLR, 10-15 Jul 2018.\n- M. C. Fu and J.-Q. Hu. Conditional Monte Carlo: Gradient estimation and Optimization Applications , volume 392. Springer Science &amp; Business Media, 2012.\n- A. Graikos, N. Malkin, N. Jojic, and D. Samaras. Diffusion models as plug-and-play priors. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 14715-14728. Curran Associates, Inc., 2022.\n- A. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation . SIAM, 2008.\n- Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel convergence analysis for algorithms of the adam family and beyond. arXiv preprint arXiv:2104.14840 , 2021.\n- M. U. Gutmann and A. Hyv¨ arinen.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-63",
      "content": "08.\n- Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel convergence analysis for algorithms of the adam family and beyond. arXiv preprint arXiv:2104.14840 , 2021.\n- M. U. Gutmann and A. Hyv¨ arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of machine learning research , 13(2), 2012.\n- A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations , 2023.\n- M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n- J. Ho, A. Jain, and P. Abbeel.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-64",
      "content": "Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n- J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems , volume 33, pages 6840-6851. Curran Associates, Inc., 2020.\n- M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A twotimescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization , 33 (1):147-180, 2023.\n- E. Hoogeboom, J. Heek, and T. Salimans. simple diffusion: End-to-end diffusion for high resolution images. In Proceedings of The 40th International Conference on Machine Learning , 2023.\n- A. Hyv¨ arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research , 6(24):695-709, 2005.\n- R. Jordan, D. Kinderlehrer, and F. Otto.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-65",
      "content": "ning , 2023.\n- A. Hyv¨ arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research , 6(24):695-709, 2005.\n- R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker-planck equation. SIAM journal on mathematical analysis , 29(1):1-17, 1998.\n- P. Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435 , 2022.\n- P. Kidger, J. Foster, X. C. Li, and T. Lyons. Efficient and accurate gradients for neural SDEs. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 1874718761. Curran Associates, Inc., 2021.\n- D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations , 2015.\n- I. Kobyzev, S. Prince, and M. A. Brubaker. Normalizing flows: Introduction and ideas. stat , 1050:25, 2019.\n- A. Korba and A. Salim.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-66",
      "content": ". In International Conference on Learning Representations , 2015.\n- I. Kobyzev, S. Prince, and M. A. Brubaker. Normalizing flows: Introduction and ideas. stat , 1050:25, 2019.\n- A. Korba and A. Salim. Sampling as first-order optimization over a space of probability measures, 2022. Tutorial at ICML 2022. Accessible at https://akorba.github.io/resources/ Baltimore\\_July2022\\_ICMLtutorial.pdf , consulted on 01/30/2024.\n- S. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications . Springer Science &amp; Business Media, 2002.\n- A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n- J. Kuntz, J. N. Lim, and A. M. Johansen. Particle algorithms for maximum likelihood training of latent variable models. In International Conference on Artificial Intelligence and Statistics , pages 5134-5180. PMLR, 2023.\n- M. Kwon, J. Jeong, and Y. Uh. Diffusion models already have a semantic latent space.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-67",
      "content": "able models. In International Conference on Artificial Intelligence and Statistics , pages 5134-5180. PMLR, 2023.\n- M. Kwon, J. Jeong, and Y. Uh. Diffusion models already have a semantic latent space. In The Eleventh International Conference on Learning Representations , 2023.\n\n- Y. LeCun and C. Cortes. MNIST handwritten digit database, 1998. URL http://yann.lecun.com/ exdb/mnist/ .\n- K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192 , 2023.\n- X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In C. Zhang, F. Ruiz, T. Bui, A. B. Dieng, and D. Liang, editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference , volume 118 of Proceedings of Machine Learning Research , pages 1-28. PMLR, 08 Dec 2020.\n- H. Liu, K. Simonyan, and Y. Yang.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-68",
      "content": "eedings of The 2nd Symposium on Advances in Approximate Bayesian Inference , volume 118 of Proceedings of Machine Learning Research , pages 1-28. PMLR, 08 Dec 2020.\n- H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations , 2019.\n- P. Marion and R. Berthier. Leveraging the two timescale regime to demonstrate convergence of neural networks. In Advances in Neural Information Processing Systems , volume 36, 2023.\n- P. Mohajerin Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming , 171(1): 115-166, 2018.\n- A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 8162-8171.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-69",
      "content": "robabilistic models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 8162-8171. PMLR, 18-24 Jul 2021.\n- A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. Advances in Neural Information Processing Systems , 27, 2014.\n- B. G. Pachpatte and W. Ames. Inequalities for Differential and Integral Equations . Elsevier, 1997.\n- G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. The Journal of Machine Learning Research , 22(1):2617-2680, 2021.\n- G. A. Pavliotis. Stochastic processes and applications . Springer, 2016.\n- F. Pedregosa. Hyperparameter optimization with approximate gradient. In M. F. Balcan and K. Q.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-70",
      "content": "rch , 22(1):2617-2680, 2021.\n- G. A. Pavliotis. Stochastic processes and applications . Springer, 2016.\n- F. Pedregosa. Hyperparameter optimization with approximate gradient. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pages 737-746, New York, New York, USA, 20-22 Jun 2016. PMLR.\n- W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 4195-4205, October 2023.\n- G. C. Pflug. Optimization of Stochastic Models: the Interface between Simulation and Optimization , volume 373. Springer Science &amp; Business Media, 2012.\n- L. S. Pontryagin. Mathematical Theory of Optimal Processes . Routledge, 1987.\n- P. Protter. Stochastic integration and differential equations. A new approach , volume 21 of Stochastic Modelling and Applied Probability .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-71",
      "content": "athematical Theory of Optimal Processes . Routledge, 1987.\n- P. Protter. Stochastic integration and differential equations. A new approach , volume 21 of Stochastic Modelling and Applied Probability . Springer Berlin, Heidelberg, 2005.\n- G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli , pages 341-363, 1996.\n- O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages 234-241. Springer, 2015.\n- L. Rosasco, S. Villa, and B. C. V˜ u. Convergence of stochastic proximal gradient algorithm. Applied Mathematics &amp; Optimization , 82:891-917, 2020.\n- L. Sharrock, D. Dodd, and C. Nemeth. Tuning-free maximum likelihood training of latent variable models via coin betting. In S. Dasgupta, S.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-72",
      "content": "Applied Mathematics &amp; Optimization , 82:891-917, 2020.\n- L. Sharrock, D. Dodd, and C. Nemeth. Tuning-free maximum likelihood training of latent variable models via coin betting. In S. Dasgupta, S. Mandt, and Y. Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics , volume 238 of Proceedings of Machine Learning Research , pages 1810-1818. PMLR, 02-04 May 2024.\n- R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems , volume 12, 1999.\n- T. Suzuki, A. Nitanda, and D. Wu. Uniform-in-time propagation of chaos for the mean-field gradient langevin dynamics. In The Eleventh International Conference on Learning Representations , 2023.\n- V. B. Tadi´ c and A. Doucet. Asymptotic bias of stochastic gradient search. Annals of Applied Probability , 27 (6):3255-3304, 2017.\n- A. Tsybakov.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-73",
      "content": "onal Conference on Learning Representations , 2023.\n- V. B. Tadi´ c and A. Doucet. Asymptotic bias of stochastic gradient search. Annals of Applied Probability , 27 (6):3255-3304, 2017.\n- A. Tsybakov. Introduction to nonparametric estimation . Springer Series in Statistics. Springer, New York, 2009.\n- B. Tzen and M. Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In A. Beygelzimer and D. Hsu, editors, Proceedings of the Thirty-Second Conference\n\n- on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pages 3084-3114. PMLR, 25-28 Jun 2019.\n- S. Vempala and A. Wibisono. Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry suffices. In Advances in Neural Information Processing Systems , volume 32, 2019.\n- P. Vincent. A connection between score matching and denoising autoencoders. Neural Computation , 23(7): 1661-1674, 2011.\n- B. Wallace, A. Gokul, S. Ermon, and N. Naik.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-74",
      "content": "ing Systems , volume 32, 2019.\n- P. Vincent. A connection between score matching and denoising autoencoders. Neural Computation , 23(7): 1661-1674, 2011.\n- B. Wallace, A. Gokul, S. Ermon, and N. Naik. End-toend diffusion latent optimization improves classifier guidance. arXiv preprint arXiv:2303.13703 , 2023.\n- Z. Wang and J. Sirignano. A forward propagation algorithm for online optimization of nonlinear stochastic differential equations. arXiv preprint arXiv:2207.04496 , 2022.\n- Z. Wang and J. Sirignano. Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations. Mathematical Finance , 34(2):348-424, 2024.\n- D. Watson, W. Chan, J. Ho, and M. Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations , 2022.\n- R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-75",
      "content": "ing through sample quality. In International Conference on Learning Representations , 2022.\n- R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 8(3-4):229-256, 1992.\n- Q. Wu, Y. Liu, H. Zhao, A. Kale, T. Bui, T. Yu, Z. Lin, Y. Zhang, and S. Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1900-1910, June 2023a.\n- X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 2096-2105, October 2023b.\n- L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization , 24(4):2057-2075, 2014.\n- J. Yang, K. Ji, and Y. Liang.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-76",
      "content": "October 2023b.\n- L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization , 24(4):2057-2075, 2014.\n- J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. Advances in Neural Information Processing Systems , 34:13670-13682, 2021.\n- F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2016.\n- Z. Zhang, L. Liu, Z. Lin, Y. Zhu, and Z. Zhao. Unsupervised discovery of interpretable directions in h-\n- space of pre-trained diffusion models. arXiv preprint arXiv:2310.09912 , 2023.\n- D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Finetuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-77",
      "content": "o, and G. Irving. Finetuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-78",
      "content": "1. For all models and algorithms presented, check if you include:\n2. (a) A clear description of the mathematical setting, assumptions, algorithm, and/or model. Yes\n3. (b) An analysis of the properties and complexity (time, space, sample size) of any algorithm. Yes\n4. (c) (Optional) Anonymized source code, with specification of all dependencies, including external libraries. No\n\nThe setting and algorithms are described in Sections 1-3, and further details are given in Section A. We open-sourced the source code related to the experiments on reward training of Langevin processes.\n\n2. For any theoretical claim, check if you include:\n2. (a) Statements of the full set of assumptions of all theoretical results. Yes\n3. (b) Complete proofs of all theoretical results. Yes\n4. (c) Clear explanations of any assumptions. Yes\n\nSee Section 4 for precise mathematical statements and assumptions, and Appendix B for proofs.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Checklist",
        "chunkIndex": 78,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-79",
      "content": "3. (b) Complete proofs of all theoretical results. Yes\n4. (c) Clear explanations of any assumptions. Yes\n\nSee Section 4 for precise mathematical statements and assumptions, and Appendix B for proofs.\n\n3. For all figures and tables that present empirical results, check if you include:\n2. (a) The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). No\n3. (b) All the training details (e.g., data splits, hyperparameters, how they were chosen). Yes\n4. (c) A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). Yes\n5. (d) A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). Yes",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Checklist",
        "chunkIndex": 79,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-80",
      "content": "with respect to the random seed after running experiments multiple times). Yes\n5. (d) A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). Yes\n\nWe open-sourced the source code related to the experiments on reward training of Langevin processes. Error bars are provided over independent repetitions for Langevin experiments, as described in Section 5 and Appendix C, while they are too costly to compute for the denoising diffusion experiments. The computing infrastructure is described in Section 5 and Appendix C.\n\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:\n2. (a) Citations of the creator If your work uses existing assets. Yes\n3. (b) The license information of the assets, if applicable. No\n4. (c) New assets either in the supplemental material or as a URL, if applicable. Not Applicable\n5. (d) Information about consent from data providers/curators.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Checklist",
        "chunkIndex": 80,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-81",
      "content": "rmation of the assets, if applicable. No\n4. (c) New assets either in the supplemental material or as a URL, if applicable. Not Applicable\n5. (d) Information about consent from data providers/curators. Not Applicable\n6. (e) Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. Not Applicable\n\nSee Section 5 and Appendix C for citations of the datasets and main code packages used in this project.\n\n5. If you used crowdsourcing or conducted research with human subjects, check if you include:\n2. (a) The full text of instructions given to participants and screenshots. Not Applicable\n3. (b) Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. Not Applicable\n4. (c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. Not Applicable\n\nAs a consequence,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Checklist",
        "chunkIndex": 81,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-82",
      "content": "applicable. Not Applicable\n4. (c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. Not Applicable\n\nAs a consequence,\n\n<!-- formula-not-decoded -->\n\nThis computation is sometimes referred to as the REINFORCE trick (Williams, 1992). This suggests taking\n\n<!-- formula-not-decoded -->\n\nIn this case, the sample version of Algorithm 2 is\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Checklist",
        "chunkIndex": 82,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-83",
      "content": "Organization of the Appendix. Section A is devoted to explanations of our methodology. Section A.1 explains the gradient estimation setting we consider. Then, in the case of Langevin dynamics (Section A.2), and denoising diffusions (Section A.3), we explain how Definition 3.1 and our Implicit Differentiation algorithms (Algorithms 2 and 3) can be instantiated. Section A.4 gives more details about the implicit differentiation approaches sketched in Section 3.2. Section B contains the proofs of our theoretical results, while Section C gives details for the experiments of Section 5 as well as additional explanations and plots. Finally, Section D is dedicated to additional related work.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "APPENDIX",
        "chunkIndex": 83,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-84",
      "content": "As discussed in Section 3.1, we focus on settings where the gradient of the loss ℓ : R p → R , defined by\n\n<!-- formula-not-decoded -->\n\ncan be estimated by using a function Γ. More precisely, following Definition 3.1, we assume that there exists a function Γ : P × R p → R p such that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ). In practice, for almost every setting there is no closed form for π ⋆ ( θ ), and even sampling from it can be challenging (e.g. here, if it is the outcome of infinitely many sampling steps). When we run our algorithms, the dynamic is in practice applied to variables, as discussed in Section 2.1. Using a batch of variables of size n , initialized independent with X i 0 ∼ p 0 , we have at each step k of joint sampling and optimization a batch of variables forming an empirical measure ˆ p ( n ) k .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.1 Gradient estimation abstraction: Γ",
        "chunkIndex": 84,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-85",
      "content": "ng a batch of variables of size n , initialized independent with X i 0 ∼ p 0 , we have at each step k of joint sampling and optimization a batch of variables forming an empirical measure ˆ p ( n ) k . We consider cases where the operator Γ is well-behaved: if ˆ p ( n ) ≈ p , then Γ(ˆ p ( n ) , θ ) ≈ Γ( p, θ ), and therefore where this finite sample approximation can be used to produce an accurate estimate of ∇ ℓ ( θ ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.1 Gradient estimation abstraction: Γ",
        "chunkIndex": 85,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-86",
      "content": "We explain how to derive the formulas (9)-(10) for Γ, and give the sample version of Algorithm 2 in these cases.\n\nRecall that the stationary distribution of the dynamics (5) is the Gibbs distribution (6), with the normalization factor Z θ = ∫ exp( -V ( x, θ ))d x . Assume that the outer objective can be written as the expectation of some (potentially non-differentiable) reward R , namely F ( p ) := -E x ∼ p [ R ( x )]. Then our objective is\n\n<!-- formula-not-decoded -->\n\nwhere (∆ B k ) k ≥ 0 are i.i.d. standard Gaussian random variables and ˆ Cov is the empirical covariance over the sample.\n\nWhen F ( p ) := KL( p ref | p ), e.g., when we want to regularize towards a reference distribution p ref with sample access, for ℓ ref ( θ ) = F ( π ⋆ ( θ )), we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nthus\n\nLeveraging the explicit formula (6) for π ⋆ ( θ ), we obtain\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.2 Langevin dynamics",
        "chunkIndex": 86,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-87",
      "content": ")), we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nthus\n\nLeveraging the explicit formula (6) for π ⋆ ( θ ), we obtain\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the third equality uses that p ref [ x ]d x = 1. This suggests taking\n\n<!-- formula-not-decoded -->\n\nThe terms in this gradient can be estimated: for a model V ( · , θ ), the gradient function w.r.t θ can be obtained by automatic differentiation. Samples from p ref are available by assumption, and samples from π ⋆ ( θ ) can be replaced by the X k in joint optimization as above. We recover the formula for contrastive learning of energy-based model to data from p ref (Gutmann and Hyv¨ arinen, 2012).\n\nThis can also be used for finetuning, combining a reward R and a KL term, with F ( p ) = -λ E X ∼ p [ R ( x )] + β KL( p || p ref ). The sample version of Algorithm 2 then can be written\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.2 Langevin dynamics",
        "chunkIndex": 87,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-88",
      "content": "used for finetuning, combining a reward R and a KL term, with F ( p ) = -λ E X ∼ p [ R ( x )] + β KL( p || p ref ). The sample version of Algorithm 2 then can be written\n\n<!-- formula-not-decoded -->\n\nwhere (∆ B k ) k ≥ 0 are i.i.d. standard Gaussian random variables, ˆ Cov is the empirical covariance over the sample, and ˜ X ( j ) k ∼ p ref .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.2 Langevin dynamics",
        "chunkIndex": 88,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-89",
      "content": "We explain how to use the adjoint method to backpropagate through differential equations, and apply this to derive instantiations of Algorithm 3 for denoising diffusions.\n\nODE sampling. We begin by recalling the adjoint method in the ODE case (Pontryagin, 1987). Consider the ODE d Y t = µ ( t, Y t , θ )d t integrated between 0 and some T &gt; 0. For some differentiable function R : R d → R , the derivative of R ( Y T ) with respect to θ can be computed by the adjoint method. More precisely, it is equal to G T defined by\n\n<!-- formula-not-decoded -->\n\nNote that sometimes the adjoint equations are written with a reversed time index ( t ′ = T -t ), which is not the formalism we adopt here.\n\nIn the setting of denoising diffusion presented in Section 2.2, we are not interested in computing the derivative of a function of a single realization of the ODE, but of the expectation over Y T ∼ π ⋆ ( θ ) of the derivative of R ( Y T ) with respect to θ .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.3 Adjoint method and denoising diffusions",
        "chunkIndex": 89,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-90",
      "content": "2.2, we are not interested in computing the derivative of a function of a single realization of the ODE, but of the expectation over Y T ∼ π ⋆ ( θ ) of the derivative of R ( Y T ) with respect to θ . In other words, we want to compute ∇ ℓ ( θ ) = ∇ ( F ◦ π ⋆ )( θ ), where F ( p ) = E x ∼ p [ R ( x )]. Rewriting the equations above in this case, we obtain that G T defined by\n\n<!-- formula-not-decoded -->\n\nis an unbiased estimator of ∇ ℓ ( θ ). Recalling Definition 3.1, this means that we can take Γ( p, θ ) := G T defined by\n\n<!-- formula-not-decoded -->\n\nThis is exactly the definition of Γ given in Section 3.2. We apply this to the case of denoising diffusions, where is µ given by (8). To avoid a notation clash between the number of iterations T = M of the sampling algorithm, and the maximum time T of the ODE in (8), we rename the latter to T horizon . A direct instantiation of Algorithm 3 with an Euler solver is the following algorithm.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.3 Adjoint method and denoising diffusions",
        "chunkIndex": 90,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-91",
      "content": "um time T of the ODE in (8), we rename the latter to T horizon . A direct instantiation of Algorithm 3 with an Euler solver is the following algorithm.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.3 Adjoint method and denoising diffusions",
        "chunkIndex": 91,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-92",
      "content": "<!-- formula-not-decoded -->\n\nSeveral comments are in order. First, the dynamics of Y ( M ) k in the previous M steps, from Y (0) k -M to Y ( M -1) k -1 , uses the M previous values of the parameter θ k -M , . . . , θ k -1 . This means that Y ( M ) k does not correspond to the result of sampling with any given parameter θ , since we are at the same time performing the sampling process and updating θ .\n\nBesides, the computation of Γ( p, θ ) is the outcome of an iterative process, namely calling an ODE solver. Therefore, it is also possible to use the same queuing trick as for sampling iterations to decrease the cost of this step by leveraging parallelization. For completeness, the variant is given below.\n\nAlgorithm 5 Implicit Diff. optimization, denoising diffusions with ODE sampling, variant with a double queue",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 4 Implicit Diff. optimization, denoising diffusions with ODE sampling",
        "chunkIndex": 92,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-93",
      "content": "of this step by leveraging parallelization. For completeness, the variant is given below.\n\nAlgorithm 5 Implicit Diff. optimization, denoising diffusions with ODE sampling, variant with a double queue\n\n```\ninput θ 0 ∈ R p , p 0 ∈ P input P M = [ Y (0) 0 , . . . , Y ( M ) 0 ] ∼ N (0 , 1) ⊗ ( m × d ) for k ∈ { 0 , . . . , K -1 } (joint single loop) do Y (0) k +1 ∼ N (0 , 1) Z (0) k +1 ← Y ( M ) k A (0) k +1 ←∇ R ( Z (0) k +1 ) G (0) k +1 ← 0 parallel Y ( m +1) k +1 ← Y ( m ) k + 1 M µ ( mT horizon M , Y ( m ) k , θ k ) for m ∈ [ M -1] parallel Z ( m +1) k +1 ← Z ( m ) k -1 M µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] parallel A ( m +1) k +1 ← A ( m ) k + 1 M ( A ( m ) k ) ⊺ ∇ 2 µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] parallel G ( m +1) k +1 ← G ( m ) k + 1 M ( G ( m ) k ) ⊺ ∇ 2 µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] θ k +1 ← θ k -ηG ( M ) k output θ K\n```",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 4 Implicit Diff. optimization, denoising diffusions with ODE sampling",
        "chunkIndex": 93,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-94",
      "content": "horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] parallel G ( m +1) k +1 ← G ( m ) k + 1 M ( G ( m ) k ) ⊺ ∇ 2 µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] θ k +1 ← θ k -ηG ( M ) k output θ K\n```\n\nSecond, each variable Y ( m ) k consists of a single sample of R d . The algorithm straightforwardly extends when each variable Y ( m ) k is a batch of samples. Finally, we consider so far the case where the size of the queue M is equal to the number of sampling steps T . We give below the variant of Algorithm 4 when M = T but M divides T . Taking M from 1 to T balances between a single-loop and a nested-loop algorithm.\n\n̸\n\n̸",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 4 Implicit Diff. optimization, denoising diffusions with ODE sampling",
        "chunkIndex": 94,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-95",
      "content": "```\ninput θ 0 ∈ R p , p 0 ∈ P input P M = [ Y (0) 0 , . . . , Y ( M ) 0 ] ∼ N (0 , 1) ⊗ ( m × d ) for k ∈ { 0 , . . . , K -1 } do Y (0) k +1 ∼ N (0 , 1) parallel Y ( m +1) k +1 / 2 ← Y ( m ) k for m ∈ [ M -1] for t ∈ { 0 , . . . , T /M -1 } in parallel for m ∈ [ M -1] do Y ( m +1) k +1 / 2 ← Y ( m ) k +1 / 2 + 1 T µ (( m M + t T ) T horizon , Y ( m ) k +1 / 2 , θ k ) parallel Y ( m +1) k +1 ← Y ( m +1) k +1 / 2 for m ∈ [ M -1] Z (0) k ← Y ( M ) k A (0) k ←∇ R ( Z (0) k ) G (0) k ← 0 for t ∈ { 0 , . . . , T -1 } do Z ( t +1) k ← Z ( t ) k -1 T µ ( tT horizon T , Z ( t ) k , θ k ) A ( t +1) k ← A ( t ) k + 1 T ( A ( t ) k ) ⊺ ∇ 2 µ ( tT horizon T , Z ( t ) k , θ k ) G ( t +1) k ← G ( t ) k + 1 T ( G ( t ) k ) ⊺ ∇ 2 µ ( tT horizon T , Z ( t ) k , θ k ) θ k +1 ← θ k -ηG ( T ) k output θ K\n```\n\nAlgorithm 5 extends to this case similarly.\n\nSDE sampling. The adjoint method is also defined in the SDE case (Li et al., 2020). Consider the SDE\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 6 Implicit Diff. optimization, denoising diffusions with ODE sampling, M = T , M divides T",
        "chunkIndex": 95,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-96",
      "content": "( T ) k output θ K\n```\n\nAlgorithm 5 extends to this case similarly.\n\nSDE sampling. The adjoint method is also defined in the SDE case (Li et al., 2020). Consider the SDE\n\n<!-- formula-not-decoded -->\n\nintegrated between 0 and some T &gt; 0. This setting encompasses the denoising diffusion SDE (7) with the appropriate choice of µ . For some differentiable function R : R d → R and for a given realization ( Y t ) 0 ≤ t ≤ T of the\n\nSDE, the derivative of R ( Y T ) with respect to θ is equal to G T defined by\n\n<!-- formula-not-decoded -->\n\nThis is a similar equation as in the ODE case. The main difference is that it is not possible to recover Y T -t only from the terminal value of the path Y T , but that we need to keep track of the randomness from the Brownian motion B t . Efficient ways to do so are presented in Li et al. (2020); Kidger et al. (2021).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 6 Implicit Diff. optimization, denoising diffusions with ODE sampling, M = T , M divides T",
        "chunkIndex": 96,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-97",
      "content": "om the terminal value of the path Y T , but that we need to keep track of the randomness from the Brownian motion B t . Efficient ways to do so are presented in Li et al. (2020); Kidger et al. (2021). In a nutshell, they consist in only keeping in memory the seed used to generate the Brownian motion, and recomputing the path from the seed.\n\nUsing the SDE sampler allows us to incorporate a KL term in the reward. Indeed, consider the SDE (13) for two different parameters θ 1 and θ 2 , with associated variables Y 1 t and Y 2 t . Then, by Girsanov's theorem (see Protter, 2005, Chapter III.8, and Tzen and Raginsky, 2019 for use in a similar context), the KL divergence between the paths Y 1 t and Y 2 t is\n\n<!-- formula-not-decoded -->\n\nwhere q 1 t denotes the distribution of Y 1 t . This term can be (stochastically) estimated at the same time as the SDE (13) is simulated, by appending a new coordinate to Y t (and to µ ) that integrates (15) over time.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 6 Implicit Diff. optimization, denoising diffusions with ODE sampling, M = T , M divides T",
        "chunkIndex": 97,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-98",
      "content": "the distribution of Y 1 t . This term can be (stochastically) estimated at the same time as the SDE (13) is simulated, by appending a new coordinate to Y t (and to µ ) that integrates (15) over time. Then, adding the KL in the reward is as simple as adding a linear term in ˜ R : R d +1 → R , that is, ˜ R ( x ) = R ( x [: -1]) + x [ -1] (using Numpy notation, where ' -1' denotes the last index). The same idea is used in Dvijotham et al. (2023) to incorporate a KL term in reward finetuning of denoising diffusion models. Finally, note that, if we had at our disposal a reward R that indicates if an image is 'close' to p data (for instance implemented by a neural network), we could use our algorithm to train from scratch a denoising diffusion.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 6 Implicit Diff. optimization, denoising diffusions with ODE sampling, M = T , M divides T",
        "chunkIndex": 98,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-99",
      "content": "Finite dimension. Take g : R m × R p → R a continuously differentiable function. Then x ⋆ ( θ ) = argmin g ( · , θ ) implies a stationary point condition ∇ 1 g ( x ⋆ ( θ 0 ) , θ 0 ) = 0. In this case, it is possible to define and analyze the function x ⋆ : R p → R m and its variations. Note that this generalizes to the case where x ⋆ ( θ ) can be written as the root of a parameterized system.\n\nMore precisely, the implicit function theorem (see, e.g., Griewank and Walther, 2008; Krantz and Parks, 2002, and references therein) can be applied. Under differentiability assumptions on g , for ( x 0 , θ 0 ) such that ∇ 1 g ( x 0 , θ 0 ) = 0 with a continuously differentiable ∇ 1 g , and if the Hessian ∇ 1 , 1 g evaluated at ( x 0 , θ 0 ) is a square invertible matrix, then there exists a function x ⋆ ( · ) over a neighborhood of θ 0 satisfying x ⋆ ( θ 0 ) = x 0 . Furthermore, for all θ in this neighborhood, we have that ∇ 1 g ( x ⋆ ( θ ) , θ ) = 0 and its Jacobian ∂x ⋆ ( θ ) exists.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.4 Implicit differentiation",
        "chunkIndex": 99,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-100",
      "content": "a function x ⋆ ( · ) over a neighborhood of θ 0 satisfying x ⋆ ( θ 0 ) = x 0 . Furthermore, for all θ in this neighborhood, we have that ∇ 1 g ( x ⋆ ( θ ) , θ ) = 0 and its Jacobian ∂x ⋆ ( θ ) exists. It is then possible to differentiate with respect to θ both sides of the equation ∇ 1 g ( x ⋆ ( θ 0 ) , θ 0 ) = 0, which yields a linear equation satisfied by this Jacobian\n\n<!-- formula-not-decoded -->\n\nThis formula can be used for automatic implicit differentiation, when both the evaluation of the derivatives in this equation and the inversion of the linear system can be done automatically Blondel et al. (2022).\n\nExtension to space of probabilities. When G : P × R p → R and π ⋆ ( θ ) = argmin G ( · , θ ) as in (3), under assumptions on differentiability and uniqueness of the solution on G , this can also be extended to a distribution setting.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.4 Implicit differentiation",
        "chunkIndex": 100,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-101",
      "content": "es. When G : P × R p → R and π ⋆ ( θ ) = argmin G ( · , θ ) as in (3), under assumptions on differentiability and uniqueness of the solution on G , this can also be extended to a distribution setting. We write here the infinite-dimensional equivalent of the above equations, involving derivatives or variations over the space of probabilities, and refer to Ambrosio et al. (2005) for more details.\n\nFirst, we have that\n\n<!-- formula-not-decoded -->\n\nwhere F ′ ( p ) : X → R denotes the first variation of F at p ∈ P (see Definition B.1). This yields ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ) with\n\n<!-- formula-not-decoded -->\n\nwhere γ ( p, θ ) is the solution of the linear system\n\n<!-- formula-not-decoded -->\n\nAlthough this gives us a general way to define gradients of π ⋆ ( θ ) with respect to θ , solving this linear system is generally not feasible. One exception is when sampling over a finite state space X , in which case P is finite-dimensional, and the integrals boil down to matrix-vector products.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.4 Implicit differentiation",
        "chunkIndex": 101,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-102",
      "content": "this linear system is generally not feasible. One exception is when sampling over a finite state space X , in which case P is finite-dimensional, and the integrals boil down to matrix-vector products.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "A.4 Implicit differentiation",
        "chunkIndex": 102,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-103",
      "content": "Notations. We denote by P 2 ( R d ) the set of probability measures on R d with bounded second moments. Given a Lebesgue measurable map T : X → X and µ ∈ P 2 ( X ), T # µ is the pushforward measure of µ by T . For any µ ∈ P 2 ( R d ), L 2 ( µ ) is the space of functions f : R d → R such that ∫ ∥ f ∥ 2 dµ &lt; ∞ . We denote by ∥ · ∥ L 2 ( µ ) and ⟨· , ·⟩ L 2 ( µ ) respectively the norm and the inner product of the Hilbert space L 2 ( µ ). We consider, for µ, ν ∈ P 2 ( R d ), the 2-Wasserstein distance W 2 ( µ, ν ) = inf s ∈S ( µ,ν ) ∫ ∥ x -y ∥ 2 ds ( x, y ), where S ( µ, ν ) is the set of couplings between µ and ν . The metric space ( P 2 ( R d ) , W 2 ) is called the Wasserstein space.\n\nLet F : P ( R d ) → R + a functional.\n\nDefinition B.1. Fix ν ∈ P ( R d ). If it exists, the first variation of F at ν is the function F ′ ( ν ) : R d → R s. t. for any µ ∈ P ( R d ), with ξ = µ -ν :\n\n<!-- formula-not-decoded -->\n\nand is defined uniquely up to an additive constant.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.1 Additional definitions",
        "chunkIndex": 103,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-104",
      "content": "xists, the first variation of F at ν is the function F ′ ( ν ) : R d → R s. t. for any µ ∈ P ( R d ), with ξ = µ -ν :\n\n<!-- formula-not-decoded -->\n\nand is defined uniquely up to an additive constant.\n\nWe will extensively apply the following formula:\n\n<!-- formula-not-decoded -->\n\nWe will also rely regularly on the definition of a Wasserstein gradient flow, since Langevin dynamics correspond to a Wasserstein gradient flow of the Kullback-Leibler (KL) divergence Jordan et al. (1998). A Wasserstein gradient flow of F Ambrosio et al. (2005) can be described by the following continuity equation:\n\n<!-- formula-not-decoded -->\n\nwhere F ′ denotes the first variation. Equation (17) holds in the sense of distributions (i.e. the equation above holds when integrated against a smooth function with compact support), see (Ambrosio et al., 2005, Chapter 8). In particular, if F = KL( ·| π ) for π ∈ P 2 ( R d ), then ∇ W 2 F ( µ ) = ∇ log( µ / π ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.1 Additional definitions",
        "chunkIndex": 104,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-105",
      "content": "olds when integrated against a smooth function with compact support), see (Ambrosio et al., 2005, Chapter 8). In particular, if F = KL( ·| π ) for π ∈ P 2 ( R d ), then ∇ W 2 F ( µ ) = ∇ log( µ / π ). In this case, the corresponding continuity equation is known as the Fokker-Planck equation, and in particular it is known that the law p t of Langevin dynamics: √\n\n<!-- formula-not-decoded -->\n\nsatisfies the Fokker-Planck equation (Pavliotis, 2016, Chapter 3).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.1 Additional definitions",
        "chunkIndex": 105,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-106",
      "content": "We begin by a more formal statement of the result alluded to in Section 4.1.\n\nProposition B.2. Let\n\n<!-- formula-not-decoded -->\n\nfor some fixed z 1 , . . . , z p ∈ R d and where\n\nThen\n\n<!-- formula-not-decoded -->\n\nis a shifted version of the logistic function for some η ∈ (0 , 1) . Then Assumptions 4.1 and 4.2 hold.\n\nProof. Assumption 4.1 holds since a mixture of Gaussians is Log-Sobolev with a bounded constant (Chen et al., 2021a, Corollary 1). Note that the constant deteriorates as the modes of the mixture get further apart.\n\nFurthermore, Assumption 4.2 holds since, for all θ ∈ R p and x ∈ R d ,\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.2 Gaussian mixtures satisfy the Assumptions",
        "chunkIndex": 106,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-107",
      "content": "In the case of the functions Γ defined by (9)-(10), we see that Γ is bounded under Assumption 4.2 and when the reward R is bounded. The Lipschitz continuity can be obtained as follows. Consider for instance the case of (10) where Γ( p, θ ) is given by\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the first inequality comes from the fact that the total variation distance is an integral probability metric generated by the set of bounded functions, and the second inequality is Pinsker's inequality (Tsybakov, 2009, Lemma 2.5). The first case of Section A.2 unfolds similarly.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.3 Proof of Proposition 4.4",
        "chunkIndex": 107,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-108",
      "content": "The dynamics (11) can be rewritten equivalently on P 2 ( R d ) and R p as\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere G ( p, θ ) = KL( p || π ∗ θ ), see Appendix B.1.1. The Wasserstein gradient in (18) is taken with respect to the first variable of G .\n\nEvolution of the loss. Recall that Γ satisfies by Definition 3.1 that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ). Thus we have, by (19),\n\n<!-- formula-not-decoded -->\n\nThen, by Assumption 4.3,\n\n<!-- formula-not-decoded -->\n\nUsing ab ≤ 1 2 ( a 2 + b 2 ), we get\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence of p t from π ∗ ( θ t ) . Recall that\n\n<!-- formula-not-decoded -->\n\nThus, by the chain rule formula (16),\n\n<!-- formula-not-decoded -->\n\nFrom an integration by parts, using (18) and by Assumption 4.1, we have\n\n<!-- formula-not-decoded -->\n\nMoving on to b , we have\n\n<!-- formula-not-decoded -->\n\nBy the chain rule and (19), we have for x ∈ X\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.4 Proof of Theorem 4.5",
        "chunkIndex": 108,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-109",
      "content": "g (18) and by Assumption 4.1, we have\n\n<!-- formula-not-decoded -->\n\nMoving on to b , we have\n\n<!-- formula-not-decoded -->\n\nBy the chain rule and (19), we have for x ∈ X\n\n<!-- formula-not-decoded -->\n\nUsing π ⋆ ( θ ) ∝ e -V ( θ, · ) (with similar computations as for ∇ ℓ ref in Section A.2), we have\n\n<!-- formula-not-decoded -->\n\nand\n\nThis yields\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the last step uses Assumptions 4.2 and 4.3. Putting everything together, we obtain\n\n<!-- formula-not-decoded -->\n\nUsing Gr¨ onwall's inequality (Pachpatte and Ames, 1997) to integrate the inequality, the KL divergence can be bounded by\n\n<!-- formula-not-decoded -->\n\nComing back to (20), we get\n\n<!-- formula-not-decoded -->\n\nIntegrating between 0 and T , we have\n\n<!-- formula-not-decoded -->\n\nSince ε t is decreasing, we can bound ε t by ε T in the first integral and rearrange terms to obtain\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.4 Proof of Theorem 4.5",
        "chunkIndex": 109,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-110",
      "content": "Integrating between 0 and T , we have\n\n<!-- formula-not-decoded -->\n\nSince ε t is decreasing, we can bound ε t by ε T in the first integral and rearrange terms to obtain\n\n<!-- formula-not-decoded -->\n\nRecall that, by assumption of the Theorem, ε t = min(1 , 1 √ t ). Thus Tε T = √ T , and the first term is bounded by a constant times T -1 / 2 . It is also the case of the second term since ∫ T 0 ε t e -2 µt d t is converging. Let us now estimate the magnitude of the last term. Let T 0 ≥ 2 (depending only on µ ) such that ln( T 0 ) 2 µ ≤ T 0 2 . For t ≥ T 0 , let α ( t ) := t -ln t 2 µ . We have, for t ≥ T 0 ,\n\n<!-- formula-not-decoded -->\n\nwhere in the last inequality we used that α ( t ) ≥ t/ 2 and ε t is decreasing. For t &lt; T 0 , we can simply bound the integral ∫ t 0 ε s e µ ( s -t ) ds by ε 0 T 0 . We obtain\n\n<!-- formula-not-decoded -->\n\nRecall that ε t = min(1 , 1 √ t ), and that T 0 ≥ 2. Thus\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.4 Proof of Theorem 4.5",
        "chunkIndex": 110,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-111",
      "content": "we can simply bound the integral ∫ t 0 ε s e µ ( s -t ) ds by ε 0 T 0 . We obtain\n\n<!-- formula-not-decoded -->\n\nRecall that ε t = min(1 , 1 √ t ), and that T 0 ≥ 2. Thus\n\n<!-- formula-not-decoded -->\n\nThe first two integrals are converging when T →∞ and the last integral is O (ln T ). Plugging this into (21), we finally obtain the existence of a constant c &gt; 0 such that\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.1.4 Proof of Theorem 4.5",
        "chunkIndex": 111,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-112",
      "content": "We take\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence of p k +1 from π ⋆ ( θ k +1 ) . Recall that p k is the law of X k . We leverage similar ideas to the proof of (Cheng and Bartlett, 2018, Theorem 3), exploiting the Log Sobolev inequality to bound the KL along one Langevin Monte Carlo iteration (an approach that was further streamlined in (Vempala and\n\nWibisono, 2019, Lemma 3)). The starting point is to notice that one Langevin Monte Carlo iteration can be equivalently written as a continuous-time process over a small time interval [0 , γ k ]. More precisely, let\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nand x t satisfying the SDE\n\nThen, following the proof of (Cheng and Bartlett, 2018, Theorem 3), p k +1 has the same distribution as the output at time γ := γ k of the continuity equation\n\n<!-- formula-not-decoded -->\n\nwhere ρ 0 | t is the conditional distribution of x 0 given x t . Similarly, θ k +1 is equal to the output at time γ of",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 112,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-113",
      "content": "output at time γ := γ k of the continuity equation\n\n<!-- formula-not-decoded -->\n\nwhere ρ 0 | t is the conditional distribution of x 0 given x t . Similarly, θ k +1 is equal to the output at time γ of\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have:\n\nWe first bound b similarly to the proof of Theorem 4.5, under Assumptions 4.2 and 4.3:\n\n<!-- formula-not-decoded -->\n\nThen we write a as\n\n<!-- formula-not-decoded -->\n\nDenote the Fisher divergence by\n\n<!-- formula-not-decoded -->\n\nThe first term a 1 is equal to -FD( ρ t || π ⋆ ( ϑ t )). To bound the second term a 2 , denote ρ 0 t the joint distribution of ( x 0 , x t ). Then\n\n<!-- formula-not-decoded -->\n\nUsing ⟨ a, b ⟩ ≤ ∥ a ∥ 2 + 1 4 ∥ b ∥ 2 and recalling that x ↦→∇ 1 V ( x, θ ) is L X -Lipschitz for all θ ∈ R p by Assumption 4.6,\n\n<!-- formula-not-decoded -->\n\nProceeding similarly for a 3 , we obtain\n\n<!-- formula-not-decoded -->\n\nSince θ ↦→∇ 1 V ( x, θ ) is L Θ -Lipschitz for all x ∈ R d by Assumption 4.6, we get",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 113,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-114",
      "content": "umption 4.6,\n\n<!-- formula-not-decoded -->\n\nProceeding similarly for a 3 , we obtain\n\n<!-- formula-not-decoded -->\n\nSince θ ↦→∇ 1 V ( x, θ ) is L Θ -Lipschitz for all x ∈ R d by Assumption 4.6, we get\n\n<!-- formula-not-decoded -->\n\nMoreover, by (23) and under Assumption 4.3, we have ∥ ϑ t -θ k ∥ 2 = t 2 ε 2 k ∥ Γ( µ k , θ k ) ∥ 2 ≤ t 2 ε 2 k C 2 , which yields\n\n<!-- formula-not-decoded -->\n\nPutting everything together,\n\n<!-- formula-not-decoded -->\n\nwhere the last inequality uses Assumption 4.1 and where the two last terms in the r.h.s. can be seen as additional bias terms compared to the analysis of (Cheng and Bartlett, 2018, Theorem 3) and (Vempala and Wibisono, 2019, Lemma 3). Let us now bound E ( x 0 ,x t ) ∼ ρ 0 t ∥ x t -x 0 ∥ 2 . Recall that x t d = x 0 -t ∇ 1 V ( x 0 , θ k ) + √ 2 tz 0 , where z 0 ∼ N (0 , I ) is independent of x 0 . Then\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 114,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-115",
      "content": ". Let us now bound E ( x 0 ,x t ) ∼ ρ 0 t ∥ x t -x 0 ∥ 2 . Recall that x t d = x 0 -t ∇ 1 V ( x 0 , θ k ) + √ 2 tz 0 , where z 0 ∼ N (0 , I ) is independent of x 0 . Then\n\n<!-- formula-not-decoded -->\n\nFinally, since x ↦→∇ 1 V ( x, θ ) is L X -Lipschitz for all x ∈ R d by Assumption 4.6, and under Assumption 4.1, we get by (Vempala and Wibisono, 2019, Lemma 12) that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nRecall that we want to integrate t between 0 and γ . For t ≤ γ , we have\n\n<!-- formula-not-decoded -->\n\nsince L X γ ≤ 1, L Θ γ 2 ≤ 1 and ε k ≤ 1 by (22). Denote by C 1 the second term and C 2 the sum of the last two terms. Then, by Gr¨ onwall's inequality (Pachpatte and Ames, 1997),\n\n<!-- formula-not-decoded -->\n\nSince µt ≤ µγ ≤ 1 by (22), we have e µt ≤ 1 + 2 µγ , and\n\n<!-- formula-not-decoded -->\n\nAll in all,\n\nSince γ ≤ µ 4 L 2 X by (22), we have 8 L 4 X γ 3 µ ≤ µγ 2 , and\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 115,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-116",
      "content": "-->\n\nSince µt ≤ µγ ≤ 1 by (22), we have e µt ≤ 1 + 2 µγ , and\n\n<!-- formula-not-decoded -->\n\nAll in all,\n\nSince γ ≤ µ 4 L 2 X by (22), we have 8 L 4 X γ 3 µ ≤ µγ 2 , and\n\n<!-- formula-not-decoded -->\n\nWe therefore obtain, by evaluating at t = γ and renaming p k +1 = ρ γ , π ⋆ ( θ k +1 ) = π ⋆ ( ϑ γ ), p k = ρ 0 , and γ k = γ ,\n\n<!-- formula-not-decoded -->\n\nBounding the KL over the whole dynamics. Let C 3 := (1 + µγ k 2 ) e -µγ k . We have C 3 &lt; 1, and by summing and telescoping,\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have by using e x ≥ 1 + x and C 3 ≤ 1. Thus\n\n<!-- formula-not-decoded -->\n\nReplacing C 2 by its value,\n\n<!-- formula-not-decoded -->\n\nSince e x ≥ 1 + x , C 3 ≤ e -µγ k / 2 , and\n\n<!-- formula-not-decoded -->\n\nWe obtain three terms in our bound that have different origins. The first term corresponds to an exponential decay of the KL divergence at initialization.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 116,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-117",
      "content": "µγ k / 2 , and\n\n<!-- formula-not-decoded -->\n\nWe obtain three terms in our bound that have different origins. The first term corresponds to an exponential decay of the KL divergence at initialization. The second term is linked to the discretization error, and is proportional to γ k . The third term is due to the fact that π ⋆ ( θ k ) is moving due to the outer problem updates. It is proportional to the ratio of learning rates ε k .\n\nEvolution of the loss. By Assumption 4.6, the loss ℓ is L -smooth, and recall that θ k +1 = θ k -γ k ε k Γ( p k , θ k ). We have\n\n<!-- formula-not-decoded -->\n\nby Assumption 4.3. Furthermore,\n\n<!-- formula-not-decoded -->\n\nThus\n\n<!-- formula-not-decoded -->\n\nby Assumption 4.3. Using ab ≤ 1 2 ( a 2 + b 2 ), we obtain\n\n<!-- formula-not-decoded -->\n\nConclusion. Summing and telescoping,\n\n<!-- formula-not-decoded -->\n\nLower bounding γ k by γ K and ε k by ε K in the first sum, then reorganizing terms, we obtain\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 117,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-118",
      "content": "ded -->\n\nConclusion. Summing and telescoping,\n\n<!-- formula-not-decoded -->\n\nLower bounding γ k by γ K and ε k by ε K in the first sum, then reorganizing terms, we obtain\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence by (24), we get\n\n<!-- formula-not-decoded -->\n\nBy definition (22) of γ k and ε k , we see that the first and last sums are converging, and the middle sums are O (ln K ). Therefore, we obtain\n\n<!-- formula-not-decoded -->\n\nfor some c &gt; 0 depending only on the constants of the problem.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.2 Langevin with discrete flow-proof of Theorem 4.7",
        "chunkIndex": 118,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-119",
      "content": "Our goal is first to find a continuous-time equivalent of Algorithm 3. To this aim, we first introduce a slightly different version of the algorithm where the queue of M versions of p k is not present at initialization, but constructed during the first M steps of the algorithm. As a consequence, θ changes for the first time after M steps of the algorithm, when the queue is completed and the M -th element of the queue has been processed through all the sampling steps.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "B.3 Denoising diffusion",
        "chunkIndex": 119,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-120",
      "content": "<!-- formula-not-decoded -->\n\nIn order to obtain the continuous-time equivalent of Algorithm 7, it is convenient to change indices defining p ( m ) k . Note that in the update of p ( m ) k in the algorithm, the quantity m -k is constant. Therefore, denoting j = m -k , the algorithm above is exactly equivalent to\n\n<!-- formula-not-decoded -->\n\nIt is then possible to translate this algorithm in a continuous setting in the case of denoising diffusions. We obtain\n\n<!-- formula-not-decoded -->\n\nLet us choose the score function s θ as in Section 4.3. The backward equation (7) then writes\n\n<!-- formula-not-decoded -->\n\nWe now turn our attention to the computation of Γ. Recall that Γ should satisfy Γ( π ⋆ ( θ ) , θ ) = ∇ ℓ ( θ ), where π ⋆ ( θ ) is the distribution of Y T and ℓ ( θ ) = E Y ∼ π ⋆ ( θ ) ( L ( Y )) for some loss function L : R → R . To this aim, for a given realization of the SDE (26), let us compute the derivative of L ( Y T ) with respect to θ using the adjoint method (14).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 7 Implicit Diff. optimization, finite time (no warm start)",
        "chunkIndex": 120,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-121",
      "content": "⋆ ( θ ) ( L ( Y )) for some loss function L : R → R . To this aim, for a given realization of the SDE (26), let us compute the derivative of L ( Y T ) with respect to θ using the adjoint method (14). We have ∇ 2 µ ( T -t, Y T -t , θ ) = -1, hence d A t d t = -1, and A t = L ′ ( Y T ) e -t . Furthermore, ∇ 3 µ ( T -t, Y T -t , θ ) = 2 e -t . Thus\n\n<!-- formula-not-decoded -->\n\nAs a consequence,\n\nThis prompts us to define\n\n<!-- formula-not-decoded -->\n\n2\n\nfor L defined by L ( x ) = ( x -θ target ) as in Section 4.3. Note that, in this case, Γ depends only on p and not on θ . Replacing s θ and Γ by their values in (25), we obtain the coupled differential equations in Y and θ\n\n<!-- formula-not-decoded -->\n\nWe can now formalize Proposition 4.8, with the following statement.\n\nProposition B.3. Consider the dynamics (27) . Then\n\n<!-- formula-not-decoded -->\n\nProof. Let us first compute the expectation of Y τ T . To this aim, denote Z τ t = e τ Y τ t . Then we have",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 7 Implicit Diff. optimization, finite time (no warm start)",
        "chunkIndex": 121,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-122",
      "content": "tatement.\n\nProposition B.3. Consider the dynamics (27) . Then\n\n<!-- formula-not-decoded -->\n\nProof. Let us first compute the expectation of Y τ T . To this aim, denote Z τ t = e τ Y τ t . Then we have\n\n<!-- formula-not-decoded -->\n\nSince E ( Z 0 t ) = E ( Y 0 t ) = 0, we obtain that E ( Z T t ) = 2 ∫ T 0 θ t + τ e 2( τ -T ) d τ , and\n\n<!-- formula-not-decoded -->\n\nTherefore, we obtain the following evolution equation for θ when t ≥ T :\n\n<!-- formula-not-decoded -->\n\nBy the change of variable τ ← T -τ in the integral, we have, for t ≥ T ,\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nLet us introduce the auxiliary variable ψ t = ∫ T 0 θ t -τ e -2 τ d τ . We have ˙ θ t = -η (2 ψ t -θ target ), and\n\n<!-- formula-not-decoded -->\n\nRecall that θ t is constant equal to θ 0 for t ∈ [0 , T ]. Therefore, for t ∈ [ T, 2 T ], ξ := ( θ, ψ ) satisfies the first order linear ODE with constant coefficients\n\n<!-- formula-not-decoded -->\n\nFor η &gt; 0, A is invertible, and",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 7 Implicit Diff. optimization, finite time (no warm start)",
        "chunkIndex": 122,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-123",
      "content": "to θ 0 for t ∈ [0 , T ]. Therefore, for t ∈ [ T, 2 T ], ξ := ( θ, ψ ) satisfies the first order linear ODE with constant coefficients\n\n<!-- formula-not-decoded -->\n\nFor η &gt; 0, A is invertible, and\n\n<!-- formula-not-decoded -->\n\nHence the linear ODE has solution, for t ∈ [ T, 2 T ],\n\n<!-- formula-not-decoded -->\n\nThus\n\n<!-- formula-not-decoded -->\n\nA straightforward computation shows that\n\n<!-- formula-not-decoded -->\n\nand that A has eigenvalues with negative real part. Putting everything together, we obtain\n\n<!-- formula-not-decoded -->\n\nFinally, recall that we have π ⋆ ( θ ) = N ( θ (1 -e -2 T ) , 1). Thus π ⋆ ( θ 2 T ) = N ( µ 2 T , 1) with µ 2 T = θ target + O ( e -T ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "Algorithm 7 Implicit Diff. optimization, finite time (no warm start)",
        "chunkIndex": 123,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-124",
      "content": "We provide here details about the experiments that we have presented in Section 5.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C EXPERIMENTAL DETAILS",
        "chunkIndex": 124,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-125",
      "content": "We provide in this section details about our experiments on Langevin processes. In Section C.1.1, we do so for the experiment described in Section 5.1, where the reward is an explicit function R ( · ). In Section C.1.2, we show additional experiments showing that Implicit Diffusion can also be used to 'train from scratch' a model: in this case the reward is a negative KL term that is evaluated from samples of a target distribution. We present results in several parametrization settings.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1 Langevin processes",
        "chunkIndex": 125,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-126",
      "content": "We consider a parameterized family of potentials for x ∈ R 2 and θ ∈ R 6 defined by\n\n<!-- formula-not-decoded -->\n\nwhere the µ i ∈ R 2 are the six vertices of a regular hexagon and σ is the softmax function mapping R 6 to the unit simplex. In this setting, for any θ ∈ R 6 ,\n\n<!-- formula-not-decoded -->\n\nFigure 10: Confidence intervals for metrics for reward training of Langevin processes. Left : Evolution of the reward. Right : Evolution of the log-likelihood.\n\n<!-- image -->\n\nwhere Z is an absolute renormalization constant that is independent of θ . This fact simplifies drawing contour lines, but we do not use this prior knowledge in our algorithms, and only use calls to functions ∇ 1 V ( · , θ ) and ∇ 2 V ( · , θ ) for various θ ∈ R 6 .\n\nWe run six sampling algorithms, all initialized with p 0 = N (0 , I 2 ). For all of them we generate a batch of variables X ( i ) of size 1 , 000, all initialized independently with X ( i ) 0 ∼ N (0 , I 2 ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 126,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-127",
      "content": "six sampling algorithms, all initialized with p 0 = N (0 , I 2 ). For all of them we generate a batch of variables X ( i ) of size 1 , 000, all initialized independently with X ( i ) 0 ∼ N (0 , I 2 ). The sampling and optimization steps are realized in parallel over the batch. The samples are represented after K = 5 , 000 steps of each algorithm in Figure 5, and used to compute the values of reward and likelihood reported in Figure 6. We also display in Figure 12 the dynamics of the probabilities throughout these algorithms.\n\nFigure 11: Comparison between Implicit Diffusion, nested loop algorithm and unrolling algorithm, for various number of inner steps T . The x-axis is the total number of gradient evaluations (roughly equal to the number of optimization steps multiplied by the number of inner loop steps T ). Left : Evolution of the reward. Right : Evolution of the log-likelihood.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 127,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-128",
      "content": "valuations (roughly equal to the number of optimization steps multiplied by the number of inner loop steps T ). Left : Evolution of the reward. Right : Evolution of the log-likelihood.\n\n<!-- image -->\n\nWe provide here additional details of and motivation for these algorithms, denoted by the colored markers that represent them in these figures.\n\n- Langevin θ 0 ( ■ ): This is the discrete-time process (a Langevin Monte Carlo process) approximating a Langevin diffusion with potential V ( · , θ 0 ) for fixed θ 0 := (1 , 0 , 1 , 0 , 1 , 0). There is no reward here; the time-continuous Langevin process converges to π ⋆ ( θ 0 ), which has some symmetries. It can be thought of as a pretrained model, and the Langevin sampling algorithm as an inference-time generative algorithm.\n- -Implicit Diffusion ( ⋆ ): We run the infinite-time horizon version of our method (Algorithm 2), aiming to minimize ℓ ( θ ) := F ( π ⋆ ( θ )) for F ( p ) = -E X ∼ p [ R ( X )] with R ( x ) = 1 ( x 1 &gt; 0) exp ( -∥ x",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 128,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-129",
      "content": "n ( ⋆ ): We run the infinite-time horizon version of our method (Algorithm 2), aiming to minimize ℓ ( θ ) := F ( π ⋆ ( θ )) for F ( p ) = -E X ∼ p [ R ( X )] with R ( x ) = 1 ( x 1 &gt; 0) exp ( -∥ x -µ ∥ 2 ) where µ = (1 , 0 . 95). This algorithm yields both a sample ˆ p K and parameters θ opt after K steps, and can be thought of as jointly sampling and reward finetuning.\n- -Nested loop ( ♦ ): We run Algorithm 1 with T inner sampling steps for each gradient step. For T = 1, this is exactly Implicit Diffusion. For T ≫ 1, it means we compute nearly perfectly π ⋆ ( θ t ) at each optimization step.\n- -Unrolling through the last step of sampling ( ▲ ): For each optimization step, we perform T sampling steps, then differentiate through the last step of sampling by automatic differentiation. This is akin to a stop gradient method. The learning rate here is chosen as 2 γ θ /γ X to improve its performance, for a fair comparison.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 129,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-130",
      "content": "hrough the last step of sampling by automatic differentiation. This is akin to a stop gradient method. The learning rate here is chosen as 2 γ θ /γ X to improve its performance, for a fair comparison. Recent studies show that differentiating through the last sampling step is an efficient and theoretically-grounded method in bilevel optimization (Bolte et al., 2023). It has been applied successfully to denoising diffusions (Clark et al., 2024).\n- Langevin θ 0 + R ( ▼ ): This is the discretization of a Langevin diffusion with reward-guided potential V ( · , θ 0 ) -λR smooth , where R smooth is a smoothed version of R (replacing the indicator by a sigmoid). Using this approach is different from finetuning: it proposes to modify the sampling algorithm, and does not yield new parameters θ . This is akin to guidance of generative models (Dhariwal and Nichol, 2021).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 130,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-131",
      "content": "is approach is different from finetuning: it proposes to modify the sampling algorithm, and does not yield new parameters θ . This is akin to guidance of generative models (Dhariwal and Nichol, 2021). Note that this requires a differentiable reward R smooth , contrarily to our approach that handles non-differentiable rewards.\n- Langevin θ opt - post Implicit Diffusion ( ): This is a discrete-time process approximating a Langevin diffusion with potential V ( · , θ opt ), where θ opt is the outcome of reward training by our algorithm. This can be thought of as doing inference with the new model parameters, post reward training with Implicit Diffusion.\n\nAs mentioned in Section 5.1, this setting illustrates the advantage of our method, which allows the efficient optimization of a function over a constrained set of distribution, without overfitting outside this class.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 131,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-132",
      "content": "Section 5.1, this setting illustrates the advantage of our method, which allows the efficient optimization of a function over a constrained set of distribution, without overfitting outside this class. We display in Figure 12 snapshots throughout some selected steps of these six algorithms (in the same order and with the same colors as indicated above). We observe that the dynamics of Implicit Diffusion are slower than those of Langevin processes (sampling), which can be observed also in the metrics reported in Figure 6. The reward and log-likelihood change slowly, plateauing several times: when θ k in this algorithm is initially close to θ 0 , the distribution gets closer to π ⋆ ( θ 0 ) (steps 0-100). It then evolves towards another distribution (steps 1000-2500), after θ has been affected by accurate gradient updates, before converging to π ⋆ ( θ opt ).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 132,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-133",
      "content": "on gets closer to π ⋆ ( θ 0 ) (steps 0-100). It then evolves towards another distribution (steps 1000-2500), after θ has been affected by accurate gradient updates, before converging to π ⋆ ( θ opt ). The two-timescale dynamics is by design: the sampling dynamics are much faster, aiming to quickly lead to an accurate evaluation of gradients with respect to θ k . This corresponds to our theoretical setting where ε k ≪ 1. To complement the comparisons between our algorithm and other baselines included in Section 5.1, we also provide in Figure 11 a comparison between Implicit Diffusion, the nested loop and unrolling approaches, in terms of reward and log-likelihood optimization, as a function of the number of gradient evaluations (i.e., number of sampling steps), rather than number of optimization steps. Again, it is apparent that the algorithmic cost of doing several steps ( T &gt; 1) of inner loop is much higher than the small improvement obtained by a better estimate of the gradients.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 133,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-134",
      "content": "zation steps. Again, it is apparent that the algorithmic cost of doing several steps ( T &gt; 1) of inner loop is much higher than the small improvement obtained by a better estimate of the gradients. Finally, the confidence intervals in Figure 6 are computed by performing 10 independent repetitions of the experiment, and reporting the largest and lowest metrics across the 10 repetitions, at each time step. For readability, Figure 10 shows the same plot with confidence intervals only (without plotting the average value).\n\nFigure 12: Dynamics of samples for four sampling algorithms after different time steps (for instance, the first column is after one step). First row: Langevin θ 0 ( ) with π ⋆ ( θ 0 ) contour lines. Second: Implicit Diffusion ( ) with π ⋆ ( θ opt ) contour lines. Third: Nested loop algorithm with T = 100 ( ). Fourth: Unrolling through the last step of sampling with T = 100 ( ). Fifth: Langevin θ 0 + smoothed Reward ( ). Sixth: Langevin θ opt ( ).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 134,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-135",
      "content": "ested loop algorithm with T = 100 ( ). Fourth: Unrolling through the last step of sampling with T = 100 ( ). Fifth: Langevin θ 0 + smoothed Reward ( ). Sixth: Langevin θ opt ( ).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.1 Reward training",
        "chunkIndex": 135,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-136",
      "content": "We present in Figure 13 a variant of this experiment where we start from a model generating a standard Gaussian, and our goal is to learn to generate a mixture of several Gaussians. For this, comparing with the setup presented above, we add a 7th potential well at the origin, and choose at initialization θ 0 = ( -7 , -7 , -7 , -7 , -7 , -7 , 11). This means that the distribution at initialization is extremely close to being a standard Gaussian, as can be seen in the top-right plot of Figure 13a. The target is θ ∗ = (1 . 5 , 0 , 1 . 5 , 0 , 1 . 5 , 0 , 0). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. This KL admits explicit gradients (see Section 3.2) which can be evaluated with samples of the target distribution. We train for K = 40 , 000 steps with a batch of size 1 , 000. Learning rates are γ X = 2 . 5 · 10 -2 and γ θ = 7 · 10 -3 .",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 136,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-137",
      "content": "Section 3.2) which can be evaluated with samples of the target distribution. We train for K = 40 , 000 steps with a batch of size 1 , 000. Learning rates are γ X = 2 . 5 · 10 -2 and γ θ = 7 · 10 -3 .\n\nTraining from scratch with a full Gaussian mixture model parameterization. We present in Figure 14 a variant of this experiment where we now parameterize the means and covariances of the Gaussians in addition to the weights of the mixture. More precisely, we consider the potential\n\n<!-- formula-not-decoded -->\n\nwhere σ is still the softmax function mapping R 2 to the unit simplex, and θ = { w, ( µ i , Σ i ) 1 ≤ i ≤ 2 } ∈ R 2 × ( R 2 , R 2 × 2 ) 2 . At initialization, the parameters are\n\n<!-- formula-not-decoded -->\n\nwhile our target is\n\n<!-- formula-not-decoded -->\n\nAs in the previous experiment, we use Implicit Diffusion where the reward is the KL between the target distribution and the current one.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 137,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-138",
      "content": "decoded -->\n\nwhile our target is\n\n<!-- formula-not-decoded -->\n\nAs in the previous experiment, we use Implicit Diffusion where the reward is the KL between the target distribution and the current one. This KL admits explicit gradients (see Section 3.2) which can be evaluated with samples of the target distribution. We train for K = 40 , 000 steps with a batch of size 1 , 000. Learning rates are γ X = 5 · 10 -2 and γ θ = 5 · 10 -4 . We observe on the Figure that Implicit Diffusion is able to learn the target distribution.\n\n(a) Contour lines and samples from sampling algorithms. We start from a model generating a standard Gaussian (top-right figure), and our goal is to learn to generate a mixture of several Gaussians (top-left figure). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. We observe that Implicit Diffusion is able to learn the target distribution (bottom-left figure).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 138,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-139",
      "content": "use Implicit Diffusion where the reward is the KL between the target distribution and the current one. We observe that Implicit Diffusion is able to learn the target distribution (bottom-left figure). After running Implicit Diffusion, it is easy to generate new samples that are close to the target distribution (bottom-right figure).\n\n<!-- image -->\n\n(b) Evolution of the reward and of the log-likelihood of the samples for the initial model, for the Implicit Diffusion algorithm, and for the trained model after Implicit Diffusion. The reward is the (opposite of the) KL between the target distribution and the current one, so a reward equal to zero means we learnt to reproduce the target distribution.\n\n<!-- image -->\n\nFigure 13: Optimizing through sampling with Implicit Diffusion to train from scratch an energy-based model (Langevin diffusion).",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 139,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-140",
      "content": "means we learnt to reproduce the target distribution.\n\n<!-- image -->\n\nFigure 13: Optimizing through sampling with Implicit Diffusion to train from scratch an energy-based model (Langevin diffusion).\n\n(a) Contour lines and samples from sampling algorithms. We start from a model generating a mixture of two Gaussians (top-right figure), and our goal is to learn to generate another mixture with different means, covariances and weights (top-left figure). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. We observe that Implicit Diffusion is able to learn the target distribution (bottom-left figure). After running Implicit Diffusion, it is easy to generate new samples that are close to the target distribution (bottom-right figure).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 140,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-141",
      "content": "earn the target distribution (bottom-left figure). After running Implicit Diffusion, it is easy to generate new samples that are close to the target distribution (bottom-right figure).\n\n<!-- image -->\n\n(b) Evolution of the reward and of the log-likelihood of the samples for the initial model, for the Implicit Diffusion algorithm, and for the trained model after Implicit Diffusion. The reward is the (opposite of the) KL between the target distribution and the current one, so a reward equal to zero means we learnt to reproduce the target distribution.\n\n<!-- image -->\n\nFigure 14: Optimizing through sampling with Implicit Diffusion to train from scratch an energy-based model (Langevin diffusion). The potential V is a logsumexp of 2 quadratic forms, where the weights, centers, and the matrix of the forms are learnable parameters, so that the outcome distribution can be any mixture of 2 Gaussians.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 141,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-142",
      "content": "the forms are learnable parameters, so that the outcome distribution can be any mixture of 2 Gaussians.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.1.2 Training from scratch",
        "chunkIndex": 142,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-143",
      "content": "We first provide additional experimental configurations that are common between both datasets before explaining details specific to each one.\n\nCommon details. The KL term in the reward is computed using Girsanov's theorem as explained in Appendix A.3. We use the Adam optimizer (Kingma and Ba, 2015), with various values for the learning rate (see e.g. Figure 7). The code was implemented in JAX (Bradbury et al., 2018). As mentioned in the main text, we use a U-Net model (Ronneberger et al., 2015).\n\nMNIST. We use an Ornstein-Uhlenbeck noise schedule, meaning that the forward diffusion is d X t = -X t d t + √ 2d B t (as presented in Section 2.2). We pretrain for 18k steps in 7 minutes on 4 TPUv2. For reward training, we train on a TPUv2 for 4 hours with a queue of size M = 4, T = 64 steps, and a batch size of 32. Further hyperparameters for pretraining and reward training are given respectively in Tables 1 and 2.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 143,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-144",
      "content": "we train on a TPUv2 for 4 hours with a queue of size M = 4, T = 64 steps, and a batch size of 32. Further hyperparameters for pretraining and reward training are given respectively in Tables 1 and 2.\n\nTable 1: Hyperparameters for pretraining of denoising diffusion models on MNIST.\n\n| Name                                                        | Value                                                                   |\n|-------------------------------------------------------------|-------------------------------------------------------------------------|\n| Noise schedule Optimizer EMA decay Learning rate Batch size | Ornstein-Uhlenbeck Adam with standard hyperparameters 0 . 995 10 - 3 32 |\n\nTable 2: Hyperparameters for reward training of denoising diffusion models pretrained on MNIST.\n\n| Name                                                      | Value                                                           |\n|-----------------------------------------------------------|---------------",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 144,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-145",
      "content": "me                                                      | Value                                                           |\n|-----------------------------------------------------------|-----------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 256 Euler Ornstein-Uhlenbeck Adam with standard hyperparameters |\n\nCIFAR-10 and LSUN. For CIFAR-10, we pretrain for 500k steps in 30 hours on 16 TPUv2, reaching an FID score (Heusel et al., 2017) of 2 . 5. For reward training, we train on a TPUv3 for 9 hours with a queue of size M = 4 and T = 64 steps, and a batch size of 32. For LSUN, we pretrain for 13 hours, reaching a FID score of 2 . 26. For reward training, we train on a TPUv3 for 9 hours with a queue of size M = 2, T = 64 steps, and a batch size of 16. Further hyperparameters for pretraining and reward training are given respectively in Tables 3 and 4.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 145,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-146",
      "content": "we train on a TPUv3 for 9 hours with a queue of size M = 2, T = 64 steps, and a batch size of 16. Further hyperparameters for pretraining and reward training are given respectively in Tables 3 and 4.\n\nTable 3: Hyperparameters for pretraining of denoising diffusion models on CIFAR-10 and LSUN.\n\n| Name                                                                                                                              | Value                                                                                                                         |\n|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer EMA decay Learning rate Batch size Number of samples for FID evaluation | 1 , 024 DDPM Cosine (Nichol and Dhariwal, 2021",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 146,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-147",
      "content": "------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer EMA decay Learning rate Batch size Number of samples for FID evaluation | 1 , 024 DDPM Cosine (Nichol and Dhariwal, 2021) Adam with β 1 = 0 . 9, β 2 = 0 . 99, ε = 10 - 12 0 . 9999 2 · 10 - 4 2048 50k |\n\nTable 4: Hyperparameters for reward training of denoising diffusion models pretrained on CIFAR-10 and LSUN.\n\n| Name                                                      | Value                                                                               |\n|-----------------------------------------------------------|-------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 1 , 024 Euler Cosine (Nichol and Dhariwal, 2021) Adam with standard hyperparameters |",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 147,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-148",
      "content": "---------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 1 , 024 Euler Cosine (Nichol and Dhariwal, 2021) Adam with standard hyperparameters |\n\nAdditional figures for MNIST. We report in Figure 16 metrics on the rewards and KL divergence with respect to the original distribution, in the case where λ &gt; 0. As in Figure 7, we observe the competition between the reward and the divergence with respect to the distribution after pretraining. The results are sensitive to the choice of hyperparameters. In particular, when the ratio λ/β is too small, we do not observe an improvement of the reward. Note that we did not perform extensive hyperparameter finetuning for these plots, so it is likely that better results could be obtained with more hyperparameter finetuning.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 148,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-149",
      "content": "improvement of the reward. Note that we did not perform extensive hyperparameter finetuning for these plots, so it is likely that better results could be obtained with more hyperparameter finetuning. We also display in Figures 15 some additional selected examples of samples generated by our denoising diffusion model with parameters θ k , at several steps k of our algorithm. Note that the random number generator system of JAX allows us, for illustration purposes, to sample for different parameters from the same seed. We take advantage of this feature to visualize the evolution of a given realization of the stochastic denoising process depending on θ .\n\nRecall that we consider\n\n<!-- formula-not-decoded -->\n\nwhere R ( x ) is the average value of all the pixels in x . The figures in the main text and in the appendix present samples for negative and positive λ , rewarding respectively darker and brighter images. We emphasize that these samples are generated for evaluation purposes.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 149,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-150",
      "content": "he main text and in the appendix present samples for negative and positive λ , rewarding respectively darker and brighter images. We emphasize that these samples are generated for evaluation purposes. To generate the samples, at various steps of the optimization procedure, we run the full denoising process for the current value of the parameters. In particular, these samples are different from the ones used to perform the joint sampling and parameter updates in Implicit Diffusion.\n\nWe have purposefully chosen, for illustration purposes, samples for experiments with the highest magnitude of λ/β , i.e. those that favor reward optimization over proximity to the original distribution. As noted in Section 5, we observe qualitatively that reward training, while shifting some aspects of the distribution (here the average brightness), and necessarily diverging from the original pretrained model, manages to do so while retaining some important global characteristics of the dataset-even though t",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 150,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-151",
      "content": "ibution (here the average brightness), and necessarily diverging from the original pretrained model, manages to do so while retaining some important global characteristics of the dataset-even though the pretraining dataset is never observed during reward training. Since we chose to display samples from experiments with the most extreme incentives towards the reward, we observe that the similarity with the pretraining dataset can be forced to break down after a certain number of reward training steps. We also observe some mode collapse; we comment further on this point below.\n\nFigure 15: Reward training for a model pretrained on MNIST. The reward favors brighter images ( λ &gt; 0, β &gt; 0). Selected examples are shown coming from a single experiment with λ/β = 30. All digits are re-sampled at the same selected steps of the Implicit Diffusion algorithm.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 151,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-152",
      "content": "&gt; 0, β &gt; 0). Selected examples are shown coming from a single experiment with λ/β = 30. All digits are re-sampled at the same selected steps of the Implicit Diffusion algorithm.\n\n<!-- image -->\n\nFigure 16: Score function reward training with Implicit Diffusion pretrained on MNIST for various λ &gt; 0 (brighter). Left: Reward, average brightness of image. Right: Divergence w.r.t. the original pretrained distribution.\n\n<!-- image -->\n\nAdditional figures for CIFAR-10. We recall that we consider, for a model with weights θ 0 pretrained on CIFAR-10, the objective function\n\n<!-- formula-not-decoded -->\n\nwhere R ( x ) is the average over the red channel, minus the average of the other channels. We show in Figure 17 the result of the denoising process for some fixed samples and various steps of the reward training, for the experiment with the most extreme incentive towards the reward.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 152,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-153",
      "content": "annels. We show in Figure 17 the result of the denoising process for some fixed samples and various steps of the reward training, for the experiment with the most extreme incentive towards the reward.\n\nWe observe as for MNIST some mode collapse, although less pronounced here. Since the pretrained model has been trained with label conditioning for CIFAR-10, it is possible that this phenomenon could be a byproduct of this pretraining feature.\n\nFigure 17: Reward training for a model pretrained on CIFAR-10. The reward favors redder images ( λ &gt; 0, β &gt; 0). Selected examples are shown coming from a single experiment with λ/β = 1 , 000. All images are re-sampled at the same selected steps of the Implicit Diffusion algorithm, as explained in Appendix C.2.\n\n<!-- image -->\n\nAdditional figures for LSUN. As for other datasets, we report in Figure 18 metrics on the rewards and KL divergence with respect to the original distribution.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 153,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-154",
      "content": "plained in Appendix C.2.\n\n<!-- image -->\n\nAdditional figures for LSUN. As for other datasets, we report in Figure 18 metrics on the rewards and KL divergence with respect to the original distribution.\n\nExtensions. We emphasize that our methodology covers a wide range of sampling processes and rewards. For instance, it could be applied to diffusions in discrete spaces for language modeling, or for more complex rewards such as aesthetic scores.\n\nFigure 18: Score function reward training with Implicit Diffusion pretrained on LSUN for various λ &gt; 0 (brighter). Left: Reward, average brightness of image. Right: Divergence w.r.t. the original pretrained distribution.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "C.2 Denoising diffusion models",
        "chunkIndex": 154,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-155",
      "content": "Reward finetuning of denoising diffusion models. A large body of work has recently tackled the task of finetuning denoising diffusion models, with various point of views. Wu et al. (2023b) update weight parameters in a supervised fashion by building a high-reward dataset, then using score matching. Other papers use reinforcement learning approaches to finetune the parameters of the model (Dvijotham et al., 2023; Fan et al., 2023; Black et al., 2024). Closer to our approach are works that propose finetuning of denoising diffusion models by backpropagating through sampling (Watson et al., 2022; Dong et al., 2023; Wallace et al., 2023; Clark et al., 2024). However, they sample only once (Lee et al., 2023), or use a nested loop approach (described in Section 3.1) and resort to implementation techniques such as gradient checkpointing or gradient rematerialization to limit the memory burden. We instead depart from this point of view and propose a single-loop approach.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 155,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-156",
      "content": "sort to implementation techniques such as gradient checkpointing or gradient rematerialization to limit the memory burden. We instead depart from this point of view and propose a single-loop approach. Furthermore, our approach is much more general than denoising diffusion models and includes any iterative sampling algorithm such as Langevin sampling.\n\nWe emphasize that the finetuning approach differs from guidance of diffusion models (see, e.g., Dhariwal and Nichol, 2021; Graikos et al., 2022; Hertz et al., 2023; Kwon et al., 2023; Wu et al., 2023a; Zhang et al., 2023). In the latter case, the sampling scheme is modified to bias sampling towards maximizing the reward. On the contrary, finetuning directly modifies the weights of the model without changing the sampling scheme. Both approaches are complementary, and it can happen that in practice people prefer to modify the weights of the model rather than the sampling scheme: e.g., to distribute weights that take into account the reward",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 156,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-157",
      "content": "s are complementary, and it can happen that in practice people prefer to modify the weights of the model rather than the sampling scheme: e.g., to distribute weights that take into account the reward and that can be used with any standard sampling scheme, without asking downstream users to modify their sampling method or requiring them to share the reward mechanism.\n\nSingle-loop approaches for bilevel optimization. Our single-loop approach for differentiating through sampling processes is inspired by recently-proposed single-loop approaches for bilevel optimization problems (Guo et al., 2021; Yang et al., 2021; Chen et al., 2022; Dagr´ eou et al., 2022; Hong et al., 2023). Closest to our setting is Dagr´ eou et al. (2022), where strong convexity assumptions are made on the inner problem while gradients for the outer problem are assumed to be Lipschitz and bounded. They also show convergence of the average of the objective gradients, akin to our Theorem 4.7.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 157,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-158",
      "content": "made on the inner problem while gradients for the outer problem are assumed to be Lipschitz and bounded. They also show convergence of the average of the objective gradients, akin to our Theorem 4.7. However, contrarily to their analysis, we study the case where the inner problem is a sampling problem (or infinite-dimensional optimization problem). Our methodology also extends to the non-stationary case, e.g. encompassing denoising diffusion models.\n\nStudy of optimization through Langevin dynamics in the linear case. In the case where the operator Γ can be written as an expectation w.r.t. p t then the dynamics of θ in (11) can be seen as a McKean-Vlasov process. Kuntz et al. (2023) and Sharrock et al. (2024) propose efficient algorithms to approximate this process\n\nusing the convergence of interacting particle systems to McKean-Vlasov process when the number of particles is large. In the same setting, where Γ can be written as an expectation w.r.t.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 158,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-159",
      "content": "this process\n\nusing the convergence of interacting particle systems to McKean-Vlasov process when the number of particles is large. In the same setting, where Γ can be written as an expectation w.r.t. p t , discretization of such dynamics have been extensively studied (Atchad´ e et al., 2017; De Bortoli et al., 2021; Xiao and Zhang, 2014; Rosasco et al., 2020; Nitanda, 2014; Tadi´ c and Doucet, 2017). In that setting, one can leverage convergence results of the Langevin algorithm under mild assumption such as Eberle (2016) to prove the convergence of a sequence ( θ k ) k ∈ N to a local minimizer such that ∇ ℓ ( θ ⋆ ) = 0, see (De Bortoli et al., 2021, Appendix B) for instance. Finally, Wang and Sirignano (2024) and Wang and Sirignano (2022) propose and analyze a single-loop algorithm to differentiate through solutions of SDEs. Their algorithm uses forward-mode differentiation, which does not scale well to large-scale machine learning problems.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 159,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-160",
      "content": "fferentiate through solutions of SDEs. Their algorithm uses forward-mode differentiation, which does not scale well to large-scale machine learning problems.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "D ADDITIONAL RELATED WORK",
        "chunkIndex": 160,
        "totalChunks": 162
      }
    },
    {
      "id": "2402.05468v3-chunk-161",
      "content": "PM worked on designing the methodology, implemented the codebase for experiments, proved theoretical guarantees for proposed method, contributed importantly to writing the paper. AK contributed to designing the methodology, worked on proving theoretical guarantees, made some contributions to the paper. PB, MB, VDB, AD, FL, CP (by alphabetical order) contributed to discussions in designing the methodology, provided references, made remarks and suggestions on the manuscript and provided some help with the codebase implementation. QB proposed the initial idea, proposed the general methodology and worked on designing it, contributed to the codebase implementation, ran experiments, and contributed importantly to writing the paper.",
      "metadata": {
        "source": "arxiv:2402.05468v3",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
          "Pierre Marion",
          "Anna Korba",
          "Peter Bartlett",
          "Mathieu Blondel",
          "Valentin De Bortoli",
          "Arnaud Doucet",
          "Felipe Llinares-López",
          "Courtney Paquette",
          "Quentin Berthet"
        ],
        "section": "AUTHOR CONTRIBUTION STATEMENT",
        "chunkIndex": 161,
        "totalChunks": 162
      }
    }
  ],
  "fullText": "## Implicit Diffusion: Efficient optimization through stochastic sampling\n\nPierre Marion 12\n\nInstitute of Mathematics, EPFL Lausanne, Switzerland\n\nAnna Korba ENSAE CREST IP Paris, France\n\n## Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-Lopez , Courtney Paquette, Quentin Berthet 1\n\nGoogle DeepMind\n\n## Abstract\n\nSampling and automatic differentiation are both ubiquitous in modern machine learning. At its intersection, differentiating through a sampling operation, with respect to the parameters of the sampling process, is a problem that is both challenging and broadly applicable. We introduce a general framework and a new algorithm for first-order optimization of parameterized stochastic diffusions, performing jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical and experimental results showcasing the performance of our method.\n\n## 1 INTRODUCTION\n\nSampling from a target distribution is a ubiquitous task at the heart of various methods in machine learning, optimization, and statistics. Increasingly, sampling algorithms rely on iteratively applying large-scale parameterized functions (e.g. neural networks), as in denoising diffusion models (Ho et al., 2020).\n\n1 Corresponding authors. Address correspondance at pierre.marion@epfl.ch and qberthet@google.com .\n\n2 Work mostly done while a student researcher at Google DeepMind.\n\nProceedings of the 28 th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume 258. Copyright 2025 by the author(s).\n\nFigure 1: Optimizing through sampling with Implicit Diffusion to finetune denoising diffusion models. Reward is brightness for MNIST and red for CIFAR-10.\n\n<!-- image -->\n\nThis iterative sampling operation implicitly maps a parameter θ ∈ R p to some distribution π ⋆ ( θ ) in P .\n\nIn this work, we focus on optimization problems over these implicitly parameterized distributions. For a space of distributions P (e.g. over R d ), and a function F : P → R , our main problem is\n\n<!-- formula-not-decoded -->\n\nThis setting encompasses for instance learning parameterized Langevin diffusions, contrastive learning of energy-based models (Gutmann and Hyv¨ arinen, 2012) or finetuning denoising diffusion models (e.g., Dvijotham et al., 2023; Clark et al., 2024), as illustrated by Figure 1. Applying first-order optimizers to this problem raises the challenge of computing gradients of functions of the target distribution with respect to the parameter: we have to differentiate through a sampling operation , where the link between θ and π ⋆ ( θ ) can be implicit (see, e.g., Figure 2).\n\nTo this aim, we propose to exploit the perspective of sampling as optimization , where the task of sampling is seen as an optimization problem over the space of probability distributions P (see Korba and Salim, 2022, and references therein). Typically, approximating a target probability distribution π can be cast as the minimization of a dissimilarity functional between probability distributions w.r.t. π , that only vanishes at the target. For instance, Langevin diffusion dynamics follow a gradient flow of a Kullback-Leibler (KL) objective w.r.t. the Wasserstein-2 distance (Jordan et al., 1998).\n\nThis allows us to draw a link between optimization through stochastic sampling and bilevel optimization , which often involves computing derivatives of the solution of a parameterized optimization problem obtained after iterative steps of an algorithm. Bilevel optimization is an active area of research with many relevant applications in machine learning, such as hyperparameter optimization (Franceschi et al., 2018) or meta-learning (Liu et al., 2019). In particular, there is a significant effort in the literature for developing tractable and provably efficient algorithms in a large-scale setting (Pedregosa, 2016; Chen et al., 2021b; Arbel and Mairal, 2022; Blondel et al., 2022; Dagr´ eou et al., 2022)-see Appendix D for additional related work. This literature focuses mostly on problems with finite-dimensional variables, in contrast with our work where the solution of the inner problem is a distribution in P .\n\nThese motivating similarities, while useful, are not limiting. We also consider settings where the sampling iterations are not readily interpretable as an optimization algorithm. Denoising diffusion cannot directly be formalized as descent dynamics of an objective functional over P , but its output is determined by a parameter θ (i.e. weights of the score matching neural networks).\n\nMain Contributions. In this work, we introduce the algorithm of Implicit Diffusion , an effective and principled technique for optimizing through a sampling operation. More precisely,\n\n- We present a general framework describing parameterized sampling algorithms, and introduce Implicit Diffusion optimization, a single-loop optimization algorithm to optimize through sampling.\n- We provide theoretical guarantees in the continuous and discrete time settings in Section 4.\n- We showcase in Section 5 its performance and efficiency in experimental settings . Applications include finetuning denoising diffusions and training energy-based models.\n\nTo allow for reproducibility, we provide an implementa- tion 3 of our algorithm in JAX (Bradbury et al., 2018).\n\nNotation. For a set X (such as R d ), we write P for the set of probability distributions on X , omitting reference to X . For f a differentiable function on R d , we denote by ∇ f its gradient function. If f is a differentiable function of k variables, we let ∇ i f denote its gradient w.r.t. its i -th variable.\n\n## 2 PROBLEM PRESENTATION\n\n## 2.1 Sampling and optimization perspectives\n\nThe core operation that we consider is sampling by running a stochastic diffusion process that depends on a parameter θ ∈ R p . We consider iterative sampling operators that map from a parameter space to a space of probabilities . We denote by π ⋆ ( θ ) ∈ P the outcome distribution of this sampling operator. This parameterized distribution is defined in an implicit manner: there is not always an explicit way to write down its dependency on θ . More formally, it is defined as follows.\n\nDefinition 2.1 (Iterative sampling operators) . For a parameter θ ∈ R p , a sequence of parameterized functions Σ s ( · , θ ) from P to P defines a diffusion sampling process , from p 0 ∈ P iterating\n\n<!-- formula-not-decoded -->\n\nThe outcome π ⋆ ( θ ) ∈ P , when s → ∞ , or for some s = T defines a sampling operator π ⋆ : R p →P .\n\nWe embrace the formalism of stochastic processes as acting on probability distributions. This perspective focuses on the dynamics of the distribution ( p s ) s ≥ 0 , and allows us to more clearly present our optimization problem and algorithms. In practice, however, in all the examples that we consider, this is realized by an iterative process on some random variable X s such that X s ∼ p s .\n\nExample 2.2 . Consider the process defined by X s +1 = X s -2 δ ( X s -θ ) + √ 2 δB s , where the B s are i.i.d. standard Gaussian, X 0 ∼ p 0 := N ( µ 0 , σ 2 0 ), and δ ∈ (0 , 1). This is the discrete-time version of Langevin dynamics for V ( x, θ ) = 0 . 5( x -θ ) 2 (see Section 2.2). The dynamics induced on probabilities p s = N ( µ s , σ 2 s ) are µ s = θ +(1 -2 δ ) s ( µ 0 -θ ) and σ 2 s = 1+(1 -2 δ ) 2 s ( σ 2 0 -1). The sampling operator for s →∞ is therefore defined by π ⋆ : θ →N ( θ, 1).\n\nMore generally, we may consider the iterates X s of the process defined for noise variables ( B s ) s ≥ 0\n\n<!-- formula-not-decoded -->\n\n3 https://github.com/google-deepmind/implicit\\_ diffusion\n\nApplying f s ( · , θ ) to X s ∼ p s implicitly defines a dynamic Σ s ( · , θ ) on the distribution. The dynamics on the variables in (2) induce dynamics on the distributions described in (1). Note that, in the special case of normalizing flows (Kobyzev et al., 2019; Papamakarios et al., 2021), explicit formulas for p s can be derived and evaluated.\n\nRemark 2.3 . i) We consider settings with discrete time steps, fitting our focus on algorithms to sample and optimize through sampling. This encompasses in particular the discretization of many continuous-time stochastic processes of interest. Most of our motivations are of this latter type, and we describe these distinctions in our examples (see Section 2.2).\n\nii) As noted above, these dynamics are often realized by an iterative process on variables X s , or even on an i.i.d. batch of samples ( X 1 s , . . . , X n s ). When the iterates Σ s ( p s , θ ) are written in our presentation (e.g. in optimization algorithms in Section 3), it is often a shorthand to mean that we have access to samples from p s , or equivalently to an empirical version ˆ p s, ( n ) of the population distribution p s . Sample versions of our algorithms are described in Appendix A.\n\niii) One of the special cases considered in our analysis are stationary processes with infinite time horizon, where the sampling operation can be interpreted as optimizing over the set of distributions\n\n<!-- formula-not-decoded -->\n\nIn this case, the iterative operations in (1) can often be directly interpreted as descent steps for the objective G ( · , θ ). However, our methodology is not limited to this setting: we also consider general sampling schemes with no stationarity and no inner G , but only a sampling process defined by Σ s .\n\nOptimization objective. We aim to optimize with respect to θ the output of the sampling operator, for a function F : P → R . In other words, we consider the optimization problem\n\n<!-- formula-not-decoded -->\n\nThis formulation transforms a problem over distributions in P to a finite-dimensional problem over θ ∈ R p . Further, this allows for convenient post-optimization sampling: for some θ opt ∈ R p obtained by solving (4), one can sample from π ⋆ ( θ opt ). This is the common paradigm in model finetuning.\n\n## 2.2 Examples\n\nLangevin dynamics. They are defined by the stochastic differential equation (SDE) (Roberts and\n\nTweedie, 1996)\n\n<!-- formula-not-decoded -->\n\nwhere V and θ ∈ R p are such that this SDE has a solution for t &gt; 0 that converges in distribution. Here π ⋆ ( θ ) is the limiting distribution of X t when t →∞ , which is the Gibbs distribution\n\n<!-- formula-not-decoded -->\n\nwhere Z θ = ∫ exp( -V ( x, θ ))d x is the normalization factor. To fit our setting of iterative sampling algorithms (2), one can consider the discretization for γ &gt; 0\n\n<!-- formula-not-decoded -->\n\nFor G ( p, θ ) = KL( p || π ⋆ ( θ )), the outcome of the sampling operator π ⋆ ( θ ) is a minimum of G ( · , θ ), and the SDE (5) implements a gradient flow for G in the space of measures, with respect to the Wasserstein-2 distance (Jordan et al., 1998). Two optimization objectives F are of particular interest in this case. First, we may want to maximize some reward R : R d → R over our samples, in which case the objective writes F ( p ) := -E x ∼ p [ R ( x )]. Second, to approximate a reference distribution p ref with sample access, it is possible to take F ( p ) := KL( p ref || p ). This case corresponds to training energy-based models (Gutmann and Hyv¨ arinen, 2012). It is also possible to consider a linear combination of these two objectives.\n\nDenoising diffusion. It consists in running the SDE for Y 0 ∼ N (0 , I ),\n\n<!-- formula-not-decoded -->\n\nwhere s θ : R d × [0 , T ] → R d is a parameterized score function (Hyv¨ arinen, 2005; Vincent, 2011; Ho et al., 2020). Its aim is to reverse a forward process d X t = -X t d t + √ 2d B t , where we have sample access to X 0 ∼ p data ∈ P . More precisely, denoting by p t the distribution of X t , if s θ ≈ ∇ log p t , then the distribution of Y T is close to p data for large T (Anderson, 1982), which allows approximate sampling from p data . Implementations of s θ include U-Nets (Ronneberger et al., 2015) or Vision Transformers that split the image into patches (Peebles and Xie, 2023). We present for simplicity an unconditioned model, but conditioning (on class, prompt, etc.) also falls in our framework.\n\nWe are interested in optimizing through diffusion sampling and consider π ⋆ ( θ ) as the distribution of Y T . A key example is when θ 0 represents the weights of a model s θ 0 that has been pretrained by score matching and one wants to finetune the target distribution π ⋆ ( θ ), e.g. in order to increase a reward R : R d → R . Figure 3 situates our contribution within the broader\n\nFigure 2: A step of optimization through sampling. For a given parameter θ 0 , the sampling process is defined by applying Σ s for s ∈ [ T ], producing π ⋆ ( θ 0 ). The goal of optimization through sampling is to update θ to minimize ℓ = F ◦ π ⋆ . Here the objective F corresponds to having lighter images (on average), which produces thicker digits.\n\n<!-- image -->\n\nliterature on this problem (see details in Appendix D). Note that this finetuning step does not require access to p data . As for Langevin dynamics, we consider in our algorithms discrete approximations of the process (7). However in this case, there exists no natural functional G minimized by the sampling process. An alternative to (7) producing the same marginal distributions is the ordinary differential equation (ODE)\n\n<!-- formula-not-decoded -->\n\n## 3 METHODS\n\nThe objective (4) presents several challenges, that we review here. We then introduce an overview of our approach, before detailing our algorithms.\n\n## 3.1 Overview\n\nEstimation of gradients through sampling. Even with samples from π ⋆ ( θ ), applying a first-order method to (4) requires evaluating gradients of ℓ = F ◦ π ⋆ . Since there is no closed form for ℓ and no explicit computational graph, we consider the following alternative setting to evaluate gradients.\n\nDefinition 3.1 (Implicit gradient estimation) . The gradient of ℓ can be implicitly estimated if Σ s , F are such that there exists Γ : P × R p → R p such that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ).\n\nIn practice we rarely reach exactly the distribution π ⋆ ( θ ), e.g. because a finite number of iterations of sampling is performed. Then, if ˆ π ≈ π ⋆ ( θ ), the gradient can be approximated by ˆ g = Γ(ˆ π, θ ). In particular, given access to approximate samples of π ⋆ ( θ ), it is possible to compute an estimate of ∇ ℓ ( θ ), and this is at the heart of our methods-see Appendix A.1 for more details. Note that when Γ is linear in its first argument, sample access to π ⋆ ( θ ) yields unbiased estimates of the gradient. This case has been studied with various approaches (see Sutton et al., 1999; Fu and Hu, 2012; Pflug, 2012; De Bortoli et al., 2021, and Appendix D).\n\nFigure 3: Main approaches for reward tuning of denoising diffusions. References are given in Appendix D.\n\n<!-- image -->\n\nRemark 3.2 . Definition 3.1 is in fact always satisfied by the tautological definition Γ( p, θ ) := ∇ ℓ ( θ ). Rather than the existence of Γ, key questions are whether Γ is easily computable, can be stochastically approximated as explained above, or enjoys theoretical guarantees. We present several sampling problems in Section 3.2\n\nwhere this is the case. This point of view should also extend beyond sampling to any setting that involves optimizing over a parameterized distribution, as for instance in Wasserstein Distributionally Robust Optimization (Mohajerin Esfahani and Kuhn, 2018), a method for robust learning. We leave these extensions to future investigations.\n\nBeyond nested-loop approaches. Sampling from π ⋆ ( θ ) is usually only feasible via iterations of the sampling process Σ s . The most straightforward method is then a nested loop: at each optimization step k , run an inner loop for a large number T of steps of Σ s to produce ˆ π k ≈ π ⋆ ( θ k ), and use it to evaluate a gradient. Algorithm 1 formalizes this baseline. This approach can be inefficient for two reasons: first, it requires solving the inner sampling problem at each optimization step . Further, nested loops are typically impractical with accelerator-oriented hardware. These issues can be partially alleviated by techniques like gradient checkpointing (see references in Appendix D).\n\nAlgorithm 1 Vanilla nested-loop approach (Baseline) input θ 0 ∈ R p , p 0 ∈ P for k ∈ { 0 , . . . , K -1 } (outer optimization loop) do p (0) k ← p 0 for s ∈ { 0 , . . . , T -1 } (inner sampling loop) do p ( s +1) k ← Σ s ( p ( s ) k , θ k ) ˆ π k ← p ( T ) k θ k +1 ← θ k -η Γ(ˆ π k , θ k ) (or another optimizer) output θ K\n\nWe rather follow an approach inspired by methods in bilevel optimization, aiming to jointly iterate on both the sampling problem (evaluation of π ⋆ -the inner problem), and the optimization problem over θ ∈ R p (the outer objective F ). We describe these methods in Section 3.3 and Algorithms 2 and 3. The connection with bilevel optimization is especially seamless when sampling can indeed be cast as an optimization problem over distributions in P , as in (3). However, as noted above, our approach generalizes beyond.\n\n## 3.2 Gradient estimation through sampling\n\nWe explain how to perform implicit gradient estimation as in Definition 3.1, that is, how to derive expressions for the function Γ, in several cases of interest.\n\nDirect analytical derivation. For Langevin dynamics, it is possible to derive analytical expressions for Γ depending on the outer objective F . We illustrate this idea for the two objectives introduced in Section 2.2. First, in the case where F rew ( p ) = -E x ∼ p [ R ( x )], a computation detailed in Appendix A.2 and the use of Definition 3.1 show that, for ℓ rew ( θ ) = F rew ( π ⋆ ( θ )),\n\n<!-- formula-not-decoded -->\n\nNote that this formula does not involve gradients of R , hence our approach handles any non-differentiable reward. Second, in the case where F ref ( p ) = KL( p ref || p ), we then have (Gutmann and Hyv¨ arinen, 2012), for ℓ ref ( θ ) = F ref ( π ⋆ ( θ )),\n\n<!-- formula-not-decoded -->\n\nThis is known as contrastive learning when p ref is given by data, and suggests taking Γ as\n\n<!-- formula-not-decoded -->\n\nThis extends to linear combinations of Γ rew and Γ ref .\n\nImplicit differentiation. When, as in (3), π ⋆ ( θ ) = argmin G ( · , θ ), under generic assumptions on G the implicit function theorem (see Krantz and Parks, 2002; Blondel et al., 2022 and Appendix A.4) shows that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ) with\n\n<!-- formula-not-decoded -->\n\nHere F ′ ( p ) : X → R denotes the first variation of F at p ∈ P (see Definition B.1) and γ ( p, θ ) is the solution of the linear system ∫ ∇ 1 , 1 G ( p, θ )[ x, x ′ ] γ ( p, θ )[ x ′ ]d x ′ = -∇ 1 , 2 G ( p, θ )[ x ]. Although this gives us a general way to define gradients of π ⋆ ( θ ) with respect to θ , solving this linear system is generally not feasible. One exception is when sampling over a finite state space X , in which case P is finite-dimensional, and the integrals boil down to matrix-vector products.\n\nDifferential adjoint method. The adjoint method allows computing gradients through differential equation solvers (Pontryagin, 1987; Li et al., 2020), applying in particular for denoising diffusion. It can be connected to implicit differentiation, by defining G over a measure path instead of a single measure p (see, e.g., Kidger, 2022). To introduce this method, consider the ODE d Y t = µ ( t, Y t , θ )d t integrated between 0 and some T &gt; 0. This setting encompasses the denoising diffusion ODE (8). Assume that the outer objective F writes as the expectation of some differentiable reward R , namely F ( p ) = E x ∼ p [ R ( x )]. Let Z 0 ∼ p, A 0 = ∇ R ( Z 0 ) , G 0 = 0, and consider the ODE system\n\n<!-- formula-not-decoded -->\n\nDefining Γ( p, θ ) := G T , the adjoint method shows that Γ( π ⋆ ( θ ) , θ ) is an unbiased estimate of ∇ ℓ ( θ ). We refer to Appendix A.3 for details and explanations on how to differentiate through the SDE sampler (7) and to incorporate a KL term in the reward by using Girsanov's theorem.\n\n## 3.3 Implicit Diffusion optimization algorithm\n\nTo circumvent solving the inner sampling problem in Algorithm 1, we propose in Algorithm 2 a joint singleloop approach that keeps track of a single dynamic of probabilities ( p k ) k ≥ 0 . At each optimization step, the probability p k is updated with one sampling step that depends on the current parameter θ k . As noted in Section 3.1, there are parallels with approaches in the literature when Γ is linear, but we go beyond in making no such assumption.\n\n## Algorithm 2 Implicit Diff. optimization, infinite time\n\n<!-- formula-not-decoded -->\n\nThis point of view is well-suited for stationary processes with infinite-time horizon, but does not directly adapt to differentiation through diffusions with a finite-time horizon (and no stationary property). Indeed, it does not make sense in this case to run sampling for an arbitrary number of steps. Our approach can then be adapted as follows.\n\n## Algorithm 3 Implicit Diff. optimization, finite time\n\n<!-- formula-not-decoded -->\n\nFinite time-horizon: queuing trick. When π ⋆ ( θ ) is obtained by a large, but finite number T of iterations of the operator Σ s , we leverage hardware parallelism to evaluate in parallel several, say M , steps of the dynamics of the distribution p k , through a queue of length M . We present for simplicity in Figure 4 and in Algorithm 3 the case where M = T and discuss extensions in Appendix A.3. At each step, the last element of the queue p ( M ) k provides a distribution to update\n\nθ through evaluation of Γ. Note that its dynamics in the previous M steps, from p (0) k -M to p ( M ) k , used the M previous values of the parameter θ k -M , . . . , θ k -1 . In particular, it provides a biased estimate of ∇ ℓ ( θ k ). Importantly, leveraging parallelism, the runtime of our algorithm is O ( K ), gaining a factor of T compared to the nested-loop approach, but at a higher memory cost.\n\n## 4 THEORETICAL ANALYSIS\n\nOur theoretical guarantees cover Langevin diffusions in the continuous and discrete settings, and a simple case of denoising diffusion. Proofs are given in Appendix B.\n\n## 4.1 Langevin with continuous flow\n\nThe continuous-time equivalent of Algorithm 2 in the case of Langevin dynamics is\n\n<!-- formula-not-decoded -->\n\nwhere p t denotes the distribution of X t and ε t &gt; 0 corresponds to the ratio of learning rates between the inner and the outer problems. In practice Γ( p t , θ t ) is approximated on a finite sample, making the dynamics in θ t stochastic. We leave the analysis of these stochastic dynamics for future work. A possible tool to do so is the propagation of chaos (Chaintron and Diez, 2022; Suzuki et al., 2023), a theory which aims at quantifying the deviation between the dynamics of a system of finitely many interacting particles and the limiting behavior described by a mean-field density.\n\nRecalling the definition (6) of π ⋆ ( θ ), we require the following assumptions.\n\nAssumption 4.1. π ⋆ ( θ t ) verifies the Log-Sobolev inequality with constant µ &gt; 0 for all t ≥ 0, i.e., for all p ∈ P , KL( p || π ⋆ ( θ t )) ≤ 1 2 µ ∥∇ log ( p π ⋆ ( θ t ) ) ∥ 2 L 2 ( p ) .\n\nAssumption 4.2. V is continuously differentiable and for θ ∈ R p , x ∈ R d , ∥∇ 2 V ( x, θ ) ∥ ≤ C , for some C &gt; 0.\n\nAssumption 4.1 generalizes µ -strong convexity of the potentials ( V ( · , θ t )) t ≥ 0 , including for instance distributions π whose potentials are bounded perturbations of a strongly convex potential (Bakry et al., 2014; Vempala and Wibisono, 2019). Assumptions 4.1 and 4.2 hold for example when the potential defines a mixture of Gaussians and the parameters θ determine the weights of the mixture (see Appendix B.1.2 for details). We also assume that the outer updates are bounded and Lipschitz continuous for the KL divergence.\n\nAssumption 4.3. For p, q ∈ P , θ ∈ R p , ∥ Γ( p, θ ) ∥ ≤ C and ∥ Γ( p, θ ) -Γ( q, θ ) ∥ ≤ K Γ √ KL( p || q ), for some C, K Γ &gt; 0.\n\nFigure 4: Illustration of the Implicit Diffusion algorithm, in the finite time setting. Left: Sampling - one step of the parameterized sampling scheme is applied in parallel to all distributions in the queue. Right: Optimization the last element of the queue is used to compute a gradient for the parameter.\n\n<!-- image -->\n\nThe next proposition shows that this assumption holds for the examples of interest given in Section 2.\n\nProposition 4.4. Consider a bounded function R : R d → R . Then, under Assumption 4.2, functions Γ rew and Γ ref defined by (9) -(10) satisfy Assumption 4.3.\n\nSince we make no strong convexity assumption, we cannot hope to prove convergence to a global minimizer, rather convergence of the average objective gradients. Note that, with strong convexity, it is a rather standard extension to prove convergence to the global minimizer (see, e.g., in a similar context, Dagr´ eou et al., 2022).\n\nTheorem 4.5. Take ε t = min(1 , 1 / √ t ) in (11) . Then, under Assumptions 4.1, 4.2, and 4.3,\n\n<!-- formula-not-decoded -->\n\nThe proof starts by noting that updates in θ would follow the gradient flow for ℓ if p t = π ⋆ ( θ t ). The deviation to these ideal dynamics can be controlled by the KL divergence of p t from π ⋆ ( θ t ), which can itself be bounded since updates in X t are gradient steps for the KL (see Section 2.2). Finally, the decay of the ratio of learning rates ε t ensures that π ⋆ ( θ t ) is not moving away from p t too fast due to updates in θ t . Taking ε t small amounts to a two-timescale approach , a tool commonly used to tackle non-convex optimization problems in machine learning (Heusel et al., 2017; Arbel and Mairal, 2022; Hong et al., 2023; Marion and Berthier, 2023).\n\n## 4.2 Langevin with discrete flow\n\nWe now consider the discrete version of (11), namely\n\n<!-- formula-not-decoded -->\n\nwhere p k denotes the distribution of X k . This setting is more challenging due to the discretization bias (Dalalyan, 2017). We make classical smoothness assumptions to analyze discrete gradient descent (e.g., Cheng and Bartlett, 2018):\n\nAssumption 4.6. The functions ∇ 1 V ( · , θ ), ∇ 1 V ( x, · ) and ∇ ℓ are respectively L X -Lipschitz for all θ ∈ R p , L Θ -Lipschitz for all x ∈ R d , and L -Lipschitz.\n\nWe can then show the following convergence result.\n\nTheorem 4.7. Take γ k = c 1 / √ k and ε k = 1 / √ k in (12) . Under Assumptions 4.1, 4.2, 4.3, and 4.6,\n\n<!-- formula-not-decoded -->\n\nThe proof technique to bound the KL in discrete iterations is inspired by Cheng and Bartlett (2018). Comparing to the continuous case, this step incurs a discretization error proportional to γ k → 0, inducing a slower convergence rate. The result is similar to Dagr´ eou et al. (2022) for finite-dimensional bilevel optimization, albeit our final convergence rate is slower.\n\n## 4.3 Denoising diffusion\n\nThe case of denoising diffusion is more challenging since π ⋆ ( θ ) can not be readily characterized as the stationary point of an iterative process. We study a 1D Gaussian case and leave more general analysis for future work. Considering p data = N ( θ data , 1) and the forward process of Section 2.2, a straightforward computation shows that the score is given by ∇ log p t ( x ) = -( x -θ data e -t ). A natural parameterization of the score function is therefore s θ ( x, t ) := -( x -θe -t ). With this parameterization, the output of the sampling process (7) is π ⋆ ( θ ) = N ( θ (1 -e -2 T ) , 1). Remarkably, π ⋆ ( θ ) is Gaussian for all θ ∈ R , making the analytical study tractable.\n\nAssume that pretraining with samples of p data yields θ = θ 0 , and we want to finetune the model towards some other θ target ∈ R by optimizing the reward R ( x ) = -( x -θ target ) 2 over samples of π ⋆ ( θ ). A short computation shows that ∇ ℓ ( θ ) = -E x ∼ π ⋆ ( θ ) R ′ ( x )(1 -e -2 T ), hence one can take Γ( p, θ ) = -E x ∼ p R ′ ( x )(1 -e -2 T ). In this setting, we study a continuous-time version of\n\nAlgorithm 3, where Σ is the denoising diffusion (7) and Γ is given above, and show convergence of θ to θ target . This shows that Algorithm 3 successfully finetunes the parameter to optimize the reward. We refer to Appendix B.3 for details.\n\nProposition 4.8. (informal) Let ( θ t ) t ≥ 0 be given by the continuous-time equivalent of Algorithm 3. Then ∥ θ 2 T -θ target ∥ = O ( e -T ) , and π ⋆ ( θ 2 T ) = N ( µ 2 T , 1) with µ 2 T = θ target + O ( e -T ) .\n\n## 5 EXPERIMENTS\n\nWe empirically illustrate the performance of Implicit Diffusion. Details are given in Appendix C.\n\n## 5.1 Reward training of Langevin processes\n\nWe consider the case where the potential V ( · , θ ) is a logsumexp of quadratics-so that the outcome distributions are mixtures of Gaussians. We optimize the reward R ( x ) = 1 ( x 1 &gt; 0) exp ( -∥ x -µ ∥ 2 ) , for µ ∈ R d , thereby illustrating the ability of our method to optimize rewards that are not differentiable. We run six sampling algorithms, including the infinite time-horizon version of Implicit Diffusion (Algorithm 2), all starting from p 0 = N (0 , I d ) and for K = 5 , 000 steps.\n\n- ■ Langevin diffusion (5) with potential V ( · , θ 0 ) for some fixed θ 0 ∈ R p , no reward.\n- ⋆ Implicit Diffusion with F ( p ) = -E X ∼ p [ R ( X )], yields both a sample ˆ p K and parameters θ opt .\n- ▼ Langevin (5) with potential V ( · , θ 0 ) -λR smooth , where R smooth is a smoothed version of R .\n- Langevin (5) with potential V ( · , θ opt ). This is inference post-training with Implicit Diffusion.\n- ♦ ♦ Nested loop (Algorithm 1) with T inner sampling steps for each gradient step.\n- ▲ ▲ Unrolling through the last step of sampling with T inner sampling steps for each outer step.\n\nFigure 5: Contour lines and samples for ( ): Langevin θ 0 - ( ) Unrolling with T = 100 inner sampling steps -( ) Implicit Diffusion.\n\n<!-- image -->\n\nBoth qualitatively (Figure 5) and quantitatively (Figure 6), we observe that our approach efficiently op- timizes through sampling. We analyze performance both in terms of steps (number of optimization stepsupdates in θ ) and gradient evaluations (number of sampling steps). After K optimization steps, our algorithm yields both θ opt := θ K and a sample ˆ p K approximately from π ⋆ ( θ opt ). Then, it is convenient and fast to sample post hoc, with a Langevin process using θ opt -as observed in Figure 6. This is similar in spirit to inference with a finetuned model, post-reward training. We compare our approach with several baselines. First, directly adding a reward term ( ▼ ) is less efficient: it tends to overfit on the reward, as the target distribution of this process is out of the family of π ⋆ ( θ )'s. Second, unrolling through the last step of sampling ( ▲ , ▲ ) leads to much slower optimization. Finally, using the nested loop approach ( ♦ , ♦ ) makes each optimization step T times more costly, while barely improving the performance after a given number of steps (due to less biased gradients). When comparing in terms of number of gradient evaluations, Implicit Diffusion strongly outperforms the nested loop approach. In other words, for a given computational budget, it is optimal to take T = 1 and perform more optimization steps, which corresponds to the Implicit Diffusion single-loop approach. Further comparison plots, as well as a variant of this experiment where we learn a reference distribution (i.e., train from scratch an energy-based model) are included in Appendix C. We also include an experiment where the potential V parameterizes the means and the covariances of the Gaussians in addition to the weights of the mixture.\n\n## 5.2 Reward training of denoising diffusion\n\nWe also apply Implicit Diffusion for reward finetuning of denoising diffusion models pretrained on image datasets. We denote by θ 0 the weights of a pretrained model, such that π ⋆ ( θ 0 ) ≈ p data . For various reward functions on the samples R : R d → R , we consider\n\n<!-- formula-not-decoded -->\n\ncommon in reward finetuning (see, e.g., Ziegler et al., 2019, and references therein), for positive and negative values of λ . We run Implicit Diffusion using the finite time-horizon variant (Algorithm 3), applying the adjoint method on SDEs for gradient estimation. We report selected samples of π ⋆ ( θ t ), as well as reward and KL divergence estimates (see Figures 1 and 7-9).\n\nWe report results on models pretrained on the image datasets MNIST (LeCun and Cortes, 1998), CIFAR-10 (Krizhevsky, 2009), and LSUN (bedrooms) (Yu et al., 2016). For MNIST, we use a 2 . 5M parameters model (no label conditioning). Our reward is the average brightness (i.e. average of all pixel values). For CIFAR10 and LSUN, we pretrain a 53 . 2M parameters model,\n\n<!-- image -->\n\nFigure 6: Metrics for reward training of Langevin processes, 10 runs. The setting and color code are detailed in Section 5.1. Left: Reward on the sample distribution, at each outer objective step, averaged on a batch. Middle: Log-likelihood of π ⋆ ( θ opt ) on the sample distribution, at each outer step, averaged on a batch-higher is better. Right: Number of gradient evaluations needed to reach a given log-likelihood threshold (y-axis)-lower is better, for various sizes of inner loop T (x-axis) and various methods (same symbols and colors as in left plots). Implicit Diffusion is T = 1. White symbols with colored edges correspond to a log-likelihood threshold of -5 . 8 (red dashed line in the middle plot) and fully-colored symbols to a threshold of -2 . 0 (black dashed line in the middle plot).\n\nFigure 7: Reward training with Implicit Diffusion for various learning rates η and reward strengths λ/β . For each dataset, we plot together the reward and the negative KL divergence w.r.t. π ⋆ ( θ 0 ).\n\n<!-- image -->\n\nFigure 8: Samples of reward training after pretraining on LSUN ( λ/β = 10). The reward incentives for redder images. Images are re-sampled with the same seed every five steps (see Appendix C.2).\n\n<!-- image -->\n\nwith label conditioning for CIFAR-10. Our reward is the average brightness of the red channel minus the average on the other channels. For pretraining, we follow the simple diffusion method (Hoogeboom et al., 2023) and use U-Net models (Ronneberger et al., 2015). We display visual examples in Figures 1, 8, 9 and in Appendix C, where we also report additional\n\nFigure 9: Samples of reward training after pretraining on MNIST. The reward favors darker images ( λ/β = -30). Images are re-sampled with the same seed every five steps (see Appendix C.2).\n\n<!-- image -->\n\nmetrics. While the finetuned models diverge from the original distribution, they retain overall semantic information (e.g. brighter digits are thicker, rather than on a gray background). We observe in Figure 7 the competition between reward and divergence to the pretrained distribution.\n\nPossible limitations of our approach include sensitivity to the choice of hyperparameters (learning rate η , reward strength λ/β , size of the queue M ), bias in the gradient estimation, practical applicability to larger scale problems or more complex rewards. We plan to investigate these questions in future research.\n\n## Acknowledgments\n\nThe authors would like to thank Fabian Pedregosa for very fruitful discussions on implicit differentiation and bilevel optimization that led to this project, Vincent Roulet for very insightful notes and comments about early drafts of this work as well as help with experiment implementation, Emiel Hoogeboom for extensive help\n\non pretraining diffusion models, and Cl´ ement Cr´ epy for help with open-sourcing. PM and AK thank Google for their academic support in the form respectively of a Google PhD Fellowship and a gift in support of her academic research.\n\n## References\n\n- L. Ambrosio, N. Gigli, and G. Savar´ e. Gradient flows: in metric spaces and in the space of probability measures . Springer Science &amp; Business Media, 2005.\n- B. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications , 12(3):313-326, 1982.\n- M. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In International Conference on Learning Representations , 2022.\n- Y. F. Atchad´ e, G. Fort, and E. Moulines. On perturbed proximal gradient algorithms. The Journal of Machine Learning Research , 18(1):310-342, 2017.\n- D. Bakry, I. Gentil, M. Ledoux, et al. Analysis and geometry of Markov diffusion operators , volume 103. Springer, 2014.\n- K. Black, M. Janner, Y. Du, I. Kostrikov, and S. Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations , 2024.\n- M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-L´ opez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. Advances in neural information processing systems , 35:52305242, 2022.\n- J. Bolte, E. Pauwels, and S. Vaiter. One-step differentiation of iterative algorithms. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems , volume 36, pages 77089-77103. Curran Associates, Inc., 2023.\n- J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/ jax .\n10. L.-P. Chaintron and A. Diez. Propagation of chaos: A review of models, methods and applications. i. models and methods. Kinetic and Related Models , 15(6):895-1015, 2022.\n11. H.-B. Chen, S. Chewi, and J. Niles-Weed. Dimensionfree log-sobolev inequalities for mixture distributions. Journal of Functional Analysis , 281(11): 109236, 2021a.\n- T. Chen, Y. Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 25294-25307. Curran Associates, Inc., 2021b.\n- T. Chen, Y. Sun, Q. Xiao, and W. Yin. A singletimescale method for stochastic bilevel optimization. In International Conference on Artificial Intelligence and Statistics , pages 2466-2488. PMLR, 2022.\n- X. Cheng and P. L. Bartlett. Convergence of Langevin MCMC in KL-divergence. In F. Janoos, M. Mohri, and K. Sridharan, editors, Proceedings of ALT2018 , volume 83 of Proceedings of Machine Learning Research , pages 186-211. PMLR, 2018.\n- K. Clark, P. Vicol, K. Swersky, and D. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations , 2024.\n- M. Dagr´ eou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 26698-26710. Curran Associates, Inc., 2022.\n- A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology , 79(3):651-676, 2017.\n- V. De Bortoli, A. Durmus, M. Pereyra, and A. F. Vidal. Efficient stochastic optimisation by unadjusted langevin monte carlo: Application to maximum marginal likelihood and empirical bayesian estimation. Statistics and Computing , 31:1-18, 2021.\n- P. Dhariwal and A. Nichol. Diffusion models beat GANs on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 8780-8794. Curran Associates, Inc., 2021.\n- H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. SHUM, and T. Zhang. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.\n- K. D. Dvijotham, S. Omidshafiei, K. Lee, K. M. Collins, D. Ramachandran, A. Weller, M. Ghavamzadeh, M. Nasr, Y. Fan, and J. Z. Liu. Algorithms for optimal adaptation of diffusion models to reward functions. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023.\n\n- A. Eberle. Reflection couplings and contraction rates for diffusions. Probability theory and related fields , 166:851-886, 2016.\n- Y. Fan, O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and K. Lee. Dpok: Reinforcement learning for finetuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381 , 2023.\n- L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 1568-1577. PMLR, 10-15 Jul 2018.\n- M. C. Fu and J.-Q. Hu. Conditional Monte Carlo: Gradient estimation and Optimization Applications , volume 392. Springer Science &amp; Business Media, 2012.\n- A. Graikos, N. Malkin, N. Jojic, and D. Samaras. Diffusion models as plug-and-play priors. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 14715-14728. Curran Associates, Inc., 2022.\n- A. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation . SIAM, 2008.\n- Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. A novel convergence analysis for algorithms of the adam family and beyond. arXiv preprint arXiv:2104.14840 , 2021.\n- M. U. Gutmann and A. Hyv¨ arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of machine learning research , 13(2), 2012.\n- A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations , 2023.\n- M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n- J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems , volume 33, pages 6840-6851. Curran Associates, Inc., 2020.\n- M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A twotimescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization , 33 (1):147-180, 2023.\n- E. Hoogeboom, J. Heek, and T. Salimans. simple diffusion: End-to-end diffusion for high resolution images. In Proceedings of The 40th International Conference on Machine Learning , 2023.\n- A. Hyv¨ arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research , 6(24):695-709, 2005.\n- R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker-planck equation. SIAM journal on mathematical analysis , 29(1):1-17, 1998.\n- P. Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435 , 2022.\n- P. Kidger, J. Foster, X. C. Li, and T. Lyons. Efficient and accurate gradients for neural SDEs. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages 1874718761. Curran Associates, Inc., 2021.\n- D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations , 2015.\n- I. Kobyzev, S. Prince, and M. A. Brubaker. Normalizing flows: Introduction and ideas. stat , 1050:25, 2019.\n- A. Korba and A. Salim. Sampling as first-order optimization over a space of probability measures, 2022. Tutorial at ICML 2022. Accessible at https://akorba.github.io/resources/ Baltimore\\_July2022\\_ICMLtutorial.pdf , consulted on 01/30/2024.\n- S. G. Krantz and H. R. Parks. The implicit function theorem: history, theory, and applications . Springer Science &amp; Business Media, 2002.\n- A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n- J. Kuntz, J. N. Lim, and A. M. Johansen. Particle algorithms for maximum likelihood training of latent variable models. In International Conference on Artificial Intelligence and Statistics , pages 5134-5180. PMLR, 2023.\n- M. Kwon, J. Jeong, and Y. Uh. Diffusion models already have a semantic latent space. In The Eleventh International Conference on Learning Representations , 2023.\n\n- Y. LeCun and C. Cortes. MNIST handwritten digit database, 1998. URL http://yann.lecun.com/ exdb/mnist/ .\n- K. Lee, H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192 , 2023.\n- X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In C. Zhang, F. Ruiz, T. Bui, A. B. Dieng, and D. Liang, editors, Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference , volume 118 of Proceedings of Machine Learning Research , pages 1-28. PMLR, 08 Dec 2020.\n- H. Liu, K. Simonyan, and Y. Yang. DARTS: Differentiable architecture search. In International Conference on Learning Representations , 2019.\n- P. Marion and R. Berthier. Leveraging the two timescale regime to demonstrate convergence of neural networks. In Advances in Neural Information Processing Systems , volume 36, 2023.\n- P. Mohajerin Esfahani and D. Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming , 171(1): 115-166, 2018.\n- A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In M. Meila and T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pages 8162-8171. PMLR, 18-24 Jul 2021.\n- A. Nitanda. Stochastic proximal gradient descent with acceleration techniques. Advances in Neural Information Processing Systems , 27, 2014.\n- B. G. Pachpatte and W. Ames. Inequalities for Differential and Integral Equations . Elsevier, 1997.\n- G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. The Journal of Machine Learning Research , 22(1):2617-2680, 2021.\n- G. A. Pavliotis. Stochastic processes and applications . Springer, 2016.\n- F. Pedregosa. Hyperparameter optimization with approximate gradient. In M. F. Balcan and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning Research , pages 737-746, New York, New York, USA, 20-22 Jun 2016. PMLR.\n- W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 4195-4205, October 2023.\n- G. C. Pflug. Optimization of Stochastic Models: the Interface between Simulation and Optimization , volume 373. Springer Science &amp; Business Media, 2012.\n- L. S. Pontryagin. Mathematical Theory of Optimal Processes . Routledge, 1987.\n- P. Protter. Stochastic integration and differential equations. A new approach , volume 21 of Stochastic Modelling and Applied Probability . Springer Berlin, Heidelberg, 2005.\n- G. O. Roberts and R. L. Tweedie. Exponential convergence of langevin distributions and their discrete approximations. Bernoulli , pages 341-363, 1996.\n- O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18 , pages 234-241. Springer, 2015.\n- L. Rosasco, S. Villa, and B. C. V˜ u. Convergence of stochastic proximal gradient algorithm. Applied Mathematics &amp; Optimization , 82:891-917, 2020.\n- L. Sharrock, D. Dodd, and C. Nemeth. Tuning-free maximum likelihood training of latent variable models via coin betting. In S. Dasgupta, S. Mandt, and Y. Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics , volume 238 of Proceedings of Machine Learning Research , pages 1810-1818. PMLR, 02-04 May 2024.\n- R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems , volume 12, 1999.\n- T. Suzuki, A. Nitanda, and D. Wu. Uniform-in-time propagation of chaos for the mean-field gradient langevin dynamics. In The Eleventh International Conference on Learning Representations , 2023.\n- V. B. Tadi´ c and A. Doucet. Asymptotic bias of stochastic gradient search. Annals of Applied Probability , 27 (6):3255-3304, 2017.\n- A. Tsybakov. Introduction to nonparametric estimation . Springer Series in Statistics. Springer, New York, 2009.\n- B. Tzen and M. Raginsky. Theoretical guarantees for sampling and inference in generative models with latent diffusions. In A. Beygelzimer and D. Hsu, editors, Proceedings of the Thirty-Second Conference\n\n- on Learning Theory , volume 99 of Proceedings of Machine Learning Research , pages 3084-3114. PMLR, 25-28 Jun 2019.\n- S. Vempala and A. Wibisono. Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry suffices. In Advances in Neural Information Processing Systems , volume 32, 2019.\n- P. Vincent. A connection between score matching and denoising autoencoders. Neural Computation , 23(7): 1661-1674, 2011.\n- B. Wallace, A. Gokul, S. Ermon, and N. Naik. End-toend diffusion latent optimization improves classifier guidance. arXiv preprint arXiv:2303.13703 , 2023.\n- Z. Wang and J. Sirignano. A forward propagation algorithm for online optimization of nonlinear stochastic differential equations. arXiv preprint arXiv:2207.04496 , 2022.\n- Z. Wang and J. Sirignano. Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations. Mathematical Finance , 34(2):348-424, 2024.\n- D. Watson, W. Chan, J. Ho, and M. Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations , 2022.\n- R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 8(3-4):229-256, 1992.\n- Q. Wu, Y. Liu, H. Zhao, A. Kale, T. Bui, T. Yu, Z. Lin, Y. Zhang, and S. Chang. Uncovering the disentanglement capability in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1900-1910, June 2023a.\n- X. Wu, K. Sun, F. Zhu, R. Zhao, and H. Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 2096-2105, October 2023b.\n- L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization , 24(4):2057-2075, 2014.\n- J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. Advances in Neural Information Processing Systems , 34:13670-13682, 2021.\n- F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2016.\n- Z. Zhang, L. Liu, Z. Lin, Y. Zhu, and Z. Zhao. Unsupervised discovery of interpretable directions in h-\n- space of pre-trained diffusion models. arXiv preprint arXiv:2310.09912 , 2023.\n- D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Finetuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.\n\n## Checklist\n\n1. For all models and algorithms presented, check if you include:\n2. (a) A clear description of the mathematical setting, assumptions, algorithm, and/or model. Yes\n3. (b) An analysis of the properties and complexity (time, space, sample size) of any algorithm. Yes\n4. (c) (Optional) Anonymized source code, with specification of all dependencies, including external libraries. No\n\nThe setting and algorithms are described in Sections 1-3, and further details are given in Section A. We open-sourced the source code related to the experiments on reward training of Langevin processes.\n\n2. For any theoretical claim, check if you include:\n2. (a) Statements of the full set of assumptions of all theoretical results. Yes\n3. (b) Complete proofs of all theoretical results. Yes\n4. (c) Clear explanations of any assumptions. Yes\n\nSee Section 4 for precise mathematical statements and assumptions, and Appendix B for proofs.\n\n3. For all figures and tables that present empirical results, check if you include:\n2. (a) The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). No\n3. (b) All the training details (e.g., data splits, hyperparameters, how they were chosen). Yes\n4. (c) A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). Yes\n5. (d) A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). Yes\n\nWe open-sourced the source code related to the experiments on reward training of Langevin processes. Error bars are provided over independent repetitions for Langevin experiments, as described in Section 5 and Appendix C, while they are too costly to compute for the denoising diffusion experiments. The computing infrastructure is described in Section 5 and Appendix C.\n\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:\n2. (a) Citations of the creator If your work uses existing assets. Yes\n3. (b) The license information of the assets, if applicable. No\n4. (c) New assets either in the supplemental material or as a URL, if applicable. Not Applicable\n5. (d) Information about consent from data providers/curators. Not Applicable\n6. (e) Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. Not Applicable\n\nSee Section 5 and Appendix C for citations of the datasets and main code packages used in this project.\n\n5. If you used crowdsourcing or conducted research with human subjects, check if you include:\n2. (a) The full text of instructions given to participants and screenshots. Not Applicable\n3. (b) Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. Not Applicable\n4. (c) The estimated hourly wage paid to participants and the total amount spent on participant compensation. Not Applicable\n\nAs a consequence,\n\n<!-- formula-not-decoded -->\n\nThis computation is sometimes referred to as the REINFORCE trick (Williams, 1992). This suggests taking\n\n<!-- formula-not-decoded -->\n\nIn this case, the sample version of Algorithm 2 is\n\n<!-- formula-not-decoded -->\n\n## APPENDIX\n\nOrganization of the Appendix. Section A is devoted to explanations of our methodology. Section A.1 explains the gradient estimation setting we consider. Then, in the case of Langevin dynamics (Section A.2), and denoising diffusions (Section A.3), we explain how Definition 3.1 and our Implicit Differentiation algorithms (Algorithms 2 and 3) can be instantiated. Section A.4 gives more details about the implicit differentiation approaches sketched in Section 3.2. Section B contains the proofs of our theoretical results, while Section C gives details for the experiments of Section 5 as well as additional explanations and plots. Finally, Section D is dedicated to additional related work.\n\n## A IMPLICIT DIFFUSION ALGORITHMS\n\n## A.1 Gradient estimation abstraction: Γ\n\nAs discussed in Section 3.1, we focus on settings where the gradient of the loss ℓ : R p → R , defined by\n\n<!-- formula-not-decoded -->\n\ncan be estimated by using a function Γ. More precisely, following Definition 3.1, we assume that there exists a function Γ : P × R p → R p such that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ). In practice, for almost every setting there is no closed form for π ⋆ ( θ ), and even sampling from it can be challenging (e.g. here, if it is the outcome of infinitely many sampling steps). When we run our algorithms, the dynamic is in practice applied to variables, as discussed in Section 2.1. Using a batch of variables of size n , initialized independent with X i 0 ∼ p 0 , we have at each step k of joint sampling and optimization a batch of variables forming an empirical measure ˆ p ( n ) k . We consider cases where the operator Γ is well-behaved: if ˆ p ( n ) ≈ p , then Γ(ˆ p ( n ) , θ ) ≈ Γ( p, θ ), and therefore where this finite sample approximation can be used to produce an accurate estimate of ∇ ℓ ( θ ).\n\n## A.2 Langevin dynamics\n\nWe explain how to derive the formulas (9)-(10) for Γ, and give the sample version of Algorithm 2 in these cases.\n\nRecall that the stationary distribution of the dynamics (5) is the Gibbs distribution (6), with the normalization factor Z θ = ∫ exp( -V ( x, θ ))d x . Assume that the outer objective can be written as the expectation of some (potentially non-differentiable) reward R , namely F ( p ) := -E x ∼ p [ R ( x )]. Then our objective is\n\n<!-- formula-not-decoded -->\n\nwhere (∆ B k ) k ≥ 0 are i.i.d. standard Gaussian random variables and ˆ Cov is the empirical covariance over the sample.\n\nWhen F ( p ) := KL( p ref | p ), e.g., when we want to regularize towards a reference distribution p ref with sample access, for ℓ ref ( θ ) = F ( π ⋆ ( θ )), we have\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nthus\n\nLeveraging the explicit formula (6) for π ⋆ ( θ ), we obtain\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the third equality uses that p ref [ x ]d x = 1. This suggests taking\n\n<!-- formula-not-decoded -->\n\nThe terms in this gradient can be estimated: for a model V ( · , θ ), the gradient function w.r.t θ can be obtained by automatic differentiation. Samples from p ref are available by assumption, and samples from π ⋆ ( θ ) can be replaced by the X k in joint optimization as above. We recover the formula for contrastive learning of energy-based model to data from p ref (Gutmann and Hyv¨ arinen, 2012).\n\nThis can also be used for finetuning, combining a reward R and a KL term, with F ( p ) = -λ E X ∼ p [ R ( x )] + β KL( p || p ref ). The sample version of Algorithm 2 then can be written\n\n<!-- formula-not-decoded -->\n\nwhere (∆ B k ) k ≥ 0 are i.i.d. standard Gaussian random variables, ˆ Cov is the empirical covariance over the sample, and ˜ X ( j ) k ∼ p ref .\n\n## A.3 Adjoint method and denoising diffusions\n\nWe explain how to use the adjoint method to backpropagate through differential equations, and apply this to derive instantiations of Algorithm 3 for denoising diffusions.\n\nODE sampling. We begin by recalling the adjoint method in the ODE case (Pontryagin, 1987). Consider the ODE d Y t = µ ( t, Y t , θ )d t integrated between 0 and some T &gt; 0. For some differentiable function R : R d → R , the derivative of R ( Y T ) with respect to θ can be computed by the adjoint method. More precisely, it is equal to G T defined by\n\n<!-- formula-not-decoded -->\n\nNote that sometimes the adjoint equations are written with a reversed time index ( t ′ = T -t ), which is not the formalism we adopt here.\n\nIn the setting of denoising diffusion presented in Section 2.2, we are not interested in computing the derivative of a function of a single realization of the ODE, but of the expectation over Y T ∼ π ⋆ ( θ ) of the derivative of R ( Y T ) with respect to θ . In other words, we want to compute ∇ ℓ ( θ ) = ∇ ( F ◦ π ⋆ )( θ ), where F ( p ) = E x ∼ p [ R ( x )]. Rewriting the equations above in this case, we obtain that G T defined by\n\n<!-- formula-not-decoded -->\n\nis an unbiased estimator of ∇ ℓ ( θ ). Recalling Definition 3.1, this means that we can take Γ( p, θ ) := G T defined by\n\n<!-- formula-not-decoded -->\n\nThis is exactly the definition of Γ given in Section 3.2. We apply this to the case of denoising diffusions, where is µ given by (8). To avoid a notation clash between the number of iterations T = M of the sampling algorithm, and the maximum time T of the ODE in (8), we rename the latter to T horizon . A direct instantiation of Algorithm 3 with an Euler solver is the following algorithm.\n\n## Algorithm 4 Implicit Diff. optimization, denoising diffusions with ODE sampling\n\n<!-- formula-not-decoded -->\n\nSeveral comments are in order. First, the dynamics of Y ( M ) k in the previous M steps, from Y (0) k -M to Y ( M -1) k -1 , uses the M previous values of the parameter θ k -M , . . . , θ k -1 . This means that Y ( M ) k does not correspond to the result of sampling with any given parameter θ , since we are at the same time performing the sampling process and updating θ .\n\nBesides, the computation of Γ( p, θ ) is the outcome of an iterative process, namely calling an ODE solver. Therefore, it is also possible to use the same queuing trick as for sampling iterations to decrease the cost of this step by leveraging parallelization. For completeness, the variant is given below.\n\nAlgorithm 5 Implicit Diff. optimization, denoising diffusions with ODE sampling, variant with a double queue\n\n```\ninput θ 0 ∈ R p , p 0 ∈ P input P M = [ Y (0) 0 , . . . , Y ( M ) 0 ] ∼ N (0 , 1) ⊗ ( m × d ) for k ∈ { 0 , . . . , K -1 } (joint single loop) do Y (0) k +1 ∼ N (0 , 1) Z (0) k +1 ← Y ( M ) k A (0) k +1 ←∇ R ( Z (0) k +1 ) G (0) k +1 ← 0 parallel Y ( m +1) k +1 ← Y ( m ) k + 1 M µ ( mT horizon M , Y ( m ) k , θ k ) for m ∈ [ M -1] parallel Z ( m +1) k +1 ← Z ( m ) k -1 M µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] parallel A ( m +1) k +1 ← A ( m ) k + 1 M ( A ( m ) k ) ⊺ ∇ 2 µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] parallel G ( m +1) k +1 ← G ( m ) k + 1 M ( G ( m ) k ) ⊺ ∇ 2 µ ( mT horizon M , Z ( m ) k , θ k ) for m ∈ [ M -1] θ k +1 ← θ k -ηG ( M ) k output θ K\n```\n\nSecond, each variable Y ( m ) k consists of a single sample of R d . The algorithm straightforwardly extends when each variable Y ( m ) k is a batch of samples. Finally, we consider so far the case where the size of the queue M is equal to the number of sampling steps T . We give below the variant of Algorithm 4 when M = T but M divides T . Taking M from 1 to T balances between a single-loop and a nested-loop algorithm.\n\n̸\n\n̸\n\n## Algorithm 6 Implicit Diff. optimization, denoising diffusions with ODE sampling, M = T , M divides T\n\n```\ninput θ 0 ∈ R p , p 0 ∈ P input P M = [ Y (0) 0 , . . . , Y ( M ) 0 ] ∼ N (0 , 1) ⊗ ( m × d ) for k ∈ { 0 , . . . , K -1 } do Y (0) k +1 ∼ N (0 , 1) parallel Y ( m +1) k +1 / 2 ← Y ( m ) k for m ∈ [ M -1] for t ∈ { 0 , . . . , T /M -1 } in parallel for m ∈ [ M -1] do Y ( m +1) k +1 / 2 ← Y ( m ) k +1 / 2 + 1 T µ (( m M + t T ) T horizon , Y ( m ) k +1 / 2 , θ k ) parallel Y ( m +1) k +1 ← Y ( m +1) k +1 / 2 for m ∈ [ M -1] Z (0) k ← Y ( M ) k A (0) k ←∇ R ( Z (0) k ) G (0) k ← 0 for t ∈ { 0 , . . . , T -1 } do Z ( t +1) k ← Z ( t ) k -1 T µ ( tT horizon T , Z ( t ) k , θ k ) A ( t +1) k ← A ( t ) k + 1 T ( A ( t ) k ) ⊺ ∇ 2 µ ( tT horizon T , Z ( t ) k , θ k ) G ( t +1) k ← G ( t ) k + 1 T ( G ( t ) k ) ⊺ ∇ 2 µ ( tT horizon T , Z ( t ) k , θ k ) θ k +1 ← θ k -ηG ( T ) k output θ K\n```\n\nAlgorithm 5 extends to this case similarly.\n\nSDE sampling. The adjoint method is also defined in the SDE case (Li et al., 2020). Consider the SDE\n\n<!-- formula-not-decoded -->\n\nintegrated between 0 and some T &gt; 0. This setting encompasses the denoising diffusion SDE (7) with the appropriate choice of µ . For some differentiable function R : R d → R and for a given realization ( Y t ) 0 ≤ t ≤ T of the\n\nSDE, the derivative of R ( Y T ) with respect to θ is equal to G T defined by\n\n<!-- formula-not-decoded -->\n\nThis is a similar equation as in the ODE case. The main difference is that it is not possible to recover Y T -t only from the terminal value of the path Y T , but that we need to keep track of the randomness from the Brownian motion B t . Efficient ways to do so are presented in Li et al. (2020); Kidger et al. (2021). In a nutshell, they consist in only keeping in memory the seed used to generate the Brownian motion, and recomputing the path from the seed.\n\nUsing the SDE sampler allows us to incorporate a KL term in the reward. Indeed, consider the SDE (13) for two different parameters θ 1 and θ 2 , with associated variables Y 1 t and Y 2 t . Then, by Girsanov's theorem (see Protter, 2005, Chapter III.8, and Tzen and Raginsky, 2019 for use in a similar context), the KL divergence between the paths Y 1 t and Y 2 t is\n\n<!-- formula-not-decoded -->\n\nwhere q 1 t denotes the distribution of Y 1 t . This term can be (stochastically) estimated at the same time as the SDE (13) is simulated, by appending a new coordinate to Y t (and to µ ) that integrates (15) over time. Then, adding the KL in the reward is as simple as adding a linear term in ˜ R : R d +1 → R , that is, ˜ R ( x ) = R ( x [: -1]) + x [ -1] (using Numpy notation, where ' -1' denotes the last index). The same idea is used in Dvijotham et al. (2023) to incorporate a KL term in reward finetuning of denoising diffusion models. Finally, note that, if we had at our disposal a reward R that indicates if an image is 'close' to p data (for instance implemented by a neural network), we could use our algorithm to train from scratch a denoising diffusion.\n\n## A.4 Implicit differentiation\n\nFinite dimension. Take g : R m × R p → R a continuously differentiable function. Then x ⋆ ( θ ) = argmin g ( · , θ ) implies a stationary point condition ∇ 1 g ( x ⋆ ( θ 0 ) , θ 0 ) = 0. In this case, it is possible to define and analyze the function x ⋆ : R p → R m and its variations. Note that this generalizes to the case where x ⋆ ( θ ) can be written as the root of a parameterized system.\n\nMore precisely, the implicit function theorem (see, e.g., Griewank and Walther, 2008; Krantz and Parks, 2002, and references therein) can be applied. Under differentiability assumptions on g , for ( x 0 , θ 0 ) such that ∇ 1 g ( x 0 , θ 0 ) = 0 with a continuously differentiable ∇ 1 g , and if the Hessian ∇ 1 , 1 g evaluated at ( x 0 , θ 0 ) is a square invertible matrix, then there exists a function x ⋆ ( · ) over a neighborhood of θ 0 satisfying x ⋆ ( θ 0 ) = x 0 . Furthermore, for all θ in this neighborhood, we have that ∇ 1 g ( x ⋆ ( θ ) , θ ) = 0 and its Jacobian ∂x ⋆ ( θ ) exists. It is then possible to differentiate with respect to θ both sides of the equation ∇ 1 g ( x ⋆ ( θ 0 ) , θ 0 ) = 0, which yields a linear equation satisfied by this Jacobian\n\n<!-- formula-not-decoded -->\n\nThis formula can be used for automatic implicit differentiation, when both the evaluation of the derivatives in this equation and the inversion of the linear system can be done automatically Blondel et al. (2022).\n\nExtension to space of probabilities. When G : P × R p → R and π ⋆ ( θ ) = argmin G ( · , θ ) as in (3), under assumptions on differentiability and uniqueness of the solution on G , this can also be extended to a distribution setting. We write here the infinite-dimensional equivalent of the above equations, involving derivatives or variations over the space of probabilities, and refer to Ambrosio et al. (2005) for more details.\n\nFirst, we have that\n\n<!-- formula-not-decoded -->\n\nwhere F ′ ( p ) : X → R denotes the first variation of F at p ∈ P (see Definition B.1). This yields ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ) with\n\n<!-- formula-not-decoded -->\n\nwhere γ ( p, θ ) is the solution of the linear system\n\n<!-- formula-not-decoded -->\n\nAlthough this gives us a general way to define gradients of π ⋆ ( θ ) with respect to θ , solving this linear system is generally not feasible. One exception is when sampling over a finite state space X , in which case P is finite-dimensional, and the integrals boil down to matrix-vector products.\n\n## B THEORETICAL ANALYSIS\n\n## B.1 Langevin with continuous flow\n\n## B.1.1 Additional definitions\n\nNotations. We denote by P 2 ( R d ) the set of probability measures on R d with bounded second moments. Given a Lebesgue measurable map T : X → X and µ ∈ P 2 ( X ), T # µ is the pushforward measure of µ by T . For any µ ∈ P 2 ( R d ), L 2 ( µ ) is the space of functions f : R d → R such that ∫ ∥ f ∥ 2 dµ &lt; ∞ . We denote by ∥ · ∥ L 2 ( µ ) and ⟨· , ·⟩ L 2 ( µ ) respectively the norm and the inner product of the Hilbert space L 2 ( µ ). We consider, for µ, ν ∈ P 2 ( R d ), the 2-Wasserstein distance W 2 ( µ, ν ) = inf s ∈S ( µ,ν ) ∫ ∥ x -y ∥ 2 ds ( x, y ), where S ( µ, ν ) is the set of couplings between µ and ν . The metric space ( P 2 ( R d ) , W 2 ) is called the Wasserstein space.\n\nLet F : P ( R d ) → R + a functional.\n\nDefinition B.1. Fix ν ∈ P ( R d ). If it exists, the first variation of F at ν is the function F ′ ( ν ) : R d → R s. t. for any µ ∈ P ( R d ), with ξ = µ -ν :\n\n<!-- formula-not-decoded -->\n\nand is defined uniquely up to an additive constant.\n\nWe will extensively apply the following formula:\n\n<!-- formula-not-decoded -->\n\nWe will also rely regularly on the definition of a Wasserstein gradient flow, since Langevin dynamics correspond to a Wasserstein gradient flow of the Kullback-Leibler (KL) divergence Jordan et al. (1998). A Wasserstein gradient flow of F Ambrosio et al. (2005) can be described by the following continuity equation:\n\n<!-- formula-not-decoded -->\n\nwhere F ′ denotes the first variation. Equation (17) holds in the sense of distributions (i.e. the equation above holds when integrated against a smooth function with compact support), see (Ambrosio et al., 2005, Chapter 8). In particular, if F = KL( ·| π ) for π ∈ P 2 ( R d ), then ∇ W 2 F ( µ ) = ∇ log( µ / π ). In this case, the corresponding continuity equation is known as the Fokker-Planck equation, and in particular it is known that the law p t of Langevin dynamics: √\n\n<!-- formula-not-decoded -->\n\nsatisfies the Fokker-Planck equation (Pavliotis, 2016, Chapter 3).\n\n## B.1.2 Gaussian mixtures satisfy the Assumptions\n\nWe begin by a more formal statement of the result alluded to in Section 4.1.\n\nProposition B.2. Let\n\n<!-- formula-not-decoded -->\n\nfor some fixed z 1 , . . . , z p ∈ R d and where\n\nThen\n\n<!-- formula-not-decoded -->\n\nis a shifted version of the logistic function for some η ∈ (0 , 1) . Then Assumptions 4.1 and 4.2 hold.\n\nProof. Assumption 4.1 holds since a mixture of Gaussians is Log-Sobolev with a bounded constant (Chen et al., 2021a, Corollary 1). Note that the constant deteriorates as the modes of the mixture get further apart.\n\nFurthermore, Assumption 4.2 holds since, for all θ ∈ R p and x ∈ R d ,\n\n<!-- formula-not-decoded -->\n\n## B.1.3 Proof of Proposition 4.4\n\nIn the case of the functions Γ defined by (9)-(10), we see that Γ is bounded under Assumption 4.2 and when the reward R is bounded. The Lipschitz continuity can be obtained as follows. Consider for instance the case of (10) where Γ( p, θ ) is given by\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the first inequality comes from the fact that the total variation distance is an integral probability metric generated by the set of bounded functions, and the second inequality is Pinsker's inequality (Tsybakov, 2009, Lemma 2.5). The first case of Section A.2 unfolds similarly.\n\n## B.1.4 Proof of Theorem 4.5\n\nThe dynamics (11) can be rewritten equivalently on P 2 ( R d ) and R p as\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere G ( p, θ ) = KL( p || π ∗ θ ), see Appendix B.1.1. The Wasserstein gradient in (18) is taken with respect to the first variable of G .\n\nEvolution of the loss. Recall that Γ satisfies by Definition 3.1 that ∇ ℓ ( θ ) = Γ( π ⋆ ( θ ) , θ ). Thus we have, by (19),\n\n<!-- formula-not-decoded -->\n\nThen, by Assumption 4.3,\n\n<!-- formula-not-decoded -->\n\nUsing ab ≤ 1 2 ( a 2 + b 2 ), we get\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence of p t from π ∗ ( θ t ) . Recall that\n\n<!-- formula-not-decoded -->\n\nThus, by the chain rule formula (16),\n\n<!-- formula-not-decoded -->\n\nFrom an integration by parts, using (18) and by Assumption 4.1, we have\n\n<!-- formula-not-decoded -->\n\nMoving on to b , we have\n\n<!-- formula-not-decoded -->\n\nBy the chain rule and (19), we have for x ∈ X\n\n<!-- formula-not-decoded -->\n\nUsing π ⋆ ( θ ) ∝ e -V ( θ, · ) (with similar computations as for ∇ ℓ ref in Section A.2), we have\n\n<!-- formula-not-decoded -->\n\nand\n\nThis yields\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the last step uses Assumptions 4.2 and 4.3. Putting everything together, we obtain\n\n<!-- formula-not-decoded -->\n\nUsing Gr¨ onwall's inequality (Pachpatte and Ames, 1997) to integrate the inequality, the KL divergence can be bounded by\n\n<!-- formula-not-decoded -->\n\nComing back to (20), we get\n\n<!-- formula-not-decoded -->\n\nIntegrating between 0 and T , we have\n\n<!-- formula-not-decoded -->\n\nSince ε t is decreasing, we can bound ε t by ε T in the first integral and rearrange terms to obtain\n\n<!-- formula-not-decoded -->\n\nRecall that, by assumption of the Theorem, ε t = min(1 , 1 √ t ). Thus Tε T = √ T , and the first term is bounded by a constant times T -1 / 2 . It is also the case of the second term since ∫ T 0 ε t e -2 µt d t is converging. Let us now estimate the magnitude of the last term. Let T 0 ≥ 2 (depending only on µ ) such that ln( T 0 ) 2 µ ≤ T 0 2 . For t ≥ T 0 , let α ( t ) := t -ln t 2 µ . We have, for t ≥ T 0 ,\n\n<!-- formula-not-decoded -->\n\nwhere in the last inequality we used that α ( t ) ≥ t/ 2 and ε t is decreasing. For t &lt; T 0 , we can simply bound the integral ∫ t 0 ε s e µ ( s -t ) ds by ε 0 T 0 . We obtain\n\n<!-- formula-not-decoded -->\n\nRecall that ε t = min(1 , 1 √ t ), and that T 0 ≥ 2. Thus\n\n<!-- formula-not-decoded -->\n\nThe first two integrals are converging when T →∞ and the last integral is O (ln T ). Plugging this into (21), we finally obtain the existence of a constant c &gt; 0 such that\n\n<!-- formula-not-decoded -->\n\n## B.2 Langevin with discrete flow-proof of Theorem 4.7\n\nWe take\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence of p k +1 from π ⋆ ( θ k +1 ) . Recall that p k is the law of X k . We leverage similar ideas to the proof of (Cheng and Bartlett, 2018, Theorem 3), exploiting the Log Sobolev inequality to bound the KL along one Langevin Monte Carlo iteration (an approach that was further streamlined in (Vempala and\n\nWibisono, 2019, Lemma 3)). The starting point is to notice that one Langevin Monte Carlo iteration can be equivalently written as a continuous-time process over a small time interval [0 , γ k ]. More precisely, let\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nand x t satisfying the SDE\n\nThen, following the proof of (Cheng and Bartlett, 2018, Theorem 3), p k +1 has the same distribution as the output at time γ := γ k of the continuity equation\n\n<!-- formula-not-decoded -->\n\nwhere ρ 0 | t is the conditional distribution of x 0 given x t . Similarly, θ k +1 is equal to the output at time γ of\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have:\n\nWe first bound b similarly to the proof of Theorem 4.5, under Assumptions 4.2 and 4.3:\n\n<!-- formula-not-decoded -->\n\nThen we write a as\n\n<!-- formula-not-decoded -->\n\nDenote the Fisher divergence by\n\n<!-- formula-not-decoded -->\n\nThe first term a 1 is equal to -FD( ρ t || π ⋆ ( ϑ t )). To bound the second term a 2 , denote ρ 0 t the joint distribution of ( x 0 , x t ). Then\n\n<!-- formula-not-decoded -->\n\nUsing ⟨ a, b ⟩ ≤ ∥ a ∥ 2 + 1 4 ∥ b ∥ 2 and recalling that x ↦→∇ 1 V ( x, θ ) is L X -Lipschitz for all θ ∈ R p by Assumption 4.6,\n\n<!-- formula-not-decoded -->\n\nProceeding similarly for a 3 , we obtain\n\n<!-- formula-not-decoded -->\n\nSince θ ↦→∇ 1 V ( x, θ ) is L Θ -Lipschitz for all x ∈ R d by Assumption 4.6, we get\n\n<!-- formula-not-decoded -->\n\nMoreover, by (23) and under Assumption 4.3, we have ∥ ϑ t -θ k ∥ 2 = t 2 ε 2 k ∥ Γ( µ k , θ k ) ∥ 2 ≤ t 2 ε 2 k C 2 , which yields\n\n<!-- formula-not-decoded -->\n\nPutting everything together,\n\n<!-- formula-not-decoded -->\n\nwhere the last inequality uses Assumption 4.1 and where the two last terms in the r.h.s. can be seen as additional bias terms compared to the analysis of (Cheng and Bartlett, 2018, Theorem 3) and (Vempala and Wibisono, 2019, Lemma 3). Let us now bound E ( x 0 ,x t ) ∼ ρ 0 t ∥ x t -x 0 ∥ 2 . Recall that x t d = x 0 -t ∇ 1 V ( x 0 , θ k ) + √ 2 tz 0 , where z 0 ∼ N (0 , I ) is independent of x 0 . Then\n\n<!-- formula-not-decoded -->\n\nFinally, since x ↦→∇ 1 V ( x, θ ) is L X -Lipschitz for all x ∈ R d by Assumption 4.6, and under Assumption 4.1, we get by (Vempala and Wibisono, 2019, Lemma 12) that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nRecall that we want to integrate t between 0 and γ . For t ≤ γ , we have\n\n<!-- formula-not-decoded -->\n\nsince L X γ ≤ 1, L Θ γ 2 ≤ 1 and ε k ≤ 1 by (22). Denote by C 1 the second term and C 2 the sum of the last two terms. Then, by Gr¨ onwall's inequality (Pachpatte and Ames, 1997),\n\n<!-- formula-not-decoded -->\n\nSince µt ≤ µγ ≤ 1 by (22), we have e µt ≤ 1 + 2 µγ , and\n\n<!-- formula-not-decoded -->\n\nAll in all,\n\nSince γ ≤ µ 4 L 2 X by (22), we have 8 L 4 X γ 3 µ ≤ µγ 2 , and\n\n<!-- formula-not-decoded -->\n\nWe therefore obtain, by evaluating at t = γ and renaming p k +1 = ρ γ , π ⋆ ( θ k +1 ) = π ⋆ ( ϑ γ ), p k = ρ 0 , and γ k = γ ,\n\n<!-- formula-not-decoded -->\n\nBounding the KL over the whole dynamics. Let C 3 := (1 + µγ k 2 ) e -µγ k . We have C 3 &lt; 1, and by summing and telescoping,\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe have by using e x ≥ 1 + x and C 3 ≤ 1. Thus\n\n<!-- formula-not-decoded -->\n\nReplacing C 2 by its value,\n\n<!-- formula-not-decoded -->\n\nSince e x ≥ 1 + x , C 3 ≤ e -µγ k / 2 , and\n\n<!-- formula-not-decoded -->\n\nWe obtain three terms in our bound that have different origins. The first term corresponds to an exponential decay of the KL divergence at initialization. The second term is linked to the discretization error, and is proportional to γ k . The third term is due to the fact that π ⋆ ( θ k ) is moving due to the outer problem updates. It is proportional to the ratio of learning rates ε k .\n\nEvolution of the loss. By Assumption 4.6, the loss ℓ is L -smooth, and recall that θ k +1 = θ k -γ k ε k Γ( p k , θ k ). We have\n\n<!-- formula-not-decoded -->\n\nby Assumption 4.3. Furthermore,\n\n<!-- formula-not-decoded -->\n\nThus\n\n<!-- formula-not-decoded -->\n\nby Assumption 4.3. Using ab ≤ 1 2 ( a 2 + b 2 ), we obtain\n\n<!-- formula-not-decoded -->\n\nConclusion. Summing and telescoping,\n\n<!-- formula-not-decoded -->\n\nLower bounding γ k by γ K and ε k by ε K in the first sum, then reorganizing terms, we obtain\n\n<!-- formula-not-decoded -->\n\nBounding the KL divergence by (24), we get\n\n<!-- formula-not-decoded -->\n\nBy definition (22) of γ k and ε k , we see that the first and last sums are converging, and the middle sums are O (ln K ). Therefore, we obtain\n\n<!-- formula-not-decoded -->\n\nfor some c &gt; 0 depending only on the constants of the problem.\n\n## B.3 Denoising diffusion\n\nOur goal is first to find a continuous-time equivalent of Algorithm 3. To this aim, we first introduce a slightly different version of the algorithm where the queue of M versions of p k is not present at initialization, but constructed during the first M steps of the algorithm. As a consequence, θ changes for the first time after M steps of the algorithm, when the queue is completed and the M -th element of the queue has been processed through all the sampling steps.\n\n## Algorithm 7 Implicit Diff. optimization, finite time (no warm start)\n\n<!-- formula-not-decoded -->\n\nIn order to obtain the continuous-time equivalent of Algorithm 7, it is convenient to change indices defining p ( m ) k . Note that in the update of p ( m ) k in the algorithm, the quantity m -k is constant. Therefore, denoting j = m -k , the algorithm above is exactly equivalent to\n\n<!-- formula-not-decoded -->\n\nIt is then possible to translate this algorithm in a continuous setting in the case of denoising diffusions. We obtain\n\n<!-- formula-not-decoded -->\n\nLet us choose the score function s θ as in Section 4.3. The backward equation (7) then writes\n\n<!-- formula-not-decoded -->\n\nWe now turn our attention to the computation of Γ. Recall that Γ should satisfy Γ( π ⋆ ( θ ) , θ ) = ∇ ℓ ( θ ), where π ⋆ ( θ ) is the distribution of Y T and ℓ ( θ ) = E Y ∼ π ⋆ ( θ ) ( L ( Y )) for some loss function L : R → R . To this aim, for a given realization of the SDE (26), let us compute the derivative of L ( Y T ) with respect to θ using the adjoint method (14). We have ∇ 2 µ ( T -t, Y T -t , θ ) = -1, hence d A t d t = -1, and A t = L ′ ( Y T ) e -t . Furthermore, ∇ 3 µ ( T -t, Y T -t , θ ) = 2 e -t . Thus\n\n<!-- formula-not-decoded -->\n\nAs a consequence,\n\nThis prompts us to define\n\n<!-- formula-not-decoded -->\n\n2\n\nfor L defined by L ( x ) = ( x -θ target ) as in Section 4.3. Note that, in this case, Γ depends only on p and not on θ . Replacing s θ and Γ by their values in (25), we obtain the coupled differential equations in Y and θ\n\n<!-- formula-not-decoded -->\n\nWe can now formalize Proposition 4.8, with the following statement.\n\nProposition B.3. Consider the dynamics (27) . Then\n\n<!-- formula-not-decoded -->\n\nProof. Let us first compute the expectation of Y τ T . To this aim, denote Z τ t = e τ Y τ t . Then we have\n\n<!-- formula-not-decoded -->\n\nSince E ( Z 0 t ) = E ( Y 0 t ) = 0, we obtain that E ( Z T t ) = 2 ∫ T 0 θ t + τ e 2( τ -T ) d τ , and\n\n<!-- formula-not-decoded -->\n\nTherefore, we obtain the following evolution equation for θ when t ≥ T :\n\n<!-- formula-not-decoded -->\n\nBy the change of variable τ ← T -τ in the integral, we have, for t ≥ T ,\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nLet us introduce the auxiliary variable ψ t = ∫ T 0 θ t -τ e -2 τ d τ . We have ˙ θ t = -η (2 ψ t -θ target ), and\n\n<!-- formula-not-decoded -->\n\nRecall that θ t is constant equal to θ 0 for t ∈ [0 , T ]. Therefore, for t ∈ [ T, 2 T ], ξ := ( θ, ψ ) satisfies the first order linear ODE with constant coefficients\n\n<!-- formula-not-decoded -->\n\nFor η &gt; 0, A is invertible, and\n\n<!-- formula-not-decoded -->\n\nHence the linear ODE has solution, for t ∈ [ T, 2 T ],\n\n<!-- formula-not-decoded -->\n\nThus\n\n<!-- formula-not-decoded -->\n\nA straightforward computation shows that\n\n<!-- formula-not-decoded -->\n\nand that A has eigenvalues with negative real part. Putting everything together, we obtain\n\n<!-- formula-not-decoded -->\n\nFinally, recall that we have π ⋆ ( θ ) = N ( θ (1 -e -2 T ) , 1). Thus π ⋆ ( θ 2 T ) = N ( µ 2 T , 1) with µ 2 T = θ target + O ( e -T ).\n\n## C EXPERIMENTAL DETAILS\n\nWe provide here details about the experiments that we have presented in Section 5.\n\n## C.1 Langevin processes\n\nWe provide in this section details about our experiments on Langevin processes. In Section C.1.1, we do so for the experiment described in Section 5.1, where the reward is an explicit function R ( · ). In Section C.1.2, we show additional experiments showing that Implicit Diffusion can also be used to 'train from scratch' a model: in this case the reward is a negative KL term that is evaluated from samples of a target distribution. We present results in several parametrization settings.\n\n## C.1.1 Reward training\n\nWe consider a parameterized family of potentials for x ∈ R 2 and θ ∈ R 6 defined by\n\n<!-- formula-not-decoded -->\n\nwhere the µ i ∈ R 2 are the six vertices of a regular hexagon and σ is the softmax function mapping R 6 to the unit simplex. In this setting, for any θ ∈ R 6 ,\n\n<!-- formula-not-decoded -->\n\nFigure 10: Confidence intervals for metrics for reward training of Langevin processes. Left : Evolution of the reward. Right : Evolution of the log-likelihood.\n\n<!-- image -->\n\nwhere Z is an absolute renormalization constant that is independent of θ . This fact simplifies drawing contour lines, but we do not use this prior knowledge in our algorithms, and only use calls to functions ∇ 1 V ( · , θ ) and ∇ 2 V ( · , θ ) for various θ ∈ R 6 .\n\nWe run six sampling algorithms, all initialized with p 0 = N (0 , I 2 ). For all of them we generate a batch of variables X ( i ) of size 1 , 000, all initialized independently with X ( i ) 0 ∼ N (0 , I 2 ). The sampling and optimization steps are realized in parallel over the batch. The samples are represented after K = 5 , 000 steps of each algorithm in Figure 5, and used to compute the values of reward and likelihood reported in Figure 6. We also display in Figure 12 the dynamics of the probabilities throughout these algorithms.\n\nFigure 11: Comparison between Implicit Diffusion, nested loop algorithm and unrolling algorithm, for various number of inner steps T . The x-axis is the total number of gradient evaluations (roughly equal to the number of optimization steps multiplied by the number of inner loop steps T ). Left : Evolution of the reward. Right : Evolution of the log-likelihood.\n\n<!-- image -->\n\nWe provide here additional details of and motivation for these algorithms, denoted by the colored markers that represent them in these figures.\n\n- Langevin θ 0 ( ■ ): This is the discrete-time process (a Langevin Monte Carlo process) approximating a Langevin diffusion with potential V ( · , θ 0 ) for fixed θ 0 := (1 , 0 , 1 , 0 , 1 , 0). There is no reward here; the time-continuous Langevin process converges to π ⋆ ( θ 0 ), which has some symmetries. It can be thought of as a pretrained model, and the Langevin sampling algorithm as an inference-time generative algorithm.\n- -Implicit Diffusion ( ⋆ ): We run the infinite-time horizon version of our method (Algorithm 2), aiming to minimize ℓ ( θ ) := F ( π ⋆ ( θ )) for F ( p ) = -E X ∼ p [ R ( X )] with R ( x ) = 1 ( x 1 &gt; 0) exp ( -∥ x -µ ∥ 2 ) where µ = (1 , 0 . 95). This algorithm yields both a sample ˆ p K and parameters θ opt after K steps, and can be thought of as jointly sampling and reward finetuning.\n- -Nested loop ( ♦ ): We run Algorithm 1 with T inner sampling steps for each gradient step. For T = 1, this is exactly Implicit Diffusion. For T ≫ 1, it means we compute nearly perfectly π ⋆ ( θ t ) at each optimization step.\n- -Unrolling through the last step of sampling ( ▲ ): For each optimization step, we perform T sampling steps, then differentiate through the last step of sampling by automatic differentiation. This is akin to a stop gradient method. The learning rate here is chosen as 2 γ θ /γ X to improve its performance, for a fair comparison. Recent studies show that differentiating through the last sampling step is an efficient and theoretically-grounded method in bilevel optimization (Bolte et al., 2023). It has been applied successfully to denoising diffusions (Clark et al., 2024).\n- Langevin θ 0 + R ( ▼ ): This is the discretization of a Langevin diffusion with reward-guided potential V ( · , θ 0 ) -λR smooth , where R smooth is a smoothed version of R (replacing the indicator by a sigmoid). Using this approach is different from finetuning: it proposes to modify the sampling algorithm, and does not yield new parameters θ . This is akin to guidance of generative models (Dhariwal and Nichol, 2021). Note that this requires a differentiable reward R smooth , contrarily to our approach that handles non-differentiable rewards.\n- Langevin θ opt - post Implicit Diffusion ( ): This is a discrete-time process approximating a Langevin diffusion with potential V ( · , θ opt ), where θ opt is the outcome of reward training by our algorithm. This can be thought of as doing inference with the new model parameters, post reward training with Implicit Diffusion.\n\nAs mentioned in Section 5.1, this setting illustrates the advantage of our method, which allows the efficient optimization of a function over a constrained set of distribution, without overfitting outside this class. We display in Figure 12 snapshots throughout some selected steps of these six algorithms (in the same order and with the same colors as indicated above). We observe that the dynamics of Implicit Diffusion are slower than those of Langevin processes (sampling), which can be observed also in the metrics reported in Figure 6. The reward and log-likelihood change slowly, plateauing several times: when θ k in this algorithm is initially close to θ 0 , the distribution gets closer to π ⋆ ( θ 0 ) (steps 0-100). It then evolves towards another distribution (steps 1000-2500), after θ has been affected by accurate gradient updates, before converging to π ⋆ ( θ opt ). The two-timescale dynamics is by design: the sampling dynamics are much faster, aiming to quickly lead to an accurate evaluation of gradients with respect to θ k . This corresponds to our theoretical setting where ε k ≪ 1. To complement the comparisons between our algorithm and other baselines included in Section 5.1, we also provide in Figure 11 a comparison between Implicit Diffusion, the nested loop and unrolling approaches, in terms of reward and log-likelihood optimization, as a function of the number of gradient evaluations (i.e., number of sampling steps), rather than number of optimization steps. Again, it is apparent that the algorithmic cost of doing several steps ( T &gt; 1) of inner loop is much higher than the small improvement obtained by a better estimate of the gradients. Finally, the confidence intervals in Figure 6 are computed by performing 10 independent repetitions of the experiment, and reporting the largest and lowest metrics across the 10 repetitions, at each time step. For readability, Figure 10 shows the same plot with confidence intervals only (without plotting the average value).\n\nFigure 12: Dynamics of samples for four sampling algorithms after different time steps (for instance, the first column is after one step). First row: Langevin θ 0 ( ) with π ⋆ ( θ 0 ) contour lines. Second: Implicit Diffusion ( ) with π ⋆ ( θ opt ) contour lines. Third: Nested loop algorithm with T = 100 ( ). Fourth: Unrolling through the last step of sampling with T = 100 ( ). Fifth: Langevin θ 0 + smoothed Reward ( ). Sixth: Langevin θ opt ( ).\n\n<!-- image -->\n\n## C.1.2 Training from scratch\n\nWe present in Figure 13 a variant of this experiment where we start from a model generating a standard Gaussian, and our goal is to learn to generate a mixture of several Gaussians. For this, comparing with the setup presented above, we add a 7th potential well at the origin, and choose at initialization θ 0 = ( -7 , -7 , -7 , -7 , -7 , -7 , 11). This means that the distribution at initialization is extremely close to being a standard Gaussian, as can be seen in the top-right plot of Figure 13a. The target is θ ∗ = (1 . 5 , 0 , 1 . 5 , 0 , 1 . 5 , 0 , 0). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. This KL admits explicit gradients (see Section 3.2) which can be evaluated with samples of the target distribution. We train for K = 40 , 000 steps with a batch of size 1 , 000. Learning rates are γ X = 2 . 5 · 10 -2 and γ θ = 7 · 10 -3 .\n\nTraining from scratch with a full Gaussian mixture model parameterization. We present in Figure 14 a variant of this experiment where we now parameterize the means and covariances of the Gaussians in addition to the weights of the mixture. More precisely, we consider the potential\n\n<!-- formula-not-decoded -->\n\nwhere σ is still the softmax function mapping R 2 to the unit simplex, and θ = { w, ( µ i , Σ i ) 1 ≤ i ≤ 2 } ∈ R 2 × ( R 2 , R 2 × 2 ) 2 . At initialization, the parameters are\n\n<!-- formula-not-decoded -->\n\nwhile our target is\n\n<!-- formula-not-decoded -->\n\nAs in the previous experiment, we use Implicit Diffusion where the reward is the KL between the target distribution and the current one. This KL admits explicit gradients (see Section 3.2) which can be evaluated with samples of the target distribution. We train for K = 40 , 000 steps with a batch of size 1 , 000. Learning rates are γ X = 5 · 10 -2 and γ θ = 5 · 10 -4 . We observe on the Figure that Implicit Diffusion is able to learn the target distribution.\n\n(a) Contour lines and samples from sampling algorithms. We start from a model generating a standard Gaussian (top-right figure), and our goal is to learn to generate a mixture of several Gaussians (top-left figure). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. We observe that Implicit Diffusion is able to learn the target distribution (bottom-left figure). After running Implicit Diffusion, it is easy to generate new samples that are close to the target distribution (bottom-right figure).\n\n<!-- image -->\n\n(b) Evolution of the reward and of the log-likelihood of the samples for the initial model, for the Implicit Diffusion algorithm, and for the trained model after Implicit Diffusion. The reward is the (opposite of the) KL between the target distribution and the current one, so a reward equal to zero means we learnt to reproduce the target distribution.\n\n<!-- image -->\n\nFigure 13: Optimizing through sampling with Implicit Diffusion to train from scratch an energy-based model (Langevin diffusion).\n\n(a) Contour lines and samples from sampling algorithms. We start from a model generating a mixture of two Gaussians (top-right figure), and our goal is to learn to generate another mixture with different means, covariances and weights (top-left figure). We use Implicit Diffusion where the reward is the KL between the target distribution and the current one. We observe that Implicit Diffusion is able to learn the target distribution (bottom-left figure). After running Implicit Diffusion, it is easy to generate new samples that are close to the target distribution (bottom-right figure).\n\n<!-- image -->\n\n(b) Evolution of the reward and of the log-likelihood of the samples for the initial model, for the Implicit Diffusion algorithm, and for the trained model after Implicit Diffusion. The reward is the (opposite of the) KL between the target distribution and the current one, so a reward equal to zero means we learnt to reproduce the target distribution.\n\n<!-- image -->\n\nFigure 14: Optimizing through sampling with Implicit Diffusion to train from scratch an energy-based model (Langevin diffusion). The potential V is a logsumexp of 2 quadratic forms, where the weights, centers, and the matrix of the forms are learnable parameters, so that the outcome distribution can be any mixture of 2 Gaussians.\n\n## C.2 Denoising diffusion models\n\nWe first provide additional experimental configurations that are common between both datasets before explaining details specific to each one.\n\nCommon details. The KL term in the reward is computed using Girsanov's theorem as explained in Appendix A.3. We use the Adam optimizer (Kingma and Ba, 2015), with various values for the learning rate (see e.g. Figure 7). The code was implemented in JAX (Bradbury et al., 2018). As mentioned in the main text, we use a U-Net model (Ronneberger et al., 2015).\n\nMNIST. We use an Ornstein-Uhlenbeck noise schedule, meaning that the forward diffusion is d X t = -X t d t + √ 2d B t (as presented in Section 2.2). We pretrain for 18k steps in 7 minutes on 4 TPUv2. For reward training, we train on a TPUv2 for 4 hours with a queue of size M = 4, T = 64 steps, and a batch size of 32. Further hyperparameters for pretraining and reward training are given respectively in Tables 1 and 2.\n\nTable 1: Hyperparameters for pretraining of denoising diffusion models on MNIST.\n\n| Name                                                        | Value                                                                   |\n|-------------------------------------------------------------|-------------------------------------------------------------------------|\n| Noise schedule Optimizer EMA decay Learning rate Batch size | Ornstein-Uhlenbeck Adam with standard hyperparameters 0 . 995 10 - 3 32 |\n\nTable 2: Hyperparameters for reward training of denoising diffusion models pretrained on MNIST.\n\n| Name                                                      | Value                                                           |\n|-----------------------------------------------------------|-----------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 256 Euler Ornstein-Uhlenbeck Adam with standard hyperparameters |\n\nCIFAR-10 and LSUN. For CIFAR-10, we pretrain for 500k steps in 30 hours on 16 TPUv2, reaching an FID score (Heusel et al., 2017) of 2 . 5. For reward training, we train on a TPUv3 for 9 hours with a queue of size M = 4 and T = 64 steps, and a batch size of 32. For LSUN, we pretrain for 13 hours, reaching a FID score of 2 . 26. For reward training, we train on a TPUv3 for 9 hours with a queue of size M = 2, T = 64 steps, and a batch size of 16. Further hyperparameters for pretraining and reward training are given respectively in Tables 3 and 4.\n\nTable 3: Hyperparameters for pretraining of denoising diffusion models on CIFAR-10 and LSUN.\n\n| Name                                                                                                                              | Value                                                                                                                         |\n|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer EMA decay Learning rate Batch size Number of samples for FID evaluation | 1 , 024 DDPM Cosine (Nichol and Dhariwal, 2021) Adam with β 1 = 0 . 9, β 2 = 0 . 99, ε = 10 - 12 0 . 9999 2 · 10 - 4 2048 50k |\n\nTable 4: Hyperparameters for reward training of denoising diffusion models pretrained on CIFAR-10 and LSUN.\n\n| Name                                                      | Value                                                                               |\n|-----------------------------------------------------------|-------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 1 , 024 Euler Cosine (Nichol and Dhariwal, 2021) Adam with standard hyperparameters |\n\nAdditional figures for MNIST. We report in Figure 16 metrics on the rewards and KL divergence with respect to the original distribution, in the case where λ &gt; 0. As in Figure 7, we observe the competition between the reward and the divergence with respect to the distribution after pretraining. The results are sensitive to the choice of hyperparameters. In particular, when the ratio λ/β is too small, we do not observe an improvement of the reward. Note that we did not perform extensive hyperparameter finetuning for these plots, so it is likely that better results could be obtained with more hyperparameter finetuning. We also display in Figures 15 some additional selected examples of samples generated by our denoising diffusion model with parameters θ k , at several steps k of our algorithm. Note that the random number generator system of JAX allows us, for illustration purposes, to sample for different parameters from the same seed. We take advantage of this feature to visualize the evolution of a given realization of the stochastic denoising process depending on θ .\n\nRecall that we consider\n\n<!-- formula-not-decoded -->\n\nwhere R ( x ) is the average value of all the pixels in x . The figures in the main text and in the appendix present samples for negative and positive λ , rewarding respectively darker and brighter images. We emphasize that these samples are generated for evaluation purposes. To generate the samples, at various steps of the optimization procedure, we run the full denoising process for the current value of the parameters. In particular, these samples are different from the ones used to perform the joint sampling and parameter updates in Implicit Diffusion.\n\nWe have purposefully chosen, for illustration purposes, samples for experiments with the highest magnitude of λ/β , i.e. those that favor reward optimization over proximity to the original distribution. As noted in Section 5, we observe qualitatively that reward training, while shifting some aspects of the distribution (here the average brightness), and necessarily diverging from the original pretrained model, manages to do so while retaining some important global characteristics of the dataset-even though the pretraining dataset is never observed during reward training. Since we chose to display samples from experiments with the most extreme incentives towards the reward, we observe that the similarity with the pretraining dataset can be forced to break down after a certain number of reward training steps. We also observe some mode collapse; we comment further on this point below.\n\nFigure 15: Reward training for a model pretrained on MNIST. The reward favors brighter images ( λ &gt; 0, β &gt; 0). Selected examples are shown coming from a single experiment with λ/β = 30. All digits are re-sampled at the same selected steps of the Implicit Diffusion algorithm.\n\n<!-- image -->\n\nFigure 16: Score function reward training with Implicit Diffusion pretrained on MNIST for various λ &gt; 0 (brighter). Left: Reward, average brightness of image. Right: Divergence w.r.t. the original pretrained distribution.\n\n<!-- image -->\n\nAdditional figures for CIFAR-10. We recall that we consider, for a model with weights θ 0 pretrained on CIFAR-10, the objective function\n\n<!-- formula-not-decoded -->\n\nwhere R ( x ) is the average over the red channel, minus the average of the other channels. We show in Figure 17 the result of the denoising process for some fixed samples and various steps of the reward training, for the experiment with the most extreme incentive towards the reward.\n\nWe observe as for MNIST some mode collapse, although less pronounced here. Since the pretrained model has been trained with label conditioning for CIFAR-10, it is possible that this phenomenon could be a byproduct of this pretraining feature.\n\nFigure 17: Reward training for a model pretrained on CIFAR-10. The reward favors redder images ( λ &gt; 0, β &gt; 0). Selected examples are shown coming from a single experiment with λ/β = 1 , 000. All images are re-sampled at the same selected steps of the Implicit Diffusion algorithm, as explained in Appendix C.2.\n\n<!-- image -->\n\nAdditional figures for LSUN. As for other datasets, we report in Figure 18 metrics on the rewards and KL divergence with respect to the original distribution.\n\nExtensions. We emphasize that our methodology covers a wide range of sampling processes and rewards. For instance, it could be applied to diffusions in discrete spaces for language modeling, or for more complex rewards such as aesthetic scores.\n\nFigure 18: Score function reward training with Implicit Diffusion pretrained on LSUN for various λ &gt; 0 (brighter). Left: Reward, average brightness of image. Right: Divergence w.r.t. the original pretrained distribution.\n\n<!-- image -->\n\n## D ADDITIONAL RELATED WORK\n\nReward finetuning of denoising diffusion models. A large body of work has recently tackled the task of finetuning denoising diffusion models, with various point of views. Wu et al. (2023b) update weight parameters in a supervised fashion by building a high-reward dataset, then using score matching. Other papers use reinforcement learning approaches to finetune the parameters of the model (Dvijotham et al., 2023; Fan et al., 2023; Black et al., 2024). Closer to our approach are works that propose finetuning of denoising diffusion models by backpropagating through sampling (Watson et al., 2022; Dong et al., 2023; Wallace et al., 2023; Clark et al., 2024). However, they sample only once (Lee et al., 2023), or use a nested loop approach (described in Section 3.1) and resort to implementation techniques such as gradient checkpointing or gradient rematerialization to limit the memory burden. We instead depart from this point of view and propose a single-loop approach. Furthermore, our approach is much more general than denoising diffusion models and includes any iterative sampling algorithm such as Langevin sampling.\n\nWe emphasize that the finetuning approach differs from guidance of diffusion models (see, e.g., Dhariwal and Nichol, 2021; Graikos et al., 2022; Hertz et al., 2023; Kwon et al., 2023; Wu et al., 2023a; Zhang et al., 2023). In the latter case, the sampling scheme is modified to bias sampling towards maximizing the reward. On the contrary, finetuning directly modifies the weights of the model without changing the sampling scheme. Both approaches are complementary, and it can happen that in practice people prefer to modify the weights of the model rather than the sampling scheme: e.g., to distribute weights that take into account the reward and that can be used with any standard sampling scheme, without asking downstream users to modify their sampling method or requiring them to share the reward mechanism.\n\nSingle-loop approaches for bilevel optimization. Our single-loop approach for differentiating through sampling processes is inspired by recently-proposed single-loop approaches for bilevel optimization problems (Guo et al., 2021; Yang et al., 2021; Chen et al., 2022; Dagr´ eou et al., 2022; Hong et al., 2023). Closest to our setting is Dagr´ eou et al. (2022), where strong convexity assumptions are made on the inner problem while gradients for the outer problem are assumed to be Lipschitz and bounded. They also show convergence of the average of the objective gradients, akin to our Theorem 4.7. However, contrarily to their analysis, we study the case where the inner problem is a sampling problem (or infinite-dimensional optimization problem). Our methodology also extends to the non-stationary case, e.g. encompassing denoising diffusion models.\n\nStudy of optimization through Langevin dynamics in the linear case. In the case where the operator Γ can be written as an expectation w.r.t. p t then the dynamics of θ in (11) can be seen as a McKean-Vlasov process. Kuntz et al. (2023) and Sharrock et al. (2024) propose efficient algorithms to approximate this process\n\nusing the convergence of interacting particle systems to McKean-Vlasov process when the number of particles is large. In the same setting, where Γ can be written as an expectation w.r.t. p t , discretization of such dynamics have been extensively studied (Atchad´ e et al., 2017; De Bortoli et al., 2021; Xiao and Zhang, 2014; Rosasco et al., 2020; Nitanda, 2014; Tadi´ c and Doucet, 2017). In that setting, one can leverage convergence results of the Langevin algorithm under mild assumption such as Eberle (2016) to prove the convergence of a sequence ( θ k ) k ∈ N to a local minimizer such that ∇ ℓ ( θ ⋆ ) = 0, see (De Bortoli et al., 2021, Appendix B) for instance. Finally, Wang and Sirignano (2024) and Wang and Sirignano (2022) propose and analyze a single-loop algorithm to differentiate through solutions of SDEs. Their algorithm uses forward-mode differentiation, which does not scale well to large-scale machine learning problems.\n\n## AUTHOR CONTRIBUTION STATEMENT\n\nPM worked on designing the methodology, implemented the codebase for experiments, proved theoretical guarantees for proposed method, contributed importantly to writing the paper. AK contributed to designing the methodology, worked on proving theoretical guarantees, made some contributions to the paper. PB, MB, VDB, AD, FL, CP (by alphabetical order) contributed to discussions in designing the methodology, provided references, made remarks and suggestions on the manuscript and provided some help with the codebase implementation. QB proposed the initial idea, proposed the general methodology and worked on designing it, contributed to the codebase implementation, ran experiments, and contributed importantly to writing the paper.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Name                                                        | Value                                                                   |\n|-------------------------------------------------------------|-------------------------------------------------------------------------|\n| Noise schedule Optimizer EMA decay Learning rate Batch size | Ornstein-Uhlenbeck Adam with standard hyperparameters 0 . 995 10 - 3 32 |"
    },
    {
      "index": 1,
      "markdown": "| Name                                                      | Value                                                           |\n|-----------------------------------------------------------|-----------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 256 Euler Ornstein-Uhlenbeck Adam with standard hyperparameters |"
    },
    {
      "index": 2,
      "markdown": "| Name                                                                                                                              | Value                                                                                                                         |\n|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer EMA decay Learning rate Batch size Number of samples for FID evaluation | 1 , 024 DDPM Cosine (Nichol and Dhariwal, 2021) Adam with β 1 = 0 . 9, β 2 = 0 . 99, ε = 10 - 12 0 . 9999 2 · 10 - 4 2048 50k |"
    },
    {
      "index": 3,
      "markdown": "| Name                                                      | Value                                                                               |\n|-----------------------------------------------------------|-------------------------------------------------------------------------------------|\n| Number of sampling steps Sampler Noise schedule Optimizer | 1 , 024 Euler Cosine (Nichol and Dhariwal, 2021) Adam with standard hyperparameters |"
    }
  ],
  "stats": {
    "pages": 40,
    "chunksCreated": 162,
    "totalCharacters": 109034,
    "totalWords": 19773,
    "numTables": 4,
    "processingTimeMs": 33522
  }
}