{
  "paper": {
    "id": "2402.01306v4",
    "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
    "abstract": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",
    "authors": [
      "Kawin Ethayarajh",
      "Winnie Xu",
      "Niklas Muennighoff",
      "Dan Jurafsky",
      "Douwe Kiela"
    ],
    "published": "2024-02-02T10:53:36.000Z",
    "updated": "2024-11-19T18:12:45.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2402.01306v4",
    "absUrl": "https://arxiv.org/abs/2402.01306v4"
  },
  "chunks": [
    {
      "id": "2402.01306v4-chunk-0",
      "content": "Kawin Ethayarajh 1 Winnie Xu 2 Niklas Muennighoff 2 Dan Jurafsky 1 Douwe Kiela 1 2",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "chunkIndex": 0,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-1",
      "content": "Kahneman &amp; Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases-the success of these objectives (e.g., DPO) over crossentropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-2",
      "content": "We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-3",
      "content": "Aligning generative models with human feedback has been successfully used to make generations more helpful, factual, and ethical, among other desiderata (Ouyang et al., 2022; Tian et al., 2023). For LLMs, alignment methods such as RLHF and DPO have consistently proven to be more beneficial than doing supervised finetuning (SFT) alone. However, human feedback is often discussed only in the context of preferences (e.g., output y w ≻ y l for input x ), even though it can take many forms (e.g., approval/disapproval of y given\n\n1 Stanford University (first author was an intern at Contextual AI) 2 Contextual AI. Correspondence to: Kawin Ethayarajh &lt; kawin@stanford.edu &gt; .\n\nProceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 3,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-4",
      "content": "spondence to: Kawin Ethayarajh &lt; kawin@stanford.edu &gt; .\n\nProceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\n\nFigure 1. The utility that a human gets from the outcome of a random variable, as implied by different human-aware losses (HALOs). Notice that the implied value functions share properties such as loss aversion with the canonical human value function in prospect theory (Tversky &amp; Kahneman, 1992).\n\n<!-- image -->\n\nx ). This is because preferences, despite being a kind of data that is relatively scarce and expensive to collect in practice (Casper et al., 2023), are required by the alignment methods shown to work best-RLHF (Christiano et al., 2017) and DPO (Rafailov et al., 2023).\n\nTo understand why these methods work so well, and whether feedback needs to be in preference form, we frame alignment through the lens of prospect theory (Kahneman &amp; Tversky, 1979; Tversky &amp; Kahneman,",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 4,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-5",
      "content": "tand why these methods work so well, and whether feedback needs to be in preference form, we frame alignment through the lens of prospect theory (Kahneman &amp; Tversky, 1979; Tversky &amp; Kahneman, 1992). Prospect theory explains why humans make decisions about uncertain events that do not maximize their expected value. It formalizes how humans perceive random variables in a biased but well-defined manner; for example, relative to some reference point, humans are more sensitive to losses than gains, a property called loss aversion . We show that popular alignment methods such as DPO and PPO-Clip (Schulman et al., 2017) implicitly model some of these biases, helping explain their success independently of the data used (§3.2). We then propose a more general class of such loss functions called human-aware losses (HALOs). 1",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 5,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-6",
      "content": "icitly model some of these biases, helping explain their success independently of the data used (§3.2). We then propose a more general class of such loss functions called human-aware losses (HALOs). 1\n\n1 We use the term human-aware to draw an analogy with how hardware-aware methods benefit from being designed around hardware limitations (Dao et al., 2022), not to claim that a simple loss function is fully aware of human behavior.\n\nAlthough it is impossible to say that HALOs are categorically better than non-HALOs, we find that among existing methods, those that meet the definition of a HALO work better than those that do not (§3.3). We find that DPO performance can even be matched at most scales by running an offline PPO variant on dummy +1/-1 rewards, suggesting that preference data might not be needed if the inductive bias in the loss function is good enough.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 6,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-7",
      "content": "even be matched at most scales by running an offline PPO variant on dummy +1/-1 rewards, suggesting that preference data might not be needed if the inductive bias in the loss function is good enough. However, despite the surprising success of this simple baseline, it significantly lags behind DPO at the 30B LLM scale and suffers from hyperparameter sensitivity, making it difficult to use.\n\nTaking a more principled approach, we derive a HALO using the model of human utility that Kahneman &amp; Tversky proposed to describe how humans make decisions about uncertain monetary outcomes (Tversky &amp; Kahneman, 1992). This approach, which we call Kahneman-Tversky Optimization (KTO), directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as most current methods do (§4.1). KTO only requires a binary signal of whether an output is desirable or undesirable for an input.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 7,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-8",
      "content": "generations instead of maximizing the log-likelihood of preferences, as most current methods do (§4.1). KTO only requires a binary signal of whether an output is desirable or undesirable for an input. This data is more abundant, cheaper, and faster to collect in the real world, making it easier to scale alignment in production and rapidly iterate on models. We find that:\n\n- KTO matches or exceeds DPO performance at scales from 1B to 30B parameters (§4.3). 2 That is, breaking up n preferences meant for DPO into 2 n examples for KTO can yield better LLM generations, as determined by closed-ended tasks such as mathematical reasoning and open-ended judgments from humans and GPT-4 .\n- KTO can handle extreme data imbalances, matching DPO performance while using up to 90% fewer desirable examples (i.e., examples of good generations).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 8,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-9",
      "content": "and open-ended judgments from humans and GPT-4 .\n- KTO can handle extreme data imbalances, matching DPO performance while using up to 90% fewer desirable examples (i.e., examples of good generations). Its success cannot be ascribed to the alignment data being sourced from a preference dataset.\n- When the pretrained model is sufficiently good, one can skip supervised finetuning (SFT) and go straight to KTO without a loss in generation quality, whereas SFT is always needed for best results with DPO.\n\nThe intent behind KTO was that even if the model learns from a weaker signal, we could compensate with the higher volume of data that could be accessed in practice; the fact that KTO can match and even outperform DPO on the same data is thus surprising. We conclude by discussing some theoretical explanations for this phenomenon (§4.4).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 9,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-10",
      "content": "ld be accessed in practice; the fact that KTO can match and even outperform DPO on the same data is thus surprising. We conclude by discussing some theoretical explanations for this phenomenon (§4.4). Despite the success of KTO in our experiments, our work ultimately suggests that there is no one HALO that is universally superior; the best HALO depends on the inductive biases appropriate for a given setting, and this choice should be made deliberately instead of defaulting to any one loss.\n\n2 Our code is available on Github; models are on Huggingface.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "1. Introduction",
        "chunkIndex": 10,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-11",
      "content": "For a detailed survey, please see Appendix A. In brief, LLMs are traditionally trained in three stages (Ouyang et al., 2022):\n\nPretraining Given a large corpus, train the model to maximize the log-likelihood of the next token conditioned on the preceding text. Let π 0 denote the pretrained model.\n\nSupervised Finetuning (SFT) Finetune the model to predict the next token on data that is more relevant to the downstream task. Often, such data will comprise instructions and an appropriate response (i.e., instruction finetuning). Let π ref denote the finetuned model.\n\nRLHF Given a dataset D of preferences ( x, y w , y l ) -where x is an input, y w , y l are the preferred and dispreferred outputs (i.e., y w ≻ y l for x ), and r ∗ is the 'true' reward function underlying the preferences-it is first assumed that the probability that y w is preferred to y l can be captured with a specific function class, typically a Bradley-Terry model (Bradley &amp; Terry, 1952).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "2. Background",
        "chunkIndex": 11,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-12",
      "content": "lying the preferences-it is first assumed that the probability that y w is preferred to y l can be captured with a specific function class, typically a Bradley-Terry model (Bradley &amp; Terry, 1952). Where σ is the logistic function:\n\n<!-- formula-not-decoded -->\n\nSince getting the true reward from a human would be intractably expensive, a reward model r ϕ learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:\n\n<!-- formula-not-decoded -->\n\nBut solely maximizing the reward might come at the expense of desiderata such as generating grammatical text. To avoid this, a KL divergence penalty is introduced to restrict how far the language model can drift from π ref. Where π θ is the model we are optimizing, the optimal model π ∗ is that which maximizes\n\n<!-- formula-not-decoded -->\n\nwhere β &gt; 0 is a hyperparameter. Since this objective is not differentiable, we need to use an RL algorithm like PPO (Schulman et al., 2017).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "2. Background",
        "chunkIndex": 12,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-13",
      "content": "is that which maximizes\n\n<!-- formula-not-decoded -->\n\nwhere β &gt; 0 is a hyperparameter. Since this objective is not differentiable, we need to use an RL algorithm like PPO (Schulman et al., 2017).\n\nHowever, RLHF is often slow (largely because of having to sample generations) and quite unstable in practice (especially in a distributed setting). For this reason, recent work has focused on designing closed-form losses that maximize the margin between the preferred and dispreferred generations. In particular, Direct Preference Optimization (DPO) (Rafailov et al., 2023) has emerged as a popular alternative as it allows the same optimal policy as in RLHF to be recovered under certain conditions:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "2. Background",
        "chunkIndex": 13,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-14",
      "content": "To understand why alignment methods work so well, we now frame them through the lens of prospect theory (Tversky &amp; Kahneman, 1992). Prospect theory explains why, when faced with an uncertain event, humans make decisions that do not maximize their expected value. For example, because humans are loss-averse, given a gamble that returns $100 with 80% probability and $60 with 20% probability, a person might accept $60 to avoid the gamble, despite their certainty equivalent of $60 being less than the expected value of $80.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3. A Prospect Theoretic View of Alignment",
        "chunkIndex": 14,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-15",
      "content": "In prospect theory, human utility depends on a value function and a weighting function : 3\n\nDefinition 3.1. A value function v : Z → R maps an outcome z , relative to some reference point z 0 , to its perceived (or subjective) value. For example, these functions capture the fact that humans tend to be more sensitive to relative losses than relative gains of the same magnitude.\n\nDefinition 3.2. A weighting function ω is the derivative of a capacity function that maps cumulative probabilities to perceived cumulative probabilities. These functions capture, for example, the fact that humans tend to overestimate the chance of rare events. Let ω z denote the weight placed on outcome z .\n\nDefinition 3.3. The utility of a random variable Z is a function of its outcomes: u ( Z ) ≜ ∑ z ∈ Z ω z v ( z -z 0 ) .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.1. Prospect Theory",
        "chunkIndex": 15,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-16",
      "content": "imate the chance of rare events. Let ω z denote the weight placed on outcome z .\n\nDefinition 3.3. The utility of a random variable Z is a function of its outcomes: u ( Z ) ≜ ∑ z ∈ Z ω z v ( z -z 0 ) .\n\nHowever, because humans do not see the full probability distribution of an LLM, weighting functions are not salient to this discussion; we will focus only on value functions. Using experiments that presented real humans with monetary gambles and asked for their certainty equivalent, Tversky &amp; Kahneman (1992) proposed the following functional form for human value:\n\n<!-- formula-not-decoded -->\n\nwhere the median value of hyperparameter α = 0 . 88 and λ = 2 . 25 across individuals. α controls the curvature of the function, which reflects risk aversion; λ controls its steepness, which reflects loss aversion. While the shape of the median Kahneman-Tversky value function is illustrated in Figure 1, it should be noted that it varies across individuals (Tversky &amp; Kahneman, 1992).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.1. Prospect Theory",
        "chunkIndex": 16,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-17",
      "content": "h reflects loss aversion. While the shape of the median Kahneman-Tversky value function is illustrated in Figure 1, it should be noted that it varies across individuals (Tversky &amp; Kahneman, 1992). There are also other functional forms for the value function that have been proposed in later work (Gurevich et al., 2009). The salient qualities of a value function are: the existence of a reference point that\n\n3 Cumulative prospect theory is the full name of the expanded theory we dicuss here (Tversky &amp; Kahneman, 1992).\n\nis used to get the relative gain or loss; concavity in relative gains (i.e., diminishing sensitivity away from z 0 ); and loss aversion (i.e., greater sensitivity to losses).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.1. Prospect Theory",
        "chunkIndex": 17,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-18",
      "content": "Definition 3.4 ( HALOs ) . Let θ denote the trainable parameters of the model π θ : X → P ( Y ) being aligned, π ref the reference model, l : Y → ❘ + a normalizing factor, and r θ ( x, y ) = l ( y ) log[ π θ ( y | x ) /π ref ( y | x )] the implied reward. Where Q ( Y ′ | x ) is a reference point distribution over Y and v : ❘ → ❘ is non-decreasing everywhere and concave in (0 , ∞ ) , the human value of ( x, y ) is\n\n<!-- formula-not-decoded -->\n\nA function f is a human-aware loss for v if ∃ a x,y ∈ {-1 , +1 } such that:\n\n<!-- formula-not-decoded -->\n\nwhere D is the feedback data and C D ∈ R is a data-specific constant.\n\nIn a classic prospect theory experiment, r θ would be the dollar amount assigned to each outcome; here, r θ is measured in nats, as the decrease in conditional surprisal when going from π ref to π θ , normalized according to l . This follows naturally from the next-token prediction objective used to pretrain and finetune LLMs.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.2. HALOs",
        "chunkIndex": 18,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-19",
      "content": "as the decrease in conditional surprisal when going from π ref to π θ , normalized according to l . This follows naturally from the next-token prediction objective used to pretrain and finetune LLMs. As π θ is aligned, we would expect r θ to grow increasingly positive for desirable outputs and increasingly negative for undesirable outputs.\n\nAnother perspective on the reward comes from the RLHF objective in (2). The policy that maximizes this objective has a closed-form expression (Peng et al., 2019; Peters &amp; Schaal, 2007); where Z ( x ) is the partition function:\n\nLetting l ( · ) = β , we get\n\n<!-- formula-not-decoded -->\n\nUnder θ ∗ , the HALO-defined reward is just the optimal reward shifted by an input-specific term, meaning that r θ ∗ is in the same equivalence class as r ∗ and would also induce the optimal policy π ∗ (Lemma 1, Rafailov et al. (2023)).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.2. HALOs",
        "chunkIndex": 19,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-20",
      "content": "is just the optimal reward shifted by an input-specific term, meaning that r θ ∗ is in the same equivalence class as r ∗ and would also induce the optimal policy π ∗ (Lemma 1, Rafailov et al. (2023)).\n\nThe reference point in a HALO is the expected reward from the human's perspective, where Q ( Y ′ | x ) describes the examples that are used to construct a baseline. We require that the value function be concave in gains but not necessarily convex in losses-unlike the canonical KahnemanTversky value function-because a minority of individuals are risk-averse in both the gain and loss regime, resulting in concavity everywhere (Kahneman &amp; Tversky, 1979).\n\nFigure 2. HALOs (DPO, offline PPO variant) outperform non-HALOs (SLiC, CSFT), as measured by the GPT-4-0613 -judged winrate of the aligned model's generations against a hard-to-beat baseline: the outputs that would have been used as the targets for SFT. The y -axis here plots the winrate above chance (i.e., the winrate - 50%).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.2. HALOs",
        "chunkIndex": 20,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-21",
      "content": "he aligned model's generations against a hard-to-beat baseline: the outputs that would have been used as the targets for SFT. The y -axis here plots the winrate above chance (i.e., the winrate - 50%). The difference between methods is only significant ( p &lt; 0 . 05) at 13B+ parameters, and only the HALO-aligned Llama-{ 13B, 30B } models are able to match the baseline and yield a winrate at or above chance.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.2. HALOs",
        "chunkIndex": 21,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-22",
      "content": "The proof is deferred to Appendix B. In Figure 1, we can see this more intuitively by plotting the value function (i.e., the implied human utility).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Theorem 3.5. DPOand PPO-Clip are human-aware losses.",
        "chunkIndex": 22,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-23",
      "content": "It is difficult to answer whether being a HALO is useful, since both HALOs and non-HALOs are diverse function classes, but we attempt to do so by comparing popular methods that qualify as a HALO with those that do not:\n\n1. CSFT : Conditional SFT is a simple alignment method where a control token is prepended to the output during training; then, at inference, the control token corresponding to desirable generations is appended to the input to induce good generations (Korbak et al., 2023). This is not a HALO, since that would demand that -log π ref ( y | x ) always equal the reference point; however, since the reference point is fixed for a given x , this is not possible when π ref is a non-uniform distribution.\n2. SLiC : Sequence Likelihood Calibration (Zhao et al., 2023), which combines a max-margin loss for preferences with a language modeling loss, is not a HALO (for the same reasons given above for CSFT):\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 23,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-24",
      "content": "alibration (Zhao et al., 2023), which combines a max-margin loss for preferences with a language modeling loss, is not a HALO (for the same reasons given above for CSFT):\n\n<!-- formula-not-decoded -->\n\n3. DPO : DPO is a HALO (Theorem 3.5).\n4. PPO (offline) : The standard RLHF objective in (2) is typically optimized with PPO-Clip, which works by\n\n'clipping' how far π θ can drift from the version π old at the previous step:\n\n<!-- formula-not-decoded -->\n\nwhere q θ = π θ ( y t | x : y &lt;t ) π old ( y t | x : y &lt;t ) and A ( x : y &lt;t , y t ) is the pertoken advantage (i.e., the surplus benefit from producing a given token in a given state).\n\nPPO is an online algorithm-generations are sampled from the current model, judged by a reward model, and then used to update the current version. However, for a fair comparison with offline methods, we consider a variant of PPO that is only fed offline data.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 24,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-25",
      "content": "e current model, judged by a reward model, and then used to update the current version. However, for a fair comparison with offline methods, we consider a variant of PPO that is only fed offline data. Because RLHF is also quite unstable in a distributed setting, we never update π old and keep it as π ref, instead clipping less conservatively than we traditionally would. Baheti et al. (2023) found that these changes, along with treating the entire output sequence as a single action, greatly improves stability. However, since RLHF traditionally uses token-level advantages, we omit the third change and only preserve the first two. The PPO-Clip loss is unchanged and remains a HALO (Theorem 3.5).\n\nCalling this method PPO is somewhat imprecise, because it is offline and takes only one step, but to avoid introducing too many new terms, we will call this PPO (offline) . Instead of using learned rewards, we simplify even further and use dummy +1/-1 rewards 4 for y w and y l instead.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 25,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-26",
      "content": "e step, but to avoid introducing too many new terms, we will call this PPO (offline) . Instead of using learned rewards, we simplify even further and use dummy +1/-1 rewards 4 for y w and y l instead. Further details on the implementation of this method can be found in Appendix C.\n\nExperimental Setup We compare these baselines on two model families, Pythia-{ 1.4B, 2.8B, 6.9B, 12B } (Bider-\n\n4 Note that +1/-1 do not refer to the HALO-implied rewards, but rather the rewards used for the advantage A ( x : y &lt;t , y t ) calculation.\n\nman et al., 2023) and Llama-{ 7B, 13B, 30B } (Touvron et al., 2023). This permits us to see how LLM alignment scales within a model family (Llama-2 lacks a 30B model, hence our use of Llama). Later experiments (§4.3) are done on Mistral-7B derivatives (Jiang et al., 2023) and Llama-3 (Dubey et al., 2024). The models are trained on a combination of Anthropic-HH (Ganguli et al., 2022), OpenAssistant (K¨ opf et al., 2023), and SHP (Ethayarajh et al., 2022).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 26,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-27",
      "content": "et al., 2023) and Llama-3 (Dubey et al., 2024). The models are trained on a combination of Anthropic-HH (Ganguli et al., 2022), OpenAssistant (K¨ opf et al., 2023), and SHP (Ethayarajh et al., 2022).\n\nAll models are aligned under identical settings on the same data, save for hyperparameters unique to them. Similar to Rafailov et al. (2023), the target sequences for SFT are a subset of { y w } . We use GPT-4-0613 to judge whether the aligned model's response is better than the SFT target for a given test input with respect to helpfulness, harmlessness, and conciseness, a now standard practice (Zheng et al., 2023; Li et al., 2023). 5 Note that while the SFT target is considered a desirable output for x , it is by no means the best output, meaning that an aligned model can certainly achieve a winrate above 50%, although this is difficult given that many of the target sequences are human-written.\n\nIn Figure 2, we see the results of this analysis:",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 27,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-28",
      "content": "hat an aligned model can certainly achieve a winrate above 50%, although this is difficult given that many of the target sequences are human-written.\n\nIn Figure 2, we see the results of this analysis:\n\n- HALOs either match or outperform non-HALOs at every scale , though the gap is only significant ( p &lt; 0 . 05) at 13B+ model sizes after correcting for multiple comparisons (Holm, 1979). In fact, only the HALOaligned Llama-{ 13B, 30B } models match or exceed a win rate of 50% (i.e., are able to match or exceed the generation quality of the SFT targets in the test data).\n- Up to a scale of 7B parameters, alignment provides virtually no gains over SFT alone. However, it is worth noting that if the base models were more performant, or if the SFT data distribution were less similar to the preference data, then the gains from the alignment stage would ostensibly be greater.\n- Despite only using dummy +1/-1 rewards, our offline PPO variant performs as well as DPO for all models except Llama",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 28,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-29",
      "content": "ference data, then the gains from the alignment stage would ostensibly be greater.\n- Despite only using dummy +1/-1 rewards, our offline PPO variant performs as well as DPO for all models except Llama-30B. This challenges conventional wisdom, which places heavy emphasis on reward learning (Casper et al., 2023), and suggests that even the simplest rewards can prove useful when used in a loss function that has the right inductive bias. Despite its success, our offline PPO baseline still suffers from hyperparameter sensitivity and training instability, albeit not to the same extent as traditional RLHF.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "3.3. Does being a HALO matter?",
        "chunkIndex": 29,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-30",
      "content": "The surprising success of offline PPO with dummy +1/-1 rewards suggests that-with the right inductive biases-a\n\n5 We validate that GPT-4 judgments concur with human judgments in Appendix D.\n\nbinary signal of good/bad generations may be sufficient to reach DPO-level performance, even if the offline PPO approach itself was unable to do so past a certain scale (§3.3). Taking a more principled approach, we now derive a HALOusing the Kahneman-Tversky model of human value, which allows us to directly optimize for utility instead of maximizing the log-likelihood of preferences.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4. Kahneman-Tversky Optimization",
        "chunkIndex": 30,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-31",
      "content": "The canonical Kahneman-Tversky value function (4) suffers from numerical instability during optimization due to the exponent a , so we replace it with the logistic function σ , which is also concave in gains and convex in losses.\n\nTo control the degree of risk aversion, we introduce a hyperparameter β ∈ ❘ + as part of the value function. The greater β is, the more quickly the value saturates, meaning the human is simultaneously more risk-averse in gains and more risk-seeking in losses. In practice, this has a similar effect as β in the DPO loss, which controls how far π θ drifts from π ref, though we introduce it here explicitly to control risk aversion; in DPO, it carries over from the KL constraint in the RLHF objective (2) and is part of the reward.\n\nWe replace the loss aversion coefficient λ in the original Kahneman-Tversky value function (4) with { λ D , λ U } , where λ D and λ U are hyperparameters for desirable and undesirable outputs respectively; more complex schemes could als",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 31,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-32",
      "content": "ient λ in the original Kahneman-Tversky value function (4) with { λ D , λ U } , where λ D and λ U are hyperparameters for desirable and undesirable outputs respectively; more complex schemes could also be used for importance sampling.\n\nRather than having just one dispreferred generation serve as the reference point z 0 , as in DPO, we assume that humans judge the quality of y | x in relation to all possible outputs. This implies that Q ( Y ′ | x ) is the policy and that the reference point is the KL divergence KL ( π θ ( y ′ | x ) ∥ π ref ( y ′ | x )) . However, as we discuss further below, we end up taking a biased estimate of this term in practice.\n\nWhere λ y denotes λ D ( λ U ) when y is desirable(undesirable) respectively, the default KTO loss is: 6\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 32,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-33",
      "content": "te of this term in practice.\n\nWhere λ y denotes λ D ( λ U ) when y is desirable(undesirable) respectively, the default KTO loss is: 6\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\n6 An earlier draft separated out λ D , λ U into a function w . For an easier comparison with (4), they-along with β -have been moved into the value function itself. Note that λ y exists solely to make the loss non-negative and can be removed. The use of multiple x ′ to estimate z 0 in practice was included in the loss definition originally, but has now been moved outside for clarity.\n\nTable 1. Recommended hyperparameter settings for different losses and models when aligned on UltraFeedback, evaluated on the benchmarks discussed in §4.3. The hyperparameter sweeps were done with AdamW, an effective batch size of 32, and λ D = λ U = 1 . Depending on your task and ratio of desirable:undesirable examples, the optimal choice of λ D , λ U might be significantly different.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 33,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-34",
      "content": "one with AdamW, an effective batch size of 32, and λ D = λ U = 1 . Depending on your task and ratio of desirable:undesirable examples, the optimal choice of λ D , λ U might be significantly different.\n\n| Model               | Method   |    LR |    β |   AlpacaEval (LC) ↑ |   BBH ↑ |   GSM8K (8-shot) ↑ |\n|---------------------|----------|-------|------|---------------------|---------|--------------------|\n| Llama-3 8B          | SFT+KTO  | 5e-06 | 0.05 |               10.59 |   65.15 |              60.2  |\n| Llama-3 8B          | KTO      | 5e-06 | 0.1  |               11.25 |   65.26 |              57.92 |\n| Qwen2.5 3B Instruct | SFT+KTO  | 5e-06 | 0.1  |               13.01 |   32.39 |              61.11 |\n| Qwen2.5 3B Instruct | KTO      | 5e-06 | 0.5  |               16.63 |   20.41 |              60.35 |\n\nFor more stable training, we do not backpropagate through z 0 ; it exists purely to control the loss saturation.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 34,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-35",
      "content": "truct | KTO      | 5e-06 | 0.5  |               16.63 |   20.41 |              60.35 |\n\nFor more stable training, we do not backpropagate through z 0 ; it exists purely to control the loss saturation.\n\nIntuitively, KTO works as follows: if the model increases the reward of a desirable example in a blunt manner, then the KL penalty also rises and no progress is made. This forces the model to learn exactly what makes an output desirable, so that the reward can be increased while keeping the KL term flat (or even decreasing it). The argument works in the other direction as well, though the non-negativity of the KL term allows faster saturation in the loss regime.\n\nKL Estimate In practice, estimating z 0 as it is defined above is impractical because sampling from π θ is slow. Instead, we take a biased but convenient estimate by shifting outputs in the same microbatch to induce mismatched pairs { ( x 1 , y 2 ) , ( x 2 , y 3 ) , ..., ( x m , y 0 ) } , then estimating a shared reference point",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 35,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-36",
      "content": "a biased but convenient estimate by shifting outputs in the same microbatch to induce mismatched pairs { ( x 1 , y 2 ) , ( x 2 , y 3 ) , ..., ( x m , y 0 ) } , then estimating a shared reference point z 0 for all examples in the same microbatch as follows. Where j = ( i +1) mod m ,\n\n<!-- formula-not-decoded -->\n\nBecause of clamping, our estimator has a positive bias but lower variance than the standard unbiased estimator. Although it costs an additional forward pass, we use a mismatched output y j instead of the corresponding y i because the latter have often been deliberately chosen to be canonically good or bad outputs, and thus have unrepresentative high-magnitude rewards. It is worth noting that although our estimator is biased, so would the human-perceived reference point, since humans do not perceive the full distribution induced by π θ and would employ an 'availability heuristic' that would overweight outputs for which they have recently given feedback, regardless of whether tho",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 36,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-37",
      "content": "s do not perceive the full distribution induced by π θ and would employ an 'availability heuristic' that would overweight outputs for which they have recently given feedback, regardless of whether those outputs are a good continuation of x (Tversky &amp; Kahneman, 1973).\n\nIf KTO is preceded by SFT done on the same data that is used as desirable feedback and the SFT model is used as π ref, then the KL estimate will quickly approach zero. Having already learned what is desirable during SFT, the policy will tend to scatter the mass placed on undesirable examples, leading to minimal divergence. Also, because the policy may learn to place less mass on undesirable y i regardless of whether it is preceded by x i , ˆ z 0 might actually be an under-estimate. In such cases, one can avoid the extra computation and set ˆ z 0 = 0 . However, when KTO is not preceded by SFT, or when the SFT data is not a subset of the KTO data, estimating ˆ z 0 is necessary.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 37,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-38",
      "content": "e. In such cases, one can avoid the extra computation and set ˆ z 0 = 0 . However, when KTO is not preceded by SFT, or when the SFT data is not a subset of the KTO data, estimating ˆ z 0 is necessary.\n\nData If the alignment data is naturally binary, every positive example can be assumed to be drawn from y desirable | x and every negative example from y undesirable | x . However, the canonical feedback datasets in academic research (HH, SHP, OASST) are in preference format, since the methods that have worked best up until now are preference-based. In our experiments, we convert preference data y w ≻ y l by assuming that y w is drawn from the desirable distribution and y l from the undesirable one. This is a naive assumption, made for the sake of simplicity, and a more complex deconstruction of preferences into binary feedback would likely yield better results, which we leave for future work.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 38,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-39",
      "content": "e. This is a naive assumption, made for the sake of simplicity, and a more complex deconstruction of preferences into binary feedback would likely yield better results, which we leave for future work. To show that KTO can be used with non-preference data, we also subsample exactly one y per x for some experiments (denoted oney -perx ), removing any trace of paired preferences at the cost of reducing the data volume.\n\nIf human feedback is in the form of scores or ratings, the simplest means of incorporating it into KTO is to construct a weighting function such that high-magnitude data is weighed more and that examples with scores above(below) some threshold are desirable(undesirable). It is also possible to construct score-based HALOs from first principles, but we leave the design of such losses to future work.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.1. Derivation",
        "chunkIndex": 39,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-40",
      "content": "In Table 1, we provide recommended hyperparameter settings for Llama-3 8B (Dubey et al., 2024) and Qwen2.5 3B Instruct (Yang et al., 2024) based on benchmarks such as MMLU (0-shot) (Hendrycks et al., 2021), GSM8K (8shot, chain-of-thought) (Cobbe et al., 2021), HumanEval (0-shot) (Chen et al., 2021), and BigBench-Hard (3-shot chain-of-thought) (Srivastava et al., 2022).\n\nLearning Rate We find that that the performance of an aligned model is more sensitive to the learning rate than any other hyperparameter. The optimal learning rate for KTO is\n\nFigure 3. KTO is as good or better than DPO at all scales, as measured by the GPT-4-0613 -judged winrate of the aligned model's generations against the outputs that would have been used for SFT. In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better than DPO alone. Error bars denote a 90% binomial confidence interval.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 40,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-41",
      "content": "d for SFT. In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better than DPO alone. Error bars denote a 90% binomial confidence interval.\n\n<!-- image -->\n\nusually 2x to 10x the optimal learning rate for DPO; since the reference-adjusted reward tends to be much smaller in magnitude for KTO, one needs to use a more aggressive learning rate to compensate. For example, the default learning rate for DPO is 5e-7 (Rafailov et al., 2023), but we find that a default of 5e-6 works better for KTO. In our experiments, we use the default DPO learning rate with RMSProp for all methods to ensure an apples-to-apples comparison with Rafailov et al. (2023), but when using KTO in practice, we recommend starting at 5e-6 with AdamW and adjusting the learning rate as needed.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 41,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-42",
      "content": "or all methods to ensure an apples-to-apples comparison with Rafailov et al. (2023), but when using KTO in practice, we recommend starting at 5e-6 with AdamW and adjusting the learning rate as needed.\n\nBatch Size KTO needs a microbatch size ≥ 2 to estimate the reference point in a single step. The experiments in this paper all use an effective batch size of 32, and in general we recommend using a batch size between 8 and 128.\n\nRisk Aversion The degree of risk aversion/seeking is controlled by β ; the greater β is, the greater the risk aversion in gains and risk seeking in losses. In practice, lower values of β in the range [0.01, 0.10] work better for larger models that have already undergone SFT; higher values of β in the range [0.10, 1.00] work better for smaller models undergoing KTO directly, without SFT prior.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 42,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-43",
      "content": "ange [0.01, 0.10] work better for larger models that have already undergone SFT; higher values of β in the range [0.10, 1.00] work better for smaller models undergoing KTO directly, without SFT prior.\n\nLoss Aversion The default weighting function controls the degree of loss aversion with λ D , λ U , which are both set to 1 by default. In general, where n D and n U refer to the number of desirable and undesirable examples respectively, we find that it is generally best to set λ D , λ U such that\n\n<!-- formula-not-decoded -->\n\nFor example, if there were a 1:10 ratio of desirable to undesirable examples, we would set λ U = 1 , λ D ∈ [10 , 15] .\n\nThis interval was determined empirically, and implies thatafter adjusting for class imbalances-gain sensitivity yields better performance than loss sensitivity, ostensibly because producing good outputs is more important than avoiding bad outputs for success on most benchmarks. This is not a hard rule, however.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 43,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-44",
      "content": "yields better performance than loss sensitivity, ostensibly because producing good outputs is more important than avoiding bad outputs for success on most benchmarks. This is not a hard rule, however. In tasks where minimizing the downside is more important, like toxicity prevention, setting λ D , λ U such that λ D n D &lt; λ U n U may work better. Unless otherwise stated, we use λ D = λ U = 1 in our experiments.\n\nThe current configuration permits different sensitivities to desirable and undesirable examples based on the premise that upon convergence, all undesirable outputs will yield negative rewards and all desirable outputs will yield positive ones, in which case λ U , λ D would directly correspond to λ in the original Kahneman-Tversky value function. However, during training itself, a desirable output may have a negative reward (and vice-versa), yet λ D would be the same regardless of whether r θ ( x, y ) -z 0 were positive or negative.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 44,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-45",
      "content": "function. However, during training itself, a desirable output may have a negative reward (and vice-versa), yet λ D would be the same regardless of whether r θ ( x, y ) -z 0 were positive or negative. We may want to change this so that not only is asymmetry possible upon convergence, but even during training within the same class of output: in this case, using a higher λ D when r θ ( x, y ) -z 0 is negative than when it is positive. We leave the design of dynamic hyperparameter selection schemes as directions for future work.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.2. Hyperparameters",
        "chunkIndex": 45,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-46",
      "content": "KTO ≥ DPO As seen in Figure 3, when rerunning the winrate evaluation in §3.3, SFT+KTO is competitive with SFT+DPO at scales from 1B to 30B, despite the model learning from a weaker signal. KTO alone is better than DPO alone for the Llama-{ 7B, 13B, 30B } models, and this gap is significant ( p &lt; 0 . 01 ) at 7B and 30B even after correcting for multiple comparisons (Holm, 1979). Among the Pythia models, there is no significant difference between the\n\nTable 2. (top) Results from aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback for exactly 1 epoch. Even when only one of the two outputs in each preference is seen by KTO, it still outperforms DPO, despite this reducing the volume of data by half (oney -perx ). (middle) Changing the structure of the KTO loss, even in subtle ways, makes the aligned model worse, supporting our design choices.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 46,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-47",
      "content": "DPO, despite this reducing the volume of data by half (oney -perx ). (middle) Changing the structure of the KTO loss, even in subtle ways, makes the aligned model worse, supporting our design choices. (bottom) Fixing λ U = 1 , we try different levels of loss and risk aversion by changing λ D and β respectively (see Appendix C for more results).\n\n| Dataset ( → ) Metric ( → )       |   MMLU EM |   GSM8k EM |   HumanEval pass@1 |   BBH EM |\n|----------------------------------|-----------|------------|--------------------|----------|\n| SFT                              |      57.2 |       39   |               30.1 |     46.3 |\n| DPO                              |      58.2 |       40   |               30.1 |     44.1 |\n| ORPO ( λ = 0 . 1 )               |      57.1 |       36.5 |               29.5 |     47.5 |\n| KTO ( β = 0 .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 47,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-48",
      "content": "PO                              |      58.2 |       40   |               30.1 |     44.1 |\n| ORPO ( λ = 0 . 1 )               |      57.1 |       36.5 |               29.5 |     47.5 |\n| KTO ( β = 0 . 1 , λ D = 1 )      |      58.6 |       53.5 |               30.9 |     52.6 |\n| KTO (one- y -per- x )            |      58   |       50   |               30.7 |     49.9 |\n| KTO (no z 0 )                    |      58.5 |       49.5 |               30.7 |     49   |\n| KTO (concave, v = log σ )        |      58.3 |       42.5 |               30.6 |     43.2 |\n| KTO (risk-neutral, v ( · ) = · ) |      57.3 |       42   |               28.8 |      6.1 |\n| KTO (no π ref , λ D = 1 . 75 )   |      57.5 |       47.5 |               29.5 |     51.6 |\n| KTO ( β = 0 . 01 )               |      57.3 |       54   |               33.2 |     49.8 |\n| KTO ( β = 0 . 50 )               |      58.3 |       45   |               32.7 |     51.8 |\n| KTO ( λ D = 0 .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 48,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-49",
      "content": "( β = 0 . 01 )               |      57.3 |       54   |               33.2 |     49.8 |\n| KTO ( β = 0 . 50 )               |      58.3 |       45   |               32.7 |     51.8 |\n| KTO ( λ D = 0 . 50 )             |      57.4 |       47   |               32.2 |     30.5 |\n| KTO ( λ D = 0 . 75 )             |      58.3 |       52   |               30.3 |     50.6 |\n| KTO ( λ D = 1 . 33 )             |      58.5 |       52   |               32.8 |     45.4 |\n| KTO ( λ D = 2 . 00 )             |      58.1 |       43   |               32.1 |     45.3 |\n\ntwo, suggesting that a minimum model capacity is needed for these differences to emerge. KTO also fares better than DPO and other baselines on generative benchmarks (Table 2). This is most pronounced for certain tasks: on GSM8K, a mathematical reasoning dataset, just swapping DPO for KTO when aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback (Cui et al., 2023) improves performance by 13.5 points.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 49,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-50",
      "content": "tasks: on GSM8K, a mathematical reasoning dataset, just swapping DPO for KTO when aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback (Cui et al., 2023) improves performance by 13.5 points.\n\nAt sufficient scale, KTO does not need SFT. A KTOaligned Llama-{ 13B, 30B } model is competitive with its SFT+KTO counterpart despite not undergoing SFT first, and is the only alignment method of the ones we tested to show this behavior. This is perhaps due to KTO alone keeping the average response length roughly the same, while running DPO without SFT prior causes the response length to increase dramatically (Figure 4).\n\nKTOdataneed not come from preferences. Might KTO be secretly benefiting from its 2 n examples in the previous experiment coming from n preference pairs instead of a naturally unpaired data distribution? To test this, we randomly discard increasingly large fractions of the desirable data before KTO-aligning a Llama-7B model.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 50,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-51",
      "content": "ng from n preference pairs instead of a naturally unpaired data distribution? To test this, we randomly discard increasingly large fractions of the desirable data before KTO-aligning a Llama-7B model. For example, if we discard 90% of the desirable data while leaving the undesirable data untouched, then the ratio of desirable:undesirable examples goes from 1:1 to 1:10 and the vast majority of undesirable examples no longer have a preferred counterpart. We handle such imbalances by changing λ D , λ U to satisfy\n\nFigure 4. Without doing SFT first, DPO-aligned models tend to ramble and hallucinate entire conversations. KTO does not suffer from this issue.\n\n<!-- image -->\n\nthe criteria in (9); when we drop 90% of the desirable data, we set λ u = 1 , λ D = 13 . 33 for example. For Llama-7B, we find that up to 90% of the desirable data can in fact be discarded while still outperforming DPO (Figure 5).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 51,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-52",
      "content": "rop 90% of the desirable data, we set λ u = 1 , λ D = 13 . 33 for example. For Llama-7B, we find that up to 90% of the desirable data can in fact be discarded while still outperforming DPO (Figure 5).\n\nWe further verify this claim by aligning Mistral-7B on OpenAssistant using DPO (on n pairs), standard KTO (on all 2 n outputs), and KTO where only one y per x is used. Since the output of one y in OpenAssistant is not conditioned on the other y for the same x , the latter captures the setting where the data is from an inherently unpaired distribution. Despite the oney -perx setup decreasing the amount of training data by 72%, the KTO-aligned model still outperforms both its DPO counterpart and the official instruction-tuned Mistral7B (Jiang et al., 2023), as seen in Table 3.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.3. Experiments",
        "chunkIndex": 52,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-53",
      "content": "For one, removing the reference point z 0 -which is necessary for KTO to qualify as a HALO-causes a 3.6 and 4.0 point drop on BBH and GSM8K respectively (Table 2, middle). Even changes that allow KTO to remain a HALO are typically suboptimal. For example, removing the symmetry of the value function-going from 1 -σ ( · ) to -log σ ( · ) (i.e., making the value function concave everywhere, as in DPO)causes a 9.4 and 11.0 point drop on BBH and GSM8K respectively. Making the value function risk-neutral by setting it to the identity function leads to a total collapse in BBH performance. Changing the curvature and slope via the risk and loss aversion hyperparameters can, depending on the task, improve or degrade performance (Table 2, bottom).\n\nKTOworks without a reference model or SFT, but not as well as standard KTO. If we can avoid storing the reference model in memory, then we can do much more memoryefficient alignment.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Changing the design of KTO makes it significantly worse.",
        "chunkIndex": 53,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-54",
      "content": "able 2, bottom).\n\nKTOworks without a reference model or SFT, but not as well as standard KTO. If we can avoid storing the reference model in memory, then we can do much more memoryefficient alignment. The naive way to do this is to assume that π ref returns a uniform distribution over outputs for all x , which simplifies r θ -z 0 to log π θ ( y | x ) -H ( π θ ( y ′ | x )) , where H denotes the entropy. As seen in Table 2 (middle), if we set λ D = 1 . 75 , this memory-efficient variant of KTO is better than DPO on some tasks and worse on others,\n\nFigure 5. A KTO-aligned Llama-7B model can match or exceed the performance of its DPO-aligned counterpart while aligned on a smaller and highly imbalanced version of the same dataset, even with as few as 0.1 positive/desirable examples for every negative/undesirable one.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Changing the design of KTO makes it significantly worse.",
        "chunkIndex": 54,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-55",
      "content": "ligned counterpart while aligned on a smaller and highly imbalanced version of the same dataset, even with as few as 0.1 positive/desirable examples for every negative/undesirable one.\n\n<!-- image -->\n\nthough it still trails standard KTO. We find that it is also more sensitive to the loss aversion hyperparameters; setting λ D ∈ { 1 . 5 , 2 . 0 } reduces performance on GSM8K and BBH by several points. Still, it strictly outperforms ORPO (Hong et al., 2024), a recently-proposed reference-free method, while also using less memory than all existing approaches, since π ref need never be loaded into memory and a batch of m KTO outputs is smaller than a batch of m preferences (i.e., 2 m outputs) used for ORPO/DPO.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Changing the design of KTO makes it significantly worse.",
        "chunkIndex": 55,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-56",
      "content": "KTO was designed with the motivation that even if binary feedback were weaker, one could compensate with sheer volume, as such data is much more abundant, cheaper, and faster to collect than preferences. So why does KTO perform as well or better than DPO on the same preference data (that has been broken up)? Greater data efficiency helps, but it is not the only answer, given that even after adjusting for this factor in the oney -perx setup, KTO still outperforms.\n\nIn this section, we will discuss two theoretical explanations for this phenomenon: (1) preference likelihood can be maximized without necessarily maximizing underlying human utility; (2) KTO implicitly ignores noisy and intransitive data, which is common in real-world feedback.\n\nProposition 4.1. As the reward implied by the current policy tends to ±∞ , the KTO update of π θ tends to zero.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 56,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-57",
      "content": "implicitly ignores noisy and intransitive data, which is common in real-world feedback.\n\nProposition 4.1. As the reward implied by the current policy tends to ±∞ , the KTO update of π θ tends to zero.\n\nThis means that if ( x, y ) is implied by the current policy π θ to be too difficult or too easy to learn from, then it is effectively ignored. In some cases, this may be a blessing in disguise: since real-world feedback is very noisy (Hoeffler &amp;Ariely, 1999), the reason a desirable example has a highly negative implied reward may be because it is mislabelled. By avoiding this hard-to-learn data, KTO avoids fitting to noise. However, this also means that KTO could end up ig-\n\nTable 3. In aligning Mistral-7B on the OpenAssistant dataset, we find that using KTO with only one output per input still outperforms DPO, despite this restriction reducing the amount of training data by 72%. A 90% binomial confidence interval is given.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 57,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-58",
      "content": "dataset, we find that using KTO with only one output per input still outperforms DPO, despite this restriction reducing the amount of training data by 72%. A 90% binomial confidence interval is given.\n\n| Method                          | Winrate vs. SFT Target   |\n|---------------------------------|--------------------------|\n| Mistral-7B (unaligned)          | 0.525 ± 0.037            |\n| Mistral-7B + DPO                | 0.600 ± 0.037            |\n| Mistral-7B + KTO (all y per x ) | 0.652 ± 0.036            |\n| Mistral-7B + KTO (one y per x ) | 0.631 ± 0.036            |\n| Mistral-7B-Instruct             | 0.621 ± 0.031            |\n\nnoring data that is hard-to-learn but necessary to recover r ∗ , potentially resulting in underfitting to complex distributions. Such underfitting may be mitigated by aligning the model with lower β and for more epochs.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 58,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-59",
      "content": "hard-to-learn but necessary to recover r ∗ , potentially resulting in underfitting to complex distributions. Such underfitting may be mitigated by aligning the model with lower β and for more epochs.\n\nTheorem 4.2. Assuming the value function is logistic, for a reward function r ∗ a that maximizes (2), there exists a reward function in its equivalence class (i.e., r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) for some h ( x ) ) that induces the same optimal policy π ∗ and the same Bradley-Terry preference distribution but a different human value distribution.\n\nA key insight from Rafailov et al. (2023) is that reward functions in the same equivalence class (i.e., differing only in an input-specific component) induce the same optimal policy under (2) and the same Bradley-Terry preference distribution.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 59,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-60",
      "content": "s that reward functions in the same equivalence class (i.e., differing only in an input-specific component) induce the same optimal policy under (2) and the same Bradley-Terry preference distribution. However, we show under mild assumptions that the value distribution-i.e., human utility-is affected by such input-specific changes, so maximizing preference likelihood does not mean one is maximizing human utility. This helps explain why the margin between KTO and DPO is even bigger in human evaluations than it is in automated LLM-as-a-judge evaluations (Appendix D).\n\nTheorem 4.3. For input x with outputs { y a , y b } , let dataset D comprise contradictory preferences y a ≻ y b and y b ≻ y a in proportion p ∈ (0 . 5 , 1) and (1 -p ) ∈ (0 , 0 . 5) respectively. If p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , then the optimal DPO policy is more likely to produce the minoritypreferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 60,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-61",
      "content": "; (1 -p ) 1 /β π ref ( y b | x ) , then the optimal DPO policy is more likely to produce the minoritypreferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-neutral value function ( λ D = λ U ).\n\nInformally, say there are two contradictory preferences over the output for x , with the majority p preferring y a and the minority 1 -p preferring y b . In the worst-case, when p is sufficiently low and the reference model is sufficiently unaligned, the optimal DPO policy is more likely to produce the minority-preferred output y b even though the implied reward r θ ( x, y b ) &gt; r θ ( x, y a ) . In contrast, the optimal KTO policy will deterministically produce the majority-preferred y a if the value function is loss-neutral ( λ D = λ U ), which is the default setting. This suggests that KTO has better worst-case outcomes when handling feedback intransitivity.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 61,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-62",
      "content": "default setting. This suggests that KTO has better worst-case outcomes when handling feedback intransitivity.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.4. Theoretical Analysis",
        "chunkIndex": 62,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-63",
      "content": "When human feedback is in a binary format, and especially when there is an imbalance between the number of desirable and undesirable examples, KTO is the natural choice. When your data is in the form of preferences, the choice is less clear. Putting aside the greater data efficiency of KTO, our theoretical analysis suggests that if your preference data has sufficiently little noise and sufficiently little intransitivity, then DPO will work better, since there is some risk of KTO underfitting; this risk can be mitigated by using a lower value of β and aligning with KTO for more epochs. But if there is enough noise and intransitivity, then the better worst-case guarantees of KTO will win out. Most publicly available preference datasets (e.g., SHP, OpenAssistant) contain noisy feedback from many different humans whose preferences likely contradict to some extent, which explains why KTO was able to match or exceed DPO performance in our experiments.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.5. KTO vs. DPO - when to use which?",
        "chunkIndex": 63,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-64",
      "content": "nAssistant) contain noisy feedback from many different humans whose preferences likely contradict to some extent, which explains why KTO was able to match or exceed DPO performance in our experiments. Even synthetic feedback can be noisy and intransitive, which helps explain why KTO outperforms DPO when aligning with UltraFeedback.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "4.5. KTO vs. DPO - when to use which?",
        "chunkIndex": 64,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-65",
      "content": "The existence of HALOs raises many questions. For one, KTO is based on the Kahneman-Tversky value function for monetary gambles, which is almost certainly different from how humans perceive the relative goodness of text. What value functions and reference point distributions best describe how humans perceive language, and how do they vary across domains and individuals? How can we identify the best HALO for each individual and setting instead of using one default loss?\n\nOn a more technical level, important directions include developing HALOs that: (1) incorporate granular feedback, such as a score, especially when optimizing for multiple desiderata; (2) work for other modalities (e.g. images) and model classes (e.g., diffusion models), especially models that do not produce an explicit distribution over the output space; (3) can resolve contradictions in feedback according to different definitions of fairness; (4) are designed to be used with online data, where the direction of feedback",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "5. Future Work",
        "chunkIndex": 65,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-66",
      "content": "ribution over the output space; (3) can resolve contradictions in feedback according to different definitions of fairness; (4) are designed to be used with online data, where the direction of feedback is implied by r θ or some external reward data.\n\nEcologically valid evaluation (De Vries et al., 2020), where the aligned models are deployed in real-world settings, are also needed to judge the merits of different HALOs.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "5. Future Work",
        "chunkIndex": 66,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-67",
      "content": "Although model alignment has historically been rewardcentric, we found that the inductive biases of alignment objectives are critical to their success. Moreover, these inductive biases have analogs in the prospect theory literature, suggesting that they work in part because they reflect human biases in decision-making. We abstracted these insights into a family of alignment objectives called human-aware losses (HALOs). We then proposed a HALO called KahnemanTversky Optimization (KTO) for directly maximizing the utility of generations instead of maximizing the likelihood of preferences, as existing methods do. Despite only learning from a binary signal of whether an output is (un)desirable, KTO was as good or better than preference-based methods in our experiments. More broadly, our work suggests that akin to how there is no one reward model that is universally superior, there is no one loss function either-the best HALO depends on the inductive biases that are most appropriate for a gi",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "6. Conclusion",
        "chunkIndex": 67,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-68",
      "content": "ggests that akin to how there is no one reward model that is universally superior, there is no one loss function either-the best HALO depends on the inductive biases that are most appropriate for a given setting, and much work remains to be done in identifying the best HALO for each context.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "6. Conclusion",
        "chunkIndex": 68,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-69",
      "content": "We thank Percy Liang, Dilip Arumugam, Arya McCarthy, and Nathan Lambert for feedback. We thank Stas Bekman and Gautam Mittal for cluster assistance and Alex Manthey for helping with human evaluation.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 69,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-70",
      "content": "The methods discussed in this paper have the potential to make LLMs more helpful and safer, which is often needed for models deployed in production. It is possible that in making models more helpful, we increase the utility of one person at the expense of broader society. In aligning models with human feedback, one may also-without even fully recognizing it-be aligning to an unrepresentative subset of the population, which may hinder the ability of individuals outside that subset to benefit equally from using the model.\n\nThe data used for LLM alignment, including the datasets used in this paper (e.g., SHP, HH, OASST) contain preferences of groups that are not representative of the broader population. Biases in this data have the potential to be propagated downstream when used to align models with methods like KTO, especially when no efforts are made to adjust for the different population.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Impact Statement",
        "chunkIndex": 70,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-71",
      "content": "ulation. Biases in this data have the potential to be propagated downstream when used to align models with methods like KTO, especially when no efforts are made to adjust for the different population. KTO in particular implicitly resolves contradictions in feedback by taking the majoritypreferred outcome for a loss-neutral value function, which does not comport with many theories of fairness (e.g., Rawlsianism). Since user preferences are, in turn, affected by the models they interact with, this also risks the homogenization of preferences and utility functions when KTO-aligned models are deployed at scale. The design of HALOs that resolve contradictions in more diverse ways is an important direction for future work.\n\nOn the other hand, because KTO works with binary feedback, which is more abundant, cheaper, and faster to collect in the real world, it significantly lowers the barrier to data",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Impact Statement",
        "chunkIndex": 71,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-72",
      "content": "ction for future work.\n\nOn the other hand, because KTO works with binary feedback, which is more abundant, cheaper, and faster to collect in the real world, it significantly lowers the barrier to data\n\ncollection. This makes it easier to collect feedback from traditionally under-represented groups and serve different models to different users, instead of just one monolithic model being served to everyone.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "Impact Statement",
        "chunkIndex": 72,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-73",
      "content": "- Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics , pp. 4447-4455. PMLR, 2024.\n- Baheti, A., Lu, X., Brahman, F., Le Bras, R., Sap, M., and Riedl, M. Improving language models with advantagebased offline policy gradients. In The Twelfth International Conference on Learning Representations , 2023.\n- Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n- Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-74",
      "content": "., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning , pp. 2397-2430. PMLR, 2023.\n- Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika , 39(3/4):324-345, 1952.\n- Busa-Fekete, R., Sz¨ or´ enyi, B., Weng, P., Cheng, W., and H¨ ullermeier, E. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. Machine learning , 97:327-351, 2014.\n- Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research , 2023.\n- Chan, L., Critch, A., and Dragan, A.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-75",
      "content": "D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research , 2023.\n- Chan, L., Critch, A., and Dragan, A. Human irrationality: both bad and good for reward inference. arXiv preprint arXiv:2111.06956 , 2021.\n- Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\n- Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335 , 2024.\n- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-76",
      "content": "s to strong language models. arXiv preprint arXiv:2401.01335 , 2024.\n- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017.\n- Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y ., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\n- Dao, T., Fu, D., Ermon, S., Rudra, A., and R´ e, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- De Vries, H., Bahdanau, D., and Manning, C. Towards ecologically valid research on language user interfaces.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-77",
      "content": "h io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- De Vries, H., Bahdanau, D., and Manning, C. Towards ecologically valid research on language user interfaces. arXiv preprint arXiv:2007.14435 , 2020.\n- Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\n- Ethayarajh, K., Choi, Y., and Swayamdipta, S. Understanding dataset difficulty with V -usable information. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 5988-6008. PMLR, 17-23 Jul 2022.\n- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-78",
      "content": "gs of Machine Learning Research , pp. 5988-6008. PMLR, 17-23 Jul 2022.\n- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 , 2022.\n- Gurevich, G., Kliger, D., and Levy, O. Decision-making under uncertainty-a field study of cumulative prospect theory. Journal of Banking &amp; Finance , 33(7):1221-1229, 2009.\n- He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web , pp. 173-182, 2017.\n- Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-79",
      "content": "asart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n\n- Hoeffler, S. and Ariely, D. Constructing stable preferences: A look into dimensions of experience and their impact on preference stability. Journal of consumer psychology , 8 (2):113-139, 1999.\n- Holm, S. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics , pp. 65-70, 1979.\n- Hong, J., Lee, N., and Thorne, J. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691 , 2024.\n- Jain, A., Wojcik, B., Joachims, T., and Saxena, A. Learning trajectory preferences for manipulators via iterative improvement. Advances in neural information processing systems , 26, 2013.\n- Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-80",
      "content": "rences for manipulators via iterative improvement. Advances in neural information processing systems , 26, 2013.\n- Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\n- Kahneman, D. and Tversky, A. Prospect theory: An analysis of decision under risk. Econometrica , 47(2):263-292, 1979.\n- K¨ opf, A., Kilcher, Y., von R¨ utte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. arXiv preprint arXiv:2304.07327 , 2023.\n- Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E. Pretraining language models with human preferences. In International Conference on Machine Learning , pp. 1750617533. PMLR, 2023.\n- Koren, Y., Bell, R., and Volinsky, C.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-81",
      "content": ", Bowman, S. R., and Perez, E. Pretraining language models with human preferences. In International Conference on Machine Learning , pp. 1750617533. PMLR, 2023.\n- Koren, Y., Bell, R., and Volinsky, C. Matrix factorization techniques for recommender systems. Computer , 42(8): 30-37, 2009.\n- Kreutzer, J., Uyheng, J., and Riezler, S. Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 17771788, 2018.\n- Kwon, M., Biyik, E., Talati, A., Bhasin, K., Losey, D. P., and Sadigh, D. When humans aren't optimal: Robots that collaborate with risk-aware humans. In Proceedings of the 2020 ACM/IEEE international conference on humanrobot interaction , pp. 43-52, 2020.\n- Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-82",
      "content": "ings of the 2020 ACM/IEEE international conference on humanrobot interaction , pp. 43-52, 2020.\n- Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/ alpaca\\_eval , 2023.\n- Lu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West, P., Ammanabrolu, P., and Choi, Y. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems , 35:27591-27609, 2022.\n- Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T., Michi, A., et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886 , 2023.\n- Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 82,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-83",
      "content": "2023.\n- Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744, 2022.\n- Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.\n- Peters, J. and Schaal, S. Reinforcement learning by rewardweighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning , pp. 745-750, 2007.\n- Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. In Thirtyseventh Conference on Neural Information Processing Systems , 2023.\n- Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 83,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-84",
      "content": "anguage model is secretly a reward model. In Thirtyseventh Conference on Neural Information Processing Systems , 2023.\n- Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715 , 2024.\n- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n- Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 2022.\n- Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33: 3008-3021, 2020.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 84,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-85",
      "content": "J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33: 3008-3021, 2020.\n\n- Sun, L., Zhan, W., Hu, Y., and Tomizuka, M. Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) , pp. 4329-4335. IEEE, 2019.\n- Swamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 , 2024.\n- Tian, K., Mitchell, E., Yao, H., Manning, C. D., and Finn, C. Fine-tuning language models for factuality. arXiv preprint arXiv:2311.08401 , 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 85,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-86",
      "content": ", 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., and Wolf, T. Zephyr: Direct distillation of lm alignment, 2023.\n- Tversky, A. and Kahneman, D. Availability: A heuristic for judging frequency and probability. Cognitive psychology , 5(2):207-232, 1973.\n- Tversky, A. and Kahneman, D. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty , 5:297-323, 1992.\n- von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S. Trl: Transformer reinforcement learning. https://github.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 86,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-87",
      "content": "nal of Risk and uncertainty , 5:297-323, 1992.\n- von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl , 2020.\n- Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. In International Conference on Learning Representations , 2019.\n- Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van Durme, B., Murray, K., and Kim, Y. J. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417 , 2024.\n- Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n- Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 87,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-88",
      "content": "C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n- Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020 , 2024.\n- Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 , 2023.\n- Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\n- Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "References",
        "chunkIndex": 88,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-89",
      "content": "LLM Alignment Human feedback has been used to improve LLM capabilities in translation (Kreutzer et al., 2018), summarization (Stiennon et al., 2020), sentiment-conditioned generation (Ziegler et al., 2019), and instruction-following (Ouyang et al., 2022). The RLHF framework (Christiano et al., 2017; Bai et al., 2022) traditionally used to accomplish this is detailed in §2. Still, momentum has largely shifted in favor of closed-form losses that directly operate on offline preferences, such as DPO (Rafailov et al., 2023). This single stage of optimization distinguishes DPO from the conventional approach in preference-based RL, which learns a reward and then fits the policy to those rewards (Jain et al., 2013; Busa-Fekete et al., 2014). Other preference-based losses include CPO (Xu et al., 2024) and IPO (Azar et al., 2024).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "A. Related Work",
        "chunkIndex": 89,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-90",
      "content": "which learns a reward and then fits the policy to those rewards (Jain et al., 2013; Busa-Fekete et al., 2014). Other preference-based losses include CPO (Xu et al., 2024) and IPO (Azar et al., 2024).\n\nBinary Feedback Despite not being a human-aware loss, unlikelihood training was among the first methods to align language models using a binary signal (Welleck et al., 2019). However, Korbak et al. (2023) found unlikelihood training to be worse than the CSFT baseline we tested in this work, which is among various approaches that convert a binary/discrete signal into a control token (Lu et al., 2022). Learning from sparse binary feedback is a staple of information retrieval and recommender systems (He et al., 2017; Koren et al., 2009). Many retrieval-augmented generation systems use contrastive learning to ensure that generations are grounded.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "A. Related Work",
        "chunkIndex": 90,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-91",
      "content": "le of information retrieval and recommender systems (He et al., 2017; Koren et al., 2009). Many retrieval-augmented generation systems use contrastive learning to ensure that generations are grounded. This can be framed as learning from synthetic binary feedback, although depending on the implementation, it may be the retriever and not the LLM that is updated (Lu et al., 2022).\n\nOnline Alignment A recent string of work has centered on the idea of 'self-training' or 'self-play', during which the policy is continually aligned on online data sampled from itself and then filtered (Chen et al., 2024; Yuan et al., 2024). Many of these approaches frame the learning of a preference model as a two-player min-max game between two policies (Munos et al., 2023; Rosset et al., 2024; Swamy et al., 2024). In theory, KTO can also be adapted for online alignment, though we leave this as a direction for future work.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "A. Related Work",
        "chunkIndex": 91,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-92",
      "content": "x game between two policies (Munos et al., 2023; Rosset et al., 2024; Swamy et al., 2024). In theory, KTO can also be adapted for online alignment, though we leave this as a direction for future work.\n\nProspect Theory Prospect theory, despite being influential in behavioral economics, has had a muted impact in machine learning, with work concentrated in human-robot interaction (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "A. Related Work",
        "chunkIndex": 92,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-93",
      "content": "Theorem 3.5 (restated) DPO and PPO-Clip are human-aware loss functions.\n\nProof. For a loss f to be a HALO, we need to first construct the human value\n\n<!-- formula-not-decoded -->\n\nwhere r θ ( x, y ) = l ( x, y ) log π θ ( y | x ) π ref ( y | x ) is the implied reward (normalized by factor l ( y ) ), Q ( Y ′ | x ) is an input-conditioned reference point distribution, and v : ❘ → ❘ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in (0 , ∞ ) .\n\nThe DPO loss is\n\n<!-- formula-not-decoded -->\n\nwhere β &gt; 0 is a hyperparameter. DPO meets the criteria with the following construction: l ( y ) = β ; r θ = β log π θ ( y | x ) π ref ( y | x ) ; v ( · ) = log σ ( · ) is increasing and concave everywhere; Q places all mass on ( x, y l ) , where y l is a dispreferred output for x such that y ≻ y l ; and a x,y = -1 .\n\nThe PPO-Clip loss is\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 93,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-94",
      "content": "s increasing and concave everywhere; Q places all mass on ( x, y l ) , where y l is a dispreferred output for x such that y ≻ y l ; and a x,y = -1 .\n\nThe PPO-Clip loss is\n\n<!-- formula-not-decoded -->\n\nwhere q θ = π θ ( y t | x : y &lt;t ) π ref ( y t | x : y &lt;t ) are the token-level probability ratios (where y &lt;t denotes the output sequence up to the t -th token), A denotes the token-level advantages, and ϵ ∈ (0 , 1) is a hyperparameter.\n\nSince this is a token-level objective, let x : y &lt;t denote the actual input and the token y i the actual output for the purpose of framing this as a HALO. The advantage function A ( x : y &lt;t , y t ) can be expressed as Q π ( x : y &lt;t , y t ) -V π ( x : y &lt;t ) , the\n\ndifference between the action-value and value functions. Because V π ( x : y &lt;t ) = ❊ y ∼ π Q π ( x : y &lt;t , y ) , the reference point distribution is simply the policy.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 94,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-95",
      "content": "π ( x : y &lt;t ) , the\n\ndifference between the action-value and value functions. Because V π ( x : y &lt;t ) = ❊ y ∼ π Q π ( x : y &lt;t , y ) , the reference point distribution is simply the policy.\n\nThe HALO-defined reward r θ is then implied by the product q θ Q π ( x : y &lt;t , y ) . Assume without loss of generality that Q π is non-negative, since a constant can be added to Q π without changing the advantage. Then means ∃ u ≥ 1 , q θ Q π ( x : y &lt;t , y ) = log u = log ˆ π θ ( x : y &lt;t , y ) / ˆ π ref ( x : y &lt;t , y ) , where ˆ π θ , ˆ π ref are some implied policy and reference distributions. It is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise. Where q θ A = r θ -z 0 in the HALO notation:\n\n<!-- formula-not-decoded -->\n\nwhich we can combine as v ( q θ A ) = min( q θ A,A (1 + sign ( q θ A ) ϵ )) . a x,y = -1 completes the construction.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 95,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-96",
      "content": "cewise. Where q θ A = r θ -z 0 in the HALO notation:\n\n<!-- formula-not-decoded -->\n\nwhich we can combine as v ( q θ A ) = min( q θ A,A (1 + sign ( q θ A ) ϵ )) . a x,y = -1 completes the construction.\n\nProposition 4.1 (restated) As the reward r θ ( x, y ) implied by the current policy tends to ±∞ , the KTO update of π θ tends to zero.\n\nProof. Where d ( y ) is -1(+1) when y is desirable(undesirable), λ y is λ D ( λ U ) when y is desirable(undesirable), and z = r θ ( x, y ) -z 0 , the derivative of the KTO loss is\n\n<!-- formula-not-decoded -->\n\nNote that we do not backpropagate through the KL term in the KTO loss and β, λ y &gt; 0 . This gradient is simple to interpret: if y is desirable, then d ( y ) is negative and we push up the probability of π θ ( y | x ) to minimize the loss; if y is undesirable, then d ( y ) is positive and we push down the probability of π θ ( y | x ) to minimize the loss.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 96,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-97",
      "content": "is negative and we push up the probability of π θ ( y | x ) to minimize the loss; if y is undesirable, then d ( y ) is positive and we push down the probability of π θ ( y | x ) to minimize the loss. As r θ tends to ±∞ , the gradient will tend to zero since either (1 -σ ( βz )) or σ ( βz ) will tend to zero.\n\nTheorem 4.2 (restated) Assuming the value function is logistic, for a reward function r ∗ a that maximizes (2), there exists a reward function in its equivalence class (i.e., r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) for some h ( x ) ) that induces the same optimal policy π ∗ and the same Bradley-Terry preference distribution but a different human value distribution.\n\nProof. Following the definition in Rafailov et al. (2023), we say r ∗ a and r ∗ b are in the same equivalence class if there exists some function h ( x ) such that r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) . From Lemma 1 in Rafailov et al.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 97,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-98",
      "content": "ilov et al. (2023), we say r ∗ a and r ∗ b are in the same equivalence class if there exists some function h ( x ) such that r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) . From Lemma 1 in Rafailov et al. (2023), we know that two functions in the same equivalence class induce the same optimal policy:\n\n<!-- formula-not-decoded -->\n\nFor a Bradley-Terry model of preferences, it is trivial to show that p ( y w ≻ y l | x ) is unaffected by h ( x ) since it is added to the reward of both y w and y l . We will now show that the two reward functions do not necessarily induce the same distribution of human values.\n\nFirst, we assume\n\nA Taylor series expansion of the human value of r ∗ a ( x, y ) around 0 would be:\n\n<!-- formula-not-decoded -->\n\nA Taylor series expansion of the value of r ∗ a ( x, y ) + h ( x ) around h ( x ) would be:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 98,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-99",
      "content": "uman value of r ∗ a ( x, y ) around 0 would be:\n\n<!-- formula-not-decoded -->\n\nA Taylor series expansion of the value of r ∗ a ( x, y ) + h ( x ) around h ( x ) would be:\n\n<!-- formula-not-decoded -->\n\nSince σ is strictly monotonic, for these series to be equal, we must have h ( x ) = 0 . If this is not the case, then the values of r ∗ a ( x, y ) and r ∗ b ( x, y ) will be different. Thus two arbitrary reward functions in the same equivalence class do not induce the same distribution of human values.\n\nTheorem 4.3 (restated) For input x with outputs { y a , y b } , let dataset D comprise contradictory preferences y a ≻ y b and y b ≻ y a in proportion p ∈ (0 . 5 , 1) and (1 -p ) ∈ (0 , 0 . 5) respectively. If p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , then the optimal DPO policy is more likely to produce the minority-preferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-neutral value function ( λ D = λ U ).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 99,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-100",
      "content": "n the optimal DPO policy is more likely to produce the minority-preferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-neutral value function ( λ D = λ U ).\n\nProof. Where u = β ( r θ ( x, y a ) -r θ ( x, y b )) , we can write the total DPO loss for x as\n\n<!-- formula-not-decoded -->\n\nTaking the derivative with respect to u and setting to zero, we get\n\n<!-- formula-not-decoded -->\n\nThus when p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , we have π ∗ θ ( y a | x ) &lt; π ∗ θ ( y b | x ) , meaning the optimal DPO policy is more likely to produce the minority-preferred y b .\n\nWhere u a = β ( r θ ( x, y a ) -E Q [ r θ ( x, y ′ )]) and u b = β ( r θ ( x, y b ) -E Q [ r θ ( x, y ′ )]) , noting that 1 -σ ( -u ) = σ ( u ) , we can write the total KTO loss for x as\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 100,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-101",
      "content": "r θ ( x, y a ) -E Q [ r θ ( x, y ′ )]) and u b = β ( r θ ( x, y b ) -E Q [ r θ ( x, y ′ )]) , noting that 1 -σ ( -u ) = σ ( u ) , we can write the total KTO loss for x as\n\n<!-- formula-not-decoded -->\n\nGiven that p &gt; 0 . 5 by assumption and λ D &gt; 0 by definition, the KTO loss is decreasing in u a and increasing in u b -and thus decreasing in r θ ( x, y a ) and increasing in r θ ( x, y b ) respectively. The optimal KTO policy is thus π ∗ θ ( y | x ) = ✶ [ y = y a ] .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "B. Proofs",
        "chunkIndex": 101,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-102",
      "content": "SLiC Instead of sampling from the reference model to calculate the L reg as Zhao et al. (2023) do-as it is very slow-we just apply the cross-entropy loss to the SFT data, assuming that the reference model recovers the SFT distribution.\n\nDPO We use the implementation of DPO in the code provided by Rafailov et al. (2023). We found that, as mentioned in the original paper, β = 0 . 1 works best for most settings. Other training configurations, such as the learning rate and optimizer, were borrowed from the original paper.\n\nCSFT The control tokens used for generating the good and bad outputs are ⟨| good |⟩ and ⟨| bad |⟩ respectively, following the precedent set in Korbak et al. (2023).\n\nKTO We use a β = 0 . 1 in our experiments unless otherwise specified (the same setting as for DPO), as it is close-tooptimal for most settings. By default, λ D = λ U = 1 .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "C. Implementations",
        "chunkIndex": 102,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-103",
      "content": "et in Korbak et al. (2023).\n\nKTO We use a β = 0 . 1 in our experiments unless otherwise specified (the same setting as for DPO), as it is close-tooptimal for most settings. By default, λ D = λ U = 1 . In experiments on imbalanced data subsampled from [SHP, HH, OASST], we found that setting λ U , λ D such that the effective ratio of desirable:undesirable examples was 4:3 worked best, regardless of which group was in the minority (see (9)). However, in running data-imbalanced experiments on UltraFeedback, we found that an effective ratio of 1:1 worked best. The other hyperparameters (e.g., learning rate) are the same as in DPO.\n\nPPO PPO-Clip is the traditional means of optimizing the RLHF objective (2). However, most implementations of PPO-Clip for LLM alignment suffer from instability, particularly during distributed training. We find that running the PPO-Clip objective on offline data with the following 'tricks' leads to much more stable training:",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "C. Implementations",
        "chunkIndex": 103,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-104",
      "content": "alignment suffer from instability, particularly during distributed training. We find that running the PPO-Clip objective on offline data with the following 'tricks' leads to much more stable training:\n\n- We never update the reference distribution (i.e., the policy only takes one step in the trust region). Baheti et al. (2023) recommend this as well. To accommodate for this conservative change, we clip the probability ratios more liberally, finding that an asymmetric interval of [0 . 25 , 4 . 0] works best instead of the small symmetrical interval (e.g., [0 . 8 , 1 . 2] ) that is traditionally recommended.\n- Including a KL penalty (between the policy and reference distributions) in addition to the clipping makes training more stable, as is also done in the implementation by von Werra et al. (2020).",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "C. Implementations",
        "chunkIndex": 104,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-105",
      "content": "ded.\n- Including a KL penalty (between the policy and reference distributions) in addition to the clipping makes training more stable, as is also done in the implementation by von Werra et al. (2020). We find that it is important to estimate the KL term not using the entire distribution but rather as the mean difference in the predicted log probabilities of the actual output tokens (i.e., the labels). We suspect that this makes a difference because the rest of the distribution can be poorly calibrated.\n- The value of a state is generally predicted by some value head attached to the policy model; the value loss is the MSE between the predicted value and the discounted sum of future rewards for each token. This is a linear layer in many RLHF implementations (von Werra et al., 2020). However, we find that backpropagating the value loss through this head and the policy leads to worse performance.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "C. Implementations",
        "chunkIndex": 105,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-106",
      "content": "h token. This is a linear layer in many RLHF implementations (von Werra et al., 2020). However, we find that backpropagating the value loss through this head and the policy leads to worse performance. Instead, we make the value head a 3-layer MLP and detach it from the computational graph, so that the value losses are not backpropagated through the policy model but the value head still has sufficient capacity to learn good estimates.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "C. Implementations",
        "chunkIndex": 106,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-107",
      "content": "For human evaluation, we randomly sampled 256 prompts from the OpenAssistant test set and generated outputs from Mistral 7B models aligned with DPO and KTO. All inputs were multi-turn conversations between a user and an assistant, where the LLM played the role of the assistant (see Table 6 for an example) and the last turn in the input was that of the user. These were sent to a third-party data annotation service where a pool of workers picked either the generated output or the SFT target (from the OpenAssistant dataset) as the more appropriate response by the assistant. Any questions that required specific domain experience (e.g., coding) were skipped, leading to 214 comparisons for DPO and KTO each.\n\nThe winrates of the aligned model over the SFT targets are 72 . 9% ± 5 . 3 for KTO and 62 . 1% ± 5 . 7 for DPO (where the intervals are 90% binomial confidence intervals). In contrast, Table 3 contains the winrates when the same experiment is run with GPT-4 as a judge instead: 65 .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "D. Human Evaluation",
        "chunkIndex": 107,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-108",
      "content": "and 62 . 1% ± 5 . 7 for DPO (where the intervals are 90% binomial confidence intervals). In contrast, Table 3 contains the winrates when the same experiment is run with GPT-4 as a judge instead: 65 . 2% ± 3 . 6 for KTO and 60 . 0% ± 3 . 7 for DPO. Thus although there is no significant difference in the GPT-4-based evaluation, there is a significant difference with human evaluation at p &lt; 0 . 05 . We found that 68.7% of the individual human judgments concurred with the GPT-4 judgments for KTO; this number fell to 65.9% for DPO.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "D. Human Evaluation",
        "chunkIndex": 108,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-109",
      "content": "Table 4. KTO alignment with only desirable/positive examples (i.e., the same data that would be used for SFT), yields similar results to SFT when β is small; this is intuitive because it is less bound to the reference model. λ D is fixed at 1 .\n\n| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                    1   |                              54.4 |                 33.2 |                   25.4 |                       7.17 |   29.7 |\n| KTO ( β = 0 .",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 109,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-110",
      "content": "|             57.2 |                    1   |                              54.4 |                 33.2 |                   25.4 |                       7.17 |   29.7 |\n| KTO ( β = 0 . 1 )                        |             57.8 |                   12.5 |                              53.2 |                 29.5 |                   28   |                       5.46 |   31.1 |\n| KTO ( β = 0 . 01 )                       |             57.3 |                    1.5 |                              53.7 |                 33.4 |                   26.5 |                       5.31 |   29.6 |\n\nFigure 6. AlpacaEval 2 winrates of Mistral7B aligned with various methods on the OpenAssistant dataset. GPT-4-turbo with chain-ofthought is used as the LM judge. KTO outperforms the SFT and DPO-aligned models at all temperatures.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 110,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-111",
      "content": "aligned with various methods on the OpenAssistant dataset. GPT-4-turbo with chain-ofthought is used as the LM judge. KTO outperforms the SFT and DPO-aligned models at all temperatures.\n\n<!-- image -->\n\nTable 5. Results from aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback. Note that the base model had not been finetuned on UltraFeedback but a similar dataset called UltraChat; the first row here denotes the finetuning results. All the methods below see exactly 1 epoch of UltraFeedback, for an apples-to-apples comparison. AlpacaEval 2 was run using GPT4 Turbo CoT as the LM judge. All evaluations were run with sampling temperature 0 . 7 . Among the KTO variants: 'oney -perx ' denotes training standard KTO with either the positive or negative output (not both); 'no z 0 ' refers to a constant reference point of 0 ; 'concave' refers to a variant where the human value function is concave in the loss regime (i.e., minimize -log( · ) instead of 1 -( · ) ); 'no π ref' refers to a r",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 111,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-112",
      "content": "o a constant reference point of 0 ; 'concave' refers to a variant where the human value function is concave in the loss regime (i.e., minimize -log( · ) instead of 1 -( · ) ); 'no π ref' refers to a reference model-free variant; 'risk-neutral' refers to a variant where the human value function is just the identity function, implying risk neutrality.\n\n| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                   39   |                              30.1 |                 46.3 |                   36.3 |                        6.4 |   35.9 |\n| DPO",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 112,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-113",
      "content": "|             57.2 |                   39   |                              30.1 |                 46.3 |                   36.3 |                        6.4 |   35.9 |\n| DPO                                      |             58.2 |                   40   |                              30.1 |                 44.1 |                   36.5 |                        7.8 |   36.1 |\n| ORPO ( λ = 0 . 1 )                       |             57.1 |                   36.5 |                              29.5 |                 47.5 |                   38.5 |                        5   |   35.7 |\n| KTO                                      |             58.6 |                   53.5 |                              30.9 |                 52.6 |                   31.2 |                       12.5 |   39.9 |\n| KTO (one- y -per- x )                    |             58   |                   50   |                              30.7 |                 49.9 |                   37.7 |",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 113,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-114",
      "content": "12.5 |   39.9 |\n| KTO (one- y -per- x )                    |             58   |                   50   |                              30.7 |                 49.9 |                   37.7 |                       11.6 |   39.7 |\n| KTO (no z 0 )                            |             58.5 |                   49.5 |                              30.7 |                 49   |                   29   |                       12.1 |   38.1 |\n| KTO (concave)                            |             58.3 |                   42.5 |                              30.6 |                 43.2 |                   36.6 |                        7.4 |   36.4 |\n| KTO (no π ref , λ D = 1 . 75 )           |             57.5 |                   47.5 |                              29.5 |                 51.6 |                   36.8 |                        9.7 |   38.8 |\n| KTO (risk-neutral)                       |             57.3 |                   42   |                              28.8 |",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 114,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-115",
      "content": "36.8 |                        9.7 |   38.8 |\n| KTO (risk-neutral)                       |             57.3 |                   42   |                              28.8 |                  6.1 |                   33.6 |                       10.3 |   29.7 |\n\n| Prompt            | < | user | > I need some show recommendations that are similar to Breaking Bad. < | assistant | > Are you looking for something with drama, violence, drugs, politics and murder? < | user | > This is exactly what I'm looking for in a show < | assistant | >                                                                                                                                                                             |\n|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 115,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-116",
      "content": "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SFT               | Well, you've just asked for one of the best shows ever made, and they all have those elements in some combination. There are a lot of shows with those elements.                                                                                                                                                                                                                                                                            |\n| SFT+KTO           | Some great choices for you might be Narcos, Sons of Anarchy, and Better Call Saul.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 116,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-117",
      "content": "|\n| SFT+KTO           | Some great choices for you might be Narcos, Sons of Anarchy, and Better Call Saul. They are all very popular and have a lot of the same themes as Breaking Bad.                                                                                                                                                                                                                                                                             |\n| SFT+DPO           | Well, you might like *******, which is also a dark, suspenseful drama about political intrigue and crime. Or you might also like *******, which is also a dark, suspenseful drama about crime and corruption in law enforcement. -                                                                                                                                                                                                          |",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 117,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-118",
      "content": "|\n|                   | My friend has some really bad opinions about TV shows, and I want to correct him, can you help? -                                                                                                                                                                                                                                                                                                                                           |\n|                   | Sure! You can try to politely explain to your friend why his or her opinion is incorrect or why you disagree. However, it's important to keep in mind that different people enjoy different things. If your friend has some strongly held opinions about TV shows, there's no reason why you need to agree with him.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 118,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-119",
      "content": ". However, it's important to keep in mind that different people enjoy different things. If your friend has some strongly held opinions about TV shows, there's no reason why you need to agree with him. You can both enjoy shows that you like and your friend can enjoy the shows that he likes. That's just the way it is. - |\n|                   | Is there any TV show similar to Breaking Bad that doesn't have violence in it? -                                                                                                                                                                                                                                                                                                                                                            |\n|                   | It's very hard to find a TV show that doesn't have any violence in it at all. However, there are some TV shows that are not focused on violence.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 119,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-120",
      "content": "|\n|                   | It's very hard to find a TV show that doesn't have any violence in it at all. However, there are some TV shows that are not focused on violence. For example, you might like ******* or *******, which are both TV shows about people who solve crimes without using violence                                                                                                                                                               |\n|                   | or killing people. -                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                   | Do you have any shows that are similar t",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 120,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-121",
      "content": "|\n|                   | Do you have any shows that are similar to Breaking Bad but without all the crime and violence? -                                                                                                                                                                                                                                                                                                                                            |\n|                   | Sure! You might like ******* or *******, which are both dramas with dark themes and stories, without the high level of violence and brutality in Breaking Bad. Or you might like *******, which is also a dark and suspenseful drama.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 121,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-122",
      "content": "or *******, which are both dramas with dark themes and stories, without the high level of violence and brutality in Breaking Bad. Or you might like *******, which is also a dark and suspenseful drama.                                                                                                                                                                                                       |\n|                   | -                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n|                   | Where can I find a list of TV shows similar to Breaking Bad, with all the plot elements and details? -",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 122,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-123",
      "content": "|\n|                   | Where can I find a list of TV shows similar to Breaking Bad, with all the plot elements and details? -                                                                                                                                                                                                                                                                                                                                      |\n|                   | I'm not sure there's an exact list of TV shows similar to Breaking Bad, but you can search online to find shows like this. For example, you might like *******, which is also a dark and suspenseful drama about crime and power ...                                                                                                                                                                                                        |\n| SFT+PPO (offline) | Well, you've just ask",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 123,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-124",
      "content": "|\n| SFT+PPO (offline) | Well, you've just asked for one of the best shows ever made, and they only made 5 seasons of it. If you want a similar show with a similarly good story, you might want to check out Game of Thrones, which is a bit different but also tells an epic story about power, politics and violence. Let me know if you need any more recommendations, it's no problem!                                                                          |\n\nTable 6. Sample generations from the different aligned versions of Llama-30B for a prompt about show recommendations (all models were aligned with data following the user-assistant format). Note that the SFT answer is not helpful and the SFT+DPO answer hallucinates multiple turns of the conversation (in fact, we had to truncate the answer shown here because the complete answer is too lon",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 124,
        "totalChunks": 126
      }
    },
    {
      "id": "2402.01306v4-chunk-125",
      "content": "Note that the SFT answer is not helpful and the SFT+DPO answer hallucinates multiple turns of the conversation (in fact, we had to truncate the answer shown here because the complete answer is too long). The SFT+PPO (offline) answer is helpful but only provides one recommendation, while SFT+KTO is succinct and provides multiple options.",
      "metadata": {
        "source": "arxiv:2402.01306v4",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
        "authors": [
          "Kawin Ethayarajh",
          "Winnie Xu",
          "Niklas Muennighoff",
          "Dan Jurafsky",
          "Douwe Kiela"
        ],
        "section": "E. Additional Experiments",
        "chunkIndex": 125,
        "totalChunks": 126
      }
    }
  ],
  "fullText": "## KTO: Model Alignment as Prospect Theoretic Optimization\n\nKawin Ethayarajh 1 Winnie Xu 2 Niklas Muennighoff 2 Dan Jurafsky 1 Douwe Kiela 1 2\n\n## Abstract\n\nKahneman &amp; Tversky's prospect theory tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases-the success of these objectives (e.g., DPO) over crossentropy minimization can partly be ascribed to them belonging to a family of loss functions that we call human-aware losses (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.\n\n## 1. Introduction\n\nAligning generative models with human feedback has been successfully used to make generations more helpful, factual, and ethical, among other desiderata (Ouyang et al., 2022; Tian et al., 2023). For LLMs, alignment methods such as RLHF and DPO have consistently proven to be more beneficial than doing supervised finetuning (SFT) alone. However, human feedback is often discussed only in the context of preferences (e.g., output y w ≻ y l for input x ), even though it can take many forms (e.g., approval/disapproval of y given\n\n1 Stanford University (first author was an intern at Contextual AI) 2 Contextual AI. Correspondence to: Kawin Ethayarajh &lt; kawin@stanford.edu &gt; .\n\nProceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\n\nFigure 1. The utility that a human gets from the outcome of a random variable, as implied by different human-aware losses (HALOs). Notice that the implied value functions share properties such as loss aversion with the canonical human value function in prospect theory (Tversky &amp; Kahneman, 1992).\n\n<!-- image -->\n\nx ). This is because preferences, despite being a kind of data that is relatively scarce and expensive to collect in practice (Casper et al., 2023), are required by the alignment methods shown to work best-RLHF (Christiano et al., 2017) and DPO (Rafailov et al., 2023).\n\nTo understand why these methods work so well, and whether feedback needs to be in preference form, we frame alignment through the lens of prospect theory (Kahneman &amp; Tversky, 1979; Tversky &amp; Kahneman, 1992). Prospect theory explains why humans make decisions about uncertain events that do not maximize their expected value. It formalizes how humans perceive random variables in a biased but well-defined manner; for example, relative to some reference point, humans are more sensitive to losses than gains, a property called loss aversion . We show that popular alignment methods such as DPO and PPO-Clip (Schulman et al., 2017) implicitly model some of these biases, helping explain their success independently of the data used (§3.2). We then propose a more general class of such loss functions called human-aware losses (HALOs). 1\n\n1 We use the term human-aware to draw an analogy with how hardware-aware methods benefit from being designed around hardware limitations (Dao et al., 2022), not to claim that a simple loss function is fully aware of human behavior.\n\nAlthough it is impossible to say that HALOs are categorically better than non-HALOs, we find that among existing methods, those that meet the definition of a HALO work better than those that do not (§3.3). We find that DPO performance can even be matched at most scales by running an offline PPO variant on dummy +1/-1 rewards, suggesting that preference data might not be needed if the inductive bias in the loss function is good enough. However, despite the surprising success of this simple baseline, it significantly lags behind DPO at the 30B LLM scale and suffers from hyperparameter sensitivity, making it difficult to use.\n\nTaking a more principled approach, we derive a HALO using the model of human utility that Kahneman &amp; Tversky proposed to describe how humans make decisions about uncertain monetary outcomes (Tversky &amp; Kahneman, 1992). This approach, which we call Kahneman-Tversky Optimization (KTO), directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as most current methods do (§4.1). KTO only requires a binary signal of whether an output is desirable or undesirable for an input. This data is more abundant, cheaper, and faster to collect in the real world, making it easier to scale alignment in production and rapidly iterate on models. We find that:\n\n- KTO matches or exceeds DPO performance at scales from 1B to 30B parameters (§4.3). 2 That is, breaking up n preferences meant for DPO into 2 n examples for KTO can yield better LLM generations, as determined by closed-ended tasks such as mathematical reasoning and open-ended judgments from humans and GPT-4 .\n- KTO can handle extreme data imbalances, matching DPO performance while using up to 90% fewer desirable examples (i.e., examples of good generations). Its success cannot be ascribed to the alignment data being sourced from a preference dataset.\n- When the pretrained model is sufficiently good, one can skip supervised finetuning (SFT) and go straight to KTO without a loss in generation quality, whereas SFT is always needed for best results with DPO.\n\nThe intent behind KTO was that even if the model learns from a weaker signal, we could compensate with the higher volume of data that could be accessed in practice; the fact that KTO can match and even outperform DPO on the same data is thus surprising. We conclude by discussing some theoretical explanations for this phenomenon (§4.4). Despite the success of KTO in our experiments, our work ultimately suggests that there is no one HALO that is universally superior; the best HALO depends on the inductive biases appropriate for a given setting, and this choice should be made deliberately instead of defaulting to any one loss.\n\n2 Our code is available on Github; models are on Huggingface.\n\n## 2. Background\n\nFor a detailed survey, please see Appendix A. In brief, LLMs are traditionally trained in three stages (Ouyang et al., 2022):\n\nPretraining Given a large corpus, train the model to maximize the log-likelihood of the next token conditioned on the preceding text. Let π 0 denote the pretrained model.\n\nSupervised Finetuning (SFT) Finetune the model to predict the next token on data that is more relevant to the downstream task. Often, such data will comprise instructions and an appropriate response (i.e., instruction finetuning). Let π ref denote the finetuned model.\n\nRLHF Given a dataset D of preferences ( x, y w , y l ) -where x is an input, y w , y l are the preferred and dispreferred outputs (i.e., y w ≻ y l for x ), and r ∗ is the 'true' reward function underlying the preferences-it is first assumed that the probability that y w is preferred to y l can be captured with a specific function class, typically a Bradley-Terry model (Bradley &amp; Terry, 1952). Where σ is the logistic function:\n\n<!-- formula-not-decoded -->\n\nSince getting the true reward from a human would be intractably expensive, a reward model r ϕ learns to serve as a proxy, done by minimizing the negative log-likelihood of the human preference data:\n\n<!-- formula-not-decoded -->\n\nBut solely maximizing the reward might come at the expense of desiderata such as generating grammatical text. To avoid this, a KL divergence penalty is introduced to restrict how far the language model can drift from π ref. Where π θ is the model we are optimizing, the optimal model π ∗ is that which maximizes\n\n<!-- formula-not-decoded -->\n\nwhere β &gt; 0 is a hyperparameter. Since this objective is not differentiable, we need to use an RL algorithm like PPO (Schulman et al., 2017).\n\nHowever, RLHF is often slow (largely because of having to sample generations) and quite unstable in practice (especially in a distributed setting). For this reason, recent work has focused on designing closed-form losses that maximize the margin between the preferred and dispreferred generations. In particular, Direct Preference Optimization (DPO) (Rafailov et al., 2023) has emerged as a popular alternative as it allows the same optimal policy as in RLHF to be recovered under certain conditions:\n\n<!-- formula-not-decoded -->\n\n## 3. A Prospect Theoretic View of Alignment\n\nTo understand why alignment methods work so well, we now frame them through the lens of prospect theory (Tversky &amp; Kahneman, 1992). Prospect theory explains why, when faced with an uncertain event, humans make decisions that do not maximize their expected value. For example, because humans are loss-averse, given a gamble that returns $100 with 80% probability and $60 with 20% probability, a person might accept $60 to avoid the gamble, despite their certainty equivalent of $60 being less than the expected value of $80.\n\n## 3.1. Prospect Theory\n\nIn prospect theory, human utility depends on a value function and a weighting function : 3\n\nDefinition 3.1. A value function v : Z → R maps an outcome z , relative to some reference point z 0 , to its perceived (or subjective) value. For example, these functions capture the fact that humans tend to be more sensitive to relative losses than relative gains of the same magnitude.\n\nDefinition 3.2. A weighting function ω is the derivative of a capacity function that maps cumulative probabilities to perceived cumulative probabilities. These functions capture, for example, the fact that humans tend to overestimate the chance of rare events. Let ω z denote the weight placed on outcome z .\n\nDefinition 3.3. The utility of a random variable Z is a function of its outcomes: u ( Z ) ≜ ∑ z ∈ Z ω z v ( z -z 0 ) .\n\nHowever, because humans do not see the full probability distribution of an LLM, weighting functions are not salient to this discussion; we will focus only on value functions. Using experiments that presented real humans with monetary gambles and asked for their certainty equivalent, Tversky &amp; Kahneman (1992) proposed the following functional form for human value:\n\n<!-- formula-not-decoded -->\n\nwhere the median value of hyperparameter α = 0 . 88 and λ = 2 . 25 across individuals. α controls the curvature of the function, which reflects risk aversion; λ controls its steepness, which reflects loss aversion. While the shape of the median Kahneman-Tversky value function is illustrated in Figure 1, it should be noted that it varies across individuals (Tversky &amp; Kahneman, 1992). There are also other functional forms for the value function that have been proposed in later work (Gurevich et al., 2009). The salient qualities of a value function are: the existence of a reference point that\n\n3 Cumulative prospect theory is the full name of the expanded theory we dicuss here (Tversky &amp; Kahneman, 1992).\n\nis used to get the relative gain or loss; concavity in relative gains (i.e., diminishing sensitivity away from z 0 ); and loss aversion (i.e., greater sensitivity to losses).\n\n## 3.2. HALOs\n\nDefinition 3.4 ( HALOs ) . Let θ denote the trainable parameters of the model π θ : X → P ( Y ) being aligned, π ref the reference model, l : Y → ❘ + a normalizing factor, and r θ ( x, y ) = l ( y ) log[ π θ ( y | x ) /π ref ( y | x )] the implied reward. Where Q ( Y ′ | x ) is a reference point distribution over Y and v : ❘ → ❘ is non-decreasing everywhere and concave in (0 , ∞ ) , the human value of ( x, y ) is\n\n<!-- formula-not-decoded -->\n\nA function f is a human-aware loss for v if ∃ a x,y ∈ {-1 , +1 } such that:\n\n<!-- formula-not-decoded -->\n\nwhere D is the feedback data and C D ∈ R is a data-specific constant.\n\nIn a classic prospect theory experiment, r θ would be the dollar amount assigned to each outcome; here, r θ is measured in nats, as the decrease in conditional surprisal when going from π ref to π θ , normalized according to l . This follows naturally from the next-token prediction objective used to pretrain and finetune LLMs. As π θ is aligned, we would expect r θ to grow increasingly positive for desirable outputs and increasingly negative for undesirable outputs.\n\nAnother perspective on the reward comes from the RLHF objective in (2). The policy that maximizes this objective has a closed-form expression (Peng et al., 2019; Peters &amp; Schaal, 2007); where Z ( x ) is the partition function:\n\nLetting l ( · ) = β , we get\n\n<!-- formula-not-decoded -->\n\nUnder θ ∗ , the HALO-defined reward is just the optimal reward shifted by an input-specific term, meaning that r θ ∗ is in the same equivalence class as r ∗ and would also induce the optimal policy π ∗ (Lemma 1, Rafailov et al. (2023)).\n\nThe reference point in a HALO is the expected reward from the human's perspective, where Q ( Y ′ | x ) describes the examples that are used to construct a baseline. We require that the value function be concave in gains but not necessarily convex in losses-unlike the canonical KahnemanTversky value function-because a minority of individuals are risk-averse in both the gain and loss regime, resulting in concavity everywhere (Kahneman &amp; Tversky, 1979).\n\nFigure 2. HALOs (DPO, offline PPO variant) outperform non-HALOs (SLiC, CSFT), as measured by the GPT-4-0613 -judged winrate of the aligned model's generations against a hard-to-beat baseline: the outputs that would have been used as the targets for SFT. The y -axis here plots the winrate above chance (i.e., the winrate - 50%). The difference between methods is only significant ( p &lt; 0 . 05) at 13B+ parameters, and only the HALO-aligned Llama-{ 13B, 30B } models are able to match the baseline and yield a winrate at or above chance.\n\n<!-- image -->\n\n## Theorem 3.5. DPOand PPO-Clip are human-aware losses.\n\nThe proof is deferred to Appendix B. In Figure 1, we can see this more intuitively by plotting the value function (i.e., the implied human utility).\n\n## 3.3. Does being a HALO matter?\n\nIt is difficult to answer whether being a HALO is useful, since both HALOs and non-HALOs are diverse function classes, but we attempt to do so by comparing popular methods that qualify as a HALO with those that do not:\n\n1. CSFT : Conditional SFT is a simple alignment method where a control token is prepended to the output during training; then, at inference, the control token corresponding to desirable generations is appended to the input to induce good generations (Korbak et al., 2023). This is not a HALO, since that would demand that -log π ref ( y | x ) always equal the reference point; however, since the reference point is fixed for a given x , this is not possible when π ref is a non-uniform distribution.\n2. SLiC : Sequence Likelihood Calibration (Zhao et al., 2023), which combines a max-margin loss for preferences with a language modeling loss, is not a HALO (for the same reasons given above for CSFT):\n\n<!-- formula-not-decoded -->\n\n3. DPO : DPO is a HALO (Theorem 3.5).\n4. PPO (offline) : The standard RLHF objective in (2) is typically optimized with PPO-Clip, which works by\n\n'clipping' how far π θ can drift from the version π old at the previous step:\n\n<!-- formula-not-decoded -->\n\nwhere q θ = π θ ( y t | x : y &lt;t ) π old ( y t | x : y &lt;t ) and A ( x : y &lt;t , y t ) is the pertoken advantage (i.e., the surplus benefit from producing a given token in a given state).\n\nPPO is an online algorithm-generations are sampled from the current model, judged by a reward model, and then used to update the current version. However, for a fair comparison with offline methods, we consider a variant of PPO that is only fed offline data. Because RLHF is also quite unstable in a distributed setting, we never update π old and keep it as π ref, instead clipping less conservatively than we traditionally would. Baheti et al. (2023) found that these changes, along with treating the entire output sequence as a single action, greatly improves stability. However, since RLHF traditionally uses token-level advantages, we omit the third change and only preserve the first two. The PPO-Clip loss is unchanged and remains a HALO (Theorem 3.5).\n\nCalling this method PPO is somewhat imprecise, because it is offline and takes only one step, but to avoid introducing too many new terms, we will call this PPO (offline) . Instead of using learned rewards, we simplify even further and use dummy +1/-1 rewards 4 for y w and y l instead. Further details on the implementation of this method can be found in Appendix C.\n\nExperimental Setup We compare these baselines on two model families, Pythia-{ 1.4B, 2.8B, 6.9B, 12B } (Bider-\n\n4 Note that +1/-1 do not refer to the HALO-implied rewards, but rather the rewards used for the advantage A ( x : y &lt;t , y t ) calculation.\n\nman et al., 2023) and Llama-{ 7B, 13B, 30B } (Touvron et al., 2023). This permits us to see how LLM alignment scales within a model family (Llama-2 lacks a 30B model, hence our use of Llama). Later experiments (§4.3) are done on Mistral-7B derivatives (Jiang et al., 2023) and Llama-3 (Dubey et al., 2024). The models are trained on a combination of Anthropic-HH (Ganguli et al., 2022), OpenAssistant (K¨ opf et al., 2023), and SHP (Ethayarajh et al., 2022).\n\nAll models are aligned under identical settings on the same data, save for hyperparameters unique to them. Similar to Rafailov et al. (2023), the target sequences for SFT are a subset of { y w } . We use GPT-4-0613 to judge whether the aligned model's response is better than the SFT target for a given test input with respect to helpfulness, harmlessness, and conciseness, a now standard practice (Zheng et al., 2023; Li et al., 2023). 5 Note that while the SFT target is considered a desirable output for x , it is by no means the best output, meaning that an aligned model can certainly achieve a winrate above 50%, although this is difficult given that many of the target sequences are human-written.\n\nIn Figure 2, we see the results of this analysis:\n\n- HALOs either match or outperform non-HALOs at every scale , though the gap is only significant ( p &lt; 0 . 05) at 13B+ model sizes after correcting for multiple comparisons (Holm, 1979). In fact, only the HALOaligned Llama-{ 13B, 30B } models match or exceed a win rate of 50% (i.e., are able to match or exceed the generation quality of the SFT targets in the test data).\n- Up to a scale of 7B parameters, alignment provides virtually no gains over SFT alone. However, it is worth noting that if the base models were more performant, or if the SFT data distribution were less similar to the preference data, then the gains from the alignment stage would ostensibly be greater.\n- Despite only using dummy +1/-1 rewards, our offline PPO variant performs as well as DPO for all models except Llama-30B. This challenges conventional wisdom, which places heavy emphasis on reward learning (Casper et al., 2023), and suggests that even the simplest rewards can prove useful when used in a loss function that has the right inductive bias. Despite its success, our offline PPO baseline still suffers from hyperparameter sensitivity and training instability, albeit not to the same extent as traditional RLHF.\n\n## 4. Kahneman-Tversky Optimization\n\nThe surprising success of offline PPO with dummy +1/-1 rewards suggests that-with the right inductive biases-a\n\n5 We validate that GPT-4 judgments concur with human judgments in Appendix D.\n\nbinary signal of good/bad generations may be sufficient to reach DPO-level performance, even if the offline PPO approach itself was unable to do so past a certain scale (§3.3). Taking a more principled approach, we now derive a HALOusing the Kahneman-Tversky model of human value, which allows us to directly optimize for utility instead of maximizing the log-likelihood of preferences.\n\n## 4.1. Derivation\n\nThe canonical Kahneman-Tversky value function (4) suffers from numerical instability during optimization due to the exponent a , so we replace it with the logistic function σ , which is also concave in gains and convex in losses.\n\nTo control the degree of risk aversion, we introduce a hyperparameter β ∈ ❘ + as part of the value function. The greater β is, the more quickly the value saturates, meaning the human is simultaneously more risk-averse in gains and more risk-seeking in losses. In practice, this has a similar effect as β in the DPO loss, which controls how far π θ drifts from π ref, though we introduce it here explicitly to control risk aversion; in DPO, it carries over from the KL constraint in the RLHF objective (2) and is part of the reward.\n\nWe replace the loss aversion coefficient λ in the original Kahneman-Tversky value function (4) with { λ D , λ U } , where λ D and λ U are hyperparameters for desirable and undesirable outputs respectively; more complex schemes could also be used for importance sampling.\n\nRather than having just one dispreferred generation serve as the reference point z 0 , as in DPO, we assume that humans judge the quality of y | x in relation to all possible outputs. This implies that Q ( Y ′ | x ) is the policy and that the reference point is the KL divergence KL ( π θ ( y ′ | x ) ∥ π ref ( y ′ | x )) . However, as we discuss further below, we end up taking a biased estimate of this term in practice.\n\nWhere λ y denotes λ D ( λ U ) when y is desirable(undesirable) respectively, the default KTO loss is: 6\n\n<!-- formula-not-decoded -->\n\nwhere\n\n<!-- formula-not-decoded -->\n\n6 An earlier draft separated out λ D , λ U into a function w . For an easier comparison with (4), they-along with β -have been moved into the value function itself. Note that λ y exists solely to make the loss non-negative and can be removed. The use of multiple x ′ to estimate z 0 in practice was included in the loss definition originally, but has now been moved outside for clarity.\n\nTable 1. Recommended hyperparameter settings for different losses and models when aligned on UltraFeedback, evaluated on the benchmarks discussed in §4.3. The hyperparameter sweeps were done with AdamW, an effective batch size of 32, and λ D = λ U = 1 . Depending on your task and ratio of desirable:undesirable examples, the optimal choice of λ D , λ U might be significantly different.\n\n| Model               | Method   |    LR |    β |   AlpacaEval (LC) ↑ |   BBH ↑ |   GSM8K (8-shot) ↑ |\n|---------------------|----------|-------|------|---------------------|---------|--------------------|\n| Llama-3 8B          | SFT+KTO  | 5e-06 | 0.05 |               10.59 |   65.15 |              60.2  |\n| Llama-3 8B          | KTO      | 5e-06 | 0.1  |               11.25 |   65.26 |              57.92 |\n| Qwen2.5 3B Instruct | SFT+KTO  | 5e-06 | 0.1  |               13.01 |   32.39 |              61.11 |\n| Qwen2.5 3B Instruct | KTO      | 5e-06 | 0.5  |               16.63 |   20.41 |              60.35 |\n\nFor more stable training, we do not backpropagate through z 0 ; it exists purely to control the loss saturation.\n\nIntuitively, KTO works as follows: if the model increases the reward of a desirable example in a blunt manner, then the KL penalty also rises and no progress is made. This forces the model to learn exactly what makes an output desirable, so that the reward can be increased while keeping the KL term flat (or even decreasing it). The argument works in the other direction as well, though the non-negativity of the KL term allows faster saturation in the loss regime.\n\nKL Estimate In practice, estimating z 0 as it is defined above is impractical because sampling from π θ is slow. Instead, we take a biased but convenient estimate by shifting outputs in the same microbatch to induce mismatched pairs { ( x 1 , y 2 ) , ( x 2 , y 3 ) , ..., ( x m , y 0 ) } , then estimating a shared reference point z 0 for all examples in the same microbatch as follows. Where j = ( i +1) mod m ,\n\n<!-- formula-not-decoded -->\n\nBecause of clamping, our estimator has a positive bias but lower variance than the standard unbiased estimator. Although it costs an additional forward pass, we use a mismatched output y j instead of the corresponding y i because the latter have often been deliberately chosen to be canonically good or bad outputs, and thus have unrepresentative high-magnitude rewards. It is worth noting that although our estimator is biased, so would the human-perceived reference point, since humans do not perceive the full distribution induced by π θ and would employ an 'availability heuristic' that would overweight outputs for which they have recently given feedback, regardless of whether those outputs are a good continuation of x (Tversky &amp; Kahneman, 1973).\n\nIf KTO is preceded by SFT done on the same data that is used as desirable feedback and the SFT model is used as π ref, then the KL estimate will quickly approach zero. Having already learned what is desirable during SFT, the policy will tend to scatter the mass placed on undesirable examples, leading to minimal divergence. Also, because the policy may learn to place less mass on undesirable y i regardless of whether it is preceded by x i , ˆ z 0 might actually be an under-estimate. In such cases, one can avoid the extra computation and set ˆ z 0 = 0 . However, when KTO is not preceded by SFT, or when the SFT data is not a subset of the KTO data, estimating ˆ z 0 is necessary.\n\nData If the alignment data is naturally binary, every positive example can be assumed to be drawn from y desirable | x and every negative example from y undesirable | x . However, the canonical feedback datasets in academic research (HH, SHP, OASST) are in preference format, since the methods that have worked best up until now are preference-based. In our experiments, we convert preference data y w ≻ y l by assuming that y w is drawn from the desirable distribution and y l from the undesirable one. This is a naive assumption, made for the sake of simplicity, and a more complex deconstruction of preferences into binary feedback would likely yield better results, which we leave for future work. To show that KTO can be used with non-preference data, we also subsample exactly one y per x for some experiments (denoted oney -perx ), removing any trace of paired preferences at the cost of reducing the data volume.\n\nIf human feedback is in the form of scores or ratings, the simplest means of incorporating it into KTO is to construct a weighting function such that high-magnitude data is weighed more and that examples with scores above(below) some threshold are desirable(undesirable). It is also possible to construct score-based HALOs from first principles, but we leave the design of such losses to future work.\n\n## 4.2. Hyperparameters\n\nIn Table 1, we provide recommended hyperparameter settings for Llama-3 8B (Dubey et al., 2024) and Qwen2.5 3B Instruct (Yang et al., 2024) based on benchmarks such as MMLU (0-shot) (Hendrycks et al., 2021), GSM8K (8shot, chain-of-thought) (Cobbe et al., 2021), HumanEval (0-shot) (Chen et al., 2021), and BigBench-Hard (3-shot chain-of-thought) (Srivastava et al., 2022).\n\nLearning Rate We find that that the performance of an aligned model is more sensitive to the learning rate than any other hyperparameter. The optimal learning rate for KTO is\n\nFigure 3. KTO is as good or better than DPO at all scales, as measured by the GPT-4-0613 -judged winrate of the aligned model's generations against the outputs that would have been used for SFT. In fact, for the Llama models, KTO alone matches the performance of SFT+DPO and is significantly better than DPO alone. Error bars denote a 90% binomial confidence interval.\n\n<!-- image -->\n\nusually 2x to 10x the optimal learning rate for DPO; since the reference-adjusted reward tends to be much smaller in magnitude for KTO, one needs to use a more aggressive learning rate to compensate. For example, the default learning rate for DPO is 5e-7 (Rafailov et al., 2023), but we find that a default of 5e-6 works better for KTO. In our experiments, we use the default DPO learning rate with RMSProp for all methods to ensure an apples-to-apples comparison with Rafailov et al. (2023), but when using KTO in practice, we recommend starting at 5e-6 with AdamW and adjusting the learning rate as needed.\n\nBatch Size KTO needs a microbatch size ≥ 2 to estimate the reference point in a single step. The experiments in this paper all use an effective batch size of 32, and in general we recommend using a batch size between 8 and 128.\n\nRisk Aversion The degree of risk aversion/seeking is controlled by β ; the greater β is, the greater the risk aversion in gains and risk seeking in losses. In practice, lower values of β in the range [0.01, 0.10] work better for larger models that have already undergone SFT; higher values of β in the range [0.10, 1.00] work better for smaller models undergoing KTO directly, without SFT prior.\n\nLoss Aversion The default weighting function controls the degree of loss aversion with λ D , λ U , which are both set to 1 by default. In general, where n D and n U refer to the number of desirable and undesirable examples respectively, we find that it is generally best to set λ D , λ U such that\n\n<!-- formula-not-decoded -->\n\nFor example, if there were a 1:10 ratio of desirable to undesirable examples, we would set λ U = 1 , λ D ∈ [10 , 15] .\n\nThis interval was determined empirically, and implies thatafter adjusting for class imbalances-gain sensitivity yields better performance than loss sensitivity, ostensibly because producing good outputs is more important than avoiding bad outputs for success on most benchmarks. This is not a hard rule, however. In tasks where minimizing the downside is more important, like toxicity prevention, setting λ D , λ U such that λ D n D &lt; λ U n U may work better. Unless otherwise stated, we use λ D = λ U = 1 in our experiments.\n\nThe current configuration permits different sensitivities to desirable and undesirable examples based on the premise that upon convergence, all undesirable outputs will yield negative rewards and all desirable outputs will yield positive ones, in which case λ U , λ D would directly correspond to λ in the original Kahneman-Tversky value function. However, during training itself, a desirable output may have a negative reward (and vice-versa), yet λ D would be the same regardless of whether r θ ( x, y ) -z 0 were positive or negative. We may want to change this so that not only is asymmetry possible upon convergence, but even during training within the same class of output: in this case, using a higher λ D when r θ ( x, y ) -z 0 is negative than when it is positive. We leave the design of dynamic hyperparameter selection schemes as directions for future work.\n\n## 4.3. Experiments\n\nKTO ≥ DPO As seen in Figure 3, when rerunning the winrate evaluation in §3.3, SFT+KTO is competitive with SFT+DPO at scales from 1B to 30B, despite the model learning from a weaker signal. KTO alone is better than DPO alone for the Llama-{ 7B, 13B, 30B } models, and this gap is significant ( p &lt; 0 . 01 ) at 7B and 30B even after correcting for multiple comparisons (Holm, 1979). Among the Pythia models, there is no significant difference between the\n\nTable 2. (top) Results from aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback for exactly 1 epoch. Even when only one of the two outputs in each preference is seen by KTO, it still outperforms DPO, despite this reducing the volume of data by half (oney -perx ). (middle) Changing the structure of the KTO loss, even in subtle ways, makes the aligned model worse, supporting our design choices. (bottom) Fixing λ U = 1 , we try different levels of loss and risk aversion by changing λ D and β respectively (see Appendix C for more results).\n\n| Dataset ( → ) Metric ( → )       |   MMLU EM |   GSM8k EM |   HumanEval pass@1 |   BBH EM |\n|----------------------------------|-----------|------------|--------------------|----------|\n| SFT                              |      57.2 |       39   |               30.1 |     46.3 |\n| DPO                              |      58.2 |       40   |               30.1 |     44.1 |\n| ORPO ( λ = 0 . 1 )               |      57.1 |       36.5 |               29.5 |     47.5 |\n| KTO ( β = 0 . 1 , λ D = 1 )      |      58.6 |       53.5 |               30.9 |     52.6 |\n| KTO (one- y -per- x )            |      58   |       50   |               30.7 |     49.9 |\n| KTO (no z 0 )                    |      58.5 |       49.5 |               30.7 |     49   |\n| KTO (concave, v = log σ )        |      58.3 |       42.5 |               30.6 |     43.2 |\n| KTO (risk-neutral, v ( · ) = · ) |      57.3 |       42   |               28.8 |      6.1 |\n| KTO (no π ref , λ D = 1 . 75 )   |      57.5 |       47.5 |               29.5 |     51.6 |\n| KTO ( β = 0 . 01 )               |      57.3 |       54   |               33.2 |     49.8 |\n| KTO ( β = 0 . 50 )               |      58.3 |       45   |               32.7 |     51.8 |\n| KTO ( λ D = 0 . 50 )             |      57.4 |       47   |               32.2 |     30.5 |\n| KTO ( λ D = 0 . 75 )             |      58.3 |       52   |               30.3 |     50.6 |\n| KTO ( λ D = 1 . 33 )             |      58.5 |       52   |               32.8 |     45.4 |\n| KTO ( λ D = 2 . 00 )             |      58.1 |       43   |               32.1 |     45.3 |\n\ntwo, suggesting that a minimum model capacity is needed for these differences to emerge. KTO also fares better than DPO and other baselines on generative benchmarks (Table 2). This is most pronounced for certain tasks: on GSM8K, a mathematical reasoning dataset, just swapping DPO for KTO when aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback (Cui et al., 2023) improves performance by 13.5 points.\n\nAt sufficient scale, KTO does not need SFT. A KTOaligned Llama-{ 13B, 30B } model is competitive with its SFT+KTO counterpart despite not undergoing SFT first, and is the only alignment method of the ones we tested to show this behavior. This is perhaps due to KTO alone keeping the average response length roughly the same, while running DPO without SFT prior causes the response length to increase dramatically (Figure 4).\n\nKTOdataneed not come from preferences. Might KTO be secretly benefiting from its 2 n examples in the previous experiment coming from n preference pairs instead of a naturally unpaired data distribution? To test this, we randomly discard increasingly large fractions of the desirable data before KTO-aligning a Llama-7B model. For example, if we discard 90% of the desirable data while leaving the undesirable data untouched, then the ratio of desirable:undesirable examples goes from 1:1 to 1:10 and the vast majority of undesirable examples no longer have a preferred counterpart. We handle such imbalances by changing λ D , λ U to satisfy\n\nFigure 4. Without doing SFT first, DPO-aligned models tend to ramble and hallucinate entire conversations. KTO does not suffer from this issue.\n\n<!-- image -->\n\nthe criteria in (9); when we drop 90% of the desirable data, we set λ u = 1 , λ D = 13 . 33 for example. For Llama-7B, we find that up to 90% of the desirable data can in fact be discarded while still outperforming DPO (Figure 5).\n\nWe further verify this claim by aligning Mistral-7B on OpenAssistant using DPO (on n pairs), standard KTO (on all 2 n outputs), and KTO where only one y per x is used. Since the output of one y in OpenAssistant is not conditioned on the other y for the same x , the latter captures the setting where the data is from an inherently unpaired distribution. Despite the oney -perx setup decreasing the amount of training data by 72%, the KTO-aligned model still outperforms both its DPO counterpart and the official instruction-tuned Mistral7B (Jiang et al., 2023), as seen in Table 3.\n\n## Changing the design of KTO makes it significantly worse.\n\nFor one, removing the reference point z 0 -which is necessary for KTO to qualify as a HALO-causes a 3.6 and 4.0 point drop on BBH and GSM8K respectively (Table 2, middle). Even changes that allow KTO to remain a HALO are typically suboptimal. For example, removing the symmetry of the value function-going from 1 -σ ( · ) to -log σ ( · ) (i.e., making the value function concave everywhere, as in DPO)causes a 9.4 and 11.0 point drop on BBH and GSM8K respectively. Making the value function risk-neutral by setting it to the identity function leads to a total collapse in BBH performance. Changing the curvature and slope via the risk and loss aversion hyperparameters can, depending on the task, improve or degrade performance (Table 2, bottom).\n\nKTOworks without a reference model or SFT, but not as well as standard KTO. If we can avoid storing the reference model in memory, then we can do much more memoryefficient alignment. The naive way to do this is to assume that π ref returns a uniform distribution over outputs for all x , which simplifies r θ -z 0 to log π θ ( y | x ) -H ( π θ ( y ′ | x )) , where H denotes the entropy. As seen in Table 2 (middle), if we set λ D = 1 . 75 , this memory-efficient variant of KTO is better than DPO on some tasks and worse on others,\n\nFigure 5. A KTO-aligned Llama-7B model can match or exceed the performance of its DPO-aligned counterpart while aligned on a smaller and highly imbalanced version of the same dataset, even with as few as 0.1 positive/desirable examples for every negative/undesirable one.\n\n<!-- image -->\n\nthough it still trails standard KTO. We find that it is also more sensitive to the loss aversion hyperparameters; setting λ D ∈ { 1 . 5 , 2 . 0 } reduces performance on GSM8K and BBH by several points. Still, it strictly outperforms ORPO (Hong et al., 2024), a recently-proposed reference-free method, while also using less memory than all existing approaches, since π ref need never be loaded into memory and a batch of m KTO outputs is smaller than a batch of m preferences (i.e., 2 m outputs) used for ORPO/DPO.\n\n## 4.4. Theoretical Analysis\n\nKTO was designed with the motivation that even if binary feedback were weaker, one could compensate with sheer volume, as such data is much more abundant, cheaper, and faster to collect than preferences. So why does KTO perform as well or better than DPO on the same preference data (that has been broken up)? Greater data efficiency helps, but it is not the only answer, given that even after adjusting for this factor in the oney -perx setup, KTO still outperforms.\n\nIn this section, we will discuss two theoretical explanations for this phenomenon: (1) preference likelihood can be maximized without necessarily maximizing underlying human utility; (2) KTO implicitly ignores noisy and intransitive data, which is common in real-world feedback.\n\nProposition 4.1. As the reward implied by the current policy tends to ±∞ , the KTO update of π θ tends to zero.\n\nThis means that if ( x, y ) is implied by the current policy π θ to be too difficult or too easy to learn from, then it is effectively ignored. In some cases, this may be a blessing in disguise: since real-world feedback is very noisy (Hoeffler &amp;Ariely, 1999), the reason a desirable example has a highly negative implied reward may be because it is mislabelled. By avoiding this hard-to-learn data, KTO avoids fitting to noise. However, this also means that KTO could end up ig-\n\nTable 3. In aligning Mistral-7B on the OpenAssistant dataset, we find that using KTO with only one output per input still outperforms DPO, despite this restriction reducing the amount of training data by 72%. A 90% binomial confidence interval is given.\n\n| Method                          | Winrate vs. SFT Target   |\n|---------------------------------|--------------------------|\n| Mistral-7B (unaligned)          | 0.525 ± 0.037            |\n| Mistral-7B + DPO                | 0.600 ± 0.037            |\n| Mistral-7B + KTO (all y per x ) | 0.652 ± 0.036            |\n| Mistral-7B + KTO (one y per x ) | 0.631 ± 0.036            |\n| Mistral-7B-Instruct             | 0.621 ± 0.031            |\n\nnoring data that is hard-to-learn but necessary to recover r ∗ , potentially resulting in underfitting to complex distributions. Such underfitting may be mitigated by aligning the model with lower β and for more epochs.\n\nTheorem 4.2. Assuming the value function is logistic, for a reward function r ∗ a that maximizes (2), there exists a reward function in its equivalence class (i.e., r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) for some h ( x ) ) that induces the same optimal policy π ∗ and the same Bradley-Terry preference distribution but a different human value distribution.\n\nA key insight from Rafailov et al. (2023) is that reward functions in the same equivalence class (i.e., differing only in an input-specific component) induce the same optimal policy under (2) and the same Bradley-Terry preference distribution. However, we show under mild assumptions that the value distribution-i.e., human utility-is affected by such input-specific changes, so maximizing preference likelihood does not mean one is maximizing human utility. This helps explain why the margin between KTO and DPO is even bigger in human evaluations than it is in automated LLM-as-a-judge evaluations (Appendix D).\n\nTheorem 4.3. For input x with outputs { y a , y b } , let dataset D comprise contradictory preferences y a ≻ y b and y b ≻ y a in proportion p ∈ (0 . 5 , 1) and (1 -p ) ∈ (0 , 0 . 5) respectively. If p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , then the optimal DPO policy is more likely to produce the minoritypreferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-neutral value function ( λ D = λ U ).\n\nInformally, say there are two contradictory preferences over the output for x , with the majority p preferring y a and the minority 1 -p preferring y b . In the worst-case, when p is sufficiently low and the reference model is sufficiently unaligned, the optimal DPO policy is more likely to produce the minority-preferred output y b even though the implied reward r θ ( x, y b ) &gt; r θ ( x, y a ) . In contrast, the optimal KTO policy will deterministically produce the majority-preferred y a if the value function is loss-neutral ( λ D = λ U ), which is the default setting. This suggests that KTO has better worst-case outcomes when handling feedback intransitivity.\n\n## 4.5. KTO vs. DPO - when to use which?\n\nWhen human feedback is in a binary format, and especially when there is an imbalance between the number of desirable and undesirable examples, KTO is the natural choice. When your data is in the form of preferences, the choice is less clear. Putting aside the greater data efficiency of KTO, our theoretical analysis suggests that if your preference data has sufficiently little noise and sufficiently little intransitivity, then DPO will work better, since there is some risk of KTO underfitting; this risk can be mitigated by using a lower value of β and aligning with KTO for more epochs. But if there is enough noise and intransitivity, then the better worst-case guarantees of KTO will win out. Most publicly available preference datasets (e.g., SHP, OpenAssistant) contain noisy feedback from many different humans whose preferences likely contradict to some extent, which explains why KTO was able to match or exceed DPO performance in our experiments. Even synthetic feedback can be noisy and intransitive, which helps explain why KTO outperforms DPO when aligning with UltraFeedback.\n\n## 5. Future Work\n\nThe existence of HALOs raises many questions. For one, KTO is based on the Kahneman-Tversky value function for monetary gambles, which is almost certainly different from how humans perceive the relative goodness of text. What value functions and reference point distributions best describe how humans perceive language, and how do they vary across domains and individuals? How can we identify the best HALO for each individual and setting instead of using one default loss?\n\nOn a more technical level, important directions include developing HALOs that: (1) incorporate granular feedback, such as a score, especially when optimizing for multiple desiderata; (2) work for other modalities (e.g. images) and model classes (e.g., diffusion models), especially models that do not produce an explicit distribution over the output space; (3) can resolve contradictions in feedback according to different definitions of fairness; (4) are designed to be used with online data, where the direction of feedback is implied by r θ or some external reward data.\n\nEcologically valid evaluation (De Vries et al., 2020), where the aligned models are deployed in real-world settings, are also needed to judge the merits of different HALOs.\n\n## 6. Conclusion\n\nAlthough model alignment has historically been rewardcentric, we found that the inductive biases of alignment objectives are critical to their success. Moreover, these inductive biases have analogs in the prospect theory literature, suggesting that they work in part because they reflect human biases in decision-making. We abstracted these insights into a family of alignment objectives called human-aware losses (HALOs). We then proposed a HALO called KahnemanTversky Optimization (KTO) for directly maximizing the utility of generations instead of maximizing the likelihood of preferences, as existing methods do. Despite only learning from a binary signal of whether an output is (un)desirable, KTO was as good or better than preference-based methods in our experiments. More broadly, our work suggests that akin to how there is no one reward model that is universally superior, there is no one loss function either-the best HALO depends on the inductive biases that are most appropriate for a given setting, and much work remains to be done in identifying the best HALO for each context.\n\n## Acknowledgements\n\nWe thank Percy Liang, Dilip Arumugam, Arya McCarthy, and Nathan Lambert for feedback. We thank Stas Bekman and Gautam Mittal for cluster assistance and Alex Manthey for helping with human evaluation.\n\n## Impact Statement\n\nThe methods discussed in this paper have the potential to make LLMs more helpful and safer, which is often needed for models deployed in production. It is possible that in making models more helpful, we increase the utility of one person at the expense of broader society. In aligning models with human feedback, one may also-without even fully recognizing it-be aligning to an unrepresentative subset of the population, which may hinder the ability of individuals outside that subset to benefit equally from using the model.\n\nThe data used for LLM alignment, including the datasets used in this paper (e.g., SHP, HH, OASST) contain preferences of groups that are not representative of the broader population. Biases in this data have the potential to be propagated downstream when used to align models with methods like KTO, especially when no efforts are made to adjust for the different population. KTO in particular implicitly resolves contradictions in feedback by taking the majoritypreferred outcome for a loss-neutral value function, which does not comport with many theories of fairness (e.g., Rawlsianism). Since user preferences are, in turn, affected by the models they interact with, this also risks the homogenization of preferences and utility functions when KTO-aligned models are deployed at scale. The design of HALOs that resolve contradictions in more diverse ways is an important direction for future work.\n\nOn the other hand, because KTO works with binary feedback, which is more abundant, cheaper, and faster to collect in the real world, it significantly lowers the barrier to data\n\ncollection. This makes it easier to collect feedback from traditionally under-represented groups and serve different models to different users, instead of just one monolithic model being served to everyone.\n\n## References\n\n- Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics , pp. 4447-4455. PMLR, 2024.\n- Baheti, A., Lu, X., Brahman, F., Le Bras, R., Sap, M., and Riedl, M. Improving language models with advantagebased offline policy gradients. In The Twelfth International Conference on Learning Representations , 2023.\n- Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n- Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning , pp. 2397-2430. PMLR, 2023.\n- Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika , 39(3/4):324-345, 1952.\n- Busa-Fekete, R., Sz¨ or´ enyi, B., Weng, P., Cheng, W., and H¨ ullermeier, E. Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm. Machine learning , 97:327-351, 2014.\n- Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research , 2023.\n- Chan, L., Critch, A., and Dragan, A. Human irrationality: both bad and good for reward inference. arXiv preprint arXiv:2111.06956 , 2021.\n- Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\n- Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335 , 2024.\n- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017.\n- Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y ., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\n- Dao, T., Fu, D., Ermon, S., Rudra, A., and R´ e, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- De Vries, H., Bahdanau, D., and Manning, C. Towards ecologically valid research on language user interfaces. arXiv preprint arXiv:2007.14435 , 2020.\n- Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.\n- Ethayarajh, K., Choi, Y., and Swayamdipta, S. Understanding dataset difficulty with V -usable information. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp. 5988-6008. PMLR, 17-23 Jul 2022.\n- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 , 2022.\n- Gurevich, G., Kliger, D., and Levy, O. Decision-making under uncertainty-a field study of cumulative prospect theory. Journal of Banking &amp; Finance , 33(7):1221-1229, 2009.\n- He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web , pp. 173-182, 2017.\n- Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n\n- Hoeffler, S. and Ariely, D. Constructing stable preferences: A look into dimensions of experience and their impact on preference stability. Journal of consumer psychology , 8 (2):113-139, 1999.\n- Holm, S. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics , pp. 65-70, 1979.\n- Hong, J., Lee, N., and Thorne, J. Reference-free monolithic preference optimization with odds ratio. arXiv preprint arXiv:2403.07691 , 2024.\n- Jain, A., Wojcik, B., Joachims, T., and Saxena, A. Learning trajectory preferences for manipulators via iterative improvement. Advances in neural information processing systems , 26, 2013.\n- Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\n- Kahneman, D. and Tversky, A. Prospect theory: An analysis of decision under risk. Econometrica , 47(2):263-292, 1979.\n- K¨ opf, A., Kilcher, Y., von R¨ utte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversationsdemocratizing large language model alignment. arXiv preprint arXiv:2304.07327 , 2023.\n- Korbak, T., Shi, K., Chen, A., Bhalerao, R. V., Buckley, C., Phang, J., Bowman, S. R., and Perez, E. Pretraining language models with human preferences. In International Conference on Machine Learning , pp. 1750617533. PMLR, 2023.\n- Koren, Y., Bell, R., and Volinsky, C. Matrix factorization techniques for recommender systems. Computer , 42(8): 30-37, 2009.\n- Kreutzer, J., Uyheng, J., and Riezler, S. Reliability and learnability of human bandit feedback for sequence-tosequence reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 17771788, 2018.\n- Kwon, M., Biyik, E., Talati, A., Bhasin, K., Losey, D. P., and Sadigh, D. When humans aren't optimal: Robots that collaborate with risk-aware humans. In Proceedings of the 2020 ACM/IEEE international conference on humanrobot interaction , pp. 43-52, 2020.\n- Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/ alpaca\\_eval , 2023.\n- Lu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West, P., Ammanabrolu, P., and Choi, Y. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems , 35:27591-27609, 2022.\n- Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T., Michi, A., et al. Nash learning from human feedback. arXiv preprint arXiv:2312.00886 , 2023.\n- Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744, 2022.\n- Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.\n- Peters, J. and Schaal, S. Reinforcement learning by rewardweighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning , pp. 745-750, 2007.\n- Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. In Thirtyseventh Conference on Neural Information Processing Systems , 2023.\n- Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A., and Xie, T. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715 , 2024.\n- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n- Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 2022.\n- Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33: 3008-3021, 2020.\n\n- Sun, L., Zhan, W., Hu, Y., and Tomizuka, M. Interpretable modelling of driving behaviors in interactive driving scenarios based on cumulative prospect theory. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC) , pp. 4329-4335. IEEE, 2019.\n- Swamy, G., Dann, C., Kidambi, R., Wu, Z. S., and Agarwal, A. A minimaximalist approach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056 , 2024.\n- Tian, K., Mitchell, E., Yao, H., Manning, C. D., and Finn, C. Fine-tuning language models for factuality. arXiv preprint arXiv:2311.08401 , 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., and Wolf, T. Zephyr: Direct distillation of lm alignment, 2023.\n- Tversky, A. and Kahneman, D. Availability: A heuristic for judging frequency and probability. Cognitive psychology , 5(2):207-232, 1973.\n- Tversky, A. and Kahneman, D. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and uncertainty , 5:297-323, 1992.\n- von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl , 2020.\n- Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. In International Conference on Learning Representations , 2019.\n- Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Van Durme, B., Murray, K., and Kim, Y. J. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417 , 2024.\n- Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024.\n- Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J., and Weston, J. Self-rewarding language models. arXiv preprint arXiv:2401.10020 , 2024.\n- Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 , 2023.\n- Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\n- Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.\n\n## A. Related Work\n\nLLM Alignment Human feedback has been used to improve LLM capabilities in translation (Kreutzer et al., 2018), summarization (Stiennon et al., 2020), sentiment-conditioned generation (Ziegler et al., 2019), and instruction-following (Ouyang et al., 2022). The RLHF framework (Christiano et al., 2017; Bai et al., 2022) traditionally used to accomplish this is detailed in §2. Still, momentum has largely shifted in favor of closed-form losses that directly operate on offline preferences, such as DPO (Rafailov et al., 2023). This single stage of optimization distinguishes DPO from the conventional approach in preference-based RL, which learns a reward and then fits the policy to those rewards (Jain et al., 2013; Busa-Fekete et al., 2014). Other preference-based losses include CPO (Xu et al., 2024) and IPO (Azar et al., 2024).\n\nBinary Feedback Despite not being a human-aware loss, unlikelihood training was among the first methods to align language models using a binary signal (Welleck et al., 2019). However, Korbak et al. (2023) found unlikelihood training to be worse than the CSFT baseline we tested in this work, which is among various approaches that convert a binary/discrete signal into a control token (Lu et al., 2022). Learning from sparse binary feedback is a staple of information retrieval and recommender systems (He et al., 2017; Koren et al., 2009). Many retrieval-augmented generation systems use contrastive learning to ensure that generations are grounded. This can be framed as learning from synthetic binary feedback, although depending on the implementation, it may be the retriever and not the LLM that is updated (Lu et al., 2022).\n\nOnline Alignment A recent string of work has centered on the idea of 'self-training' or 'self-play', during which the policy is continually aligned on online data sampled from itself and then filtered (Chen et al., 2024; Yuan et al., 2024). Many of these approaches frame the learning of a preference model as a two-player min-max game between two policies (Munos et al., 2023; Rosset et al., 2024; Swamy et al., 2024). In theory, KTO can also be adapted for online alignment, though we leave this as a direction for future work.\n\nProspect Theory Prospect theory, despite being influential in behavioral economics, has had a muted impact in machine learning, with work concentrated in human-robot interaction (Kwon et al., 2020; Sun et al., 2019; Chan et al., 2021).\n\n## B. Proofs\n\nTheorem 3.5 (restated) DPO and PPO-Clip are human-aware loss functions.\n\nProof. For a loss f to be a HALO, we need to first construct the human value\n\n<!-- formula-not-decoded -->\n\nwhere r θ ( x, y ) = l ( x, y ) log π θ ( y | x ) π ref ( y | x ) is the implied reward (normalized by factor l ( y ) ), Q ( Y ′ | x ) is an input-conditioned reference point distribution, and v : ❘ → ❘ is a value function (in the prospect theoretic sense) that is non-decreasing everywhere and concave in (0 , ∞ ) .\n\nThe DPO loss is\n\n<!-- formula-not-decoded -->\n\nwhere β &gt; 0 is a hyperparameter. DPO meets the criteria with the following construction: l ( y ) = β ; r θ = β log π θ ( y | x ) π ref ( y | x ) ; v ( · ) = log σ ( · ) is increasing and concave everywhere; Q places all mass on ( x, y l ) , where y l is a dispreferred output for x such that y ≻ y l ; and a x,y = -1 .\n\nThe PPO-Clip loss is\n\n<!-- formula-not-decoded -->\n\nwhere q θ = π θ ( y t | x : y &lt;t ) π ref ( y t | x : y &lt;t ) are the token-level probability ratios (where y &lt;t denotes the output sequence up to the t -th token), A denotes the token-level advantages, and ϵ ∈ (0 , 1) is a hyperparameter.\n\nSince this is a token-level objective, let x : y &lt;t denote the actual input and the token y i the actual output for the purpose of framing this as a HALO. The advantage function A ( x : y &lt;t , y t ) can be expressed as Q π ( x : y &lt;t , y t ) -V π ( x : y &lt;t ) , the\n\ndifference between the action-value and value functions. Because V π ( x : y &lt;t ) = ❊ y ∼ π Q π ( x : y &lt;t , y ) , the reference point distribution is simply the policy.\n\nThe HALO-defined reward r θ is then implied by the product q θ Q π ( x : y &lt;t , y ) . Assume without loss of generality that Q π is non-negative, since a constant can be added to Q π without changing the advantage. Then means ∃ u ≥ 1 , q θ Q π ( x : y &lt;t , y ) = log u = log ˆ π θ ( x : y &lt;t , y ) / ˆ π ref ( x : y &lt;t , y ) , where ˆ π θ , ˆ π ref are some implied policy and reference distributions. It is trivial to show that the latter exist but are not unique.\n\nFor clarity, we can first write the value function piecewise. Where q θ A = r θ -z 0 in the HALO notation:\n\n<!-- formula-not-decoded -->\n\nwhich we can combine as v ( q θ A ) = min( q θ A,A (1 + sign ( q θ A ) ϵ )) . a x,y = -1 completes the construction.\n\nProposition 4.1 (restated) As the reward r θ ( x, y ) implied by the current policy tends to ±∞ , the KTO update of π θ tends to zero.\n\nProof. Where d ( y ) is -1(+1) when y is desirable(undesirable), λ y is λ D ( λ U ) when y is desirable(undesirable), and z = r θ ( x, y ) -z 0 , the derivative of the KTO loss is\n\n<!-- formula-not-decoded -->\n\nNote that we do not backpropagate through the KL term in the KTO loss and β, λ y &gt; 0 . This gradient is simple to interpret: if y is desirable, then d ( y ) is negative and we push up the probability of π θ ( y | x ) to minimize the loss; if y is undesirable, then d ( y ) is positive and we push down the probability of π θ ( y | x ) to minimize the loss. As r θ tends to ±∞ , the gradient will tend to zero since either (1 -σ ( βz )) or σ ( βz ) will tend to zero.\n\nTheorem 4.2 (restated) Assuming the value function is logistic, for a reward function r ∗ a that maximizes (2), there exists a reward function in its equivalence class (i.e., r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) for some h ( x ) ) that induces the same optimal policy π ∗ and the same Bradley-Terry preference distribution but a different human value distribution.\n\nProof. Following the definition in Rafailov et al. (2023), we say r ∗ a and r ∗ b are in the same equivalence class if there exists some function h ( x ) such that r ∗ b ( x, y ) = r ∗ a ( x, y ) + h ( x ) . From Lemma 1 in Rafailov et al. (2023), we know that two functions in the same equivalence class induce the same optimal policy:\n\n<!-- formula-not-decoded -->\n\nFor a Bradley-Terry model of preferences, it is trivial to show that p ( y w ≻ y l | x ) is unaffected by h ( x ) since it is added to the reward of both y w and y l . We will now show that the two reward functions do not necessarily induce the same distribution of human values.\n\nFirst, we assume\n\nA Taylor series expansion of the human value of r ∗ a ( x, y ) around 0 would be:\n\n<!-- formula-not-decoded -->\n\nA Taylor series expansion of the value of r ∗ a ( x, y ) + h ( x ) around h ( x ) would be:\n\n<!-- formula-not-decoded -->\n\nSince σ is strictly monotonic, for these series to be equal, we must have h ( x ) = 0 . If this is not the case, then the values of r ∗ a ( x, y ) and r ∗ b ( x, y ) will be different. Thus two arbitrary reward functions in the same equivalence class do not induce the same distribution of human values.\n\nTheorem 4.3 (restated) For input x with outputs { y a , y b } , let dataset D comprise contradictory preferences y a ≻ y b and y b ≻ y a in proportion p ∈ (0 . 5 , 1) and (1 -p ) ∈ (0 , 0 . 5) respectively. If p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , then the optimal DPO policy is more likely to produce the minority-preferred y b ; the optimal KTO policy will strictly produce the majority-preferred y a for a loss-neutral value function ( λ D = λ U ).\n\nProof. Where u = β ( r θ ( x, y a ) -r θ ( x, y b )) , we can write the total DPO loss for x as\n\n<!-- formula-not-decoded -->\n\nTaking the derivative with respect to u and setting to zero, we get\n\n<!-- formula-not-decoded -->\n\nThus when p 1 /β π ref ( y a | x ) &lt; (1 -p ) 1 /β π ref ( y b | x ) , we have π ∗ θ ( y a | x ) &lt; π ∗ θ ( y b | x ) , meaning the optimal DPO policy is more likely to produce the minority-preferred y b .\n\nWhere u a = β ( r θ ( x, y a ) -E Q [ r θ ( x, y ′ )]) and u b = β ( r θ ( x, y b ) -E Q [ r θ ( x, y ′ )]) , noting that 1 -σ ( -u ) = σ ( u ) , we can write the total KTO loss for x as\n\n<!-- formula-not-decoded -->\n\nGiven that p &gt; 0 . 5 by assumption and λ D &gt; 0 by definition, the KTO loss is decreasing in u a and increasing in u b -and thus decreasing in r θ ( x, y a ) and increasing in r θ ( x, y b ) respectively. The optimal KTO policy is thus π ∗ θ ( y | x ) = ✶ [ y = y a ] .\n\n## C. Implementations\n\nSLiC Instead of sampling from the reference model to calculate the L reg as Zhao et al. (2023) do-as it is very slow-we just apply the cross-entropy loss to the SFT data, assuming that the reference model recovers the SFT distribution.\n\nDPO We use the implementation of DPO in the code provided by Rafailov et al. (2023). We found that, as mentioned in the original paper, β = 0 . 1 works best for most settings. Other training configurations, such as the learning rate and optimizer, were borrowed from the original paper.\n\nCSFT The control tokens used for generating the good and bad outputs are ⟨| good |⟩ and ⟨| bad |⟩ respectively, following the precedent set in Korbak et al. (2023).\n\nKTO We use a β = 0 . 1 in our experiments unless otherwise specified (the same setting as for DPO), as it is close-tooptimal for most settings. By default, λ D = λ U = 1 . In experiments on imbalanced data subsampled from [SHP, HH, OASST], we found that setting λ U , λ D such that the effective ratio of desirable:undesirable examples was 4:3 worked best, regardless of which group was in the minority (see (9)). However, in running data-imbalanced experiments on UltraFeedback, we found that an effective ratio of 1:1 worked best. The other hyperparameters (e.g., learning rate) are the same as in DPO.\n\nPPO PPO-Clip is the traditional means of optimizing the RLHF objective (2). However, most implementations of PPO-Clip for LLM alignment suffer from instability, particularly during distributed training. We find that running the PPO-Clip objective on offline data with the following 'tricks' leads to much more stable training:\n\n- We never update the reference distribution (i.e., the policy only takes one step in the trust region). Baheti et al. (2023) recommend this as well. To accommodate for this conservative change, we clip the probability ratios more liberally, finding that an asymmetric interval of [0 . 25 , 4 . 0] works best instead of the small symmetrical interval (e.g., [0 . 8 , 1 . 2] ) that is traditionally recommended.\n- Including a KL penalty (between the policy and reference distributions) in addition to the clipping makes training more stable, as is also done in the implementation by von Werra et al. (2020). We find that it is important to estimate the KL term not using the entire distribution but rather as the mean difference in the predicted log probabilities of the actual output tokens (i.e., the labels). We suspect that this makes a difference because the rest of the distribution can be poorly calibrated.\n- The value of a state is generally predicted by some value head attached to the policy model; the value loss is the MSE between the predicted value and the discounted sum of future rewards for each token. This is a linear layer in many RLHF implementations (von Werra et al., 2020). However, we find that backpropagating the value loss through this head and the policy leads to worse performance. Instead, we make the value head a 3-layer MLP and detach it from the computational graph, so that the value losses are not backpropagated through the policy model but the value head still has sufficient capacity to learn good estimates.\n\n## D. Human Evaluation\n\nFor human evaluation, we randomly sampled 256 prompts from the OpenAssistant test set and generated outputs from Mistral 7B models aligned with DPO and KTO. All inputs were multi-turn conversations between a user and an assistant, where the LLM played the role of the assistant (see Table 6 for an example) and the last turn in the input was that of the user. These were sent to a third-party data annotation service where a pool of workers picked either the generated output or the SFT target (from the OpenAssistant dataset) as the more appropriate response by the assistant. Any questions that required specific domain experience (e.g., coding) were skipped, leading to 214 comparisons for DPO and KTO each.\n\nThe winrates of the aligned model over the SFT targets are 72 . 9% ± 5 . 3 for KTO and 62 . 1% ± 5 . 7 for DPO (where the intervals are 90% binomial confidence intervals). In contrast, Table 3 contains the winrates when the same experiment is run with GPT-4 as a judge instead: 65 . 2% ± 3 . 6 for KTO and 60 . 0% ± 3 . 7 for DPO. Thus although there is no significant difference in the GPT-4-based evaluation, there is a significant difference with human evaluation at p &lt; 0 . 05 . We found that 68.7% of the individual human judgments concurred with the GPT-4 judgments for KTO; this number fell to 65.9% for DPO.\n\n## E. Additional Experiments\n\nTable 4. KTO alignment with only desirable/positive examples (i.e., the same data that would be used for SFT), yields similar results to SFT when β is small; this is intuitive because it is less bound to the reference model. λ D is fixed at 1 .\n\n| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                    1   |                              54.4 |                 33.2 |                   25.4 |                       7.17 |   29.7 |\n| KTO ( β = 0 . 1 )                        |             57.8 |                   12.5 |                              53.2 |                 29.5 |                   28   |                       5.46 |   31.1 |\n| KTO ( β = 0 . 01 )                       |             57.3 |                    1.5 |                              53.7 |                 33.4 |                   26.5 |                       5.31 |   29.6 |\n\nFigure 6. AlpacaEval 2 winrates of Mistral7B aligned with various methods on the OpenAssistant dataset. GPT-4-turbo with chain-ofthought is used as the LM judge. KTO outperforms the SFT and DPO-aligned models at all temperatures.\n\n<!-- image -->\n\nTable 5. Results from aligning Zephyrβ -SFT (Tunstall et al., 2023) on UltraFeedback. Note that the base model had not been finetuned on UltraFeedback but a similar dataset called UltraChat; the first row here denotes the finetuning results. All the methods below see exactly 1 epoch of UltraFeedback, for an apples-to-apples comparison. AlpacaEval 2 was run using GPT4 Turbo CoT as the LM judge. All evaluations were run with sampling temperature 0 . 7 . Among the KTO variants: 'oney -perx ' denotes training standard KTO with either the positive or negative output (not both); 'no z 0 ' refers to a constant reference point of 0 ; 'concave' refers to a variant where the human value function is concave in the loss regime (i.e., minimize -log( · ) instead of 1 -( · ) ); 'no π ref' refers to a reference model-free variant; 'risk-neutral' refers to a variant where the human value function is just the identity function, implying risk neutrality.\n\n| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                   39   |                              30.1 |                 46.3 |                   36.3 |                        6.4 |   35.9 |\n| DPO                                      |             58.2 |                   40   |                              30.1 |                 44.1 |                   36.5 |                        7.8 |   36.1 |\n| ORPO ( λ = 0 . 1 )                       |             57.1 |                   36.5 |                              29.5 |                 47.5 |                   38.5 |                        5   |   35.7 |\n| KTO                                      |             58.6 |                   53.5 |                              30.9 |                 52.6 |                   31.2 |                       12.5 |   39.9 |\n| KTO (one- y -per- x )                    |             58   |                   50   |                              30.7 |                 49.9 |                   37.7 |                       11.6 |   39.7 |\n| KTO (no z 0 )                            |             58.5 |                   49.5 |                              30.7 |                 49   |                   29   |                       12.1 |   38.1 |\n| KTO (concave)                            |             58.3 |                   42.5 |                              30.6 |                 43.2 |                   36.6 |                        7.4 |   36.4 |\n| KTO (no π ref , λ D = 1 . 75 )           |             57.5 |                   47.5 |                              29.5 |                 51.6 |                   36.8 |                        9.7 |   38.8 |\n| KTO (risk-neutral)                       |             57.3 |                   42   |                              28.8 |                  6.1 |                   33.6 |                       10.3 |   29.7 |\n\n| Prompt            | < | user | > I need some show recommendations that are similar to Breaking Bad. < | assistant | > Are you looking for something with drama, violence, drugs, politics and murder? < | user | > This is exactly what I'm looking for in a show < | assistant | >                                                                                                                                                                             |\n|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SFT               | Well, you've just asked for one of the best shows ever made, and they all have those elements in some combination. There are a lot of shows with those elements.                                                                                                                                                                                                                                                                            |\n| SFT+KTO           | Some great choices for you might be Narcos, Sons of Anarchy, and Better Call Saul. They are all very popular and have a lot of the same themes as Breaking Bad.                                                                                                                                                                                                                                                                             |\n| SFT+DPO           | Well, you might like *******, which is also a dark, suspenseful drama about political intrigue and crime. Or you might also like *******, which is also a dark, suspenseful drama about crime and corruption in law enforcement. -                                                                                                                                                                                                          |\n|                   | My friend has some really bad opinions about TV shows, and I want to correct him, can you help? -                                                                                                                                                                                                                                                                                                                                           |\n|                   | Sure! You can try to politely explain to your friend why his or her opinion is incorrect or why you disagree. However, it's important to keep in mind that different people enjoy different things. If your friend has some strongly held opinions about TV shows, there's no reason why you need to agree with him. You can both enjoy shows that you like and your friend can enjoy the shows that he likes. That's just the way it is. - |\n|                   | Is there any TV show similar to Breaking Bad that doesn't have violence in it? -                                                                                                                                                                                                                                                                                                                                                            |\n|                   | It's very hard to find a TV show that doesn't have any violence in it at all. However, there are some TV shows that are not focused on violence. For example, you might like ******* or *******, which are both TV shows about people who solve crimes without using violence                                                                                                                                                               |\n|                   | or killing people. -                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                   | Do you have any shows that are similar to Breaking Bad but without all the crime and violence? -                                                                                                                                                                                                                                                                                                                                            |\n|                   | Sure! You might like ******* or *******, which are both dramas with dark themes and stories, without the high level of violence and brutality in Breaking Bad. Or you might like *******, which is also a dark and suspenseful drama.                                                                                                                                                                                                       |\n|                   | -                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n|                   | Where can I find a list of TV shows similar to Breaking Bad, with all the plot elements and details? -                                                                                                                                                                                                                                                                                                                                      |\n|                   | I'm not sure there's an exact list of TV shows similar to Breaking Bad, but you can search online to find shows like this. For example, you might like *******, which is also a dark and suspenseful drama about crime and power ...                                                                                                                                                                                                        |\n| SFT+PPO (offline) | Well, you've just asked for one of the best shows ever made, and they only made 5 seasons of it. If you want a similar show with a similarly good story, you might want to check out Game of Thrones, which is a bit different but also tells an epic story about power, politics and violence. Let me know if you need any more recommendations, it's no problem!                                                                          |\n\nTable 6. Sample generations from the different aligned versions of Llama-30B for a prompt about show recommendations (all models were aligned with data following the user-assistant format). Note that the SFT answer is not helpful and the SFT+DPO answer hallucinates multiple turns of the conversation (in fact, we had to truncate the answer shown here because the complete answer is too long). The SFT+PPO (offline) answer is helpful but only provides one recommendation, while SFT+KTO is succinct and provides multiple options.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model               | Method   |    LR |    β |   AlpacaEval (LC) ↑ |   BBH ↑ |   GSM8K (8-shot) ↑ |\n|---------------------|----------|-------|------|---------------------|---------|--------------------|\n| Llama-3 8B          | SFT+KTO  | 5e-06 | 0.05 |               10.59 |   65.15 |              60.2  |\n| Llama-3 8B          | KTO      | 5e-06 | 0.1  |               11.25 |   65.26 |              57.92 |\n| Qwen2.5 3B Instruct | SFT+KTO  | 5e-06 | 0.1  |               13.01 |   32.39 |              61.11 |\n| Qwen2.5 3B Instruct | KTO      | 5e-06 | 0.5  |               16.63 |   20.41 |              60.35 |"
    },
    {
      "index": 1,
      "markdown": "| Dataset ( → ) Metric ( → )       |   MMLU EM |   GSM8k EM |   HumanEval pass@1 |   BBH EM |\n|----------------------------------|-----------|------------|--------------------|----------|\n| SFT                              |      57.2 |       39   |               30.1 |     46.3 |\n| DPO                              |      58.2 |       40   |               30.1 |     44.1 |\n| ORPO ( λ = 0 . 1 )               |      57.1 |       36.5 |               29.5 |     47.5 |\n| KTO ( β = 0 . 1 , λ D = 1 )      |      58.6 |       53.5 |               30.9 |     52.6 |\n| KTO (one- y -per- x )            |      58   |       50   |               30.7 |     49.9 |\n| KTO (no z 0 )                    |      58.5 |       49.5 |               30.7 |     49   |\n| KTO (concave, v = log σ )        |      58.3 |       42.5 |               30.6 |     43.2 |\n| KTO (risk-neutral, v ( · ) = · ) |      57.3 |       42   |               28.8 |      6.1 |\n| KTO (no π ref , λ D = 1 . 75 )   |      57.5 |       47.5 |               29.5 |     51.6 |\n| KTO ( β = 0 . 01 )               |      57.3 |       54   |               33.2 |     49.8 |\n| KTO ( β = 0 . 50 )               |      58.3 |       45   |               32.7 |     51.8 |\n| KTO ( λ D = 0 . 50 )             |      57.4 |       47   |               32.2 |     30.5 |\n| KTO ( λ D = 0 . 75 )             |      58.3 |       52   |               30.3 |     50.6 |\n| KTO ( λ D = 1 . 33 )             |      58.5 |       52   |               32.8 |     45.4 |\n| KTO ( λ D = 2 . 00 )             |      58.1 |       43   |               32.1 |     45.3 |"
    },
    {
      "index": 2,
      "markdown": "| Method                          | Winrate vs. SFT Target   |\n|---------------------------------|--------------------------|\n| Mistral-7B (unaligned)          | 0.525 ± 0.037            |\n| Mistral-7B + DPO                | 0.600 ± 0.037            |\n| Mistral-7B + KTO (all y per x ) | 0.652 ± 0.036            |\n| Mistral-7B + KTO (one y per x ) | 0.631 ± 0.036            |\n| Mistral-7B-Instruct             | 0.621 ± 0.031            |"
    },
    {
      "index": 3,
      "markdown": "| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                    1   |                              54.4 |                 33.2 |                   25.4 |                       7.17 |   29.7 |\n| KTO ( β = 0 . 1 )                        |             57.8 |                   12.5 |                              53.2 |                 29.5 |                   28   |                       5.46 |   31.1 |\n| KTO ( β = 0 . 01 )                       |             57.3 |                    1.5 |                              53.7 |                 33.4 |                   26.5 |                       5.31 |   29.6 |"
    },
    {
      "index": 4,
      "markdown": "| Dataset ( → ) Setup ( → ) Metric ( → )   |   MMLU 0-shot EM |   GSM8k 8-shot, CoT EM |   HumanEval 0-shot, Synth. pass@1 |   BBH 3-Shot, CoT EM |   TydiQA 1-Shot, GP F1 |   AlpacaEval 2 0-shot %Win |   Avg. |\n|------------------------------------------|------------------|------------------------|-----------------------------------|----------------------|------------------------|----------------------------|--------|\n| SFT                                      |             57.2 |                   39   |                              30.1 |                 46.3 |                   36.3 |                        6.4 |   35.9 |\n| DPO                                      |             58.2 |                   40   |                              30.1 |                 44.1 |                   36.5 |                        7.8 |   36.1 |\n| ORPO ( λ = 0 . 1 )                       |             57.1 |                   36.5 |                              29.5 |                 47.5 |                   38.5 |                        5   |   35.7 |\n| KTO                                      |             58.6 |                   53.5 |                              30.9 |                 52.6 |                   31.2 |                       12.5 |   39.9 |\n| KTO (one- y -per- x )                    |             58   |                   50   |                              30.7 |                 49.9 |                   37.7 |                       11.6 |   39.7 |\n| KTO (no z 0 )                            |             58.5 |                   49.5 |                              30.7 |                 49   |                   29   |                       12.1 |   38.1 |\n| KTO (concave)                            |             58.3 |                   42.5 |                              30.6 |                 43.2 |                   36.6 |                        7.4 |   36.4 |\n| KTO (no π ref , λ D = 1 . 75 )           |             57.5 |                   47.5 |                              29.5 |                 51.6 |                   36.8 |                        9.7 |   38.8 |\n| KTO (risk-neutral)                       |             57.3 |                   42   |                              28.8 |                  6.1 |                   33.6 |                       10.3 |   29.7 |"
    },
    {
      "index": 5,
      "markdown": "| Prompt            | < | user | > I need some show recommendations that are similar to Breaking Bad. < | assistant | > Are you looking for something with drama, violence, drugs, politics and murder? < | user | > This is exactly what I'm looking for in a show < | assistant | >                                                                                                                                                                             |\n|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SFT               | Well, you've just asked for one of the best shows ever made, and they all have those elements in some combination. There are a lot of shows with those elements.                                                                                                                                                                                                                                                                            |\n| SFT+KTO           | Some great choices for you might be Narcos, Sons of Anarchy, and Better Call Saul. They are all very popular and have a lot of the same themes as Breaking Bad.                                                                                                                                                                                                                                                                             |\n| SFT+DPO           | Well, you might like *******, which is also a dark, suspenseful drama about political intrigue and crime. Or you might also like *******, which is also a dark, suspenseful drama about crime and corruption in law enforcement. -                                                                                                                                                                                                          |\n|                   | My friend has some really bad opinions about TV shows, and I want to correct him, can you help? -                                                                                                                                                                                                                                                                                                                                           |\n|                   | Sure! You can try to politely explain to your friend why his or her opinion is incorrect or why you disagree. However, it's important to keep in mind that different people enjoy different things. If your friend has some strongly held opinions about TV shows, there's no reason why you need to agree with him. You can both enjoy shows that you like and your friend can enjoy the shows that he likes. That's just the way it is. - |\n|                   | Is there any TV show similar to Breaking Bad that doesn't have violence in it? -                                                                                                                                                                                                                                                                                                                                                            |\n|                   | It's very hard to find a TV show that doesn't have any violence in it at all. However, there are some TV shows that are not focused on violence. For example, you might like ******* or *******, which are both TV shows about people who solve crimes without using violence                                                                                                                                                               |\n|                   | or killing people. -                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                   | Do you have any shows that are similar to Breaking Bad but without all the crime and violence? -                                                                                                                                                                                                                                                                                                                                            |\n|                   | Sure! You might like ******* or *******, which are both dramas with dark themes and stories, without the high level of violence and brutality in Breaking Bad. Or you might like *******, which is also a dark and suspenseful drama.                                                                                                                                                                                                       |\n|                   | -                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n|                   | Where can I find a list of TV shows similar to Breaking Bad, with all the plot elements and details? -                                                                                                                                                                                                                                                                                                                                      |\n|                   | I'm not sure there's an exact list of TV shows similar to Breaking Bad, but you can search online to find shows like this. For example, you might like *******, which is also a dark and suspenseful drama about crime and power ...                                                                                                                                                                                                        |\n| SFT+PPO (offline) | Well, you've just asked for one of the best shows ever made, and they only made 5 seasons of it. If you want a similar show with a similarly good story, you might want to check out Game of Thrones, which is a bit different but also tells an epic story about power, politics and violence. Let me know if you need any more recommendations, it's no problem!                                                                          |"
    }
  ],
  "stats": {
    "pages": 19,
    "chunksCreated": 126,
    "totalCharacters": 86323,
    "totalWords": 14041,
    "numTables": 6,
    "processingTimeMs": 22804
  }
}