{
  "doi": "10.18653/v1/2020.acl-main.747",
  "title": "Longformer: The Long-Document Transformer",
  "category": "Architecture",
  "year": 2020,
  "paper": {
    "doi": "10.18653/v1/2020.acl-main.747",
    "doi_url": "https://doi.org/10.18653/v1/2020.acl-main.747",
    "title": "Unsupervised Cross-lingual Representation Learning at Scale",
    "genre": "proceedings-article",
    "is_paratext": false,
    "published_date": "2020-01-01",
    "year": 2020,
    "journal_name": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    "journal_issns": null,
    "journal_issn_l": null,
    "journal_is_oa": false,
    "journal_is_in_doaj": false,
    "publisher": "Association for Computational Linguistics",
    "is_oa": true,
    "oa_status": "gold",
    "has_repository_copy": true,
    "best_oa_location": {
      "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
      "url_for_pdf": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
      "url_for_landing_page": "https://doi.org/10.18653/v1/2020.acl-main.747",
      "evidence": "deprecated",
      "license": "cc-by",
      "version": "publishedVersion",
      "host_type": null,
      "is_best": true,
      "pmh_id": null,
      "endpoint_id": null,
      "repository_institution": null,
      "oa_date": "2020-01-01",
      "updated": "deprecated"
    },
    "first_oa_location": {
      "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
      "url_for_pdf": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
      "url_for_landing_page": "https://doi.org/10.18653/v1/2020.acl-main.747",
      "evidence": "deprecated",
      "license": "cc-by",
      "version": "publishedVersion",
      "host_type": null,
      "is_best": true,
      "pmh_id": null,
      "endpoint_id": null,
      "repository_institution": null,
      "oa_date": "2020-01-01",
      "updated": "deprecated"
    },
    "oa_locations": [
      {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
        "url_for_pdf": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
        "url_for_landing_page": "https://doi.org/10.18653/v1/2020.acl-main.747",
        "evidence": "deprecated",
        "license": "cc-by",
        "version": "publishedVersion",
        "host_type": null,
        "is_best": true,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": null,
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      },
      {
        "url": "https://arxiv.org/pdf/1911.02116",
        "url_for_pdf": "https://arxiv.org/pdf/1911.02116",
        "url_for_landing_page": "http://arxiv.org/abs/1911.02116",
        "evidence": "deprecated",
        "license": null,
        "version": "submittedVersion",
        "host_type": "repository",
        "is_best": false,
        "pmh_id": "oai:arXiv.org:1911.02116",
        "endpoint_id": "ca8f8d56758a80a4f86",
        "repository_institution": "Cornell University",
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      },
      {
        "url": "https://arxiv.org/pdf/1911.02116v2",
        "url_for_pdf": null,
        "url_for_landing_page": "https://arxiv.org/pdf/1911.02116v2",
        "evidence": "deprecated",
        "license": null,
        "version": "submittedVersion",
        "host_type": "repository",
        "is_best": false,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": "Cornell University",
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      },
      {
        "url": "https://doi.org/10.48550/arxiv.1911.02116",
        "url_for_pdf": null,
        "url_for_landing_page": "https://doi.org/10.48550/arxiv.1911.02116",
        "evidence": "deprecated",
        "license": null,
        "version": null,
        "host_type": "repository",
        "is_best": false,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": "Cornell University",
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      }
    ],
    "oa_locations_embargoed": [],
    "data_standard": 2,
    "z_authors": [
      {
        "author_position": "first",
        "raw_author_name": "Alexis Conneau",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Kartikay Khandelwal",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Microsoft (United States), Redmond, United States"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Naman Goyal",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Vishrav Chaudhary",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Guillaume Wenzek",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Francisco Guzmán",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Johns Hopkins University, Baltimore, United States"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Edouard Grave",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Myle Ott",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Luke Zettlemoyer",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      },
      {
        "author_position": "last",
        "raw_author_name": "Veselin Stoyanov",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "Meta (Israel), Tel Aviv, Israel"
        ]
      }
    ],
    "updated": "2025-12-01T22:21:40Z"
  },
  "chunks": [
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-0",
      "content": "## Unsupervised Cross-lingual Representation Learning at Scale ∗ ∗\n\nAlexis Conneau Kartikay Khandelwal\n\nNaman Goyal Vishrav Chaudhary Guillaume Wenzek Francisco Guzm´ an\n\nEdouard Grave Myle Ott Luke Zettlemoyer Veselin Stoyanov\n\n## Facebook AI\n\n## Abstract",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 0
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-1",
      "content": "Stoyanov ## Facebook AI ## Abstract \n\nThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks. We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R , significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 1
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-2",
      "content": "first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available. 1 \n\n## 1 Introduction\n\nThe goal of this paper is to improve cross-lingual language understanding (XLU), by carefully studying the effects of training unsupervised crosslingual representations at a very large scale. We present XLM-R a transformer-based multilingual masked language model pre-trained on text in 100 languages, which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering.\n\n∗ Equal contribution.\n\nCorrespondence to { aconneau,kartikayk } @fb.com\n\n1 https://github.com/facebookresearch/(fairseq-py,pytext,xlm)",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 2
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-3",
      "content": "performance on cross-lingual classification, sequence labeling and question answering. ∗ Equal contribution. Correspondence to { aconneau,kartikayk } @fb.com 1 https://github.com/facebookresearch/(fairseq-py,pytext,xlm) \n\nMultilingual masked language models (MLM) like mBERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019) have pushed the stateof-the-art on cross-lingual understanding tasks by jointly pretraining large Transformer models (Vaswani et al., 2017) on many languages. These models allow for effective cross-lingual transfer, as seen in a number of benchmarks including cross-lingual natural language inference (Bowman et al., 2015; Williams et al., 2017; Conneau et al., 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2019), and named entity recognition (Pires et al., 2019; Wu and Dredze, 2019). However, all of these studies pre-train on Wikipedia, which provides a relatively limited scale especially for lower resource languages.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 3
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-4",
      "content": "al., 2019; Wu and Dredze, 2019). However, all of these studies pre-train on Wikipedia, which provides a relatively limited scale especially for lower resource languages. \n\nIn this paper, we first present a comprehensive analysis of the trade-offs and limitations of multilingual language models at scale, inspired by recent monolingual scaling efforts (Liu et al., 2019). We measure the trade-off between high-resource and low-resource languages and the impact of language sampling and vocabulary size. The experiments expose a trade-off as we scale the number of languages for a fixed model capacity: more languages leads to better cross-lingual performance on low-resource languages up until a point, after which the overall performance on monolingual and cross-lingual benchmarks degrades. We refer to this tradeoff as the curse of multilinguality , and show that it can be alleviated by simply increasing model capacity. We argue, however, that this remains an important limitation for future XLU systems which may aim to improve performance with more modest computational budgets.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 4
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-5",
      "content": "be alleviated by simply increasing model capacity. We argue, however, that this remains an important limitation for future XLU systems which may aim to improve performance with more modest computational budgets. \n\nOur best model XLM-RoBERTa ( XLM-R ) outperforms mBERT on cross-lingual classification by up to 23% accuracy on low-resource languages. It outperforms the previous state of the art by 5.1% average accuracy on XNLI, 2.42% average F1-score",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 5
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-6",
      "content": "of the art by 5.1% average accuracy on XNLI, 2.42% average F1-score \n\non Named Entity Recognition, and 9.1% average F1-score on cross-lingual Question Answering. We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-R obtains results competitive with state-of-the-art monolingual models, including RoBERTa (Liu et al., 2019). These results demonstrate, for the first time, that it is possible to have a single large model for all languages, without sacrificing per-language performance. We will make our code, models and data publicly available, with the hope that this will help research in multilingual NLP and low-resource language understanding.\n\n## 2 Related Work",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 6
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-7",
      "content": "available, with the hope that this will help research in multilingual NLP and low-resource language understanding. ## 2 Related Work \n\nFrom pretrained word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) to pretrained contextualized representations (Peters et al., 2018; Schuster et al., 2019) and transformer based language models (Radford et al., 2018; Devlin et al., 2018), unsupervised representation learning has significantly improved the state of the art in natural language understanding. Parallel work on cross-lingual understanding (Mikolov et al., 2013a; Schuster et al., 2019; Lample and Conneau, 2019) extends these systems to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 7
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-8",
      "content": "to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages. \n\nMost recently, Devlin et al. (2018) and Lample and Conneau (2019) introduced mBERT and XLM - masked language models trained on multiple languages, without any cross-lingual supervision. Lample and Conneau (2019) propose translation language modeling (TLM) as a way to leverage parallel data and obtain a new state of the art on the cross-lingual natural language inference (XNLI) benchmark (Conneau et al., 2018). They further show strong improvements on unsupervised machine translation and pretraining for sequence generation. Wu et al. (2019) shows that monolingual BERT representations are similar across languages, explaining in part the natural emergence of multilinguality in bottleneck architectures. Separately, Pires et al. (2019) demonstrated the effectiveness of multilingual models like mBERT on sequence labeling tasks. Huang et al. (2019) showed gains over XLM using cross-lingual multi-task learning, and Singh et al. (2019) demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI. However, all of this work was at a relatively modest scale, in terms of the amount of training data, as compared to our approach.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 8
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-9",
      "content": "et al. (2019) demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI. However, all of this work was at a relatively modest scale, in terms of the amount of training data, as compared to our approach. \n\nThe benefits of scaling language model pretraining by increasing the size of the model as well as the training data has been extensively studied in the literature. For the monolingual case, Jozefowicz et al. (2016) show how large-scale LSTM models can obtain much stronger performance on language modeling benchmarks when trained on billions of tokens. GPT (Radford et al., 2018) also highlights the importance of scaling the amount of data and RoBERTa (Liu et al., 2019) shows that training BERT longer on more data leads to significant boost in performance. Inspired by RoBERTa, we show that mBERT and XLM are undertuned, and that simple improvements in the learning procedure of unsupervised MLM leads to much better performance. We train on cleaned CommonCrawls (Wenzek et al., 2019), which increase the amount of data for low-resource languages by two orders of magnitude on average. Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages (Grave et al., 2018).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 9
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-10",
      "content": "2019), which increase the amount of data for low-resource languages by two orders of magnitude on average. Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages (Grave et al., 2018). \n\nSeveral efforts have trained massively multilingual machine translation models from large parallel corpora. They uncover the high and low resource trade-off and the problem of capacity dilution (Johnson et al., 2017; Tan et al., 2019). The work most similar to ours is Arivazhagan et al. (2019), which trains a single model in 103 languages on over 25 billion parallel sentences. Siddhant et al. (2019) further analyze the representations obtained by the encoder of a massively multilingual machine translation system and show that it obtains similar results to mBERT on cross-lingual NLI. Our work, in contrast, focuses on the unsupervised learning of cross-lingual representations and their transfer to discriminative tasks.\n\n## 3 Model and Data",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 10
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-11",
      "content": "results to mBERT on cross-lingual NLI. Our work, in contrast, focuses on the unsupervised learning of cross-lingual representations and their transfer to discriminative tasks. ## 3 Model and Data \n\nIn this section, we present the training objective, languages, and data we use. We follow the XLM approach (Lample and Conneau, 2019) as closely as possible, only introducing changes that improve performance at scale.\n\nMasked Language Models. We use a Transformer model (Vaswani et al., 2017) trained with the multilingual MLM objective (Devlin et al., 2018; Lample and Conneau, 2019) using only monolingual data. We sample streams of text from each language and train the model to predict the masked tokens in the input. We apply subword tok-",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 11
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-12",
      "content": "streams of text from each language and train the model to predict the masked tokens in the input. We apply subword tok- \n\nFigure 1: Amount of data in GiB (log-scale) for the 88 languages that appear in both the Wiki-100 corpus used for mBERT and XLM-100, and the CC-100 used for XLM-R. CC-100 increases the amount of data by several orders of magnitude, in particular for low-resource languages.\n\n<!-- image -->",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 12
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-13",
      "content": "by several orders of magnitude, in particular for low-resource languages. <!-- image --> \n\nenization directly on raw text data using Sentence Piece (Kudo and Richardson, 2018) with a unigram language model (Kudo, 2018). We sample batches from different languages using the same sampling distribution as Lample and Conneau (2019), but with α = 0 . 3 . Unlike Lample and Conneau (2019), we do not use language embeddings, which allows our model to better deal with code-switching. We use a large vocabulary size of 250K with a full softmax and train two different models: XLM-R Base (L = 12, H = 768, A = 12, 270M params) and XLM-R (L = 24, H = 1024, A = 16, 550M params). For all of our ablation studies, we use a BERTBase architecture with a vocabulary of 150K tokens. Appendix B goes into more details about the architecture of the different models referenced in this paper.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 13
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-14",
      "content": "ablation studies, we use a BERTBase architecture with a vocabulary of 150K tokens. Appendix B goes into more details about the architecture of the different models referenced in this paper. \n\nScaling to a hundred languages. XLM-R is trained on 100 languages; we provide a full list of languages and associated statistics in Appendix A. Figure 1 specifies the iso codes of 88 languages that are shared across XLM-R and XLM-100, the model from Lample and Conneau (2019) trained on Wikipedia text in 100 languages.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 14
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-15",
      "content": "and XLM-100, the model from Lample and Conneau (2019) trained on Wikipedia text in 100 languages. \n\nCompared to previous work, we replace some languages with more commonly used ones such as romanized Hindi and traditional Chinese. In our ablation studies, we always include the 7 languages for which we have classification and sequence labeling evaluation benchmarks: English, French, German, Russian, Chinese, Swahili and Urdu. We chose this set as it covers a suitable range of language families and includes low-resource languages such as Swahili and Urdu. We also consider larger sets of 15, 30, 60 and all 100 languages. When reporting results on high-resource and lowresource, we refer to the average of English and French results, and the average of Swahili and Urdu results respectively.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 15
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-16",
      "content": "results on high-resource and lowresource, we refer to the average of English and French results, and the average of Swahili and Urdu results respectively. \n\nScaling the Amount of Training Data. Following Wenzek et al. (2019) 2 , we build a clean CommonCrawl Corpus in 100 languages. We use an internal language identification model in combination with the one from fastText (Joulin et al., 2017). We train language models in each language and use it to filter documents as described in Wenzek et al. (2019). We consider one CommonCrawl dump for English and twelve dumps for all other languages, which significantly increases dataset sizes, especially for low-resource languages like Burmese and Swahili.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 16
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-17",
      "content": "English and twelve dumps for all other languages, which significantly increases dataset sizes, especially for low-resource languages like Burmese and Swahili. \n\nFigure 1 shows the difference in size between the Wikipedia Corpus used by mBERT and XLM100, and the CommonCrawl Corpus we use. As we show in Section 5.3, monolingual Wikipedia corpora are too small to enable unsupervised representation learning. Based on our experiments, we found that a few hundred MiB of text data is usually a minimal size for learning a BERT model.\n\n## 4 Evaluation\n\nWeconsider four evaluation benchmarks. For crosslingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 17
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-18",
      "content": "entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models. \n\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other lan-\n\n2 https://github.com/facebookresearch/cc net",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 18
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-19",
      "content": "We evaluate our model on cross-lingual transfer from English to other lan- 2 https://github.com/facebookresearch/cc net \n\nguages. We also consider three machine translation baselines: (i) translate-test : dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 19
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-20",
      "content": "training sets from translate-train. For the translations, we use the official data provided by the XNLI project. \n\nNamed Entity Recognition. For NER, we consider the CoNLL-2002 (Sang, 2002) and CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018).\n\nCross-lingual Question Answering. Weuse the MLQAbenchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 20
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-21",
      "content": "to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. \n\nGLUEBenchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018). We use BERTLarge and RoBERTa as baselines.\n\n## 5 Analysis and Results\n\nIn this section, we perform a comprehensive analysis of multilingual masked language models. We conduct most of the analysis on XNLI, which we found to be representative of our findings on other tasks. We then present the results of XLM-R on cross-lingual understanding and GLUE. Finally, we compare multilingual and monolingual models, and present results on low-resource languages.\n\n## 5.1 Improving and Understanding Multilingual Masked Language Models",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 21
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-22",
      "content": "XLM-R on cross-lingual understanding and GLUE. Finally, we compare multilingual and monolingual models, and present results on low-resource languages. ## 5.1 Improving and Understanding Multilingual Masked Language Models \n\nMuch of the work done on understanding the crosslingual effectiveness of mBERT or XLM (Pires et al., 2019; Wu and Dredze, 2019; Lewis et al.,\n\n2019) has focused on analyzing the performance of fixed pretrained models on downstream tasks. In this section, we present a comprehensive study of different factors that are important to pretraining large scale multilingual models. We highlight the trade-offs and limitations of these models as we scale to one hundred languages.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 22
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-23",
      "content": "scale multilingual models. We highlight the trade-offs and limitations of these models as we scale to one hundred languages. \n\nTransfer-dilution Trade-off and Curse of Multilinguality. Model capacity (i.e. the number of parameters in the model) is constrained due to practical considerations such as memory and speed during training and inference. For a fixed sized model, the per-language capacity decreases as we increase the number of languages. While low-resource language performance can be improved by adding similar higher-resource languages during pretraining, the overall downstream performance suffers from this capacity dilution (Arivazhagan et al., 2019). Positive transfer and capacity dilution have to be traded off against each other.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 23
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-24",
      "content": "this capacity dilution (Arivazhagan et al., 2019). Positive transfer and capacity dilution have to be traded off against each other. \n\nWe illustrate this trade-off in Figure 2, which shows XNLI performance vs the number of languages the model is pretrained on. Initially, as we go from 7 to 15 languages, the model is able to take advantage of positive transfer which improves performance, especially on low resource languages. Beyond this point the curse of multilinguality kicks in and degrades performance across all languages. Specifically, the overall XNLI accuracy decreases from 71.8% to 67.7% as we go from XLM-7 to XLM-100. The same trend can be observed for models trained on the larger CommonCrawl Corpus.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 24
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-25",
      "content": "67.7% as we go from XLM-7 to XLM-100. The same trend can be observed for models trained on the larger CommonCrawl Corpus. \n\nThe issue is even more prominent when the capacity of the model is small. To show this, we pretrain models on Wikipedia Data in 7, 30 and 100 languages. As we add more languages, we make the Transformer wider by increasing the hidden size from 768 to 960 to 1152. In Figure 4, we show that the added capacity allows XLM-30 to be on par with XLM-7, thus overcoming the curse of multilinguality. The added capacity for XLM-100, however, is not enough and it still lags behind due to higher vocabulary dilution (recall from Section 3 that we used a fixed vocabulary size of 150K for all models).\n\nHigh-resource vs Low-resource Trade-off. The allocation of the model capacity across languages is controlled by several parameters: the training set size, the size of the shared subword\n\n<!-- image -->",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 25
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-26",
      "content": "models). High-resource vs Low-resource Trade-off. The allocation of the model capacity across languages is controlled by several parameters: the training set size, the size of the shared subword <!-- image --> \n\nFigure 2: The transferinterference trade-off: Lowresource languages benefit from scaling to more languages, until dilution (interference) kicks in and degrades overall performance.\n\n<!-- image -->\n\nFigure 3: Wikipedia versus CommonCrawl: An XLM-7 obtains significantly better performance when trained on CC, in particular on low-resource languages.\n\n<!-- image -->\n\nFigure 5: On the high-resource versus low-resource trade-off: impact of batch language sampling for XLM-100.\n\n<!-- image -->\n\nFigure 6: On the impact of vocabulary size at fixed capacity and with increasing capacity for XLM-100.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 26
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-27",
      "content": "for XLM-100. <!-- image --> Figure 6: On the impact of vocabulary size at fixed capacity and with increasing capacity for XLM-100. \n\nvocabulary, and the rate at which we sample training examples from each language. We study the effect of sampling on the performance of highresource (English and French) and low-resource (Swahili and Urdu) languages for an XLM-100 model trained on Wikipedia (we observe a similar trend for the construction of the subword vocab). Specifically, we investigate the impact of varying the α parameter which controls the exponential smoothing of the language sampling rate. Similar to Lample and Conneau (2019), we use a sampling rate proportional to the number of sentences in each corpus. Models trained with higher values of α see batches of high-resource languages more often. Figure 5 shows that the higher the value of α , the better the performance on high-resource languages, and vice-versa. When considering overall performance, we found 0 . 3 to be an optimal value for α , and use this for XLM-R .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 27
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-28",
      "content": ", the better the performance on high-resource languages, and vice-versa. When considering overall performance, we found 0 . 3 to be an optimal value for α , and use this for XLM-R . \n\n<!-- image -->\n\nFigure 4: Adding more capacity to the model alleviates the curse of multilinguality, but remains an issue for models of moderate size.\n\n<!-- image -->\n\n.\n\nFigure 7: On the impact of largescale training, and preprocessing simplification from BPE with tokenization to SPM on raw text data.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 28
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-29",
      "content": "largescale training, and preprocessing simplification from BPE with tokenization to SPM on raw text data. \n\nshared vocabulary (the vocabulary capacity) can improve the performance of multilingual models on downstream tasks. To illustrate this effect, we train XLM-100 models on Wikipedia data with different vocabulary sizes. We keep the overall number of parameters constant by adjusting the width of the transformer. Figure 6 shows that even with a fixed capacity, we observe a 2.8% increase in XNLI average accuracy as we increase the vocabulary size from 32K to 256K. This suggests that multilingual models can benefit from allocating a higher proportion of the total number of parameters to the embedding layer even though this reduces the size of the Transformer. For simplicity and given the softmax computational constraints, we use a vocabulary of 250k for XLM-R .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 29
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-30",
      "content": "even though this reduces the size of the Transformer. For simplicity and given the softmax computational constraints, we use a vocabulary of 250k for XLM-R . \n\nImportance of Capacity and Vocabulary. In previous sections and in Figure 4, we showed the importance of scaling the model size as we increase the number of languages. Similar to the overall model size, we argue that scaling the size of the\n\nWe further illustrate the importance of this parameter, by training three models with the same transformer architecture (BERTBase) but with different vocabulary sizes: 128K, 256K and 512K. We observe more than 3% gains in overall accuracy on XNLI by simply increasing the vocab size from 128k to 512k.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 30
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-31",
      "content": "512K. We observe more than 3% gains in overall accuracy on XNLI by simply increasing the vocab size from 128k to 512k. \n\nLarger-scale Datasets and Training. As shown in Figure 1, the CommonCrawl Corpus that we collected has significantly more monolingual data than the previously used Wikipedia corpora. Figure 3 shows that for the same BERTBase architecture, all models trained on CommonCrawl obtain significantly better performance.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 31
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-32",
      "content": "same BERTBase architecture, all models trained on CommonCrawl obtain significantly better performance. \n\nApart from scaling the training data, Liu et al. (2019) also showed the benefits of training MLMs longer. In our experiments, we observed similar effects of large-scale training, such as increasing batch size (see Figure 7) and training time, on model performance. Specifically, we found that using validation perplexity as a stopping criterion for pretraining caused the multilingual MLM in Lample and Conneau (2019) to be under-tuned. In our experience, performance on downstream tasks continues to improve even after validation perplexity has plateaued. Combining this observation with our implementation of the unsupervised XLM-MLM objective, we were able to improve the performance of Lample and Conneau (2019) from 71.3% to more than 75% average accuracy on XNLI, which was on par with their supervised translation language modeling (TLM) objective. Based on these results, and given our focus on unsupervised learning, we decided to not use the supervised TLM objective for training our models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 32
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-33",
      "content": "with their supervised translation language modeling (TLM) objective. Based on these results, and given our focus on unsupervised learning, we decided to not use the supervised TLM objective for training our models. \n\nSimplifying Multilingual Tokenization with Sentence Piece. The different language-specific tokenization tools used by mBERT and XLM-100 make these models more difficult to use on raw text. Instead, we train a Sentence Piece model (SPM) and apply it directly on raw text data for all languages. We did not observe any loss in performance for models trained with SPM when compared to models trained with language-specific preprocessing and byte-pair encoding (see Figure 7) and hence use SPM for XLM-R .\n\n## 5.2 Cross-lingual Understanding Results",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 33
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-34",
      "content": "trained with language-specific preprocessing and byte-pair encoding (see Figure 7) and hence use SPM for XLM-R . ## 5.2 Cross-lingual Understanding Results \n\nBased on these results, we adapt the setting of Lample and Conneau (2019) and use a large Transformer model with 24 layers and 1024 hidden states, with a 250k vocabulary. We use the multilingual MLM loss and train our XLM-R model for 1.5 Million updates on five-hundred 32GB Nvidia V100 GPUs with a batch size of 8192. We leverage the SPM-preprocessed text data from CommonCrawl in 100 languages and sample languages with α = 0 . 3 . In this section, we show that it out- performs all previous techniques on cross-lingual benchmarks while getting performance on par with RoBERTa on the GLUE benchmark.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 34
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-35",
      "content": "section, we show that it out- performs all previous techniques on cross-lingual benchmarks while getting performance on par with RoBERTa on the GLUE benchmark. \n\nXNLI. Table 1 shows XNLI results and adds some additional details: (i) the number of models the approach induces (#M), (ii) the data on which the model was trained (D), and (iii) the number of languages the model was pretrained on (#lg). As we show in our results, these parameters significantly impact performance. Column #M specifies whether model selection was done separately on the dev set of each language ( N models), or on the joint dev set of all the languages (single model). Weobserve a 0.6 decrease in overall accuracy when we go from N models to a single model - going from 71.3 to 70.7. We encourage the community to adopt this setting. For cross-lingual transfer, while this approach is not fully zero-shot transfer, we argue that in real applications, a small amount of supervised data is often available for validation in each language.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 35
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-36",
      "content": "this setting. For cross-lingual transfer, while this approach is not fully zero-shot transfer, we argue that in real applications, a small amount of supervised data is often available for validation in each language. \n\nXLM-R sets a new state of the art on XNLI. On cross-lingual transfer, XLM-R obtains 80.9% accuracy, outperforming the XLM-100 and mBERT open-source models by 10.2% and 14.6% average accuracy. On the Swahili and Urdu lowresource languages, XLM-R outperforms XLM-100 by 15.7% and 11.4%, and mBERT by 23.5% and 15.8%. While XLM-R handles 100 languages, we also show that it outperforms the former state of the art Unicoder (Huang et al., 2019) and XLM (MLM+TLM), which handle only 15 languages, by 5.5% and 5.8% average accuracy respectively. Using the multilingual training of translate-train-all, XLM-R further improves performance and reaches 83.6% accuracy, a new overall state of the art for XNLI, outperforming Unicoder by 5.1%. Multilingual training is similar to practical applications where training sets are available in various languages for the same task. In the case of XNLI, datasets have been translated, and translate-trainall can be seen as some form of cross-lingual data augmentation (Singh et al., 2019), similar to backtranslation (Xie et al., 2019).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 36
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-37",
      "content": "various languages for the same task. In the case of XNLI, datasets have been translated, and translate-trainall can be seen as some form of cross-lingual data augmentation (Singh et al., 2019), similar to backtranslation (Xie et al., 2019). \n\nNamed Entity Recognition. In Table 2, we report results of XLM-R and mBERT on CoNLL2002 and CoNLL-2003. We consider the LSTM + CRF approach from Lample et al. (2016) and the Flair model from Akbik et al. (2018) as baselines. We evaluate the performance of the model",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 37
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-38",
      "content": "Flair model from Akbik et al. (2018) as baselines. We evaluate the performance of the model \n\nTable 1: Results on cross-lingual classification. Wereport the accuracy on each of the 15 XNLI languages and the average accuracy. We specify the dataset D used for pretraining, the number of models #M the approach requires and the number of languages #lg the model handles. Our XLM-R results are averaged over five different seeds. We show that using the translate-train-all approach which leverages training sets from multiple languages, XLM-R obtains a new state of the art on XNLI of 83 . 6 %average accuracy. Results with † are from Huang et al. (2019).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 38
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-39",
      "content": "state of the art on XNLI of 83 . 6 %average accuracy. Results with † are from Huang et al. (2019). \n\n| Model                                                                         | D                                                                             | #M                                                                            | #lg                                                                           | en                                                                            | fr                                                                            | es                                                                            | de                                                                            | el                                                                            | bg                                                                            | ru                                                                            | tr                                                                            | ar                                                                            | vi                                                                            | th                                                                            | zh                                                                            | hi                                                                            | sw                                                                            | ur                                                                            | Avg                                                                           |\n|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) |\n| Lample and Conneau (2019)                                                     | Wiki+MT                                                                       | N                                                                             | 15                                                                            | 85.0                                                                          | 78.7                                                                          | 78.9                                                                          | 77.8                                                                          | 76.6                                                                          | 77.4                                                                          | 75.3                                                                          | 72.5                                                                          | 73.1                                                                          | 76.1                                                                          | 73.2                                                                          | 76.5                                                                          | 69.6                                                                          | 68.4                                                                          | 67.3                                                                          | 75.1                                                                          |\n| Huang et al. (2019)                                                           | Wiki+MT                                                                       | N                                                                             | 15                                                                            | 85.1                                                                          | 79.0                                                                          | 79.4                                                                          | 77.8                                                                          | 77.2                                                                          | 77.2                                                                          | 76.3                                                                          | 72.8                                                                          | 73.5                                                                          | 76.4                                                                          | 73.6                                                                          | 76.2                                                                          | 69.4                                                                          | 69.7                                                                          | 66.7                                                                          | 75.4                                                                          |\n| Devlin et al. (2018)                                                          | Wiki                                                                          | N                                                                             | 102                                                                           | 82.1                                                                          | 73.8                                                                          | 74.3                                                                          | 71.1                                                                          | 66.4                                                                          | 68.9                                                                          | 69.0                                                                          | 61.6                                                                          | 64.9                                                                          | 69.5                                                                          | 55.8                                                                          | 69.3                                                                          | 60.0                                                                          | 50.4                                                                          | 58.0                                                                          | 66.3                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | N                                                                             | 100                                                                           | 83.7                                                                          | 76.2                                                                          | 76.6                                                                          | 73.7                                                                          | 72.4                                                                          | 73.0                                                                          | 72.1                                                                          | 68.1                                                                          | 68.4                                                                          | 72.0                                                                          | 68.2                                                                          | 71.5                                                                          | 64.5                                                                          | 58.0                                                                          | 62.4                                                                          | 71.3                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | 1                                                                             | 100                                                                           | 83.2                                                                          | 76.7                                                                          | 77.7                                                                          | 74.0                                                                          | 72.7                                                                          | 74.1                                                                          | 72.7                                                                          | 68.7                                                                          | 68.6                                                                          | 72.9                                                                          | 68.9                                                                          | 72.5                                                                          | 65.6                                                                          | 58.2                                                                          | 62.4                                                                          | 70.7                                                                          |\n| XLM-R Base                                                                    | CC                                                                            | 1                                                                             | 100                                                                           | 85.8                                                                          | 79.7                                                                          | 80.7                                                                          | 78.7                                                                          | 77.5                                                                          | 79.6                                                                          | 78.1                                                                          | 74.2                                                                          | 73.8                                                                          | 76.5                                                                          | 74.6                                                                          | 76.7                                                                          | 72.4                                                                          | 66.5                                                                          | 68.3                                                                          | 76.2                                                                          |\n| XLM-R                                                                         | CC                                                                            | 1                                                                             | 100                                                                           | 89.1                                                                          | 84.1                                                                          | 85.1                                                                          | 83.9                                                                          | 82.9                                                                          | 84.0                                                                          | 81.2                                                                          | 79.6                                                                          | 79.8                                                                          | 80.8                                                                          | 78.1                                                                          | 80.2                                                                          | 76.9                                                                          | 73.9                                                                          | 73.8                                                                          | 80.9                                                                          |\n| Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   |\n| BERT-en                                                                       | Wiki                                                                          | 1                                                                             | 1                                                                             | 88.8                                                                          | 81.4                                                                          | 82.3                                                                          | 80.1                                                                          | 80.3                                                                          | 80.9                                                                          | 76.2                                                                          | 76.0                                                                          | 75.4                                                                          | 72.0                                                                          | 71.9                                                                          | 75.6                                                                          | 70.0                                                                          | 65.8                                                                          | 65.8                                                                          | 76.2                                                                          |\n| RoBERTa                                                                       | Wiki+CC                                                                       | 1                                                                             | 1                                                                             | 91.3                                                                          | 82.9                                                                          | 84.3                                                                          | 81.2                                                                          | 81.7                                                                          | 83.1                                                                          | 78.3                                                                          | 76.8                                                                          | 76.6                                                                          | 74.2                                                                          | 74.1                                                                          | 77.5                                                                          | 70.9                                                                          | 66.7                                                                          | 66.8                                                                          | 77.8                                                                          |\n| Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | N                                                                             | 100                                                                           | 82.9                                                                          | 77.6                                                                          | 77.9                                                                          | 77.9                                                                          | 77.1                                                                          | 75.7                                                                          | 75.5                                                                          | 72.6                                                                          | 71.2                                                                          | 75.8                                                                          | 73.1                                                                          | 76.2                                                                          | 70.4                                                                          | 66.5                                                                          | 62.4                                                                          | 74.2                                                                          |\n| Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       |\n| Lample and Conneau (2019) †                                                   | Wiki+MT                                                                       | 1                                                                             | 15                                                                            | 85.0                                                                          | 80.8                                                                          | 81.3                                                                          | 80.3                                                                          | 79.1                                                                          | 80.9                                                                          | 78.3                                                                          | 75.6                                                                          | 77.6                                                                          | 78.5                                                                          | 76.0                                                                          | 79.5                                                                          | 72.9                                                                          | 72.8                                                                          | 68.5                                                                          | 77.8                                                                          |\n| Huang et al. (2019)                                                           | Wiki+MT                                                                       | 1                                                                             | 15                                                                            | 85.6                                                                          | 81.1                                                                          | 82.3                                                                          | 80.9                                                                          | 79.5                                                                          | 81.4                                                                          | 79.7                                                                          | 76.8                                                                          | 78.2                                                                          | 77.9                                                                          | 77.1                                                                          | 80.5                                                                          | 73.4                                                                          | 73.8                                                                          | 69.6                                                                          | 78.5                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | 1                                                                             | 100                                                                           | 84.5                                                                          | 80.1                                                                          | 81.3                                                                          | 79.3                                                                          | 78.6                                                                          | 79.4                                                                          | 77.5                                                                          | 75.2                                                                          | 75.6                                                                          | 78.3                                                                          | 75.7                                                                          | 78.3                                                                          | 72.1                                                                          | 69.2                                                                          | 67.7                                                                          | 76.9                                                                          |\n| XLM-R Base                                                                    | CC                                                                            | 1                                                                             | 100                                                                           | 85.4                                                                          | 81.4                                                                          | 82.2                                                                          | 80.3                                                                          | 80.4                                                                          | 81.3                                                                          | 79.7                                                                          | 78.6                                                                          | 77.3                                                                          | 79.7                                                                          | 77.9                                                                          | 80.2                                                                          | 76.1                                                                          | 73.1                                                                          | 73.0                                                                          | 79.1                                                                          |\n| XLM-R                                                                         | CC                                                                            | 1                                                                             | 100                                                                           | 89.1                                                                          | 85.1                                                                          | 86.6                                                                          | 85.7                                                                          | 85.3                                                                          | 85.9                                                                          | 83.5                                                                          | 83.2                                                                          | 83.1                                                                          | 83.7                                                                          | 81.5                                                                          | 83.7                                                                          | 81.6                                                                          | 78.0                                                                          | 78.1                                                                          | 83.6                                                                          |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 39
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-40",
      "content": "all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL) | | Lample and Conneau (2019) † | Wiki+MT | 1 | 15 | 85.0 | 80.8 | 81.3 | 80.3 | 79.1 | 80.9 | 78.3 | 75.6 | 77.6 | 78.5 | 76.0 | 79.5 | 72.9 | 72.8 | 68.5 | 77.8 | | Huang et al. (2019) | Wiki+MT | 1 | 15 | 85.6 | 81.1 | 82.3 | 80.9 | 79.5 | 81.4 | 79.7 | 76.8 | 78.2 | 77.9 | 77.1 | 80.5 | 73.4 | 73.8 | 69.6 | 78.5 | | Lample and Conneau (2019) | Wiki | 1 | 100 | 84.5 | 80.1 | 81.3 | 79.3 | 78.6 | 79.4 | 77.5 | 75.2 | 75.6 | 78.3 | 75.7 | 78.3 | 72.1 | 69.2 | 67.7 | 76.9 | | XLM-R Base | CC | 1 | 100 | 85.4 | 81.4 | 82.2 | 80.3 | 80.4 | 81.3 | 79.7 | 78.6 | 77.3 | 79.7 | 77.9 | 80.2 | 76.1 | 73.1 | 73.0 | 79.1 | | XLM-R | CC | 1 | 100 | 89.1 | 85.1 | 86.6 | 85.7 | 85.3 | 85.9 | 83.5 | 83.2 | 83.1 | 83.7 | 81.5 | 83.7 | 81.6 | 78.0 | 78.1 | 83.6 | \n\nTable 2: Results on named entity recognition on CoNLL-2002 and CoNLL-2003 (F1 score). Results with † are from Wu and Dredze (2019). Note that mBERT and XLM-R do not use a linear-chain CRF, as opposed to Akbik et al. (2018) and Lample et al. (2016).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 40
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-41",
      "content": "| 83.5 | 83.2 | 83.1 | 83.7 | 81.5 | 83.7 | 81.6 | 78.0 | 78.1 | 83.6 | Table 2: Results on named entity recognition on CoNLL-2002 and CoNLL-2003 (F1 score). Results with † are from Wu and Dredze (2019). Note that mBERT and XLM-R do not use a linear-chain CRF, as opposed to Akbik et al. (2018) and Lample et al. (2016). \n\n| Model                                    | train     | #M   | en          | nl          | es      | de          | Avg     |\n|------------------------------------------|-----------|------|-------------|-------------|---------|-------------|---------|\n| Lample et al. (2016) Akbik et al. (2018) | each each | N    | 90.74 93.18 | 81.74 90.44 | 85.75 - | 78.76 88.27 | 84.25 - |\n|                                          |           | N    |             |             | 87.38   | 82.82       | 88.28   |\n| mBERT †                                  | each en   | N 1  | 91.97 91.97 | 90.94 77.57 | 74.96   | 69.56       | 78.52   |\n| XLM-R Base                               | each      | N    | 92.25       | 90.39       | 87.99   | 84.60       | 88.81   |\n|                                          | en        | 1    | 92.25       | 78.08       | 76.53   | 69.60       | 79.11   |\n|                                          | all       | 1    | 91.08       | 89.09       | 87.28   | 83.17       | 87.66   |\n| XLM-R                                    | each      | N    | 92.92       | 92.53       | 89.72   | 85.81       | 90.24   |\n|                                          | en        | 1    | 92.92       | 80.80       | 78.64   | 71.40       | 80.94   |\n|                                          | all       | 1    | 92.00       | 91.60       | 89.52   | 84.60       | 89.43   |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 41
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-42",
      "content": "XLM-R | each | N | 92.92 | 92.53 | 89.72 | 85.81 | 90.24 | | | en | 1 | 92.92 | 80.80 | 78.64 | 71.40 | 80.94 | | | all | 1 | 92.00 | 91.60 | 89.52 | 84.60 | 89.43 | \n\non each of the target languages in three different settings: (i) train on English data only (en) (ii) train on data in target language (each) (iii) train on data in all languages (all). Results of mBERT are reported from Wu and Dredze (2019). Note that we do not use a linear-chain CRF on top of XLM-R and mBERT representations, which gives an advantage to Akbik et al. (2018). Without the CRF, our XLM-R model still performs on par with the state of the art, outperforming Akbik et al. (2018) on Dutch by 2 . 09 points. On this task, XLM-R also outperforms mBERT by 2.42 F1 on average for cross-lingual transfer, and 1.86 F1 when trained on each language. Training on all languages leads to an average F1 score of 89.43%, outperforming cross-lingual transfer approach by 8.49%.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 42
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-43",
      "content": "outperforms mBERT by 2.42 F1 on average for cross-lingual transfer, and 1.86 F1 when trained on each language. Training on all languages leads to an average F1 score of 89.43%, outperforming cross-lingual transfer approach by 8.49%. \n\nQuestion Answering. We also obtain new state of the art results on the MLQA cross-lingual question answering benchmark, introduced by Lewis et al. (2019). We follow their procedure by training on the English training data and evaluating on the 7 languages of the dataset. We report results in Table 3. XLM-R obtains F1 and accuracy scores of 70.7% and 52.7% while the previous state of the art was 61.6% and 43.5%. XLM-R also outperforms mBERT by 13.0% F1-score and 11.1% accuracy. It even outperforms BERT-Large on English, confirming its strong monolingual performance.\n\n## 5.3 Multilingual versus Monolingual\n\nIn this section, we present results of multilingual XLM models against monolingual BERT models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 43
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-44",
      "content": "outperforms BERT-Large on English, confirming its strong monolingual performance. ## 5.3 Multilingual versus Monolingual In this section, we present results of multilingual XLM models against monolingual BERT models. \n\nGLUE: XLM-R versus RoBERTa. Our goal is to obtain a multilingual model with strong performance on both, cross-lingual understanding tasks as well as natural language understanding tasks for each language. To that end, we evaluate XLMR on the GLUE benchmark. We show in Table 4, that XLM-R obtains better average dev performance than BERTLarge by 1.6% and reaches performance on par with XLNetLarge. The RoBERTa model outperforms XLM-R by only 1.0% on average. We believe future work can reduce this gap even further by alleviating the curse of multilinguality and",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 44
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-45",
      "content": "by only 1.0% on average. We believe future work can reduce this gap even further by alleviating the curse of multilinguality and \n\nTable 3: Results on MLQA question answering We report the F1 and EM (exact match) scores for zero-shot classification where models are fine-tuned on the English Squad dataset and evaluated on the 7 languages of MLQA. Results with † are taken from the original MLQA paper Lewis et al. (2019).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 45
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-46",
      "content": "with † are taken from the original MLQA paper Lewis et al. (2019). \n\n| Model        | train   |   #lgs | en          | es          | de          | ar          | hi          | vi          | zh          | Avg         |\n|--------------|---------|--------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n| BERT-Large † | en      |      1 | 80.2 / 67.4 | -           | -           | -           | -           | -           | -           | -           |\n| mBERT †      | en      |    102 | 77.7 / 65.2 | 64.3 / 46.6 | 57.9 / 44.3 | 45.7 / 29.8 | 43.8 / 29.7 | 57.1 / 38.6 | 57.5 / 37.3 | 57.7 / 41.6 |\n| XLM-15 †     | en      |     15 | 74.9 / 62.4 | 68.0 / 49.8 | 62.2 / 47.6 | 54.8 / 36.3 | 48.8 / 27.3 | 61.4 / 41.8 | 61.1 / 39.6 | 61.6 / 43.5 |\n| XLM-R Base   | en      |    100 | 77.1 / 64.6 | 67.4 / 49.6 | 60.9 / 46.7 | 54.9 / 36.6 | 59.4 / 42.9 | 64.5 / 44.7 | 61.8 / 39.3 | 63.7 / 46.3 |\n| XLM-R        | en      |    100 | 80.6 / 67.8 | 74.1 / 56.0 | 68.5 / 53.6 | 63.1 / 43.5 | 69.2 / 51.6 | 71.3 / 50.9 | 68.0 / 45.4 | 70.7 / 52.7 |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 46
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-47",
      "content": "63.7 / 46.3 | | XLM-R | en | 100 | 80.6 / 67.8 | 74.1 / 56.0 | 68.5 / 53.6 | 63.1 / 43.5 | 69.2 / 51.6 | 71.3 / 50.9 | 68.0 / 45.4 | 70.7 / 52.7 | \n\nvocabulary dilution. These results demonstrate the possibility of learning one model for many languages while maintaining strong performance on per-language downstream tasks.\n\nTable 4: GLUE dev results. Results with † are from Liu et al. (2019). We compare the performance of XLMR to BERTLarge, XLNet and RoBERTa on the English GLUE benchmark.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 47
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-48",
      "content": "al. (2019). We compare the performance of XLMR to BERTLarge, XLNet and RoBERTa on the English GLUE benchmark. \n\n| Model         |   #lgs | MNLI-m/mm   |   QNLI |   QQP |   SST |   MRPC |   STS-B |   Avg |\n|---------------|--------|-------------|--------|-------|-------|--------|---------|-------|\n| BERT Large †  |      1 | 86.6/-      |   92.3 |  91.3 |  93.2 |   88   |    90   |  90.2 |\n| XLNet Large † |      1 | 89.8/-      |   93.9 |  91.8 |  95.6 |   89.2 |    91.8 |  92   |\n| RoBERTa †     |      1 | 90.2/90.2   |   94.7 |  92.2 |  96.4 |   90.9 |    92.4 |  92.8 |\n| XLM-R         |    100 | 88.9/89.0   |   93.8 |  92.3 |  95   |   89.5 |    91.2 |  91.8 |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 48
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-49",
      "content": "92.4 | 92.8 | | XLM-R | 100 | 88.9/89.0 | 93.8 | 92.3 | 95 | 89.5 | 91.2 | 91.8 | \n\nXNLI: XLM versus BERT. A recurrent criticism against multilingual models is that they obtain worse performance than their monolingual counterparts. In addition to the comparison of XLM-R and RoBERTa, we provide the first comprehensive study to assess this claim on the XNLI benchmark. We extend our comparison between multilingual XLM models and monolingual BERT models on 7 languages and compare performance in Table 5. We train 14 monolingual BERT models on Wikipedia and CommonCrawl (capped at 60 GiB), and two XLM-7 models. We increase the vocabulary size of the multilingual model for a better comparison. We found that multilingual models can outperform their monolingual BERT counterparts . Specifically, in Table 5, we show that for cross-lingual transfer, monolingual baselines outperform XLM-7 for both Wikipedia and CC by 1.6% and 1.3% average accuracy. However, by making use of multilingual training (translate-trainall) and leveraging training sets coming from multiple languages, XLM-7 can outperform the BERT models: our XLM-7 trained on CC obtains 80.0% average accuracy on the 7 languages, while the average performance of BERT models trained on CC is 77.5%. This is a surprising result that shows that the capacity of multilingual models to leverage training data coming from multiple languages for a particular task can overcome the capacity dilution problem to obtain better overall performance.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 49
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-50",
      "content": "average performance of BERT models trained on CC is 77.5%. This is a surprising result that shows that the capacity of multilingual models to leverage training data coming from multiple languages for a particular task can overcome the capacity dilution problem to obtain better overall performance. \n\nTable 5: Multilingual versus monolingual models (BERT-BASE). Wecompare the performance of monolingual models (BERT) versus multilingual models (XLM) on seven languages, using a BERT-BASE architecture. We choose a vocabulary size of 40k and 150k for monolingual and multilingual models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 50
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-51",
      "content": "BERT-BASE architecture. We choose a vocabulary size of 40k and 150k for monolingual and multilingual models. \n\n| Model                                        | D                                            | #vocab                                       | en                                           | fr                                           | de                                           | ru                                           | zh                                           | sw                                           | ur                                           | Avg                                          |\n|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\n| Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        |\n| BERT                                         | Wiki CC                                      | 40k 40k                                      | 84.5 86.7                                    | 78.6 81.2                                    | 80.0 81.2                                    | 75.5 78.2                                    | 77.7 79.5                                    | 60.1 70.8                                    | 57.3 65.1                                    | 73.4 77.5                                    |\n| Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) |\n| XLM-7                                        | Wiki CC                                      | 150k 150k                                    | 82.3 85.7                                    | 76.8 78.6                                    | 74.7 79.5                                    | 72.5 76.4                                    | 73.1 74.8                                    | 60.8 71.2                                    | 62.3 66.9                                    | 71.8 76.2                                    |\n| Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    |\n| XLM-7                                        | Wiki CC                                      | 150k 150k                                    | 84.6 87.2                                    | 80.1 82.5                                    | 80.2 82.9                                    | 75.7 79.7                                    | 78 80.4                                      | 68.7 75.7                                    | 66.7 71.5                                    | 76.3 80.0                                    |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 51
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-52",
      "content": "| Multilingual models (translate-train-all) | Multilingual models (translate-train-all) | Multilingual models (translate-train-all) | Multilingual models (translate-train-all) | Multilingual models (translate-train-all) | | XLM-7 | Wiki CC | 150k 150k | 84.6 87.2 | 80.1 82.5 | 80.2 82.9 | 75.7 79.7 | 78 80.4 | 68.7 75.7 | 66.7 71.5 | 76.3 80.0 | \n\n## 5.4 Representation Learning for Low-resource Languages",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 52
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-53",
      "content": "| 76.3 80.0 | ## 5.4 Representation Learning for Low-resource Languages \n\nWe observed in Table 5 that pretraining on Wikipedia for Swahili and Urdu performed similarly to a randomly initialized model; most likely due to the small size of the data for these languages. On the other hand, pretraining on CC improved performance by up to 10 points. This confirms our assumption that mBERT and XLM-100 rely heavily on cross-lingual transfer but do not model the low-resource languages as well as XLM-R . Specifically, in the translate-train-all setting, we observe that the biggest gains for XLM models trained on CC, compared to their Wikipedia counterparts, are on low-resource languages; 7% and 4.8% improvement on Swahili and Urdu respectively.\n\n## 6 Conclusion\n\nIn this work, we introduced XLM-R , our new state of the art multilingual masked language model trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages. We show that it provides strong gains over previous multilingual",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 53
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-54",
      "content": "state of the art multilingual masked language model trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages. We show that it provides strong gains over previous multilingual \n\nmodels like mBERT and XLM on classification, sequence labeling and question answering. We exposed the limitations of multilingual MLMs, in particular by uncovering the high-resource versus low-resource trade-off, the curse of multilinguality and the importance of key hyperparameters. We also expose the surprising effectiveness of multilingual models over monolingual models, and show strong improvements on low-resource languages.\n\n## References",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 54
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-55",
      "content": "surprising effectiveness of multilingual models over monolingual models, and show strong improvements on low-resource languages. ## References \n\n- Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In COLING , pages 1638-1649.\n- Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019 .\n- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP .\n- Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. In EMNLP . Association for Computational Linguistics.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL .\n- Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 2018. Learning word vectors for 157 languages. In LREC .\n- Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. ACL .\n- Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´ egas, Martin Wattenberg, Greg Corrado, et al. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. TACL , 5:339-351.\n- Armand Joulin, Edouard Grave, and Piotr Bojanowski Tomas Mikolov. 2017. Bag of tricks for efficient text classification. EACL 2017 , page 427.\n- Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 .\n- Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In ACL , pages 66-75.\n- Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. EMNLP .\n- Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL , pages 260-270, San Diego, California. Association for Computational Linguistics.\n- Guillaume Lample and Alexis Conneau. 2019. Crosslingual language model pretraining. NeurIPS .\n- Patrick Lewis, Barlas O˘ guz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019. Mlqa: Evaluating cross-lingual extractive question answering. arXiv preprint arXiv:1910.07475 .\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 .\n- Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168 .\n- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS , pages 3111-3119.\n- Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP , pages 1532-1543.\n- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL .\n- Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? In ACL .\n- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openaiassets/research-covers/languageunsupervised/language understanding paper.pdf .\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog , 1(8).\n- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 55
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-56",
      "content": "- Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL . - Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? In ACL . - Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openaiassets/research-covers/languageunsupervised/language understanding paper.pdf . - Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog , 1(8). - Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 . \n\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. ACL .\n- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP , pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n- Erik F Sang. 2002. Introduction to the conll-2002 shared task: Language-independent named entity recognition. CoNLL .\n- Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. 2019. Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing. NAACL .\n- Aditya Siddhant, Melvin Johnson, Henry Tsai, Naveen Arivazhagan, Jason Riesa, Ankur Bapna, Orhan Firat, and Karthik Raman. 2019. Evaluating the crosslingual effectiveness of massively multilingual neural machine translation. AAAI .\n- Jasdeep Singh, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2019. Xlda: Cross-lingual data augmentation for natural language inference and question answering. arXiv preprint arXiv:1905.11471 .\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP , pages 1631-1642.\n- Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and TieYan Liu. 2019. Multilingual neural machine translation with knowledge distillation. ICLR .\n- Erik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: languageindependent named entity recognition. In CoNLL , pages 142-147. Association for Computational Linguistics.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems , pages 6000-6010.\n- Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 .\n- Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2019. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359 .\n- Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. Proceedings of the 2nd Workshop on Evaluating VectorSpace Representations for NLP .\n- Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Emerging cross-lingual structure in pretrained language models. ACL .\n- Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert. EMNLP .\n- Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848 .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 56
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-57",
      "content": "2019. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359 . - Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. Proceedings of the 2nd Workshop on Evaluating VectorSpace Representations for NLP . - Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Emerging cross-lingual structure in pretrained language models. ACL . - Shijie Wu and Mark Dredze. 2019. Beto, bentz, becas: The surprising cross-lingual effectiveness of bert. EMNLP . - Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848 . \n\n## Appendix\n\n## A Languages and statistics for CC-100 used by XLM-R",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 57
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-58",
      "content": "2019. Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848 . ## Appendix ## A Languages and statistics for CC-100 used by XLM-R \n\nIn this section we present the list of languages in the CC-100 corpus we created for training XLM-R . We also report statistics such as the number of tokens and the size of each monolingual corpus.\n\nTable 6: Languages and statistics of the CC-100 corpus. We report the list of 100 languages and include the number of tokens (Millions) and the size of the data (in GiB) for each language. Note that we also include romanized variants of some non latin languages such as Bengali, Hindi, Tamil, Telugu and Urdu.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 58
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-59",
      "content": "language. Note that we also include romanized variants of some non latin languages such as Bengali, Hindi, Tamil, Telugu and Urdu. \n\n| ISO code   | Language                  | Tokens (M)   | Size (GiB)   | ISO code   | Language                     | Tokens (M)   | Size (GiB)   |\n|------------|---------------------------|--------------|--------------|------------|------------------------------|--------------|--------------|\n| af         | Afrikaans                 | 242          | 1.3          | lo         | Lao                          | 17           | 0.6          |\n| am         | Amharic                   | 68           | 0.8          | lt         | Lithuanian                   | 1835         | 13.7         |\n| ar         | Arabic                    | 2869         | 28.0         | lv         | Latvian                      | 1198         | 8.8          |\n| as         | Assamese                  | 5            | 0.1          | mg         | Malagasy                     | 25           | 0.2          |\n| az         | Azerbaijani               | 783          | 6.5          | mk         | Macedonian                   | 449          | 4.8          |\n| be         | Belarusian                | 362          | 4.3          | ml         | Malayalam                    | 313          | 7.6          |\n| bg         | Bulgarian                 | 5487         | 57.5         | mn         | Mongolian                    | 248          | 3.0          |\n| bn         | Bengali                   | 525          | 8.4          | mr         | Marathi                      | 175          | 2.8          |\n| -          | Bengali Romanized         | 77           | 0.5          | ms         | Malay                        | 1318         | 8.5          |\n| br         | Breton                    | 16           | 0.1          | my         | Burmese                      | 15           | 0.4          |\n| bs         | Bosnian                   | 14           | 0.1          | my         | Burmese                      | 56           | 1.6          |\n| ca         | Catalan                   | 1752         | 10.1         | ne         | Nepali                       | 237          | 3.8          |\n| cs         | Czech                     | 2498         | 16.3         | nl         | Dutch                        | 5025         | 29.3         |\n| cy         | Welsh                     | 141          | 0.8          | no         | Norwegian                    | 8494         | 49.0         |\n| da         | Danish                    | 7823         | 45.6         | om         | Oromo                        | 8            | 0.1          |\n| de         | German                    | 10297        | 66.6         | or         | Oriya                        | 36           | 0.6          |\n| el         | Greek                     | 4285         | 46.9         | pa         | Punjabi                      | 68           | 0.8          |\n| en         | English                   | 55608        | 300.8        | pl         | Polish                       | 6490         | 44.6         |\n| eo         | Esperanto                 | 157          | 0.9          | ps         | Pashto                       | 96           | 0.7          |\n| es         | Spanish                   | 9374         | 53.3         | pt         | Portuguese                   | 8405         | 49.1         |\n| et         | Estonian                  | 843          | 6.1          | ro         | Romanian                     | 10354        | 61.4         |\n| eu         | Basque                    | 270          | 2.0          | ru         | Russian                      | 23408        | 278.0        |\n| fa         | Persian                   | 13259        | 111.6        | sa         | Sanskrit                     | 17           | 0.3          |\n| fi         | Finnish                   | 6730         | 54.3         | sd         | Sindhi                       | 50           | 0.4          |\n| fr         | French                    | 9780         | 56.8         | si         | Sinhala                      | 243          | 3.6          |\n| fy         | Western Frisian           | 29           | 0.2          | sk         | Slovak                       | 3525         | 23.2         |\n| ga         | Irish                     | 86           | 0.5          | sl         | Slovenian                    | 1669         | 10.3         |\n| gd         | Scottish Gaelic           | 21           | 0.1          | so         | Somali                       | 62           | 0.4          |\n| gl         | Galician                  | 495          | 2.9          | sq         | Albanian                     | 918          | 5.4          |\n| gu         | Gujarati                  | 140          | 1.9          | sr         | Serbian                      | 843          | 9.1          |\n| ha         | Hausa                     | 56           | 0.3          | su         | Sundanese                    | 10           | 0.1          |\n| he         | Hebrew                    | 3399         | 31.6         | sv         | Swedish                      | 77.8         | 12.1         |\n| hi         | Hindi                     | 1715         | 20.2         | sw         | Swahili                      | 275          | 1.6          |\n| -          | Hindi Romanized           | 88           | 0.5          | ta         | Tamil                        | 595          | 12.2         |\n| hr         | Croatian                  | 3297         | 20.5         | -          | Tamil Romanized              | 36           | 0.3          |\n| hu         | Hungarian                 | 7807         | 58.4         | te         | Telugu                       | 249          | 4.7          |\n| hy         | Armenian                  | 421          | 5.5          | -          | Telugu Romanized             | 39           | 0.3          |\n| id         | Indonesian                | 22704        | 148.3        | th         | Thai                         | 1834         | 71.7         |\n| is         | Icelandic                 | 505          | 3.2          | tl         | Filipino                     | 556          | 3.1          |\n| it         | Italian                   | 4983         | 30.2         | tr         | Turkish                      | 2736         | 20.9         |\n| ja         | Japanese                  | 530          | 69.3         | ug         | Uyghur                       | 27           | 0.4          |\n| jv         | Javanese                  | 24           | 0.2          | uk         |                              | 6.5          | 84.6         |\n| ka         | Georgian                  |              |              |            | Ukrainian                    |              |              |\n| kk         |                           | 469          | 9.1          | ur         | Urdu                         | 730          | 5.7          |\n| km         | Kazakh Khmer              | 476 36       | 6.4 1.5      | - uz       | Urdu Romanized Uzbek         | 85 91        | 0.5 0.7      |\n| kn         | Kannada                   | 169          | 3.3          | vi         | Vietnamese                   | 24757        | 137.3        |\n|            |                           |              |              | xh         |                              |              | 0.1          |\n| ko         | Korean                    | 5644         | 54.2         |            | Xhosa                        | 13           | 0.3          |\n| ku ky      | Kurdish (Kurmanji) Kyrgyz | 66 94        | 0.4 1.2      | yi zh      | Yiddish Chinese (Simplified) | 34 259       | 46.9         |\n| la         | Latin                     | 390          | 2.5          | zh         | Chinese (Traditional)        | 176          | 16.6         |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 59
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-60",
      "content": "Turkish | 2736 | 20.9 | | ja | Japanese | 530 | 69.3 | ug | Uyghur | 27 | 0.4 | | jv | Javanese | 24 | 0.2 | uk | | 6.5 | 84.6 | | ka | Georgian | | | | Ukrainian | | | | kk | | 469 | 9.1 | ur | Urdu | 730 | 5.7 | | km | Kazakh Khmer | 476 36 | 6.4 1.5 | - uz | Urdu Romanized Uzbek | 85 91 | 0.5 0.7 | | kn | Kannada | 169 | 3.3 | vi | Vietnamese | 24757 | 137.3 | | | | | | xh | | | 0.1 | | ko | Korean | 5644 | 54.2 | | Xhosa | 13 | 0.3 | | ku ky | Kurdish (Kurmanji) Kyrgyz | 66 94 | 0.4 1.2 | yi zh | Yiddish Chinese (Simplified) | 34 259 | 46.9 | | la | Latin | 390 | 2.5 | zh | Chinese (Traditional) | 176 | 16.6 | \n\n## B Model Architectures and Sizes\n\nAs we showed in section 5, capacity is an important parameter for learning strong cross-lingual representations. In the table below, we list multiple monolingual and multilingual models used by the research community and summarize their architectures and total number of parameters.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 60
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-61",
      "content": "Architectures and Sizes As we showed in section 5, capacity is an important parameter for learning strong cross-lingual representations. In the table below, we list multiple monolingual and multilingual models used by the research community and summarize their architectures and total number of parameters. \n\nTable 7: Details on model sizes. We show the tokenization used by each Transformer model, the number of layers L, the number of hidden states of the model H m , the dimension of the feed-forward layer H ff , the number of attention heads A, the size of the vocabulary V and the total number of parameters #params. For Transformer encoders, the number of parameters can be approximated by 4 LH 2 m + 2 LH m H ff + V H m . GPT2 numbers are from Radford et al. (2019), mm-NMT models are from the work of Arivazhagan et al. (2019) on massively multilingual neural machine translation (mmNMT), and T5 numbers are from Raffel et al. (2019). While XLM-R is among the largest models partly due to its large embedding layer, it has a similar number of parameters than XLM-100, and remains significantly smaller that recently introduced Transformer models for multilingual MT and transfer learning. While this table gives more hindsight on the difference of capacity of each model, note it does not highlight other critical differences between the models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 61
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.acl-main.747-chunk-62",
      "content": "of parameters than XLM-100, and remains significantly smaller that recently introduced Transformer models for multilingual MT and transfer learning. While this table gives more hindsight on the difference of capacity of each model, note it does not highlight other critical differences between the models. \n\n| Model        |   #lgs | tokenization   |   L |   H m |   H ff |   A | V    | #params   |\n|--------------|--------|----------------|-----|-------|--------|-----|------|-----------|\n| BERT Base    |      1 | WordPiece      |  12 |   768 |   3072 |  12 | 30k  | 110M      |\n| BERT Large   |      1 | WordPiece      |  24 |  1024 |   4096 |  16 | 30k  | 335M      |\n| mBERT        |    104 | WordPiece      |  12 |   768 |   3072 |  12 | 110k | 172M      |\n| RoBERTa Base |      1 | bBPE           |  12 |   768 |   3072 |   8 | 50k  | 125M      |\n| RoBERTa      |      1 | bBPE           |  24 |  1024 |   4096 |  16 | 50k  | 355M      |\n| XLM-15       |     15 | BPE            |  12 |  1024 |   4096 |   8 | 95k  | 250M      |\n| XLM-17       |     17 | BPE            |  16 |  1280 |   5120 |  16 | 200k | 570M      |\n| XLM-100      |    100 | BPE            |  16 |  1280 |   5120 |  16 | 200k | 570M      |\n| Unicoder     |     15 | BPE            |  12 |  1024 |   4096 |   8 | 95k  | 250M      |\n| XLM-R Base   |    100 | SPM            |  12 |   768 |   3072 |  12 | 250k | 270M      |\n| XLM-R        |    100 | SPM            |  24 |  1024 |   4096 |  16 | 250k | 550M      |\n| GPT2         |      1 | bBPE           |  48 |  1600 |   6400 |  32 | 50k  | 1.5B      |\n| wide-mmNMT   |    103 | SPM            |  12 |  2048 |  16384 |  32 | 64k  | 3B        |\n| deep-mmNMT   |    103 | SPM            |  24 |  1024 |  16384 |  32 | 64k  | 3B        |\n| T5-3B        |      1 | WordPiece      |  24 |  1024 |  16384 |  32 | 32k  | 3B        |\n| T5-11B       |      1 | WordPiece      |  24 |  1024 |  65536 |  32 | 32k  | 11B       |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.acl-main.747",
        "title": "Longformer: The Long-Document Transformer",
        "category": "Architecture",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 62
      }
    }
  ],
  "tables": [
    "| Model                                                                         | D                                                                             | #M                                                                            | #lg                                                                           | en                                                                            | fr                                                                            | es                                                                            | de                                                                            | el                                                                            | bg                                                                            | ru                                                                            | tr                                                                            | ar                                                                            | vi                                                                            | th                                                                            | zh                                                                            | hi                                                                            | sw                                                                            | ur                                                                            | Avg                                                                           |\n|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) | Fine-tune multilingual model on English training set (Cross-lingual Transfer) |\n| Lample and Conneau (2019)                                                     | Wiki+MT                                                                       | N                                                                             | 15                                                                            | 85.0                                                                          | 78.7                                                                          | 78.9                                                                          | 77.8                                                                          | 76.6                                                                          | 77.4                                                                          | 75.3                                                                          | 72.5                                                                          | 73.1                                                                          | 76.1                                                                          | 73.2                                                                          | 76.5                                                                          | 69.6                                                                          | 68.4                                                                          | 67.3                                                                          | 75.1                                                                          |\n| Huang et al. (2019)                                                           | Wiki+MT                                                                       | N                                                                             | 15                                                                            | 85.1                                                                          | 79.0                                                                          | 79.4                                                                          | 77.8                                                                          | 77.2                                                                          | 77.2                                                                          | 76.3                                                                          | 72.8                                                                          | 73.5                                                                          | 76.4                                                                          | 73.6                                                                          | 76.2                                                                          | 69.4                                                                          | 69.7                                                                          | 66.7                                                                          | 75.4                                                                          |\n| Devlin et al. (2018)                                                          | Wiki                                                                          | N                                                                             | 102                                                                           | 82.1                                                                          | 73.8                                                                          | 74.3                                                                          | 71.1                                                                          | 66.4                                                                          | 68.9                                                                          | 69.0                                                                          | 61.6                                                                          | 64.9                                                                          | 69.5                                                                          | 55.8                                                                          | 69.3                                                                          | 60.0                                                                          | 50.4                                                                          | 58.0                                                                          | 66.3                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | N                                                                             | 100                                                                           | 83.7                                                                          | 76.2                                                                          | 76.6                                                                          | 73.7                                                                          | 72.4                                                                          | 73.0                                                                          | 72.1                                                                          | 68.1                                                                          | 68.4                                                                          | 72.0                                                                          | 68.2                                                                          | 71.5                                                                          | 64.5                                                                          | 58.0                                                                          | 62.4                                                                          | 71.3                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | 1                                                                             | 100                                                                           | 83.2                                                                          | 76.7                                                                          | 77.7                                                                          | 74.0                                                                          | 72.7                                                                          | 74.1                                                                          | 72.7                                                                          | 68.7                                                                          | 68.6                                                                          | 72.9                                                                          | 68.9                                                                          | 72.5                                                                          | 65.6                                                                          | 58.2                                                                          | 62.4                                                                          | 70.7                                                                          |\n| XLM-R Base                                                                    | CC                                                                            | 1                                                                             | 100                                                                           | 85.8                                                                          | 79.7                                                                          | 80.7                                                                          | 78.7                                                                          | 77.5                                                                          | 79.6                                                                          | 78.1                                                                          | 74.2                                                                          | 73.8                                                                          | 76.5                                                                          | 74.6                                                                          | 76.7                                                                          | 72.4                                                                          | 66.5                                                                          | 68.3                                                                          | 76.2                                                                          |\n| XLM-R                                                                         | CC                                                                            | 1                                                                             | 100                                                                           | 89.1                                                                          | 84.1                                                                          | 85.1                                                                          | 83.9                                                                          | 82.9                                                                          | 84.0                                                                          | 81.2                                                                          | 79.6                                                                          | 79.8                                                                          | 80.8                                                                          | 78.1                                                                          | 80.2                                                                          | 76.9                                                                          | 73.9                                                                          | 73.8                                                                          | 80.9                                                                          |\n| Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   | Translate everything to English and use English-only model (TRANSLATE-TEST)   |\n| BERT-en                                                                       | Wiki                                                                          | 1                                                                             | 1                                                                             | 88.8                                                                          | 81.4                                                                          | 82.3                                                                          | 80.1                                                                          | 80.3                                                                          | 80.9                                                                          | 76.2                                                                          | 76.0                                                                          | 75.4                                                                          | 72.0                                                                          | 71.9                                                                          | 75.6                                                                          | 70.0                                                                          | 65.8                                                                          | 65.8                                                                          | 76.2                                                                          |\n| RoBERTa                                                                       | Wiki+CC                                                                       | 1                                                                             | 1                                                                             | 91.3                                                                          | 82.9                                                                          | 84.3                                                                          | 81.2                                                                          | 81.7                                                                          | 83.1                                                                          | 78.3                                                                          | 76.8                                                                          | 76.6                                                                          | 74.2                                                                          | 74.1                                                                          | 77.5                                                                          | 70.9                                                                          | 66.7                                                                          | 66.8                                                                          | 77.8                                                                          |\n| Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           | Fine-tune multilingual model on each training set (TRANSLATE-TRAIN)           |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | N                                                                             | 100                                                                           | 82.9                                                                          | 77.6                                                                          | 77.9                                                                          | 77.9                                                                          | 77.1                                                                          | 75.7                                                                          | 75.5                                                                          | 72.6                                                                          | 71.2                                                                          | 75.8                                                                          | 73.1                                                                          | 76.2                                                                          | 70.4                                                                          | 66.5                                                                          | 62.4                                                                          | 74.2                                                                          |\n| Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       | Fine-tune multilingual model on all training sets (TRANSLATE-TRAIN-ALL)       |\n| Lample and Conneau (2019) †                                                   | Wiki+MT                                                                       | 1                                                                             | 15                                                                            | 85.0                                                                          | 80.8                                                                          | 81.3                                                                          | 80.3                                                                          | 79.1                                                                          | 80.9                                                                          | 78.3                                                                          | 75.6                                                                          | 77.6                                                                          | 78.5                                                                          | 76.0                                                                          | 79.5                                                                          | 72.9                                                                          | 72.8                                                                          | 68.5                                                                          | 77.8                                                                          |\n| Huang et al. (2019)                                                           | Wiki+MT                                                                       | 1                                                                             | 15                                                                            | 85.6                                                                          | 81.1                                                                          | 82.3                                                                          | 80.9                                                                          | 79.5                                                                          | 81.4                                                                          | 79.7                                                                          | 76.8                                                                          | 78.2                                                                          | 77.9                                                                          | 77.1                                                                          | 80.5                                                                          | 73.4                                                                          | 73.8                                                                          | 69.6                                                                          | 78.5                                                                          |\n| Lample and Conneau (2019)                                                     | Wiki                                                                          | 1                                                                             | 100                                                                           | 84.5                                                                          | 80.1                                                                          | 81.3                                                                          | 79.3                                                                          | 78.6                                                                          | 79.4                                                                          | 77.5                                                                          | 75.2                                                                          | 75.6                                                                          | 78.3                                                                          | 75.7                                                                          | 78.3                                                                          | 72.1                                                                          | 69.2                                                                          | 67.7                                                                          | 76.9                                                                          |\n| XLM-R Base                                                                    | CC                                                                            | 1                                                                             | 100                                                                           | 85.4                                                                          | 81.4                                                                          | 82.2                                                                          | 80.3                                                                          | 80.4                                                                          | 81.3                                                                          | 79.7                                                                          | 78.6                                                                          | 77.3                                                                          | 79.7                                                                          | 77.9                                                                          | 80.2                                                                          | 76.1                                                                          | 73.1                                                                          | 73.0                                                                          | 79.1                                                                          |\n| XLM-R                                                                         | CC                                                                            | 1                                                                             | 100                                                                           | 89.1                                                                          | 85.1                                                                          | 86.6                                                                          | 85.7                                                                          | 85.3                                                                          | 85.9                                                                          | 83.5                                                                          | 83.2                                                                          | 83.1                                                                          | 83.7                                                                          | 81.5                                                                          | 83.7                                                                          | 81.6                                                                          | 78.0                                                                          | 78.1                                                                          | 83.6                                                                          |",
    "| Model                                    | train     | #M   | en          | nl          | es      | de          | Avg     |\n|------------------------------------------|-----------|------|-------------|-------------|---------|-------------|---------|\n| Lample et al. (2016) Akbik et al. (2018) | each each | N    | 90.74 93.18 | 81.74 90.44 | 85.75 - | 78.76 88.27 | 84.25 - |\n|                                          |           | N    |             |             | 87.38   | 82.82       | 88.28   |\n| mBERT †                                  | each en   | N 1  | 91.97 91.97 | 90.94 77.57 | 74.96   | 69.56       | 78.52   |\n| XLM-R Base                               | each      | N    | 92.25       | 90.39       | 87.99   | 84.60       | 88.81   |\n|                                          | en        | 1    | 92.25       | 78.08       | 76.53   | 69.60       | 79.11   |\n|                                          | all       | 1    | 91.08       | 89.09       | 87.28   | 83.17       | 87.66   |\n| XLM-R                                    | each      | N    | 92.92       | 92.53       | 89.72   | 85.81       | 90.24   |\n|                                          | en        | 1    | 92.92       | 80.80       | 78.64   | 71.40       | 80.94   |\n|                                          | all       | 1    | 92.00       | 91.60       | 89.52   | 84.60       | 89.43   |",
    "| Model        | train   |   #lgs | en          | es          | de          | ar          | hi          | vi          | zh          | Avg         |\n|--------------|---------|--------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|\n| BERT-Large † | en      |      1 | 80.2 / 67.4 | -           | -           | -           | -           | -           | -           | -           |\n| mBERT †      | en      |    102 | 77.7 / 65.2 | 64.3 / 46.6 | 57.9 / 44.3 | 45.7 / 29.8 | 43.8 / 29.7 | 57.1 / 38.6 | 57.5 / 37.3 | 57.7 / 41.6 |\n| XLM-15 †     | en      |     15 | 74.9 / 62.4 | 68.0 / 49.8 | 62.2 / 47.6 | 54.8 / 36.3 | 48.8 / 27.3 | 61.4 / 41.8 | 61.1 / 39.6 | 61.6 / 43.5 |\n| XLM-R Base   | en      |    100 | 77.1 / 64.6 | 67.4 / 49.6 | 60.9 / 46.7 | 54.9 / 36.6 | 59.4 / 42.9 | 64.5 / 44.7 | 61.8 / 39.3 | 63.7 / 46.3 |\n| XLM-R        | en      |    100 | 80.6 / 67.8 | 74.1 / 56.0 | 68.5 / 53.6 | 63.1 / 43.5 | 69.2 / 51.6 | 71.3 / 50.9 | 68.0 / 45.4 | 70.7 / 52.7 |",
    "| Model         |   #lgs | MNLI-m/mm   |   QNLI |   QQP |   SST |   MRPC |   STS-B |   Avg |\n|---------------|--------|-------------|--------|-------|-------|--------|---------|-------|\n| BERT Large †  |      1 | 86.6/-      |   92.3 |  91.3 |  93.2 |   88   |    90   |  90.2 |\n| XLNet Large † |      1 | 89.8/-      |   93.9 |  91.8 |  95.6 |   89.2 |    91.8 |  92   |\n| RoBERTa †     |      1 | 90.2/90.2   |   94.7 |  92.2 |  96.4 |   90.9 |    92.4 |  92.8 |\n| XLM-R         |    100 | 88.9/89.0   |   93.8 |  92.3 |  95   |   89.5 |    91.2 |  91.8 |",
    "| Model                                        | D                                            | #vocab                                       | en                                           | fr                                           | de                                           | ru                                           | zh                                           | sw                                           | ur                                           | Avg                                          |\n|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\n| Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        | Monolingual baselines                        |\n| BERT                                         | Wiki CC                                      | 40k 40k                                      | 84.5 86.7                                    | 78.6 81.2                                    | 80.0 81.2                                    | 75.5 78.2                                    | 77.7 79.5                                    | 60.1 70.8                                    | 57.3 65.1                                    | 73.4 77.5                                    |\n| Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) | Multilingual models (cross-lingual transfer) |\n| XLM-7                                        | Wiki CC                                      | 150k 150k                                    | 82.3 85.7                                    | 76.8 78.6                                    | 74.7 79.5                                    | 72.5 76.4                                    | 73.1 74.8                                    | 60.8 71.2                                    | 62.3 66.9                                    | 71.8 76.2                                    |\n| Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    | Multilingual models (translate-train-all)    |\n| XLM-7                                        | Wiki CC                                      | 150k 150k                                    | 84.6 87.2                                    | 80.1 82.5                                    | 80.2 82.9                                    | 75.7 79.7                                    | 78 80.4                                      | 68.7 75.7                                    | 66.7 71.5                                    | 76.3 80.0                                    |",
    "| ISO code   | Language                  | Tokens (M)   | Size (GiB)   | ISO code   | Language                     | Tokens (M)   | Size (GiB)   |\n|------------|---------------------------|--------------|--------------|------------|------------------------------|--------------|--------------|\n| af         | Afrikaans                 | 242          | 1.3          | lo         | Lao                          | 17           | 0.6          |\n| am         | Amharic                   | 68           | 0.8          | lt         | Lithuanian                   | 1835         | 13.7         |\n| ar         | Arabic                    | 2869         | 28.0         | lv         | Latvian                      | 1198         | 8.8          |\n| as         | Assamese                  | 5            | 0.1          | mg         | Malagasy                     | 25           | 0.2          |\n| az         | Azerbaijani               | 783          | 6.5          | mk         | Macedonian                   | 449          | 4.8          |\n| be         | Belarusian                | 362          | 4.3          | ml         | Malayalam                    | 313          | 7.6          |\n| bg         | Bulgarian                 | 5487         | 57.5         | mn         | Mongolian                    | 248          | 3.0          |\n| bn         | Bengali                   | 525          | 8.4          | mr         | Marathi                      | 175          | 2.8          |\n| -          | Bengali Romanized         | 77           | 0.5          | ms         | Malay                        | 1318         | 8.5          |\n| br         | Breton                    | 16           | 0.1          | my         | Burmese                      | 15           | 0.4          |\n| bs         | Bosnian                   | 14           | 0.1          | my         | Burmese                      | 56           | 1.6          |\n| ca         | Catalan                   | 1752         | 10.1         | ne         | Nepali                       | 237          | 3.8          |\n| cs         | Czech                     | 2498         | 16.3         | nl         | Dutch                        | 5025         | 29.3         |\n| cy         | Welsh                     | 141          | 0.8          | no         | Norwegian                    | 8494         | 49.0         |\n| da         | Danish                    | 7823         | 45.6         | om         | Oromo                        | 8            | 0.1          |\n| de         | German                    | 10297        | 66.6         | or         | Oriya                        | 36           | 0.6          |\n| el         | Greek                     | 4285         | 46.9         | pa         | Punjabi                      | 68           | 0.8          |\n| en         | English                   | 55608        | 300.8        | pl         | Polish                       | 6490         | 44.6         |\n| eo         | Esperanto                 | 157          | 0.9          | ps         | Pashto                       | 96           | 0.7          |\n| es         | Spanish                   | 9374         | 53.3         | pt         | Portuguese                   | 8405         | 49.1         |\n| et         | Estonian                  | 843          | 6.1          | ro         | Romanian                     | 10354        | 61.4         |\n| eu         | Basque                    | 270          | 2.0          | ru         | Russian                      | 23408        | 278.0        |\n| fa         | Persian                   | 13259        | 111.6        | sa         | Sanskrit                     | 17           | 0.3          |\n| fi         | Finnish                   | 6730         | 54.3         | sd         | Sindhi                       | 50           | 0.4          |\n| fr         | French                    | 9780         | 56.8         | si         | Sinhala                      | 243          | 3.6          |\n| fy         | Western Frisian           | 29           | 0.2          | sk         | Slovak                       | 3525         | 23.2         |\n| ga         | Irish                     | 86           | 0.5          | sl         | Slovenian                    | 1669         | 10.3         |\n| gd         | Scottish Gaelic           | 21           | 0.1          | so         | Somali                       | 62           | 0.4          |\n| gl         | Galician                  | 495          | 2.9          | sq         | Albanian                     | 918          | 5.4          |\n| gu         | Gujarati                  | 140          | 1.9          | sr         | Serbian                      | 843          | 9.1          |\n| ha         | Hausa                     | 56           | 0.3          | su         | Sundanese                    | 10           | 0.1          |\n| he         | Hebrew                    | 3399         | 31.6         | sv         | Swedish                      | 77.8         | 12.1         |\n| hi         | Hindi                     | 1715         | 20.2         | sw         | Swahili                      | 275          | 1.6          |\n| -          | Hindi Romanized           | 88           | 0.5          | ta         | Tamil                        | 595          | 12.2         |\n| hr         | Croatian                  | 3297         | 20.5         | -          | Tamil Romanized              | 36           | 0.3          |\n| hu         | Hungarian                 | 7807         | 58.4         | te         | Telugu                       | 249          | 4.7          |\n| hy         | Armenian                  | 421          | 5.5          | -          | Telugu Romanized             | 39           | 0.3          |\n| id         | Indonesian                | 22704        | 148.3        | th         | Thai                         | 1834         | 71.7         |\n| is         | Icelandic                 | 505          | 3.2          | tl         | Filipino                     | 556          | 3.1          |\n| it         | Italian                   | 4983         | 30.2         | tr         | Turkish                      | 2736         | 20.9         |\n| ja         | Japanese                  | 530          | 69.3         | ug         | Uyghur                       | 27           | 0.4          |\n| jv         | Javanese                  | 24           | 0.2          | uk         |                              | 6.5          | 84.6         |\n| ka         | Georgian                  |              |              |            | Ukrainian                    |              |              |\n| kk         |                           | 469          | 9.1          | ur         | Urdu                         | 730          | 5.7          |\n| km         | Kazakh Khmer              | 476 36       | 6.4 1.5      | - uz       | Urdu Romanized Uzbek         | 85 91        | 0.5 0.7      |\n| kn         | Kannada                   | 169          | 3.3          | vi         | Vietnamese                   | 24757        | 137.3        |\n|            |                           |              |              | xh         |                              |              | 0.1          |\n| ko         | Korean                    | 5644         | 54.2         |            | Xhosa                        | 13           | 0.3          |\n| ku ky      | Kurdish (Kurmanji) Kyrgyz | 66 94        | 0.4 1.2      | yi zh      | Yiddish Chinese (Simplified) | 34 259       | 46.9         |\n| la         | Latin                     | 390          | 2.5          | zh         | Chinese (Traditional)        | 176          | 16.6         |",
    "| Model        |   #lgs | tokenization   |   L |   H m |   H ff |   A | V    | #params   |\n|--------------|--------|----------------|-----|-------|--------|-----|------|-----------|\n| BERT Base    |      1 | WordPiece      |  12 |   768 |   3072 |  12 | 30k  | 110M      |\n| BERT Large   |      1 | WordPiece      |  24 |  1024 |   4096 |  16 | 30k  | 335M      |\n| mBERT        |    104 | WordPiece      |  12 |   768 |   3072 |  12 | 110k | 172M      |\n| RoBERTa Base |      1 | bBPE           |  12 |   768 |   3072 |   8 | 50k  | 125M      |\n| RoBERTa      |      1 | bBPE           |  24 |  1024 |   4096 |  16 | 50k  | 355M      |\n| XLM-15       |     15 | BPE            |  12 |  1024 |   4096 |   8 | 95k  | 250M      |\n| XLM-17       |     17 | BPE            |  16 |  1280 |   5120 |  16 | 200k | 570M      |\n| XLM-100      |    100 | BPE            |  16 |  1280 |   5120 |  16 | 200k | 570M      |\n| Unicoder     |     15 | BPE            |  12 |  1024 |   4096 |   8 | 95k  | 250M      |\n| XLM-R Base   |    100 | SPM            |  12 |   768 |   3072 |  12 | 250k | 270M      |\n| XLM-R        |    100 | SPM            |  24 |  1024 |   4096 |  16 | 250k | 550M      |\n| GPT2         |      1 | bBPE           |  48 |  1600 |   6400 |  32 | 50k  | 1.5B      |\n| wide-mmNMT   |    103 | SPM            |  12 |  2048 |  16384 |  32 | 64k  | 3B        |\n| deep-mmNMT   |    103 | SPM            |  24 |  1024 |  16384 |  32 | 64k  | 3B        |\n| T5-3B        |      1 | WordPiece      |  24 |  1024 |  16384 |  32 | 32k  | 3B        |\n| T5-11B       |      1 | WordPiece      |  24 |  1024 |  65536 |  32 | 32k  | 11B       |"
  ],
  "stats": {
    "totalCharacters": 90877,
    "chunkCount": 63,
    "tableCount": 7,
    "oaStatus": "gold",
    "pdfUrl": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf"
  }
}