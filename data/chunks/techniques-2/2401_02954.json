{
  "paper": {
    "id": "2401.02954v1",
    "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
    "authors": [
      "DeepSeek-AI",
      ":",
      "Xiao Bi",
      "Deli Chen",
      "Guanting Chen",
      "Shanhuang Chen",
      "Damai Dai",
      "Chengqi Deng",
      "Honghui Ding",
      "Kai Dong",
      "Qiushi Du",
      "Zhe Fu",
      "Huazuo Gao",
      "Kaige Gao",
      "Wenjun Gao",
      "Ruiqi Ge",
      "Kang Guan",
      "Daya Guo",
      "Jianzhong Guo",
      "Guangbo Hao",
      "Zhewen Hao",
      "Ying He",
      "Wenjie Hu",
      "Panpan Huang",
      "Erhang Li",
      "Guowei Li",
      "Jiashi Li",
      "Yao Li",
      "Y. K. Li",
      "Wenfeng Liang",
      "Fangyun Lin",
      "A. X. Liu",
      "Bo Liu",
      "Wen Liu",
      "Xiaodong Liu",
      "Xin Liu",
      "Yiyuan Liu",
      "Haoyu Lu",
      "Shanghao Lu",
      "Fuli Luo",
      "Shirong Ma",
      "Xiaotao Nie",
      "Tian Pei",
      "Yishi Piao",
      "Junjie Qiu",
      "Hui Qu",
      "Tongzheng Ren",
      "Zehui Ren",
      "Chong Ruan",
      "Zhangli Sha",
      "Zhihong Shao",
      "Junxiao Song",
      "Xuecheng Su",
      "Jingxiang Sun",
      "Yaofeng Sun",
      "Minghui Tang",
      "Bingxuan Wang",
      "Peiyi Wang",
      "Shiyu Wang",
      "Yaohui Wang",
      "Yongji Wang",
      "Tong Wu",
      "Y. Wu",
      "Xin Xie",
      "Zhenda Xie",
      "Ziwei Xie",
      "Yiliang Xiong",
      "Hanwei Xu",
      "R. X. Xu",
      "Yanhong Xu",
      "Dejian Yang",
      "Yuxiang You",
      "Shuiping Yu",
      "Xingkai Yu",
      "B. Zhang",
      "Haowei Zhang",
      "Lecong Zhang",
      "Liyue Zhang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Wentao Zhang",
      "Yichao Zhang",
      "Chenggang Zhao",
      "Yao Zhao",
      "Shangyan Zhou",
      "Shunfeng Zhou",
      "Qihao Zhu",
      "Yuheng Zou"
    ],
    "published": "2024-01-05T18:59:13.000Z",
    "updated": "2024-01-05T18:59:13.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2401.02954v1",
    "absUrl": "https://arxiv.org/abs/2401.02954v1"
  },
  "chunks": [
    {
      "id": "2401.02954v1-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "chunkIndex": 0,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-1",
      "content": "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Scaling Open-Source Language Models with Longtermism",
        "chunkIndex": 1,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-2",
      "content": ", Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou *\n\n* DeepSeek-AI",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Scaling Open-Source Language Models with Longtermism",
        "chunkIndex": 2,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-3",
      "content": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling laws described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate the scaling of large scale models in two prevalent used opensource configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Abstract",
        "chunkIndex": 3,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-4",
      "content": "and is continuously expanding. We further conduct supervised fine-tuning (SFT) and direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.\n\n*Authors are ordered alphabetically by the last name.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Abstract",
        "chunkIndex": 4,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-5",
      "content": "| 1 Introduction   | 1 Introduction                          | 1 Introduction                                                                                                                                                                           | 1 Introduction                                                                                                                                                                           | 3     |\n|------------------|-----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|\n| 2                |                                         |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 5,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-6",
      "content": "--------------------------------------------------------------------------------|-------|\n| 2                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          |       |\n|                  | Pre-Training                            | Pre-Training                                                                                                                                                                             | Pre-Training                                                                                                                                                                             | 4     |\n|                  |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 6,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-7",
      "content": "| 4     |\n|                  | 2.1                                     | Data . . . . . . . .                                                                                                                                                                     | . . . . . . . . . . . . . . . . . .                                                                                                                                                      | 4     |\n|                  | 2.2                                     | Architecture                                                                                                                                                                             | . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 7,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-8",
      "content": "| . . . . . . . . . . . . . . . . . . . . .                                                                                                                                                | 5     |\n|                  | 2.3                                     | Hyperparameters . . .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 5     |\n|                  | 2.4                                     | Infrastructures",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 8,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-9",
      "content": "| 2.4                                     | Infrastructures                                                                                                                                                                          | . . . . . . . . . . . . . . . . . . . .                                                                                                                                                  | 6     |\n| 3                | Scaling Laws 7                          | Scaling Laws 7                                                                                                                                                                           | Scaling Laws 7                                                                                                                                                                           |       |\n|                  | 3.1                                     | Scaling Laws for Hyperparameters",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 9,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-10",
      "content": "|       |\n|                  | 3.1                                     | Scaling Laws for Hyperparameters                                                                                                                                                         | . . . . . . . .                                                                                                                                                                          | 8     |\n|                  | 3.2                                     | Estimating Optimal Model and Data Scaling                                                                                                                                                | . .                                                                                                                                                                                      | 9     |\n|                  |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 10,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-11",
      "content": "| 9     |\n|                  | 3.3                                     | Scaling Laws with Different Data                                                                                                                                                         | . . . . . . . . .                                                                                                                                                                        | 12    |\n| 4                |                                         |                                                                                                                                                                                          |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 11,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-12",
      "content": "|                                                                                                                                                                                          | 12    |\n|                  | Alignment                               | Alignment                                                                                                                                                                                | Alignment                                                                                                                                                                                |       |\n| 5                | 5.1                                     | Public Benchmark                                                                                                                                                                         | Evaluation . . . . . . . . . . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 12,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-13",
      "content": "| Evaluation . . . . . . . . . . .                                                                                                                                                         | 13    |\n|                  |                                         | 5.1.1                                                                                                                                                                                    | Base Model . . . . . . . . . . . . . . . . .                                                                                                                                             | 14    |\n|                  |                                         | 5.1.2 . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 13,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-14",
      "content": "| 14    |\n|                  |                                         | 5.1.2 . .                                                                                                                                                                                | Chat Model . . . . . . . . . . . . . . .                                                                                                                                                 | 14    |\n|                  | 5.2                                     | Open-Ended                                                                                                                                                                               | Evaluation . . . . . . . . . . . . . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 14,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-15",
      "content": "| Evaluation . . . . . . . . . . . . . .                                                                                                                                                   | 17    |\n|                  |                                         | 5.2.1 Chinese Open-Ended Evaluation                                                                                                                                                      | . . . . .                                                                                                                                                                                | 17    |\n|                  |                                         | 5.2.2 English",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 15,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-16",
      "content": "|\n|                  |                                         | 5.2.2 English                                                                                                                                                                            | Open-Ended Evaluation . . . . .                                                                                                                                                          |       |\n|                  | 5.3                                     | Held-Out Evaluation .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 18 18 |\n|                  |                                         |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 16,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-17",
      "content": "| 18 18 |\n|                  |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 19    |\n|                  | 5.4 5.5                                 | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 17,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-18",
      "content": ". . . . . . . | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 20    |\n| 6                | Conclusion, Limitation, and Future Work | Conclusion, Limitation, and Future Work                                                                                                                                                  | Conclusion, Limitation, and Future Work                                                                                                                                                  | 23    |\n| A                |                                         |                                                                                                                                                                                          |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 18,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-19",
      "content": "|                                                                                                                                                                                          | 30    |\n|                  | Appendix                                | Appendix                                                                                                                                                                                 | Appendix                                                                                                                                                                                 |       |\n|                  | A.1                                     | Acknowledgments . .                                                                                                                                                                      | . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 19,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-20",
      "content": "| Acknowledgments . .                                                                                                                                                                      | . . . . . . . . . . . . . . .                                                                                                                                                            | 30    |\n|                  | A.2                                     | Different Model Scale Representations                                                                                                                                                    | . . . . . .                                                                                                                                                                              | 30    |\n|                  | A.3                                     | Benchmark Metrics Curves",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 20,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-21",
      "content": "| 30    |\n|                  | A.3                                     | Benchmark Metrics Curves                                                                                                                                                                 | . . . . . . . . . . . .                                                                                                                                                                  | 31    |\n|                  | A.4                                     | Comparison with Code or Math Specific Models                                                                                                                                             |                                                                                                                                                                                          | 32    |\n|                  | A.5                                     | Benchmark",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 21,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-22",
      "content": "| 32    |\n|                  | A.5                                     | Benchmark Results w/ DPO                                                                                                                                                                 | Stage . . . . . . . .                                                                                                                                                                    | 32    |\n|                  | A.6                                     | Evaluation Formats . .                                                                                                                                                                   | . . . . . . . . . . . . . . .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 22,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-23",
      "content": "ts . .                                                                                                                                                                   | . . . . . . . . . . . . . . .                                                                                                                                                            | 32    |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Contents",
        "chunkIndex": 23,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-24",
      "content": "Over the past few years, Large Language Models (LLMs) based on decoder-only Transformers (Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Artificial General Intelligence (AGI). By predicting the next word in continuous text, LLMs undergo self-supervised pre-training on massive datasets, enabling them to achieve various purposes and possess many abilities, such as novel creation, text summarization, code completion, and more. Subsequent developments like supervised fine-tuning and reward modeling have enabled Large Language Models (LLMs) to better follow user intentions and instructions. This has endowed them with more versatile conversational capabilities and rapidly expanded their influence.\n\nThis wave is sparked with closed products, such as ChatGPT (OpenAI, 2022), Claude (Anthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational resources and substantial annotation costs.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 24,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-25",
      "content": "ed with closed products, such as ChatGPT (OpenAI, 2022), Claude (Anthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational resources and substantial annotation costs. These products have significantly raised the community's expectations for the capabilities of open-source LLMs, consequently inspiring a series of work (Bai et al., 2023; Du et al., 2022; Jiang et al., 2023; Touvron et al., 2023a,b; Yang et al., 2023). Among these, the LLaMA series models (Touvron et al., 2023a,b) stand out. It consolidates a range of works to create an efficient and stable architecture, building well-performing models ranging from 7B to 70B parameters. Consequently, the LLaMA series has become the de facto benchmark for architecture and performance among open-source models.\n\nFollowing LLaMA, the open-source community has primarily focused on training fixed-size (7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM scaling laws (Hoff",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 25,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-26",
      "content": "Following LLaMA, the open-source community has primarily focused on training fixed-size (7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020). Nonetheless, research on scaling laws is of utmost importance, considering that the current open-source models are merely at the initial stage of Artificial General Intelligence (AGI) development. In addition, early works (Hoffmann et al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data with increased compute budgets and inadequately addressed hyperparameter discussions. In this paper, we extensively investigate the scaling behavior of language models and apply our findings in two widely used large-scale model configurations, namely 7B and 67B. Our study aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further advancements in this domain.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 26,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-27",
      "content": "wo widely used large-scale model configurations, namely 7B and 67B. Our study aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further advancements in this domain. Specifically, we first examined the scaling laws of batch size and learning rate, and found their trends with model size. Building on this, we conducted a comprehensive study of the scaling laws of the data and model scale, successfully revealing the optimal model/data scaling-up allocation strategy and predicting the expected performance of our large-scale models. Additionally, during development, we discovered that the scaling laws derived from different datasets show significant differences. This suggests that choice of dataset remarkably affects the scaling behavior, indicating that caution should be exercised when generalizing scaling laws across datasets.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 27,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-28",
      "content": "show significant differences. This suggests that choice of dataset remarkably affects the scaling behavior, indicating that caution should be exercised when generalizing scaling laws across datasets.\n\nUnder the guidance of our scaling laws, we build from scratch open-source large language models, and release as much information as possible for community reference. We collect 2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we generally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler with a multi-step learning rate scheduler, maintaining performance while facilitating continual training. We collected over 1 million instances for supervised fine-tuning (SFT) (Ouyang et al., 2022) from diverse sources. This paper shares our experiences with different SFT strategies and findings in data ablation techniques.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 28,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-29",
      "content": "million instances for supervised fine-tuning (SFT) (Ouyang et al., 2022) from diverse sources. This paper shares our experiences with different SFT strategies and findings in data ablation techniques. Additionally, we have utilized direct preference optimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the model.\n\nWe conduct extensive evaluations using our base and chat models. The evaluation results demonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particularly in the fields of code, mathematics, and reasoning. Following SFT and DPO, the DeepSeek 67B chat model outperforms GPT-3.5 in both Chinese and English open-ended evaluations. This highlights the superior performance of DeepSeek 67B in generating high-quality responses and engaging in meaningful conversations in both languages. Furthermore, the safety evaluation indicates that DeepSeek 67B Chat can provide harmless responses in practice.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 29,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-30",
      "content": "ating high-quality responses and engaging in meaningful conversations in both languages. Furthermore, the safety evaluation indicates that DeepSeek 67B Chat can provide harmless responses in practice.\n\nIn the rest of this paper, we first introduce our pre-training basic concepts of DeepSeek LLM in Section 2, including the composition of data, model architecture, infrastructure, and hyperparameters. In Section 3, we provide a detailed explanation of the scaling laws we have discovered and its implications. Additionally, we discuss the rationale behind our selection of pre-training hyperparameters, taking into account the insights gained from the scaling laws analysis. In Section 4, we discuss our fine-tuning methodology, encompassing the composition of fine-tuning data and specific methods during the SFT and DPO stages.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 30,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-31",
      "content": "sights gained from the scaling laws analysis. In Section 4, we discuss our fine-tuning methodology, encompassing the composition of fine-tuning data and specific methods during the SFT and DPO stages. We then present the detailed evaluation results of DeepSeek LLM in Section 5, covering both the base and chat models, as well as their performance in open-ended evaluations and safety evaluations. Finally, we discuss the current limitations and future directions of DeepSeek LLM in Section 6.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "1. Introduction",
        "chunkIndex": 31,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-32",
      "content": "Our main objective is to comprehensively enhance the richness and diversity of the dataset. We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a). To achieve these goals, we have organized our approach into three essential stages: deduplication, filtering, and remixing. The deduplication and remixing stages ensure a diverse representation of the data by sampling unique instances. The filtering stage enhances the density of information, thereby enabling more efficient and effective model training.\n\nWe adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump. Table 1 illustrates that deduplicating across 91 dumps eliminates four times more documents than a single dump method.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.1. Data",
        "chunkIndex": 32,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-33",
      "content": "er removal of duplicate instances compared to deduplicating within a single dump. Table 1 illustrates that deduplicating across 91 dumps eliminates four times more documents than a single dump method.\n\nTable 1 | Deduplication ratios for various Common Crawl dumps.\n\n| Dumps Used             |    1 |    2 |    6 |   12 |   16 |   22 |   41 |   91 |\n|------------------------|------|------|------|------|------|------|------|------|\n| Deduplication Rate (%) | 22.2 | 46.7 | 55.7 | 69.9 | 75.7 | 76.3 | 81.6 | 89.8 |\n\nIn the filtering stage, we focus on developing robust criteria for document quality assessment. This involves a detailed analysis incorporating both linguistic and semantic evaluations, providing a view of data quality from individual and global perspectives. In the remixing phase, we adjust our approach to address data imbalances, focusing on increasing the presence of underrepresented domains.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.1. Data",
        "chunkIndex": 33,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-34",
      "content": "view of data quality from individual and global perspectives. In the remixing phase, we adjust our approach to address data imbalances, focusing on increasing the presence of underrepresented domains. This adjustment aims to achieve a more balanced and inclusive dataset, ensuring that diverse perspectives and information are adequately represented.\n\nFor our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm based on the tokenizers library (Huggingface Team, 2019). Pre-tokenization was employed to\n\nprevent the merging of tokens from different character categories such as new lines, punctuation, and Chinese-Japanese-Korean (CJK) symbols, similar to GPT-2 (Radford et al., 2019). We also chose to split numbers into individual digits following the approach used in (Touvron et al., 2023a,b). Based on our prior experience, we set the number of conventional tokens in the vocabulary at 100000.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.1. Data",
        "chunkIndex": 34,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-35",
      "content": "e to split numbers into individual digits following the approach used in (Touvron et al., 2023a,b). Based on our prior experience, we set the number of conventional tokens in the vocabulary at 100000. The tokenizer was trained on a multilingual corpus of approximately 24 GB, and we augmented the final vocabulary with 15 special tokens, bringing the total size to 100015. To ensure computational efficiency during training and to reserve space for any additional special tokens that might be needed in the future, we configured the model's vocabulary size to 102400 for training.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.1. Data",
        "chunkIndex": 35,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-36",
      "content": "Table 2 | Detailed specs of DeepSeek LLM family of models. We choose the hyper-parameters based on our findings in Section 3\n\n| Params   |    layers |    model |    heads |    kv_heads |   Context Length |   Sequence Batch Size |   Learning Rate | Tokens   |\n|----------|------------|-----------|-----------|--------------|------------------|-----------------------|-----------------|----------|\n| 7B       |         30 |      4096 |        32 |           32 |             4096 |                  2304 |         0.00042 | 2.0T     |\n| 67B      |         95 |      8192 |        64 |            8 |             4096 |                  4608 |         0.00032 | 2.0T     |\n\nThe micro design of DeepSeek LLM largely follows the design of LLaMA (Touvron et al., 2023a,b), adopting a Pre-Norm structure with RMSNorm (Zhang and Sennrich, 2019) function and using SwiGLU (Shazeer, 2020) as the activation function for the Feed-Forward Network (FFN), with an intermediate layer dimension of 8 3 ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.2. Architecture",
        "chunkIndex": 36,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-37",
      "content": "ture with RMSNorm (Zhang and Sennrich, 2019) function and using SwiGLU (Shazeer, 2020) as the activation function for the Feed-Forward Network (FFN), with an intermediate layer dimension of 8 3  . It also incorporates Rotary Embedding (Su et al., 2024) for positional encoding. To optimize inference cost, the 67B model uses GroupedQuery Attention (GQA) (Ainslie et al., 2023) instead of the traditional Multi-Head Attention (MHA).\n\nHowever, in terms of macro design, DeepSeek LLM differs slightly. Specifically, DeepSeek LLM 7B is a 30-layer network, while DeepSeek LLM 67B has 95 layers. These layer adjustments, while maintaining parameter consistency with other open-source models, also facilitate model pipeline partitioning to optimize training and inference.\n\nUnlike most works using Grouped-Query Attention (GQA), we expanded the 67B model's parameters in network depth rather than the common practice of widening the intermediate width of FFN layers, aiming for better performanc",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.2. Architecture",
        "chunkIndex": 37,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-38",
      "content": "ng Grouped-Query Attention (GQA), we expanded the 67B model's parameters in network depth rather than the common practice of widening the intermediate width of FFN layers, aiming for better performance. Detailed network specifications can be found in Table 2.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.2. Architecture",
        "chunkIndex": 38,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-39",
      "content": "DeepSeek LLM is initialized with a standard deviation of 0.006 and trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with the following hyperparameters:  1 = 0.9,  2 = 0.95, and weight\\_decay = 0.1.\n\nAmulti-step learning rate scheduler is employed during pre-training instead of the typical cosine scheduler. Specifically, the learning rate of the model reaches its maximum value after 2000 warmup steps, and then decreases to 31.6% of the maximum value after processing 80% of the training tokens. It further reduces to 10% of the maximum value after 90% of the tokens. The gradient clipping during the training phase is set to 1.0.\n\nBased on our empirical findings, we observed that despite differences in the loss reduction\n\nFigure 1 | Training loss curves with different learning rate schedulers or different parameters for schedulers. The model size is 1.6 billion parameters, trained on a dataset of 100 billion tokens.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.3. Hyperparameters",
        "chunkIndex": 39,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-40",
      "content": "aining loss curves with different learning rate schedulers or different parameters for schedulers. The model size is 1.6 billion parameters, trained on a dataset of 100 billion tokens.\n\n<!-- image -->\n\ntrend during training, the final performance using a multi-step learning rate scheduler is essentially consistent with that of a cosine scheduler, as shown in Figure 1(a). When adjusting the training scale while keeping the model size fixed, the multi-step learning rate scheduler allows for the reuse of training from the first phase, offering a unique convenience for continual training. Therefore, we chose the multi-step learning rate scheduler as our default setting. We also demonstrate in Figure 1(b) that adjusting the proportions of different stages in the multi-step learning rate scheduler can yield slightly better performance.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.3. Hyperparameters",
        "chunkIndex": 40,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-41",
      "content": "scheduler as our default setting. We also demonstrate in Figure 1(b) that adjusting the proportions of different stages in the multi-step learning rate scheduler can yield slightly better performance. However, for the sake of balancing reuse ratios in continual training and model performance, we opted for the aforementioned distribution of 80%, 10%, and 10% for the three stages respectively.\n\nThe batch size and learning rate vary with the model size. Specific parameters for the pre-training phases of the 7B and 67B models can be found in Table 2.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.3. Hyperparameters",
        "chunkIndex": 41,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-42",
      "content": "We use an efficient and light-weight training framework named HAI-LLM (High-flyer, 2023) to train and evaluate large language models. Data parallelism, tensor parallelism, sequence parallelism, and 1F1B pipeline parallelism are integrated into this framework as done in Megatron (Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019). We also leverage the flash attention (Dao, 2023; Dao et al., 2022) technique to improve hardware utilization. ZeRO-1 (Rajbhandari et al., 2020) is exploited to partition optimizer states over data parallel ranks. Efforts are also made to overlap computation and communication to minimize additional waiting overhead, including the backward procedure of the last micro-batch and reduce-scatter operation in ZeRO-1, and GEMM computation and all-gather/reduce-scatter in sequence parallel. Some layers/operators are fused to speed up training, including LayerNorm, GEMM whenever possible, and Adam updates.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.4. Infrastructures",
        "chunkIndex": 42,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-43",
      "content": "in ZeRO-1, and GEMM computation and all-gather/reduce-scatter in sequence parallel. Some layers/operators are fused to speed up training, including LayerNorm, GEMM whenever possible, and Adam updates. To improve model training stability, we train the model in bf16 precision but accumulate gradients in fp32 precision. In-place cross-entropy is performed to reduce GPU memory consumption, i.e.: we convert bf16 logits to fp32 precision on the fly in the cross-entropy CUDA kernel (instead of converting it beforehand in HBM), calculate the corresponding bf16 gradient, and overwrite logits with its gradient.\n\nModel weights and optimizer states are saved every 5 minutes asynchronously, which means we will lose no more than 5 minutes of training in the worst case of occasional hardware or network failures. These temporary model checkpoints are cleared up regularly to avoid",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.4. Infrastructures",
        "chunkIndex": 43,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-44",
      "content": "hronously, which means we will lose no more than 5 minutes of training in the worst case of occasional hardware or network failures. These temporary model checkpoints are cleared up regularly to avoid\n\nconsuming too much storage space. We also support resuming training from a different 3D parallel configuration to cope with dynamic changes in computing cluster load.\n\nAs for evaluation, we employ vLLM (Kwon et al., 2023) in generative tasks, and continuous batching in non-generative tasks to avoid manual batch size tuning and reduce token padding.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "2.4. Infrastructures",
        "chunkIndex": 44,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-45",
      "content": "Research on scaling laws (Hestness et al., 2017) predates the emergence of large language models. Scaling laws (Henighan et al., 2020; Hoffmann et al., 2022; Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget  , model scale  , and data scale  . When model scale  is represented by model parameters and data scale  by the number of tokens,  can be approximated as  = 6  . Therefore, how to optimize the allocation between model and data scales when increasing the compute budget is also a crucial research objective in scaling laws.\n\nThe development of LLMs (Dai et al., 2019; Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 45,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-46",
      "content": "s.\n\nThe development of LLMs (Dai et al., 2019; Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak. Results in scaling laws demonstrate that expanding the compute budget continues to yield significant benefits, which further encourages the increase in model scales (Brown et al., 2020; Smith et al., 2022).\n\nHowever, as shown in Table 4, early works (Hoffmann et al., 2022; Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 46,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-47",
      "content": "f scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance. Therefore, we revisit scaling laws in this section to address these uncertainties and ensure we are on the right path to efficiently scaleup compute, which reflects the long-term perspective and is key to developing continuously improving models.\n\nTo ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters. Empirically, it has been observed that the optimal values of most parameters during training do not change when varying compute budgets. Therefore, these parameters are consistent with those outlined in Section 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 47,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-48",
      "content": "ion 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.\n\nEarly works (Goyal et al., 2017; McCandlish et al., 2018; Shallue et al., 2019; Smith et al., 2017; Zhang et al., 2019) provided some empirical observations for setting batch size and learning rate, but we found these observations have limited applicability in our preliminary experiments. Through extensive experiments, we modeled the power law relationship between the compute budget  and the optimal batch size and learning rate. This relationship, which we refer to as the scaling laws of hyperparameters, provides an empirical framework for determining the optimal hyperparameters. This methodology ensures that models across different compute budgets can reach their near-optimal performance.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 48,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-49",
      "content": "parameters, provides an empirical framework for determining the optimal hyperparameters. This methodology ensures that models across different compute budgets can reach their near-optimal performance.\n\nWe then study the scaling laws of the model and data scales. To reduce experimental costs and fitting difficulties, we adopted the IsoFLOP profile approach from Chinchilla (Hoffmann et al., 2022) to fit the scaling curve. To represent the model scale more accurately, we utilized a new model scale representation, non-embedding FLOPs/token  , replacing the earlier-used model parameters  , and substituted the approximate compute budget formula  = 6 \n\nwith the more precise  =  . The experimental results provided insights into the optimal model/data scaling-up allocation strategy and performance predictions, and also accurately forecasted the expected performance of DeepSeek LLM 7B and 67B models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 49,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-50",
      "content": "lts provided insights into the optimal model/data scaling-up allocation strategy and performance predictions, and also accurately forecasted the expected performance of DeepSeek LLM 7B and 67B models.\n\nAdditionally, in the process of exploring scaling laws, the data we used underwent multiple iterations, continually improving in quality. We attempted to fit the scaling curve on various datasets and found that the data quality significantly influences the optimal model/data scalingup allocation strategy. The higher the data quality, the more the increased compute budget should be allocated to model scaling. This implies that high-quality data can drive the training of larger models given the same data scale. The differences in the optimal model/data scaling-up allocation strategy may also serve as an indirect approach to assess the quality of data.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 50,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-51",
      "content": "the training of larger models given the same data scale. The differences in the optimal model/data scaling-up allocation strategy may also serve as an indirect approach to assess the quality of data. We will continue to pay close attention to the changes in data quality and its impact on scaling laws, and provide more analysis in future works.\n\nIn summary, our contributions and findings in scaling laws can be summarized as follows:\n\n- We established the scaling laws for hyperparameters, providing an empirical framework for determining the optimal hyperparameters.\n- Instead of model parameters  , we adopt non-embedding FLOPs/token  to represent the model scale, leading to a more accurate optimal model/data scaling-up allocation strategy and a better prediction of generalization loss for large-scale models.\n- The quality of pre-training data impacts the optimal model/data scaling-up allocation strategy.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 51,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-52",
      "content": "scaling-up allocation strategy and a better prediction of generalization loss for large-scale models.\n- The quality of pre-training data impacts the optimal model/data scaling-up allocation strategy. The higher the data quality, the more the increased compute budget should be allocated to model scaling.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3. Scaling Laws",
        "chunkIndex": 52,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-53",
      "content": "We initially conducted a grid search for batch size and learning rate on small-scale experiments with a compute budget of 1e17, and the results of a specific model size (177M FLOPs/token) are illustrated in Figure 2(a). The results demonstrate that the generalization error remains stable across a wide range of choices of batch sizes and learning rates. This indicates that near-optimal performance can be achieved within a relatively wide parameter space.\n\nFigure 2 | Training loss w.r.t. batch size and learning rate with 1e17 and 1e20 FLOPs.\n\n<!-- image -->\n\nThen, we utilized the aforementioned multi-step learning rate scheduler to effectively train multiple models with different batch sizes, learning rates, and compute budgets ranging from\n\n1e17 to 2e19 by reusing the first stage. Considering the redundancy in the parameter space, we regarded the parameters used by models whose generalization error exceeded the minimum by no more than 0.25% as near-optimal hyperparameters.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.1. Scaling Laws for Hyperparameters",
        "chunkIndex": 53,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-54",
      "content": "age. Considering the redundancy in the parameter space, we regarded the parameters used by models whose generalization error exceeded the minimum by no more than 0.25% as near-optimal hyperparameters. We then fitted the batch size  and learning rate  with respect to the compute budget  . The fitting results, as shown in Figure 3, reveal that the optimal batch size  gradually increases with the increase in compute budget  , while the optimal learning rate  gradually decreases. This is in line with the intuitive empirical settings for batch size and learning rate when scaling up models. Moreover, all near-optimal hyperparameters fall within a broad band range, indicating that it is relatively easy to choose near-optimal parameters within this interval. The final formulae we fitted for batch size and learning rate are as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- image -->\n\n(a) Batch size scaling curve\n\n(b) Learning rate scaling curve",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.1. Scaling Laws for Hyperparameters",
        "chunkIndex": 54,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-55",
      "content": "his interval. The final formulae we fitted for batch size and learning rate are as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- image -->\n\n(a) Batch size scaling curve\n\n(b) Learning rate scaling curve\n\nFigure 3 | Scaling curves of batch size and learning rate. The grey circles represent models whose generalization error exceeded the minimum by no more than 0.25%. The dotted line represents the power law fitting the smaller model. The blue stars represent DeepSeek LLM 7B and 67B.\n\nWe validated our formulae on a series of models with a 1e20 compute budget, and the results of a specific model size (2.94B FLOPs per token) are shown in Figure 2(b). The results indicate that the fitted parameters are centered in the optimal parameter space. Subsequent sections also show that the parameters we fitted for DeepSeek LLM 7B and 67B models similarly achieved good performance.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.1. Scaling Laws for Hyperparameters",
        "chunkIndex": 55,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-56",
      "content": "t the fitted parameters are centered in the optimal parameter space. Subsequent sections also show that the parameters we fitted for DeepSeek LLM 7B and 67B models similarly achieved good performance.\n\nHowever, it's important to note that we have not yet considered the impact of factors beyond the compute budget  on the optimal hyperparameters. This is inconsistent with some earlier works (Kaplan et al., 2020; McCandlish et al., 2018) which suggested that the optimal batch size can be modeled as being solely related to the generalization error  . Furthermore, we observed that in models with the same compute budget but different model/data allocations, the optimal parameter space varies slightly. This suggests that further research is needed to understand the selection of hyperparameters and training dynamics. We will explore these aspects in future works.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.1. Scaling Laws for Hyperparameters",
        "chunkIndex": 56,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-57",
      "content": "After deriving the formulae for fitting near-optimal hyperparameters, we started fitting the scaling curve and analyzing the optimal model/data scaling-up allocation strategy. This strategy involves finding model scaling exponent  and data scaling exponent  that satisfy  opt   \n\nand  opt    , respectively. The data scale  can be consistently represented by the number of tokens in the dataset. In previous works, the model scale was typically represented by model parameters, with non-embedding parameters  1 (Kaplan et al., 2020) and complete parameters  2 (Hoffmann et al., 2022). The relationship between compute budget  and model/data scale could be approximately described as  = 6  , meaning we could use 6  1 or 6  2 to approximate the model scale. However, since both 6  1 and 6  2 do not account for the computational overhead of attention operation, and 6  2 also includes the vocabulary computation, which contributes less to the model's capacity",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 57,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-58",
      "content": ", since both 6  1 and 6  2 do not account for the computational overhead of attention operation, and 6  2 also includes the vocabulary computation, which contributes less to the model's capacity, they both have significant approximation errors under certain settings.\n\nTo mitigate these errors, we introduced a new model scale representation: non-embedding FLOPs/token  .  includes the computational overhead of attention operation but does not take into account the vocabulary computation. With the model scale represented by  , the compute budget  can be simply expressed as  =  . The specific differences between 6  1, 6  2, and  are as shown in the following formulae:\n\n<!-- formula-not-decoded -->\n\nwhere  layer represents the number of layers,  model represents the model width,  vocab is the vocabulary size, and  seq is the sequence length.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 58,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-59",
      "content": "lowing formulae:\n\n<!-- formula-not-decoded -->\n\nwhere  layer represents the number of layers,  model represents the model width,  vocab is the vocabulary size, and  seq is the sequence length. We assessed the differences between these three representations across models of varying scales, as shown in Table 3. The results indicate that both 6  1 and 6  2 either overestimate or underestimate the computational cost in models of different scales. This discrepancy is particularly pronounced in small-scale models, with differences reaching up to 50%. Such inaccuracies can introduce substantial statistical errors when fitting the scaling curve. Please refer to Appendix A.2 for further analysis regarding different representations of model scale.\n\nTable 3 | Difference in model scale representations and disparities of non-embedding parameters  1 and complete parameters  2 relative to non-embedding FLOPs/token  .",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 59,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-60",
      "content": "sentations of model scale.\n\nTable 3 | Difference in model scale representations and disparities of non-embedding parameters  1 and complete parameters  2 relative to non-embedding FLOPs/token  .\n\n|    layers |    model |    vocab |  seq   |  1   |  2   |      |   6  1  |   6  2  |\n|------------|-----------|-----------|---------|-------|-------|-------|-----------|-----------|\n|          8 |       512 |    102400 | 4096    | 25.2M | 77.6M | 352M  |      0.43 |      1.32 |\n|         12 |       768 |    102400 | 4096    | 84.9M | 164M  | 963M  |      0.53 |      1.02 |\n|         24 |      1024 |    102400 | 4096    | 302M  | 407M  | 3.02B |      0.6  |      0.81 |\n|         24 |      2048 |    102400 |         | 1.21B | 1.42B | 9.66B |      0.75 |      0.88 |\n|         32 |      4096 |    102400 |         | 6.44B | 6.86B | 45.1B |      0.85 |      0.91 |\n|         40 |      5120 |    102400 |         | 12.6B | 13.1B | 85.6B |      0.88 |      0.92 |\n|         8",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 60,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-61",
      "content": "32 |      4096 |    102400 |         | 6.44B | 6.86B | 45.1B |      0.85 |      0.91 |\n|         40 |      5120 |    102400 |         | 12.6B | 13.1B | 85.6B |      0.88 |      0.92 |\n|         80 |      8192 |    102400 |         | 64.4B | 65.3B | 419B  |      0.92 |      0.94 |\n\nAfter adopting  to represent the model scale, our objective could be described more clearly as: Given a computing budget  =  , find the optimal model scale  opt and data scale  opt that minimize the generalization error of the model. This target could be formalized as:\n\n<!-- formula-not-decoded -->\n\nTo reduce experimental costs and fitting difficulties, the IsoFLOP profile approach from Chinchilla (Hoffmann et al., 2022) was used to fit the scaling curve. We selected 8 different\n\nFigure 4 | IsoFLOP curve and optimal model/data allocation. The metric in IsoFLOP curve is bits-per-byte on the validation set.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 61,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-62",
      "content": "t al., 2022) was used to fit the scaling curve. We selected 8 different\n\nFigure 4 | IsoFLOP curve and optimal model/data allocation. The metric in IsoFLOP curve is bits-per-byte on the validation set. The dotted lines in optimal model/data scaling curves represent the power law fitting the smaller model (grey circles).\n\n<!-- image -->\n\ncompute budgets ranging from 1e17 to 3e20, and designed around 10 different model/data scale allocations for each budget. The hyperparameters for each budget were determined by Formula(1), and the generalization error was calculated on an independent validation set, distributed similarly to the training set and containing 100M tokens.\n\nFigure 4 demonstrates the IsoFLOP curve and model/data scaling curves, which are fitted by using the optimal model/data allocation for each compute budget. The specific formulae for the optimal non-embedding FLOPs/token  opt and optimal tokens  opt are as follows:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 62,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-63",
      "content": "he optimal model/data allocation for each compute budget. The specific formulae for the optimal non-embedding FLOPs/token  opt and optimal tokens  opt are as follows:\n\n<!-- formula-not-decoded -->\n\nAdditionally, we fitted the loss scaling curve according to compute budget  and optimal generalization error, and predicted the generalization error for DeepSeek LLM 7B and 67B, as shown in Figure 5. The results indicate that using small-scale experiments can accurately predict\n\nFigure 5 | Performance scaling curve. The metric is the bits-per-byte on the validation set. The dotted line represents the power law fitting the smaller model (grey circles). The blue stars represent DeepSeek LLM 7B and 67B. Their performance is well-predicted by the scaling curve.\n\n<!-- image -->\n\nthe performance of models with 1000  compute budget. This provides both confidence and guidance for training models on a larger scale.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 63,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-64",
      "content": "of models with 1000  compute budget. This provides both confidence and guidance for training models on a larger scale.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.2. Estimating Optimal Model and Data Scaling",
        "chunkIndex": 64,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-65",
      "content": "In the development process of DeepSeek LLM, the dataset was iteratively refined multiple times, with adjustments in the proportions of different data sources while enhancing the overall quality. This allowed us to further analyze the impact of different datasets on scaling laws.\n\nWe studied the scaling laws using three different datasets: early in-house data, current inhouse data, and OpenWebText2, which was utilized in the previous study of scaling laws (Kaplan et al., 2020). Our internal data assessment revealed that current in-house data has higher data quality than early in-house data. Furthermore, the quality of OpenWebText2 even surpasses the current in-house data, due to its smaller scale which allows for more meticulous processing.\n\nTable 4 | Coefficients of model scaling and data scaling vary with training data distribution.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.3. Scaling Laws with Different Data",
        "chunkIndex": 65,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-66",
      "content": "rpasses the current in-house data, due to its smaller scale which allows for more meticulous processing.\n\nTable 4 | Coefficients of model scaling and data scaling vary with training data distribution.\n\n| Approach                 |   Coeff.  where  opt (  opt )    |   Coeff.  where  opt    |\n|--------------------------|----------------------------------------|------------------------------|\n| OpenAI (OpenWebText2)    |                                  0.73  |                        0.27  |\n| Chinchilla (MassiveText) |                                  0.49  |                        0.51  |\n| Ours (Early Data)        |                                  0.45  |                        0.55  |\n| Ours (Current Data)      |                                  0.524 |                        0.476 |\n| Ours (OpenWebText2)      |                                  0.578 |                        0.422 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.3. Scaling Laws with Different Data",
        "chunkIndex": 66,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-67",
      "content": "Ours (Current Data)      |                                  0.524 |                        0.476 |\n| Ours (OpenWebText2)      |                                  0.578 |                        0.422 |\n\nAn interesting observation from the analysis is that the optimal model/data scaling-up allocation strategy across these three datasets showed consistency with data quality. As illustrated in Table 4, as data quality improves, the model scaling exponent  gradually increases, while the data scaling exponent  decreases, which suggests that the increased compute budget should be allocated more to the model instead of the data. This finding might also explain the significant differences in optimal model/data scaling-up allocation observed in earlier studies of scaling laws.\n\nAn intuitive speculation for this finding is that high-quality data usually implies logical clarity and less predictive difficulty after sufficient training.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.3. Scaling Laws with Different Data",
        "chunkIndex": 67,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-68",
      "content": "erved in earlier studies of scaling laws.\n\nAn intuitive speculation for this finding is that high-quality data usually implies logical clarity and less predictive difficulty after sufficient training. Therefore, it's more advantageous to scale up the model size when increasing compute budget. We will continue to pay close attention to the changes in data quality and its impact on scaling laws, and provide more analysis in future works.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "3.3. Scaling Laws with Different Data",
        "chunkIndex": 68,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-69",
      "content": "We collect around 1.5 million instruction data instances in English and Chinese, covering a wide range of helpfulness and harmlessness topics. Our helpful data contains 1.2 million instances, with a distribution of 31.2% for general language tasks, 46.6% for mathematical problems, and 22.2% for coding exercises. The safety data consists of 300K instances, covering various sensitive topics.\n\nOur alignment pipeline contains two stages.\n\nSupervised Fine-Tuning: We fine-tuned our 7B model with 4 epochs, but only 2 epochs for the 67B model, since we observed the overfitting problem is serious on the 67B model. We\n\nobserved that GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021) are improved consistently for the 7B model, while the 67B model hits the upper bound soon. The learning rate is 1e-5 and 5e-6 for 7B and 67B models, respectively. In addition to monitoring the benchmark accuracy, we also assess the repetition ratio of a chat model during the fine-tuning process.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "4. Alignment",
        "chunkIndex": 69,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-70",
      "content": "earning rate is 1e-5 and 5e-6 for 7B and 67B models, respectively. In addition to monitoring the benchmark accuracy, we also assess the repetition ratio of a chat model during the fine-tuning process. We gathered a total of 3868 Chinese and English prompts and determined the proportion of generated responses that fail to terminate and instead endlessly repeat a sequence of text. We observed that the repetition ratio tends to rise as the quantity of math SFT data increases. This can be attributed to the fact that math SFT data occasionally includes similar patterns in reasoning. Consequently, weaker models struggle to grasp such reasoning patterns, resulting in repetitive responses. To tackle the problem, we tried two-stage fine-tuning and DPO (Rafailov et al., 2023), both of which could almost keep the benchmark score and reduce the repetition significantly.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "4. Alignment",
        "chunkIndex": 70,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-71",
      "content": "epetitive responses. To tackle the problem, we tried two-stage fine-tuning and DPO (Rafailov et al., 2023), both of which could almost keep the benchmark score and reduce the repetition significantly.\n\nDPO: To further enhance the model's ability, we used the direct preference optimization algorithm (Rafailov et al., 2023), which is proven to be a simple but effective method for LLM alignment. We constructed the preference data for DPO training in terms of helpfulness and harmlessness. For helpfulness data, we collected multilingual prompts, which cover categories including creative writing, question answering, instruction following, and so on. Then we generated responses using our DeepSeek Chat models as response candidates. Similar operations are applied to harmlessness preference data construction.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "4. Alignment",
        "chunkIndex": 71,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-72",
      "content": "ering, instruction following, and so on. Then we generated responses using our DeepSeek Chat models as response candidates. Similar operations are applied to harmlessness preference data construction.\n\nWe trained an epoch for DPO, with a learning rate of 5e-6 and batch size of 512, and we used a learning rate warmup and cosine learning rate scheduler. We found out that DPO can strengthen the model's open-ended generation skill, while engendering little difference in performance among standard benchmarks.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "4. Alignment",
        "chunkIndex": 72,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-73",
      "content": "We evaluate our models on a series of public benchmarks both in English and Chinese, based on the internal evaluation framework.\n\nMulti-subject multiple-choice datasets including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023) and CMMLU (Li et al., 2023).\n\nLanguage understanding and reasoning datasets including HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and BigBench Hard (BBH) (Suzgun et al., 2022).\n\nClosed-book question answering datasets including TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019).\n\nReading comprehension datasets including RACE Lai et al. (2017) and DROP (Dua et al., 2019), C3 (Sun et al., 2019).\n\nReference disambiguation datasets including WinoGrande Sakaguchi et al. (2019) and CLUEWSC (Xu et al., 2020).\n\nLanguage modeling datasets including Pile (Gao et al., 2020).",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1. Public Benchmark Evaluation",
        "chunkIndex": 73,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-74",
      "content": "9), C3 (Sun et al., 2019).\n\nReference disambiguation datasets including WinoGrande Sakaguchi et al. (2019) and CLUEWSC (Xu et al., 2020).\n\nLanguage modeling datasets including Pile (Gao et al., 2020).\n\nChinese understanding and culture datasets including CHID (Zheng et al., 2019) and CCPM (Li et al., 2021).\n\nMath datasets including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and CMath (Wei et al., 2023).\n\nCode datasets including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).\n\nStandardized exams including AGIEval (Zhong et al., 2023).\n\nWe apply perplexity-based evaluation to datasets that require answers to be chosen from several options. These datasets include HellaSwag, PIQA, WinoGrande, RACE-Middle, RACEHigh, MMLU, ARC-Easy, ARC-Challenge, OpenBookQA, CHID, C-Eval, CMMLU, C3 and CCPM. The perplexity-based evaluation here refers to calculating the perplexity of each option and selecting the lowest one as the model prediction.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1. Public Benchmark Evaluation",
        "chunkIndex": 74,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-75",
      "content": "-Challenge, OpenBookQA, CHID, C-Eval, CMMLU, C3 and CCPM. The perplexity-based evaluation here refers to calculating the perplexity of each option and selecting the lowest one as the model prediction. For ARC and OpenBookQA, we calculate the perplexity with unconditional normalization (Brown et al., 2020), and for other datasets we use length normalization.\n\nWe apply generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, HumanEval, MBPP, BBH, AGIEval, CLUEWSC, and CMath. The generation-based evaluation here refers to letting the model generate free texts and parsing results from generated texts. For generation-based evaluation, we use greedy decoding.\n\nWe apply language-modeling-based evaluation for Pile-test, which means calculating the bits-per-byte on the test corpus.\n\nWe use 2048 or 4096 as the maximum sequence length for different benchmarks. Details of evaluation formats can be found in Appendix A.6.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1. Public Benchmark Evaluation",
        "chunkIndex": 75,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-76",
      "content": "rpus.\n\nWe use 2048 or 4096 as the maximum sequence length for different benchmarks. Details of evaluation formats can be found in Appendix A.6.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1. Public Benchmark Evaluation",
        "chunkIndex": 76,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-77",
      "content": "Table 5 presents the main results on the evaluation benchmark. Despite DeepSeek models are pre-trained on 2T bilingual corpus, they show comparable performance on English language understanding benchmarks with LLaMA2 models, which also consume 2T tokens but focus on English. Furthermore, DeepSeek 67B achieves considerably better performance on MATH, GSM8K, HumanEval, MBPP, BBH, and Chinese benchmarks compared to LLaMA2 70B. We show the benchmark curve in the Appendix A.3. We can see some task performance is boosted as model scaling, such as GSM8K and BBH. Given that we train both 7B and 67B on the same dataset, the emergence of this improvement can be attributed to the powerful few-shot learning ability of large models. However, as the proportion of mathematical data increases, the disparity between small and large models may diminish.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.1. Base Model",
        "chunkIndex": 77,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-78",
      "content": "ement can be attributed to the powerful few-shot learning ability of large models. However, as the proportion of mathematical data increases, the disparity between small and large models may diminish.\n\nAn interesting observation is that the advantage of DeepSeek 67B over LLaMA2 70B is larger than that of DeepSeek 7B over LLaMA2 7B. This phenomenon highlights the greater influence of language conflict on smaller models. Additionally, LLaMA2 demonstrates impressive performance on certain Chinese tasks, such as CMath, despite not being specifically trained on Chinese data. This suggests that certain fundamental abilities, such as mathematical reasoning, can be effectively transferred across languages. However, tasks like CHID, which involve evaluating the usage of Chinese idioms, require the model to consume a significant number of Chinese tokens during pre-training. In this case, LLaMA2 significantly underperforms compared to DeepSeek LLM.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.1. Base Model",
        "chunkIndex": 78,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-79",
      "content": "model to consume a significant number of Chinese tokens during pre-training. In this case, LLaMA2 significantly underperforms compared to DeepSeek LLM.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.1. Base Model",
        "chunkIndex": 79,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-80",
      "content": "Table 6 demonstrates the results of the DeepSeek Chat models, showcasing overall improvements in most tasks following tuning. However, there were a few instances where the performance of\n\n| Language   | Benchmark        | Test-shots   |   LLaMA2 7B |   DeepSeek 7B |   LLaMA2 70B |   DeepSeek 67B |\n|------------|------------------|--------------|-------------|---------------|--------------|----------------|\n| English    | HellaSwag        | 0-shot       |      75.6   |        75.4   |       84     |         84     |\n| English    | PIQA             | 0-shot       |      78     |        79.2   |       82     |         83.6   |\n| English    | WinoGrande       | 0-shot       |      69.6   |        70.5   |       80.4   |         79.8   |\n| English    | RACE-Middle      | 5-shot       |      60.7   |        63.2   |       70.1   |         69.9   |\n| English    | RACE-High        | 5-shot       |      45.8   |        46.5   |       54.3   |         50.7   |\n| English    | TriviaQA         | 5",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.2. Chat Model",
        "chunkIndex": 80,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-81",
      "content": "7   |        63.2   |       70.1   |         69.9   |\n| English    | RACE-High        | 5-shot       |      45.8   |        46.5   |       54.3   |         50.7   |\n| English    | TriviaQA         | 5-shot       |      63.8   |        59.7   |       79.5   |         78.9   |\n| English    | NaturalQuestions | 5-shot       |      25.5   |        22.2   |       36.1   |         36.6   |\n| English    | MMLU             | 5-shot       |      45.8   |        48.2   |       69     |         71.3   |\n| English    | ARC-Easy         | 0-shot       |      69.1   |        67.9   |       76.5   |         76.9   |\n|            | ARC-Challenge    | 0-shot       |      49     |        48.1   |       59.5   |         59     |\n|            | OpenBookQA       | 0-shot       |      57.4   |        55.8   |       60.4   |         60.2   |\n|            | DROP             | 1-shot       |      39.8   |        41     |       69.2   |         67.9   |\n|            | MATH             | 4-shot       |       2.5",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.2. Chat Model",
        "chunkIndex": 81,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-82",
      "content": "60.4   |         60.2   |\n|            | DROP             | 1-shot       |      39.8   |        41     |       69.2   |         67.9   |\n|            | MATH             | 4-shot       |       2.5   |         6     |       13.5   |         18.7   |\n|            | GSM8K            | 8-shot       |      15.5   |        17.4   |       58.4   |         63.4   |\n|            | HumanEval        | 0-shot       |      14.6   |        26.2   |       28.7   |         42.7   |\n|            | MBPP             | 3-shot       |      21.8   |        39     |       45.6   |         57.4   |\n|            | BBH              | 3-shot       |      38.5   |        39.5   |       62.9   |         68.7   |\n|            | AGIEval          | 0-shot       |      22.8   |        26.4   |       37.2   |         41.3   |\n|            | Pile-test        | -            |       0.741 |         0.725 |        0.649 |          0.642 |\n| Chinese    | CLUEWSC          | 5-shot       |      64     |        73.1   |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.2. Chat Model",
        "chunkIndex": 82,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-83",
      "content": "1.3   |\n|            | Pile-test        | -            |       0.741 |         0.725 |        0.649 |          0.642 |\n| Chinese    | CLUEWSC          | 5-shot       |      64     |        73.1   |       76.5   |         81     |\n| Chinese    | CHID             | 0-shot       |      37.9   |        89.3   |       55.5   |         92.1   |\n| Chinese    | C-Eval           | 5-shot       |      33.9   |        45     |       51.4   |         66.1   |\n|            | CMMLU            | 5-shot       |      32.6   |        47.2   |       53.1   |         70.8   |\n|            | CMath            | 3-shot       |      25.1   |        34.5   |       53.9   |         63     |\n|            | C3               | 0-shot       |      47.4   |        65.4   |       61.7   |         75.3   |\n|            | CCPM             | 0-shot       |      60.7   |        76.9   |       66.2   |         88.5   |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.2. Chat Model",
        "chunkIndex": 83,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-84",
      "content": "| 0-shot       |      47.4   |        65.4   |       61.7   |         75.3   |\n|            | CCPM             | 0-shot       |      60.7   |        76.9   |       66.2   |         88.5   |\n\nTable 5 | Main results. The evaluation results we report are based on the internal evaluation framework. Bold numbers indicate the best results among the 4 models. For Pile-test we report bits-per-byte (BPB), for DROP we report F1 score and for other tasks we report accuracy. Note that the test-shots is the maximum value and fewer shots might be applied because of limited context length or limited few-shot examples available in the same passage for reading comprehension tasks such as RACE.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.1.2. Chat Model",
        "chunkIndex": 84,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-85",
      "content": "Knowledge : We have observed fluctuations of base and chat models in knowledge-related tasks, such as TriviaQA, MMLU, and C-Eval. However, we do not believe that such minor fluctuations indicate the acquisition or loss of knowledge after SFT. The value of SFT lies in the ability to learn to achieve comparable scores to the base model's few-shot setting in the chat model's zero-shot setting, which is aligned with real scenarios. For example, 0-shot MMLU performance of a chat model is comparable with 5-shot MMLU performance of a base model.\n\nReasoning : As a significant proportion of the SFT instances are in the CoT format Wei et al. (2022), the chat models demonstrate slight improvements in reasoning tasks, such as BBH and NaturalQuestions. However, we believe that the SFT stage does not learn reasoning capabilities but rather the correct format for reasoning paths.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 85,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-86",
      "content": "light improvements in reasoning tasks, such as BBH and NaturalQuestions. However, we believe that the SFT stage does not learn reasoning capabilities but rather the correct format for reasoning paths.\n\n| Language   | Benchmark        |   DeepSeek 7B Base |   DeepSeek 7B Chat |   DeepSeek 67B Base |   DeepSeek 67B Chat |\n|------------|------------------|--------------------|--------------------|---------------------|---------------------|\n| English    | HellaSwag        |               75.4 |               68.5 |                84   |                75.7 |\n| English    | PIQA             |               79.2 |               77.6 |                83.6 |                82.6 |\n| English    | WinoGrande       |               70.5 |               66.9 |                79.8 |                76   |\n| English    | RACE-Middle      |               63.2 |               65.2 |                69.9 |                70.9 |\n| English    | RACE-High        |               46.5 |               50.8 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 86,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-87",
      "content": "|\n| English    | RACE-Middle      |               63.2 |               65.2 |                69.9 |                70.9 |\n| English    | RACE-High        |               46.5 |               50.8 |                50.7 |                56   |\n| English    | TriviaQA         |               59.7 |               57.9 |                78.9 |                81.5 |\n| English    | NaturalQuestions |               22.2 |               32.5 |                36.6 |                47   |\n| English    | MMLU             |               48.2 |               49.4 |                71.3 |                71.1 |\n| English    | ARC-Easy         |               67.9 |               71   |                76.9 |                81.6 |\n| English    | ARC-Challenge    |               48.1 |               49.4 |                59   |                64.1 |\n| English    | GSM8K            |               17.4 |               63   |                63.4 |                84.1 |\n| English    | MATH             |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 87,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-88",
      "content": "59   |                64.1 |\n| English    | GSM8K            |               17.4 |               63   |                63.4 |                84.1 |\n| English    | MATH             |                6   |               15.8 |                18.7 |                32.6 |\n| English    | HumanEval        |               26.2 |               48.2 |                42.7 |                73.8 |\n| English    | MBPP             |               39   |               35.2 |                57.4 |                61.4 |\n| English    | DROP             |               41   |               49.1 |                67.9 |                71.9 |\n| English    | OpenBookQA       |               55.8 |               54.8 |                60.2 |                63.2 |\n| English    | BBH              |               39.5 |               42.3 |                68.7 |                71.7 |\n| English    | AGIEval          |               26.4 |               19.3 |                41.3 |                46.4",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 88,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-89",
      "content": "39.5 |               42.3 |                68.7 |                71.7 |\n| English    | AGIEval          |               26.4 |               19.3 |                41.3 |                46.4 |\n| Chinese    | CLUEWSC          |               73.1 |               71.9 |                81   |                60   |\n| Chinese    | CHID             |               89.3 |               64.9 |                92.1 |                72.6 |\n| Chinese    | C-Eval           |               45   |               47   |                66.1 |                65.2 |\n| Chinese    | CMMLU            |               47.2 |               49.7 |                70.8 |                67.8 |\n| Chinese    | CMath            |               34.5 |               68.4 |                63   |                80.3 |\n| Chinese    | C3               |               65.4 |               66.4 |                75.3 |                77   |\n| Chinese    | CCPM             |               76.9 |               76.5 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 89,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-90",
      "content": "|\n| Chinese    | C3               |               65.4 |               66.4 |                75.3 |                77   |\n| Chinese    | CCPM             |               76.9 |               76.5 |                88.5 |                84.9 |\n\nTable 6 | The comparison between base and chat models. We evaluate chat models with 0-shot for MMLU, GSM8K, MATH, C-Eval, and CMMLU, while base model results are still obtained in the few-shot setting.\n\nPerformance Drop Tasks : The performance of a few tasks consistently declines after finetuning, regardless of the model size or pre-trained checkpoint selected. These particular tasks typically involve cloze tasks or sentence completion tasks, such as HellaSwag. It is reasonable to assume that pure language models are better equipped to handle such tasks.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 90,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-91",
      "content": "ed. These particular tasks typically involve cloze tasks or sentence completion tasks, such as HellaSwag. It is reasonable to assume that pure language models are better equipped to handle such tasks.\n\nMath and Code : Our model exhibits significant improvements in math and coding tasks after fine-tuning. For instance, HumanEval and GSM8K scores are improved by over 20 points. Our explanation for this is that the base model was initially underfitted for these tasks, and the SFT stage has learned additional knowledge in coding and mathematics through the extensive SFT data. However, it is important to note that the model's capabilities may be primarily focused on code completion and algebraic questions. To develop a comprehensive understanding of mathematics and coding, it is crucial to incorporate a diverse range of data during the pretraining stage, which is left as future work. We conducted a detailed analysis of code and math tasks in Appendix A.4.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 91,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-92",
      "content": "cs and coding, it is crucial to incorporate a diverse range of data during the pretraining stage, which is left as future work. We conducted a detailed analysis of code and math tasks in Appendix A.4.\n\nIn the 7B model fine-tuning, we initially fine-tune the model using all data. Subsequently, a second stage is introduced, which excludes math and code data. The motivation behind this approach is that the stage-1 model exhibits a repetition ratio of 2.0%, which is reduced to 1.4%\n\n| Model                     | Overall     | Reasoning       | Reasoning       | Reasoning       | Language       | Language       | Language       | Language       | Language       | Language       | Language       |\n|---------------------------|---------------|---------------------|---------------------|---------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 92,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-93",
      "content": "---------|---------------------|---------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n|                         | Overall     | Avg.            | Math.           | Logi.           | Avg.           | Fund.          | Chi.           | Open.          | Writ.          | Role.          | Pro.           |\n| gpt-4-1106-preview        | 8.01          | 7.73                | 7.80                | 7.66                | 8.29               | 7.99               | 7.33               | 8.61               | 8.67               | 8.47               | 8.65               |\n| gpt-4-0613                | 7.53          | 7.47                | 7.56                | 7.37                | 7.59               | 7.81               | 6.93               | 7.42               | 7.93               | 7.51               | 7.94               |\n| DeepSeek-67B-Chat-D",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 93,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-94",
      "content": "| 7.37                | 7.59               | 7.81               | 6.93               | 7.42               | 7.93               | 7.51               | 7.94               |\n| DeepSeek-67B-Chat-DPO*    | 6.69          | 5.77                | 6.13                | 5.41                | 7.60               | 7.29               | 7.47               | 7.82               | 7.51               | 7.83               | 7.71               |\n| DeepSeek-67B-Chat*        | 6.43          | 5.75                | 5.71                | 5.79                | 7.11               | 7.12               | 6.52               | 7.58               | 7.20               | 6.91               | 7.37               |\n| chatglm-turbo       | 6.24          | 5.00                | 4.74                | 5.26                | 7.49               | 6.82               | 7.17               | 8.16               | 7.77               | 7.76               | 7.24               |\n| erniebot-3.5        | 6.14",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 94,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-95",
      "content": "| 7.49               | 6.82               | 7.17               | 8.16               | 7.77               | 7.76               | 7.24               |\n| erniebot-3.5        | 6.14          | 5.15                | 5.03                | 5.27                | 7.13               | 6.62               | 7.60               | 7.26               | 7.56               | 6.83               | 6.90               |\n| gpt-3.5-turbo-0613        | 6.08          | 5.35                | 5.68                | 5.02                | 6.82               | 6.71               | 5.81               | 7.29               | 7.03               | 7.28               | 6.77               |\n| chatglm-pro         | 5.83          | 4.65                | 4.54                | 4.75                | 7.01               | 6.51               | 6.76               | 7.47               | 7.07               | 7.34               | 6.89               |\n| spark_desk_v2       | 5.74          | 4.73                |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 95,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-96",
      "content": "| 6.51               | 6.76               | 7.47               | 7.07               | 7.34               | 6.89               |\n| spark_desk_v2       | 5.74          | 4.73                | 4.71                | 4.74                | 6.76               | 5.84               | 6.97               | 7.29               | 7.18               | 6.92               | 6.34               |\n| Qwen-14B-Chat             | 5.72          | 4.81                | 4.91                | 4.71                | 6.63               | 6.90               | 6.36               | 6.74               | 6.64               | 6.59               | 6.56               |\n| Baichuan2-13B-Chat        | 5.25          | 3.92                | 3.76                | 4.07                | 6.59               | 6.22               | 6.05               | 7.11               | 6.97               | 6.75               | 6.43               |\n| ChatGLM3-6B               | 4.97          | 3.85                | 3.55                |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 96,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-97",
      "content": "| 6.05               | 7.11               | 6.97               | 6.75               | 6.43               |\n| ChatGLM3-6B               | 4.97          | 3.85                | 3.55                | 4.14                | 6.10               | 5.75               | 5.29               | 6.71               | 6.83               | 6.28               | 5.73               |\n| Baichuan2-7B-Chat         | 4.97          | 3.66                | 3.56                | 3.75                | 6.28               | 5.81               | 5.50               | 7.13               | 6.84               | 6.53               | 5.84               |\n| InternLM-20B              | 4.96          | 3.66                | 3.39                | 3.92                | 6.26               | 5.96               | 5.50               | 7.18               | 6.19               | 6.49               | 6.22               |\n| Qwen-7B-Chat              | 4.91          | 3.73                | 3.62                | 3.83                | 6",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 97,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-98",
      "content": "| 7.18               | 6.19               | 6.49               | 6.22               |\n| Qwen-7B-Chat              | 4.91          | 3.73                | 3.62                | 3.83                | 6.09               | 6.40               | 5.74               | 6.26               | 6.31               | 6.19               | 5.66               |\n| ChatGLM2-6B               | 4.48          | 3.39                | 3.16                | 3.61                | 5.58               | 4.91               | 4.52               | 6.66               | 6.25               | 6.08               | 5.08               |\n| InternLM-Chat-7B          | 3.65          | 2.56                | 2.45                | 2.66                | 4.75               | 4.34               | 4.09               | 5.82               | 4.89               | 5.32               | 4.06               |\n| Chinese-LLaMA-2-7B-Chat   | 3.57          | 2.68                | 2.29                | 3.07                | 4.46               | 4.3",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 98,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-99",
      "content": "4.89               | 5.32               | 4.06               |\n| Chinese-LLaMA-2-7B-Chat   | 3.57          | 2.68                | 2.29                | 3.07                | 4.46               | 4.31               | 4.26               | 4.50               | 4.63               | 4.91               | 4.13               |\n| LLaMA-2-13B-Chinese-Chat  | 3.35          | 2.47                | 2.21                | 2.73                | 4.23               | 4.13               | 3.31               | 4.79               | 3.93               | 4.53               | 4.71               |\n\nTable 7 | AlignBench leaderboard rated by gpt-4-0613. Models are ranked in descending order of total score. Results with * are our evaluation results based on the official AlignBench repository, whereas all other results are derived from the AlignBench paper.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 99,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-100",
      "content": "s are ranked in descending order of total score. Results with * are our evaluation results based on the official AlignBench repository, whereas all other results are derived from the AlignBench paper. We found that our Deepseek-67B-Chat model surpasses ChatGPT and other baseline models by a clear margin, which indicates the superior performance of our model in both basic Chinese language tasks and advanced Chinese reasoning tasks. Besides, we can find that the DPO process has brought improvements in almost all fields.\n\nafter stage-2 tuning, while maintaining the benchmark score. In the case of the 67B model, the repetition ratio is already below 1% following the first stage fine-tuning, and the second stage hurts the model score on the benchmark. Therefore, only one stage of SFT is done for the 67B model.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "certain tasks declined.",
        "chunkIndex": 100,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-101",
      "content": "For chat models, in addition to observing metrics on standard benchmarks, the quality of results generated in open domains and open-ended questions directly affects the actual user experience. Hence, we separately tested the open-ended generation capabilities of our chat model in both Chinese and English tasks.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2. Open-Ended Evaluation",
        "chunkIndex": 101,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-102",
      "content": "For Chinese open-ended evaluation, we tested the comprehensive of our chat model in different domains on a high-quality open-ended question testset AlignBench (Liu et al., 2023). AlignBench includes a total of 8 primary categories, 36 secondary categories, and encompasses 683 questions. For each question, in addition to the prompt, AlignBench also provides professional reference answers and rating templates for GPT-4 to judge the quality of the response.\n\nWe utilized the official AlignBench Github code repository to implement the evaluation of\n\nour model. We strictly aligned the key temperature parameter with the original setting: for role-playing, writing ability, and open-ended questions, the generation temperature was set to 0.7; whereas for other tasks, the generation temperature was set to 0.1.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.1. Chinese Open-Ended Evaluation",
        "chunkIndex": 102,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-103",
      "content": "r with the original setting: for role-playing, writing ability, and open-ended questions, the generation temperature was set to 0.7; whereas for other tasks, the generation temperature was set to 0.1.\n\nThe AlignBench leaderboard is shown in Table 7. We can find that our DeepSeek 67B Chat model surpasses ChatGPT and other baseline models, and is only after the two versions of GPT-4. This demonstrates the excellent performance of our model across various Chinese tasks, compared to other open-source or proprietary Chinese Large Language Models. The DPO model has shown improvement across almost all metrics, which demonstrates the positive impact of the DPO training process on model alignment.\n\nFor the basic Chinese Language tasks, our model is in the first tier among all models, and the Chinese fundamental language ability of our DPO model is even higher than the newest version of GPT-4.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.1. Chinese Open-Ended Evaluation",
        "chunkIndex": 103,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-104",
      "content": ".\n\nFor the basic Chinese Language tasks, our model is in the first tier among all models, and the Chinese fundamental language ability of our DPO model is even higher than the newest version of GPT-4. For the advanced Chinese Reasoning tasks, our model's scores are significantly higher than those of other Chinese LLMs with a clear margin, demonstrating the superior performance of our model in more complex Chinese logical reasoning and mathematical calculations.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.1. Chinese Open-Ended Evaluation",
        "chunkIndex": 104,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-105",
      "content": "For English open-ended evaluation, we use the MT-Bench benchmark (Zheng et al., 2023), which contains 8 different categories of multi-turn questions. As illustrated in Table 8, our DeepSeek LLM 67B Chat outperforms other open-source models such as LLaMA-2-Chat Touvron et al. (2023b) 70B, Xwin 70b v0.1, and TLU 2+DPO 70B (Ivison et al., 2023), and achieves 8.35 score comparable with GPT-3.5-turbo. Besides, after the DPO stage, our DeepSeek LLM 67B Chat DPO further improves the average score to 8.76, which is only behind GPT-4 (OpenAI, 2023). These results illustrate the strong multi-turn open-ended generation ability of DeepSeek LLM.\n\nTable 8 | MT-Bench Evaluation. Results with  are reported in Ivison et al. (2023)\n\n| Model                    |   STEM |   Humanities |   Reasoning |   Coding |   Math |   Extraction |   Roleplay |   Writing |   Average |\n|--------------------------|--------|--------------|-------------|----------|--------|--------------|------------|-----------|--------",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.2. English Open-Ended Evaluation",
        "chunkIndex": 105,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-106",
      "content": "ng |   Math |   Extraction |   Roleplay |   Writing |   Average |\n|--------------------------|--------|--------------|-------------|----------|--------|--------------|------------|-----------|-----------|\n| GPT-4-1106-preview      |   9.9  |         9.95 |        8.1  |     9.05 |   7.95 |         9.9  |       9.5  |      9.7  |      9.26 |\n| GPT-3.5-turbo-0613      |   9.55 |         9.95 |        6.2  |     7.05 |   7.05 |         9    |       8.65 |      9.65 |      8.39 |\n| LLAMA-2-Chat 7B         |   8.65 |         8.75 |        4.25 |     3    |   2.4  |         6.5  |       7.7  |      8.9  |      6.27 |\n| LLAMA-2-Chat 13B        |   8.63 |         9.75 |        5.1  |     3    |   3.45 |         6.93 |       7.5  |      8.85 |      6.65 |\n| LLAMA-2-Chat 70B        |   8.93 |         9.63 |        5.8  |     3.15 |   3.3  |         7.25 |       7.5  |      9.3  |      6.86 |\n| Zephyr-Beta 7B          |   9.03 |         9.63 |        5.6  |     5.1  |   4.45 |         7.45",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.2. English Open-Ended Evaluation",
        "chunkIndex": 106,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-107",
      "content": "9.63 |        5.8  |     3.15 |   3.3  |         7.25 |       7.5  |      9.3  |      6.86 |\n| Zephyr-Beta 7B          |   9.03 |         9.63 |        5.6  |     5.1  |   4.45 |         7.45 |       8.2  |      9.35 |      7.35 |\n| Xwin 70b v0.1           |   9.68 |         9.95 |        6.55 |     4.25 |   3.3  |         8.75 |       8.25 |      9.55 |      7.53 |\n| Xwin 13b v0.2           |   9.55 |         9.88 |        5.2  |     3.6  |   2.85 |         7.7  |       8.6  |      8.68 |      7.01 |\n| TLU 2+DPO 70B          |   9    |         9.9  |        7    |     4.7  |   4.65 |         9.35 |       9.25 |      9.25 |      7.89 |\n| DeepSeek LLM 67B Chat    |   9.6  |         9.7  |        8    |     7.35 |   6.25 |         8.4  |       8.2  |      9.3  |      8.35 |\n| DeepSeek LLM 67B ChatDPO |   9.7  |         9.8  |        9.05 |     6.75 |   6.65 |         9.3  |       9.1  |      9.75 |      8.76 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.2. English Open-Ended Evaluation",
        "chunkIndex": 107,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-108",
      "content": "pSeek LLM 67B ChatDPO |   9.7  |         9.8  |        9.05 |     6.75 |   6.65 |         9.3  |       9.1  |      9.75 |      8.76 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.2.2. English Open-Ended Evaluation",
        "chunkIndex": 108,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-109",
      "content": "Data contamination and benchmark overfitting are two challenges in evaluating LLMs. One common practice is to utilize testsets published recently to evaluate the model as held-out testsets.\n\nLeetCode: To assess the coding proficiency of the model, we have utilized problems from the LeetCode Weekly Contest (Weekly Contest 351-372, Bi-Weekly Contest 108-117, from July 2023 to Nov 2023). We have obtained these problems by crawling data from LeetCode, which consists of 126 problems with over 20 test cases for each. The evaluation metric employed is akin to that of HumanEval. In this regard, if a model's outputs successfully pass all test cases, the model is considered to have effectively solved the problem. The model's coding capabilities are\n\ndepicted in the Figure below, where the y-axis represents the pass@1 score on in-domain human evaluation testing, and the x-axis represents the pass@1 score on out-domain LeetCode Weekly Contest problems.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 109,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-110",
      "content": "cted in the Figure below, where the y-axis represents the pass@1 score on in-domain human evaluation testing, and the x-axis represents the pass@1 score on out-domain LeetCode Weekly Contest problems. The LeetCode test data will be released accompanied with the DeepSeek Coder technique report soon.\n\nHungarian National High-School Exam: In line with Grok-1, we have evaluated the model's mathematical capabilities using the Hungarian National High School Exam. This exam comprises 33 problems, and the model's scores are determined through human annotation. We follow the scoring metric in the solution.pdf to evaluate all models.\n\nInstruction Following Evaluation: On Nov 15th, 2023, Google released an instruction following the evaluation dataset (Zhou et al., 2023). They identified 25 types of verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We use the prompt-level loose metric to evaluate all models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 110,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-111",
      "content": "ed 25 types of verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We use the prompt-level loose metric to evaluate all models.\n\nTable 9 | Held-out Dataset Evaluation.\n\n| Model                 |   LeetCode |   Hungarian Exam |   IFEval |\n|-----------------------|------------|------------------|----------|\n| GPT-4                 |       48.4 |             68   |     79.3 |\n| ChatGLM3 6B           |        2.4 |             32   |     29.7 |\n| DeepSeek LLM 7B Chat  |        4.7 |             28.5 |     41.2 |\n| Baichuan2-Chat 13B    |        1.6 |             19.5 |     44.5 |\n| Yi-Chat 34B           |        7.9 |             39   |     48.4 |\n| Qwen 72B Chat         |       12.7 |             52   |     50.8 |\n| DeepSeek LLM 67B Chat |       17.5 |             58   |     55.5 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 111,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-112",
      "content": "hat 34B           |        7.9 |             39   |     48.4 |\n| Qwen 72B Chat         |       12.7 |             52   |     50.8 |\n| DeepSeek LLM 67B Chat |       17.5 |             58   |     55.5 |\n\nWe have conducted a comparative analysis of our model against various baseline models of different sizes, namely Qwen 72B Chat (Bai et al., 2023), ChatGLM3 (Du et al., 2022), Baichuan2 (Yang et al., 2023), and Yi-34B Chat. Our observations indicate that there exists a significant performance gap between large models and small models on these held-out datasets, even if certain small models achieve promising results on conventional benchmarks. For instance, ChatGLM3 achieves a score of 52.4 on MBPP, a code testset, which is close to DeepSeek 67B. However, when evaluated on new benchmarks, its performance falls considerably short compared to DeepSeek 67B.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 112,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-113",
      "content": "ChatGLM3 achieves a score of 52.4 on MBPP, a code testset, which is close to DeepSeek 67B. However, when evaluated on new benchmarks, its performance falls considerably short compared to DeepSeek 67B. A similar trend is also observed in math datasets, where ChatGLM3 is very strong on GSM8K (72.3), but its performance in the Hungarian Exam score is inferior to large models. Furthermore, the capability of instruction following demonstrates that total computing plays a crucial role.\n\nThe DeepSeek 7B and 67B models utilize the same training pipeline, but there is a significant disparity in their performance. Through our subjective evaluation, we have observed a notable discrepancy in intelligence across various tasks when scaling model size to 67B. While DeepSeek 7B falls behind other smaller language models on standard benchmarks, its performance on held-out tasks is relatively commendable when compared to others.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 113,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-114",
      "content": "language models on standard benchmarks, its performance on held-out tasks is relatively commendable when compared to others.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.3. Held-Out Evaluation",
        "chunkIndex": 114,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-115",
      "content": "We profoundly recognize the importance of safety for general artificial intelligence. The premise for establishing a truly helpful artificial intelligence model is that it possesses values consistent with those of humans and exhibits friendliness towards humanity. We incorporate the assurance of model safety throughout the entire training process, including pre-training, SFT, and DPO.\n\nTo validate the safety of our model, we established a 20-person expert team from various\n\n| Category                                                           | Subcategory                                                                                                                                                                                                                                                                                                   | #Safety Answers / #Total Cases   |\n|--------------------------------------------------------------------|-----------------------------------------",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 115,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-116",
      "content": "| #Safety Answers / #Total Cases   |\n|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n|       (Discrimination and Prejudice Questions)               |     (Ethnic and Racial),     (Religious Belief),    (Nationality and Geography),   (Gender),   (Age),   (Occupation),   (Health),       (Discrimination in Other Aspects)                                                                                                           | 486/500                          |\n|        (Infringement of Others' Legal Rights)              |     (Physical an",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 116,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-117",
      "content": "| 486/500                          |\n|        (Infringement of Others' Legal Rights)              |     (Physical and Mental Health),     (Legitimate Property),    (Portrait Rights),    (Reputation Rights),    (Honor Rights),    (Privacy Rights),     (Information Rights),       (Other Legal Rights)                                                                         | 473/500                          |\n|          (Trade Secrets and Intellectual Property Rights) |        (Infringing Others' Intellectual Property Rights),           (Monopolistic and Unfair Competitive Actions),           (Other Commercially Illegal and Non-compliant Behaviors),       (Violating Business Ethics),        (Disclosing Others' Trade Secrets) | 281/300                          |\n|       (Illegal and Non-compliant Behavior)                   |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 117,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-118",
      "content": "      (Violating Business Ethics),        (Disclosing Others' Trade Secrets) | 281/300                          |\n|       (Illegal and Non-compliant Behavior)                   |     (Cults and Superstition),   (Pornography),   (Gambling),      (Drugs and Prohibited Items),     (Insults and Abuse),     (Violent Behavior),     (Involvement in Organized Crime),         (Other Illegal and Non-compliant Behaviors)                                  | 290/300                          |\n|       (Other Safety Issues)                                  |         (Issues of Illusion and Reality),     (Time-sensitive Issues),       (Self-recognition Problems),       (Other Sensitive Topics),                                                                                                                                            | 767/800                          |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 118,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-119",
      "content": "Other Sensitive Topics),                                                                                                                                            | 767/800                          |\n\nTable 10 | Our taxonomy for safety evaluation. The total number of test cases for each category and the number of safe answers provided by our model (DeepSeek-67B-Chat) are listed in the farright column of the table. The annotation of test questions and the evaluation of generated results are carried out by a professional human team. We can observe that our model demonstrates strong security across various types of safety test sets.\n\ndisciplines and constructed a safety content classification system that aligns with human values (the safety evaluation taxonomy is shown in Table 10). Subsequently, the expert team constructed dozens of high-quality test cases for each safety subcategory manually.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 119,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-120",
      "content": "m that aligns with human values (the safety evaluation taxonomy is shown in Table 10). Subsequently, the expert team constructed dozens of high-quality test cases for each safety subcategory manually. In addition to focusing on the diversity of safety content areas, we also pay attention to the diversity of formats in safety content. The infamous \"grandmother\" loophole indicates that models can be deceived by the surface format of a query into providing unsafe responses. Therefore, when devising questions, the expert team also pays attention to diversifying the ways of inquiry. They construct diverse safety issues through means such as inducement, role-playing, multi-turn dialogues, preset positions, and etc.. Ultimately, we obtained a safety test set comprising 2400 questions. In addition, the expert team has constructed a basic guideline constitution for safety reviews for each different content type and format type.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 120,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-121",
      "content": "we obtained a safety test set comprising 2400 questions. In addition, the expert team has constructed a basic guideline constitution for safety reviews for each different content type and format type.\n\nFor the output results of our model on this test set, we manually inspected its safety. Our review team was well-trained and cross-verification was performed on the annotation results. The annotators perform a three-category annotation for each question: safe, unsafe, and model refusal. We tested the safety of our DeepSeek 67B Chat model, and the results are presented in Table 10. The number of test questions for each safety category and the number of safety tests passed by our model are listed in the table. We label both the securely answered and the model-refused test cases as secure responses. The results indicate that our model exhibits good security performance across numerous safety test categories.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 121,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-122",
      "content": "We label both the securely answered and the model-refused test cases as secure responses. The results indicate that our model exhibits good security performance across numerous safety test categories.\n\nComplementing our existing approach to safety, we further enriched our evaluation using the \"Do-Not-Answer\" dataset (Wang et al., 2023) to evaluate the safety mechanisms of our DeepSeek 67B Chat model. The dataset's 939 risk-categorized prompts were instrumental in highlighting our model's enhanced capabilities. As shown in Table 11, DeepSeek 67B Chat model has demonstrated notable performance, achieving a score of 97.8, which is higher than both ChatGPT and GPT-4. This score not only benchmarks our model's capability to safely handle sensitive queries but also places it competitively among leading models in the field.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.4. Safety Evaluation",
        "chunkIndex": 122,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-123",
      "content": "Throughout the development process, we have discovered some interesting findings in building LLMs.\n\nTable 11 | Do-Not-Answer Score (Wang et al., 2023), a higher score signifies greater model safety. Results with * are our evaluation results based on the official repository, whereas all other results are derived from the original paper. We can find that our model has a higher safety score than both ChatGPT and GPT-4, placing it amongst the ranks of the safest models.\n\n| Model              |   Do-Not-Answer |\n|--------------------|-----------------|\n| LLAMA-2-7B-Chat    |            99.4 |\n| Claude             |            98.3 |\n| DeepSeek-67B-Chat* |            97.8 |\n| ChatGPT            |            97.7 |\n| GPT-4              |            96.5 |\n| Vicuna-7B          |            94.9 |\n| ChatGLM2           |            92.9 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 123,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-124",
      "content": "epSeek-67B-Chat* |            97.8 |\n| ChatGPT            |            97.7 |\n| GPT-4              |            96.5 |\n| Vicuna-7B          |            94.9 |\n| ChatGLM2           |            92.9 |\n\nStaged Fine-Tuning: As we mentioned above, small models need longer fine-tuning on math and code dataset, but it will hurt the model conversation ability, such as increasing repetition behavior. To address this issue, we have implemented a staged fine-tuning process. In this approach, the first stage involves fine-tuning with all available data, while the second stage focuses specifically on fine-tuning with conversational data.\n\nTable 12 | Two-stage fine-tuning results. The repetition ratio is computed when the temperature is 0. The lower repetition ratio is better. The IFEval result is the prompt-level loose accuracy.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 124,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-125",
      "content": "data.\n\nTable 12 | Two-stage fine-tuning results. The repetition ratio is computed when the temperature is 0. The lower repetition ratio is better. The IFEval result is the prompt-level loose accuracy.\n\n| Model                       |   HumanEval |   GSM8K |   Repetition |   IFEval |\n|-----------------------------|-------------|---------|--------------|----------|\n| DeepSeek LLM 7B Chat Stage1 |        48.2 |    63.9 |        0.02  |     38   |\n| DeepSeek LLM 7B Chat Stage2 |        48.2 |    63   |        0.014 |     41.2 |\n\nTable 12 displays the results obtained from the two-stage training process. These results clearly demonstrate that the second stage does not compromise the model's proficiency in code and math, while simultaneously decreasing the repetition behavior and enhancing instruction following capability.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 125,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-126",
      "content": "demonstrate that the second stage does not compromise the model's proficiency in code and math, while simultaneously decreasing the repetition behavior and enhancing instruction following capability.\n\nMulti-Choice Question: It is a common practice to test a model with multi-choice style evaluation data, such as MMLU, AGI Eval, and C-Eval. Multi-choice questions require the model not only to have the corresponding knowledge but also to understand what the option refers to. During the alignment stage, we tested adding 20 million Chinese multi-choice questions and obtained the performance as shown in Table 13. It is important to note that we conducted deduplication for the C-Eval validation set and CMMLU test set to prevent data contamination.\n\nTable 13 | The impact of adding multi-choice question data.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 126,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-127",
      "content": "13. It is important to note that we conducted deduplication for the C-Eval validation set and CMMLU test set to prevent data contamination.\n\nTable 13 | The impact of adding multi-choice question data.\n\n| Model                   |   MMLU |   C-Eval |   CMMLU |   TriviaQA |   ChineseQA |\n|-------------------------|--------|----------|---------|------------|-------------|\n| DeepSeek LLM 7B Chat    |   49.4 |     47   |    49.7 |       57.9 |        75   |\n| DeepSeek LLM 7B Chat+MC |   60.9 |     71.3 |    73.8 |       57.9 |        74.4 |\n\nThe inclusion of an additional 20M MC (multiple-choice) data has proven to be beneficial not only for Chinese multiple-choice benchmarks but also for improving English benchmarks. This indicates that the model's capability to solve MC problems has been enhanced.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 127,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-128",
      "content": "as proven to be beneficial not only for Chinese multiple-choice benchmarks but also for improving English benchmarks. This indicates that the model's capability to solve MC problems has been enhanced. However, we have observed that this improvement does not extend to the model's performance on other evaluations that do not utilize the multiple-choice format, such as TriviaQA and our in-house\n\nChineseQA testsets, which are generative evaluation benchmarks. This suggests that users may not perceive the model as becoming more intelligent during conversational interactions, as these interactions involve generating responses rather than solving multiple-choice problems.\n\nTherefore, we have chosen to exclude MC data from both the pre-training and fine-tuning stages , as including it would result in overfitting to benchmarks and would not contribute to achieving true intelligence in the model.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 128,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-129",
      "content": "n to exclude MC data from both the pre-training and fine-tuning stages , as including it would result in overfitting to benchmarks and would not contribute to achieving true intelligence in the model.\n\nInstruction Data in Pre-Training: It is widely acknowledged that incorporating instruction data during the latter part of the pre-training phase enhances the performance of a base model on benchmark tasks. In our study, we integrated 5 million instruction data, primarily consisting of multi-choice questions, during the final 10% of the pre-training stage. We observed that the base model did exhibit improved performance on the benchmark. However, the final outcomes were nearly identical to those achieved by adding the same data during the SFT stage. We conclude that while this approach strengthens the base model's performance on the benchmark, its overall potential is equivalent to not incorporating these instruction data.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 129,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-130",
      "content": "a during the SFT stage. We conclude that while this approach strengthens the base model's performance on the benchmark, its overall potential is equivalent to not incorporating these instruction data. If the instruction data is substantial in size, it is acceptable to incorporate it into the pre-training process. Due to our preference for excluding multi-choice questions and the limited availability of non-multi-choice questions we have, we made the decision not to include instruction data in the pre-training process.\n\nSystem Prompt: A well-designed system prompt should effectively guide a model to generate responses that are both helpful and respectful. We slightly changed the prompt introduced by LLaMA-2 as our system prompt.\n\nSystem prompt: You are DeepSeek Chat, a helpful, respectful and honest AI assistant developed by DeepSeek. The knowledge cut-off date for your training data is up to May 2023. Always answer as helpfully as possible, while being safe.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 130,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-131",
      "content": "hat, a helpful, respectful and honest AI assistant developed by DeepSeek. The knowledge cut-off date for your training data is up to May 2023. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nWe have observed an intriguing phenomenon wherein the performance of a 7B LLM experiences a slight degradation when a system prompt is introduced. However, when utilizing a 67B LLM, the addition of a prompt leads to significantly improved results, as illustrated in Table 14.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 131,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-132",
      "content": "LM experiences a slight degradation when a system prompt is introduced. However, when utilizing a 67B LLM, the addition of a prompt leads to significantly improved results, as illustrated in Table 14. Our explanation for this disparity is that larger models possess a better understanding of the intended meaning behind the system prompt, enabling them to follow instructions more effectively and generate superior responses. On the other hand, smaller models struggle to grasp the system prompt adequately, and the inconsistency between training and testing might negatively impact their performance.\n\nTable 14 | The impact of adding a system prompt.\n\n| Model                                 |   MTBench |\n|---------------------------------------|-----------|\n| DeepSeek LLM 7B Chat                  |      7.15 |\n| DeepSeek LLM 7B Chat + System Prompt  |      7.11 |\n| DeepSeek LLM 67B Chat                 |      8.35 |\n| DeepSeek LLM 67B Chat + System Prompt |      8.58 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 132,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-133",
      "content": "|      7.15 |\n| DeepSeek LLM 7B Chat + System Prompt  |      7.11 |\n| DeepSeek LLM 67B Chat                 |      8.35 |\n| DeepSeek LLM 67B Chat + System Prompt |      8.58 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "5.5. Discussion",
        "chunkIndex": 133,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-134",
      "content": "We introduce DeepSeek LLMs, a series of open-source models trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In this paper, we provide an in-depth explanation of hyper-parameters selection, scaling laws, as well as the various fine-tuning attempts we made. We calibrate the scaling laws in the previous work and propose a new optimal model/data scaling-up allocation strategy. In addition, we present a method to predict the near-optimal batch size and learning rate with given compute budget. We further conclude that the scaling laws is related to the data quality, which might be the root cause of varying scaling behavior in different works. Guided by the scaling laws, we conduct pre-training with the best hyper-parameter and provide a comprehensive evaluation. We avoid benchmark decoration and dark secrets in all training stages.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "6. Conclusion, Limitation, and Future Work",
        "chunkIndex": 134,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-135",
      "content": "works. Guided by the scaling laws, we conduct pre-training with the best hyper-parameter and provide a comprehensive evaluation. We avoid benchmark decoration and dark secrets in all training stages.\n\nDeepSeek Chat shares the acknowledged limitations commonly found in other LLMs, which include the lack of ongoing knowledge updates after pre-training, the possibility of generating non-factual information such as unverified advice, and a tendency to produce hallucinations. Moreover, it is important to note that our initial version of Chinese data is not exhaustive, which may result in suboptimal performance on certain Chinese-specific topics. Since our data primarily consists of Chinese and English sources, the model's proficiency in other languages remains delicate and should be approached with caution.\n\nDeepSeek LLM is a long-term project committed to advancing open-source language models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "6. Conclusion, Limitation, and Future Work",
        "chunkIndex": 135,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-136",
      "content": "ish sources, the model's proficiency in other languages remains delicate and should be approached with caution.\n\nDeepSeek LLM is a long-term project committed to advancing open-source language models.\n\n- Soon, we will release our technique reports in code intelligence and Mixture-of-Experts (MoE), respectively. They show how we create high-quality code data for pre-training, and design a sparse model to achieve dense model performance.\n- At present, we are constructing a larger and improved dataset for the upcoming version of DeepSeek LLM. We hope the reasoning, Chinese knowledge, math, and code capabilities will be significantly improved in the next version.\n- Our alignment team is dedicated to studying ways to deliver a model that is helpful, honest, and safe to the public. Our initial experiments prove that reinforcement learning could boost model complex reasoning capability.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "6. Conclusion, Limitation, and Future Work",
        "chunkIndex": 136,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-137",
      "content": "- J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrn, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.\n2. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/introd ucing-claude .\n- J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n- J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n- Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 137,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-138",
      "content": "609, 2023.\n- Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n\n- 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432-7439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239 .\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.\n- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 138,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-139",
      "content": "er, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.\n- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\n- P. Clark, I. Cowhey, O. Etzioni, T. Khot, A.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 139,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-140",
      "content": "dlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\n- P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457 .\n- K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n- T. Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n- T. Dao.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 140,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-141",
      "content": "ta .\n- Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n- T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n- T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.\n- Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335, 2022.\n- D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 141,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-142",
      "content": "-335, 2022.\n- D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 23682378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246 .\n\n- L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n2. Google. An important next step on our AI journey, 2023. URL https://blog.google/tech nology/ai/bard-google-ai-search-updates/ .\n- Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 142,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-143",
      "content": "027, 2020.\n2. Google. An important next step on our AI journey, 2023. URL https://blog.google/tech nology/ai/bard-google-ai-search-updates/ .\n- Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A toolintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023. doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745 2 .\n- P. Goyal, P. Dollr, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n- D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n- D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n- T. Henighan, J.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 143,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-144",
      "content": "C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n- T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\n- J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. M. A. Patwary, Y. Yang, and Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n9. High-flyer. Hai-llm:              , 2023. URL https://www.high-flyer.c n/en/blog/hai-llm .\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 144,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-145",
      "content": "de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550 /ARXIV.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556 .\n- Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n12. Huggingface Team. Tokenizers: Fast state-of-the-art tokenizers optimized for research and production, 2019. URL https://github.com/huggingface/tokenizers .\n- F. i, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 145,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-146",
      "content": "e/tokenizers .\n- F. i, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id= fR3wGCk-IXp .\n\n- H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, and H. Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2. 2023.\n- A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n- M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 146,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-147",
      "content": "iv preprint arXiv:2310.06825, 2023.\n- M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361 .\n- V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\n- T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 147,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-148",
      "content": "Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\n- T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_00276. URL https://doi.org/10.1162/tacl\\_a\\_00276 .\n- W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\n- G. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 148,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-149",
      "content": "29th Symposium on Operating Systems Principles, 2023.\n- G. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785-794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082 .\n- H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.\n- W. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset, 2021.\n- X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 149,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-150",
      "content": "e classical poetry matching dataset, 2021.\n- X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang. Alignbench: Benchmarking chinese alignment of large language models. CoRR, abs/2311.18743, 2023. doi: 10.48550/A RXIV.2311.18743. URL https://doi.org/10.48550/arXiv.2311.18743 .\n- I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\n- S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018.\n- T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 150,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-151",
      "content": "583, 2023.\n- S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018.\n- T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.\n- D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15, 2021.\n\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt .\n\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 151,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-152",
      "content": "3.08774, 2023.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n- G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n- A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n- R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. 2023.\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 152,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-153",
      "content": "preference optimization: Your language model is secretly a reward model. 2023.\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE, 2020.\n- K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n- C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research, 20(112):1-49, 2019.\n- N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- S. Smith, M. Patwary, B. Norick, P.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 153,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-154",
      "content": "resley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n- S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.\n\n- J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n- K. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019.\n- M. Suzgun, N. Scales, N. Schrli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 154,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-155",
      "content": "ng prior knowledge for challenging chinese machine reading comprehension, 2019.\n- M. Suzgun, N. Scales, N. Schrli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n- H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n- H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 155,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-156",
      "content": "Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288 .\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n- Y. Wang, H. Li, X. Han, P. Nakov, and T. Baldwin. Do-not-answer: A dataset for evaluating safeguards in llms.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 156,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-157",
      "content": "Attention is all you need. Advances in neural information processing systems, 30, 2017.\n- Y. Wang, H. Li, X. Han, P. Nakov, and T. Baldwin. Do-not-answer: A dataset for evaluating safeguards in llms. CoRR, abs/2308.13387, 2023. doi: 10.48550/ARXIV.2308.13387. URL https://doi.org/10.48550/arXiv.2308.13387 .\n- J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL http://papers.nips.cc/paper\\_files/paper/2022/hash/9d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html .\n- T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.\n- L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 157,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-158",
      "content": ". Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762-4772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419 .\n- A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie, J. Dai,",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 158,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-159",
      "content": "419 .\n- A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie, J. Dai,\n\nK. Fang, L. Su, L. Song, L. Liu, L. Ru, L. Ma, M. Wang, M. Liu, M. Lin, N. Nie, P. Guo, R. Sun, T. Zhang, T. Li, T. Li, W. Cheng, W. Chen, X. Zeng, X. Wang, X. Chen, X. Men, X. Yu, X. Pan, Y. Shen, Y. Wang, Y. Li, Y. Jiang, Y. Gao, Y. Zhang, Z. Zhou, and Z. Wu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023. URL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf .\n\n- L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. CoRR, abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284 .\n- R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 159,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-160",
      "content": "ons for large language models. CoRR, abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284 .\n- R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Mrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791-4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472 .\n- B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.\n- G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.\n- C. Zheng, M.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 160,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-161",
      "content": "C. Shallue, and R. B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.\n- C. Zheng, M. Huang, and A. Sun. Chid: A large-scale chinese idiom dataset for cloze test. In A. Korhonen, D. R. Traum, and L. Mrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 778-787. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1075. URL https://doi.org/10.18653/v1/p19-1075 .\n- L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. 2023.\n- W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 161,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-162",
      "content": "-judge with mt-bench and chatbot arena. 2023.\n- W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364 .\n- J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "References",
        "chunkIndex": 162,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-163",
      "content": "This project was realized thanks to the efforts of numerous contributors. We offer our extended thanks to the following individuals for their help 1 :\n\n- Data Annotation Team: Jialu Cai, Ruijian Chen, Ruyi Chen, Bei Feng, Yanping Huang, Zhen Huang, Pin Jiang, Rongli Jin, Xiangyue Jin, Ziyun Ke, Hui Li, Meng Li, Sangsang Li, Xiaoqian Li, Yaohui Li, Yunxian Ma, Jiaqi Ni, Xiaojin Shen, Xinnan Song, Tianyu Sun, Xiaosha Chen, Haoyuan Tian, Xiaohan Wang, Xiaoxiang Wang, Yuhao Wang, Fanyi Xia, Lei Xu, Zeyuan Xu, Zhipeng Xu, Tian Yuan, Zhongyu Zhang, Yi Zheng, Shuang Zhou, Xinyi Zhou, Yuchen Zhu, Yuxuan Zhu.\n- Compliance Team: Jin Chen, Ying Tang, Miaojun Wang, Xianzu Wang, Shaoqing Wu, Leyi Xia, W.L. Xiao.\n- Business Team: Jian Liang, Mingming Li, T. Wang, Xianzu Wang, Zhiniu Wen, Shengfeng Ye, Peng Zhang, Zhen Zhang.\n- Design Team: Wei An, Yukun Zha.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.1. Acknowledgments",
        "chunkIndex": 163,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-164",
      "content": "We refitted the scaling curve for different model scale representations, reusing the experiments from the IsoFLOP profile. We recalculated the compute FLOPs using 6  1 and 6  2 as model scale representations and refitted the performance scaling curves. As shown in Figure 6, the results indicate that the deviation of optimal model/data allocation among these three representations is not significant at higher compute budgets, but there are noticeable differences at lower budgets.\n\n<!-- image -->\n\n- (a) Compute budget  = 6  1 \n- (b) Compute budget  = 6  2 \n\n<!-- image -->\n\n(c) Compute budget  = \n\n<!-- image -->\n\nFigure 6 | Performance scaling curves using different model scale representations. The metric is the bits-per-byte on the validation set. The dotted line represents the power law fitting the smaller model (grey circles). The blue stars represent DeepSeek LLM 7B and 67B.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.2. Different Model Scale Representations",
        "chunkIndex": 164,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-165",
      "content": "esentations. The metric is the bits-per-byte on the validation set. The dotted line represents the power law fitting the smaller model (grey circles). The blue stars represent DeepSeek LLM 7B and 67B.  1,  2, and  represent the non-embedding parameters, complete parameters, and non-embedding FLOPs/token of the model, respectively.\n\nWhen using 6  1 as the model scale representation, the fitted performance scaling curve tends to overestimate the performance of large-scale models. Conversely, when using 6  2, the\n\n1 Authors are ordered alphabetically by the last name.\n\ncurve tends to underestimate their performance. Using  as the model scale representation, however, achieves the most accurate predictions.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.2. Different Model Scale Representations",
        "chunkIndex": 165,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-166",
      "content": "Figure 7 | Benchmark metrics curves of DeepSeek LLM Base. ChineseQA is our in-house test set, constructed in a manner akin to TriviaQA.\n\n<!-- image -->\n\nFigure 7 shows benchmark metrics curves across different training steps. We can see consistent improvement on these benchmarks from the start to the end of training. We believe the performance will further be improved if the training continues.\n\nTable 15 | Comparison with code-specific models.\n\n| Model                    | Size                     | HumanEval                | HumanEval                | MBPP                     |\n|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                          |                          | Python                   | Multilingual             |                          |\n| Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       |\n| C",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.3. Benchmark Metrics Curves",
        "chunkIndex": 166,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-167",
      "content": "| Multilingual             |                          |\n| Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       |\n| Codex-001                | -                        | 33.5%                    | 26.1%                    | 45.9%                    |\n| StarCoder                | 16B                      | 36.0%                    | 28.7%                    | 46.8%                    |\n| CodeGeeX2                | 6B                       | 36.0%                    | 24.5%                    | 42.4%                    |\n| CodeLlama                | 7B                       | 31.7%                    | 29.2%                    | 41.6%                    |\n| CodeLlama                | 13B                      | 36.0%                    | 35.4%                    | 48.4%                    |\n| CodeLlama                | 34B                      | 48.2 %                   | 41.0 %                   | 55.2%",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.3. Benchmark Metrics Curves",
        "chunkIndex": 167,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-168",
      "content": "36.0%                    | 35.4%                    | 48.4%                    |\n| CodeLlama                | 34B                      | 48.2 %                   | 41.0 %                   | 55.2%                    |\n| DeepSeek-LLM-Base        | 67B                      | 42.7%                    | 37.2%                    | 57.4%                    |\n| Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models |\n| Wizard-Coder             | 34B                      | 73.2%                    | 48.8%                    | 61.2%                    |\n| DeepSeek-LLM-Chat        | 67B                      | 73.8%                    | 53.3 %                   | 61.4%                    |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.3. Benchmark Metrics Curves",
        "chunkIndex": 168,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-169",
      "content": "We have conducted a comparison between our model and specific code and math language models (LLMs). Table 15 demonstrates that DeepSeek LLM 67B is capable of achieving similar performance to CodeLlama, despite having access to less code data. It is worth noting that DeepSeek LLM possesses greater capabilities in areas other than code.\n\nLikewise, Table 16 presents the results obtained from various math-related benchmarks, such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM-zh (i et al., 2023), and CMath (Wei et al., 2023). DeepSeek 67B exhibits exceptional performance on math-related tasks across different languages, showcasing its superiority in this domain. In addition, DeepSeek LLM can utilize programs to solve math problems, which demonstrates better performance than chain-of-thoughts. It is significantly better than the previous SOTA model, ToRA (Gou et al., 2023), on the benchmarks.\n\nTable 16 | Comparison with math-specific models.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.4. Comparison with Code or Math Specific Models",
        "chunkIndex": 169,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-170",
      "content": "trates better performance than chain-of-thoughts. It is significantly better than the previous SOTA model, ToRA (Gou et al., 2023), on the benchmarks.\n\nTable 16 | Comparison with math-specific models.\n\n|                                   | Inference                 | GSM8K                     | MATH                      | MGSM-zh                   | CMath                     |\n|-----------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|\n| Chain-of-Thoughts                 | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         |\n| MetaMath 70B (Yu et al., 2023)    | CoT                       | 82.3%                     | 26.6%                     | 66.4%                     | 70.9%                     |\n| WizardMath 70B (Luo et al., 2023) | CoT                       | 81.6%",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.4. Comparison with Code or Math Specific Models",
        "chunkIndex": 170,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-171",
      "content": "| 82.3%                     | 26.6%                     | 66.4%                     | 70.9%                     |\n| WizardMath 70B (Luo et al., 2023) | CoT                       | 81.6%                     | 22.7%                     | 64.8%                     | 65.4%                     |\n| DeepSeek LLM 67B Chat             | CoT                       | 84.1 %                    | 32.6 %                    | 74.0%                     | 80.3%                     |\n| Tool-Integrated Reasoning         | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning |\n| ToRA-Code 34B (Gou et al., 2023)  | Tool-Integrated           | 80.7%                     | 50.8%                     | 41.2%                     | 53.4%                     |\n| DeepSeek LLM 67B Chat             | Tool-Integrated           | 86.7%                     | 51.1%                     | 76.4%                     | 85.4%",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.4. Comparison with Code or Math Specific Models",
        "chunkIndex": 171,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-172",
      "content": ".4%                     |\n| DeepSeek LLM 67B Chat             | Tool-Integrated           | 86.7%                     | 51.1%                     | 76.4%                     | 85.4%                     |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.4. Comparison with Code or Math Specific Models",
        "chunkIndex": 172,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-173",
      "content": "Table 17 presents the benchmark results obtained with the DPO stage. Based on these results, we can conclude that the DPO stage does not significantly impact the fundamental capability of an LLM.\n\nTable 17 | The benchmark metrics before and after DPO stage.\n\n|                  |   DeepSeek 67B Chat |   DeepSeek 67B Chat DPO |\n|------------------|---------------------|-------------------------|\n| HellaSwag        |                75.7 |                    76.1 |\n| TriviaQA         |                81.5 |                    82.9 |\n| NaturalQuestions |                47   |                    48.8 |\n| MMLU             |                71.1 |                    70.9 |\n| GSM8K            |                84.1 |                    85.2 |\n| MATH             |                32.6 |                    30.2 |\n| HumanEval        |                73.8 |                    71.3 |\n| BBH              |                71.7 |                    70.8 |\n| AGIEval          |                46.4 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.5. Benchmark Results w/ DPO Stage",
        "chunkIndex": 173,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-174",
      "content": "30.2 |\n| HumanEval        |                73.8 |                    71.3 |\n| BBH              |                71.7 |                    70.8 |\n| AGIEval          |                46.4 |                    46.1 |\n| CEval            |                65.2 |                    64.3 |\n| CMMLU            |                67.8 |                    68.2 |",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.5. Benchmark Results w/ DPO Stage",
        "chunkIndex": 174,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-175",
      "content": "Table 18  Table 40 present examples of our evaluation formats on different benchmarks.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "A.6. Evaluation Formats",
        "chunkIndex": 175,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-176",
      "content": "                      \n\n                      ,        (A)          (B)      DNA(C)     ATP      (D)             \n\n  \n\n A  D,     \n\nTable 18 | An example of AGIEval.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 176,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-177",
      "content": "Question: Use the information below to answer the question. Cotton is a plant product used to make fabric. Cotton is made of cellulose, a fiber not digestible by humans. Cellulose is composed of many sugar molecules bonded together into long chains. Each sugar molecule contains carbon, hydrogen, and oxygen atoms. When cotton fabric is washed, wrinkles often form. The clothing industry uses chemicals to manufacture some cotton fabrics that are wrinkle-free. Dyes are also added to color the cellulose fibers in cotton. How would a clothing manufacturer separate colors to determine the purity of the dyes?\n\nAnswer:",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 177,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-178",
      "content": "- through filtration\n- by their boiling points\n- by their freezing points\n- through paper chromatography\n\nTable 19 | An example of ARC.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 178,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-179",
      "content": "Evaluate the result of a random Boolean expression.\n\nQ: not ( ( not not True ) ) is\n\nA: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = not ( ( not not True ) ) = not ( ( A ) )\" where \"A = not not True\". Let's evaluate A: A=not not True = not (not True) = not False = True. Plugging in A, we get: Z = not ( ( A ) ) = not ( ( True ) ) = not True = False. So the answer is False.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 179,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-180",
      "content": "A: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = True and False and not True and True = A and B\" where \"A = True and False\" and \"B = not True and True\". Let's evaluate A: A = True and False = False. Let's evaluate B: B = not True and True = not (True and True) = not (True) = False. Plugging in A and B, we get: Z = A and B = False and False = False. So the answer is False.\n\nQ: not not ( not ( False ) ) is\n\nA: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = not not ( not ( False ) ) = not not ( A )\" where \"A = not ( False )\".",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Q: True and False and not True and True is",
        "chunkIndex": 180,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-181",
      "content": "om highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = not not ( not ( False ) ) = not not ( A )\" where \"A = not ( False )\". Let's evaluate A: A = not ( False ) = not False = True. Plugging in A, we get: Z = not not ( A ) = not not (True) = not not False = True. So the answer is True.\n\nQ: False and False and False or not False is A: Let's think step by step.\n\nTable 20 | An example of BBH.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "Q: True and False and not True and True is",
        "chunkIndex": 181,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-182",
      "content": "                           \n\n                           \\_\\_\\_\\_ \n\n- A.     \n\n- B.     \n\n- C.       \n\n- D.     \n\n  \n\nC\n\n                          \\_\\_\\_\\_ \n\n- A.     \n\n- B.     \n\n- C.     \n\n- D.     \n\n  \n\nA\n\n-         \\_\\_\\_\\_ \n\n- A.           \n\n- B.           \n\n- C.           \n\n- D.           \n\n\n\n\n\n\n\nB\n\n-                        \\_\\_\\_\\_ \n\n- A.         \n\n- B.              \n\n- C.          \n\n- D.             \n\n  \n\nB\n\n                                                 \\_\\_\\_\\_ \n\n- A.      \n\n- B.   \n\n- C.    \n\n- D.     \n\n  \n\nB\n\n                             \\_\\_\\_\\_\n\n- A.           \n\n- B.            ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 182,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-183",
      "content": "- A.      \n\n- B.   \n\n- C.    \n\n- D.     \n\n  \n\nB\n\n                             \\_\\_\\_\\_\n\n- A.           \n\n- B.            \n\n- C.          \n\n- D.            \n\n-   ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 183,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-184",
      "content": "- A\n\n- B\n\n- C\n\n- D\n\nTable 21 | An example of C-Eval.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 184,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-185",
      "content": "        ?\n\n           \n\n         \n\n    ?   ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 185,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-186",
      "content": "- - \n- - \n- - \n- - ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 186,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-187",
      "content": "                                                                                               \n\n           ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 187,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-188",
      "content": "- -      \n- -      \n- -     \n- -      \n\nTable 22 | An example of C3.\n\nTable 23 | An example of CCPM.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 188,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-189",
      "content": "Q:     '    -        '             8000        1500           200       1600             3  5         \n\nA:     1500           200         1500+200=1700               8000              =     -              8000-15001700-1600=3200                 3  5        3200/(3+5)*3=1200         1200 \n\nQ:                         800                    100              \n\nA:                                    -       =100       100+800=900         900 \n\nQ: A   B                5                       3      A       135    B       165              ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 189,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-190",
      "content": "0=900         900 \n\nQ: A   B                5                       3      A       135    B       165              \n\nA:   A      x       B      y          A  B    A     5    A    3         135    B    3          165             =5x+5y=135+8x=165+8y       10(x+y)=300+8(x+y)    x+y=150       5(x+y)=750          750 \n\nQ:        10             10                4     6                         \n\nA:\n\nTable 24 | An example of CMATH.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 190,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-191",
      "content": "                          \n\n           \n\nA.  \n\nB.  \n\nC.   \n\nD.   \n\n   \n\nB\n\n           \n\nA.   \n\nB.  \n\nC.   \n\nD.    \n\n   \n\nB\n\n           \n\nA.  \n\nB.   \n\nC.   \n\nD.   \n\n   \n\nD\n\n      \n\nA.   \n\nB.   \n\nC.   \n\nD.   \n\n   \n\nC\n\n         \n\nA.  \n\nB.  \n\nC.  \n\nD.  \n\n   \n\nB\n\n              \n\nA.   \n\nB.    \n\nC.    \n\nD.    \n\n   ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 191,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-192",
      "content": "- A\n\n- B\n\n- C\n\n- D\n\nTable 25 | An example of CMMLU.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 192,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-193",
      "content": "Passage: The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. The gender makeup of the city was 64.3% male and 35.7% female.\n\nAnswer the following questions based on the above passage, please calculate carefully if calculation is necessary.\n\nQ: How many percent were not from 25 to 44?\n\nA: The answer type is number. So according to above Passage, the answer is 83.9.\n\nQ: How many in percent weren't 25 to 44?\n\nA: The answer type is number. So according to above Passage, the answer is\n\nTable 26 | An example of DROP.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 193,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-194",
      "content": "   12  7       6    ,         ,                   (Joseph Varon)       260  ,        2              ,            ,        ' ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 194,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-195",
      "content": "- -    ' \n- -   ' \n- -    ' \n- -    ' \n- -    ' \n- -    ' \n- -   ' \n\nTable 27 | An example of CHID.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 195,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-196",
      "content": "                                                       \n\n       \"  \"   \n\n  \n\n                                \n\n       \"   \"     \n\n'       JulesTellier                                                                   '\n\n         \n\n     \n\n                                                                  \"  \"   \n\n    \n\n                                                                    \"   \"   \n\n \n\n                             1962         '\n\n       \"  \"   \n\nTable 28 | An example of CLUEWSC.\n\n\" \" ",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 196,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-197",
      "content": "Q: Max can mow the lawn in 40 minutes. If it takes him twice that long to fertilize the lawn, how long will it take him to both mow and fertilize the lawn?\n\nA: Let's think step by step. It takes Max 2 * 40 minutes = 80 minutes to fertilize the lawn. In total, Max takes 80 minutes + 40 minutes = 120 minutes to both mow and fertilize the lawn. The answer is 120.\n\nQ: The bagels cost $2.25 each, or a dozen for $24. How much is saved, per bagel, in cents, by buying a dozen at a time?\n\nA: Let's think step by step. They cost 2.25*100=225 cents each. At the bulk rate, they are 24/12=2 dollar each. They cost 2*100=200 cents each. 225-200=25 cents are saved per bagel. The answer is 25.\n\nQ: Tim is 5 years old. His cousin, Rommel, is thrice as old as he is. His other cousin, Jenny, is 2 years older than Rommel. How many years younger is Tim than Jenny? A: Let's think step by step. Rommel is 5 x 3 = 15 years old. Jenny is 15 + 2 = 17 years old. So, Tim is 17 - 5 = 12 years younger than Jenny.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 197,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-198",
      "content": "der than Rommel. How many years younger is Tim than Jenny? A: Let's think step by step. Rommel is 5 x 3 = 15 years old. Jenny is 15 + 2 = 17 years old. So, Tim is 17 - 5 = 12 years younger than Jenny. The answer is 12.\n\nQ: The school has 14 boys and 10 girls. If 4 boys and 3 girls drop out, how many boys and girls are left?\n\nA: Let's think step by step. There are 14 boys - 4 boys = 10 boys left. There are 10 girls - 3 girls = 7 girls left. In total there are 10 boys + 7 girls = 17 boys and girls left. The answer is 17.\n\nQ: Building one birdhouse requires 7 planks and 20 nails. If 1 nail costs 0.05, and one plank costs 3, what is the cost, in dollars, to build 4 birdhouses?\n\nA: Let's think step by step. The cost of the planks for one birdhouse is 7 * 3 = 21. And the nails are a cost of 20 * 0.05 = 1 for each birdhouse. So to build one birdhouse one will need 21 + 1 = 22. So the cost of building 4 birdhouses is at 4 * 22 = 88. The answer is 88.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 198,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-199",
      "content": "7 * 3 = 21. And the nails are a cost of 20 * 0.05 = 1 for each birdhouse. So to build one birdhouse one will need 21 + 1 = 22. So the cost of building 4 birdhouses is at 4 * 22 = 88. The answer is 88.\n\nQ: Danny brings 3 watermelons to his family picnic. He cuts each watermelon into 10 slices. His sister brings 1 watermelon to the family picnic, and she cuts the watermelon into 15 slices. How many watermelon slices are there in total at the picnic?\n\nA: Let's think step by step. From Danny, there are 3 * 10 = 30 watermelon slices. From his sister, there are 1 * 15 = 15 watermelon slices. There are a total of 30 + 15 = 45 watermelon slices. The answer is 45.\n\nQ: Angela is a bike messenger in New York. She needs to deliver 8 times as many packages as meals. If she needs to deliver 27 meals and packages combined, how many meals does she deliver?",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 199,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-200",
      "content": "swer is 45.\n\nQ: Angela is a bike messenger in New York. She needs to deliver 8 times as many packages as meals. If she needs to deliver 27 meals and packages combined, how many meals does she deliver?\n\nA: Let's think step by step. Let p be the number of packages Angela delivers and mbe the number of meals. We know that p + m = 27 and p = 8m. Substituting the second equation into the first equation, we get 8m + m = 27. Combining like terms, we get 9m = 27. Dividing both sides by 9, we get m = 3. The answer is 3.\n\nQ: Cori is 3 years old today. In 5 years, she will be one-third the age of her aunt. How old is her aunt today?\n\nA: Let's think step by step. In 5 years, Cori will be 3 + 5 = 8 years old. In 5 years, Cori's aunt will be 8 x 3 = 24 years old. Today, her aunt is 24 - 5 = 19 years old. The answer is 19.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 200,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-201",
      "content": "unt today?\n\nA: Let's think step by step. In 5 years, Cori will be 3 + 5 = 8 years old. In 5 years, Cori's aunt will be 8 x 3 = 24 years old. Today, her aunt is 24 - 5 = 19 years old. The answer is 19.\n\nQ: Indras has 6 letters in her name. Her sister's name has 4 more letters than half of the letters in Indras' name. How many letters are in Indras and her sister's names? A: Let's think step by step.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 201,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-202",
      "content": "Playing piano: A man is seated at a piano. He",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 202,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-203",
      "content": "- is playing the piano with his hands and his face.\n- bigins to play a song by timbaland on the piano.\n- plays slowly, and pauses to snap his fingers.\n- is playing a song in front of him.\n\nTable 30 | An example of HellaSwag.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 203,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-204",
      "content": "def starts\\_one\\_ends(n):\n\n\"\"\"\n\nGiven a positive integer n, return the count of the numbers of n-digit positive integers that start or end with 1. \"\"\"\n\nTable 31 | An example of HumanEval.\n\n```\nPROMPT Problem: Find the domain of the expression $\\frac{\\sqrt{x-2}}{\\sqrt{5-x}}$.} Solution: The expressions inside each square root must be non-negative. Therefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge 0$, so $x \\le 5$. Also, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$. Therefore, the domain of the expression is $\\boxed{[2,5)}$. Final Answer: The final answer is $[2,5)$. I hope it is correct. Problem: If $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ then find $\\det (\\mathbf{A} \\mathbf{B}).$ Solution: We have that $\\det (\\mathbf{A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\boxed{24}.$ Final Answer: The final answer is $24$. I hope it is correct. Problem: Terrell usually lifts two 20-pound weights 12 times.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 204,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-205",
      "content": "A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\boxed{24}.$ Final Answer: The final answer is $24$. I hope it is correct. Problem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight? Solution: If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight. If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight. Equating this to 480 pounds, we can solve for $n$: \\begin{align*} 30n&=480\\\\ \\Rightarrow\\qquad n&=480/30=\\boxed{16} \\end{align*} Final Answer: The final answer is $16$. I hope it is correct. Problem: If the system of equations \\begin{align*} 6x-4y&=a,\\\\ 6y-9x &=b. \\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 205,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-206",
      "content": "correct. Problem: If the system of equations \\begin{align*} 6x-4y&=a,\\\\ 6y-9x &=b. \\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero. Solution: If we multiply the first equation by $-\\frac{3}{2}$, we obtain $$6y-9x=-\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have $$-\\frac{3}{2}a=b\\Rightarrow\\frac{a}{b}=\\boxed{-\\frac{2}{3}}.$$ Final Answer: The final answer is $-\\frac{2}{3}$. I hope it is correct. Problem: Evaluate $\\log_21$. Solution:\n```",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 206,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-207",
      "content": "You are an expert Python programmer, and here is your task: Write a function\n\n```\nto find the similar elements from the given two tuple lists. Your code should pass these tests: assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5) assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4) assert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14) [BEGIN] def similar_elements(test_tup1, test_tup2): res = tuple(set(test_tup1) & set(test_tup2)) return (res) [DONE] You are an expert Python programmer, and here is your task: Write a python function to identify non-prime numbers. Your code should pass these tests: assert is_not_prime(2) == False assert is_not_prime(10) == True assert is_not_prime(35) == True [BEGIN] import math def is_not_prime(n): result = False for i in range(2,int(math.sqrt(n)) + 1): if n % i == 0: result = True return result [DONE] You are an expert Python programmer, and here is your task: Write a function to find the largest integers from a given",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 207,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-208",
      "content": "range(2,int(math.sqrt(n)) + 1): if n % i == 0: result = True return result [DONE] You are an expert Python programmer, and here is your task: Write a function to find the largest integers from a given list of numbers using heap queue algorithm. Your code should pass these tests: assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35] [BEGIN] import heapq as hq def heap_queue_largest(nums,n): largest_nums = hq.nlargest(n, nums) return largest_nums [DONE] You are an expert Python programmer, and here is your task: Write a function to return the sum of all divisors of a number. Your code should pass these tests: assert sum_div(8)==7 assert sum_div(12)==16 assert sum_div(7)==1 [BEGIN]\n```\n\nTable 33 | An example of MBPP.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 208,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-209",
      "content": "tests: assert sum_div(8)==7 assert sum_div(12)==16 assert sum_div(7)==1 [BEGIN]\n```\n\nTable 33 | An example of MBPP.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 209,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-210",
      "content": "The following are multiple choice questions (with answers) about miscellaneous.\n\nHow many axles does a standard automobile have?\n\nA. one\n\nB. two\n\nC. four\n\nD. eight\n\nAnswer: B\n\nWhat place is named in the title of the 1979 live album by rock legends Cheap\n\nTrick?\n\nA. Budapest\n\nB. Budokan\n\nC. Bhutan\n\nD. Britain\n\nAnswer: B\n\nWho is the shortest man to ever win an NBA slam dunk competition?\n\nA. Anthony 'Spud' Webb\n\nB. Michael 'Air' Jordan\n\nC. Tyrone 'Muggsy' Bogues\n\nD. Julius 'Dr J' Erving\n\nAnswer: A\n\nWhat is produced during photosynthesis?\n\nA. hydrogen\n\nB. nylon\n\nC. oxygen\n\nD. light\n\nAnswer: C\n\nWhich of these songs was a Top 10 hit for the rock band The Police?\n\nA. 'Radio Ga-Ga'\n\nB. 'Ob-la-di Ob-la-da'\n\nC. 'De Do Do Do De Da Da Da'\n\nD. 'In-a-Gadda-Da-Vida'\n\nAnswer: C\n\nWhich of the Three Stooges was not related to the others?\n\nA. Moe\n\nB. Larry\n\nC. Curly\n\nD. Shemp\n\nAnswer:",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 210,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-211",
      "content": "- A\n\n- B\n\n- C\n\n- D\n\nTable 34 | An example of MMLU.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 211,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-212",
      "content": "Answer these questions:\n\n- Q: Who is hosting the fifa world cup in 2022?\n- A: Qatar\n- Q: Who won the first women 's fifa world cup?\n- A: United States\n- Q: When did miami vice go off the air?\n- A: 1989\n- Q: Who wrote the song shout to the lord?\n- A: Darlene Zschech\n- Q: Who was thrown in the lion 's den?\n- A: Daniel\n- Q: What is the meaning of the name habib?\n- A:",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 212,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-213",
      "content": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 213,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-214",
      "content": "- flowers blooming\n- grass turning brown\n- trees growing\n- blossoms blooming\n\nTable 36 | An example of OpenBookQA.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 214,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-215",
      "content": "To make it easier to push the reset button of the garbage disposable machine which is located underneath the machine,",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 215,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-216",
      "content": "- place a wall mirror on the floor of the cabinet\n- hold a hand mirror under the garbage disposable machine\n\nTable 37 | An example of PIQA.\n\nTable 35 | An example of NaturalQuestions.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 216,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-217",
      "content": "Article:\n\nWhen you read an article you will understand and remember it better if you can work out how the writer has put the ideas together. Sometimes a writer puts ideas together by asking questions and then answering them. For example, if the article is about groundhogs, the set of questions in the writer's head might be:\n\nWhat does a groundhog look like?\n\nWhere do groundhogs live?\n\nWhat do they eat?...\n\nIn the article,the author might answer those questions.\n\nSometimes an author writes out her questions in the article. These questions give you signals. They tell you what the author is going to write next.Often an author has a question in her head but she doesn't write it out for you. You have to work out her question for yourself. Here's a sample reading for you to practice this method.\n\nEarthworms",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 217,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-218",
      "content": "xt.Often an author has a question in her head but she doesn't write it out for you. You have to work out her question for yourself. Here's a sample reading for you to practice this method.\n\nEarthworms\n\nDo you know how many kinds of earthworms there are? There are about 1800 kinds in the world! They can be brown, purple, green. They can be as small as 3 cm long and as large as 3 m long.\n\nThe best time to see earthworms is at night, especially a cool, damp night. That's when they come up from their burrows to hunt for food. Earthworms don't like to be in the sun. That's because they breathe through their skin,and they can't breathe if their skin gets too dry. Earthworms must come out of the earth if it rains a lot, because they can't breathe in their flooded burrows. What a dangerous life!\n\nEarthworms don't have eyes, so how can they tell when it's dark? They have special places on their skin that are sensitive to light. These spots tell whether it's light or dark.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 218,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-219",
      "content": "hat a dangerous life!\n\nEarthworms don't have eyes, so how can they tell when it's dark? They have special places on their skin that are sensitive to light. These spots tell whether it's light or dark. If you shine a flashlight on an earthworm at night,it will quickly disappear into the ground.\n\nEarthworms don't have ears either, but they can hear by feeling movements in the earth.If you want to hear like an earthworm, lie on the ground with your fingers in your ears. Then have a friend stamp his or her feet near you. This is how earthworms feel birds and people walking, and moles digging, near them.\n\nEarthworms are useful. Farmers and gardeners like having lots of earthworms in their land because the worms help to make better soil when they dig. That digging keeps the soil loose and airy. In one year earthworms can pile up as much as 23,000 kg of castings in an area about the size of a football field.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 219,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-220",
      "content": "s help to make better soil when they dig. That digging keeps the soil loose and airy. In one year earthworms can pile up as much as 23,000 kg of castings in an area about the size of a football field.\n\n- Q: What's the purpose of reading Earthworms?\n- A: To put the writer's idea into real use.\n- Q: Which question CANNOT be answered in the passage?\n- A: Why can human listen like earthworms?\n- Q: How can you understand Earthworms better according to this passage?\n- A: Read to work out all the questions in the writer's head while reading.\n- Q: What's the best title for the passage?\n\nA:",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 220,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-221",
      "content": "- One way to help with understanding\n- One way to practice with a new idea\n- One way to learn to be a wise writer\n- One way to be clearer about worms",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "OPTIONS",
        "chunkIndex": 221,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-222",
      "content": "Answer these questions:\n\nQ: A Jayhawker was a term applied to anti-slavery militant bands from a certain US state that clashed with pro-slavery factions from Missouri. Which state is this, sometimes referred to as the Jayhawk State?\n\nA: Kans.\n\nQ: Which Swedish DJ and record producer had a UK Number One single in 2013 with 'Wake Me Up'?\n\nA: Tim Bergling\n\nQ: Who is the MP for Sheffield Hallam?\n\nA: Nick clegg\n\nQ: A case that riveted the nation, the case of The State of Tennessee v. John Thomas Scopes concluded on July 21, 1925, with the jury finding Mr. Scopes guilty of teaching what?\n\nA: Survival of species\n\nQ: What cartoon series featured a character called Little My?\n\nA: Muumi\n\nQ: \"What English model, with her short-haired androgynous look, born Lesley Hornby, was discovered in 1966 by Nigel Davies when she was 16 and weighed 6 stone (41 kg, 91 lbs), and became \"\"The Face of '66\"\" with her high fashion mod look created by Mary Quant?\"\n\nA:",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PROMPT",
        "chunkIndex": 222,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-223",
      "content": "- So Monica\n\n- So Jessica",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "PREFIXES",
        "chunkIndex": 223,
        "totalChunks": 225
      }
    },
    {
      "id": "2401.02954v1-chunk-224",
      "content": "avoids eating carrots for their eye health because Emily needs good eyesight while Monica doesn't.\n\nTable 40 | An example of WinoGrande. Note that there are multiple prefixes and only one completion for WinoGrande, and we choose the predicted prefix with the lowest perplexity of the completion.\n\nTable 39 | An example of TriviaQA.",
      "metadata": {
        "source": "arxiv:2401.02954v1",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
          "DeepSeek-AI",
          ":",
          "Xiao Bi",
          "Deli Chen",
          "Guanting Chen",
          "Shanhuang Chen",
          "Damai Dai",
          "Chengqi Deng",
          "Honghui Ding",
          "Kai Dong",
          "Qiushi Du",
          "Zhe Fu",
          "Huazuo Gao",
          "Kaige Gao",
          "Wenjun Gao",
          "Ruiqi Ge",
          "Kang Guan",
          "Daya Guo",
          "Jianzhong Guo",
          "Guangbo Hao",
          "Zhewen Hao",
          "Ying He",
          "Wenjie Hu",
          "Panpan Huang",
          "Erhang Li",
          "Guowei Li",
          "Jiashi Li",
          "Yao Li",
          "Y. K. Li",
          "Wenfeng Liang",
          "Fangyun Lin",
          "A. X. Liu",
          "Bo Liu",
          "Wen Liu",
          "Xiaodong Liu",
          "Xin Liu",
          "Yiyuan Liu",
          "Haoyu Lu",
          "Shanghao Lu",
          "Fuli Luo",
          "Shirong Ma",
          "Xiaotao Nie",
          "Tian Pei",
          "Yishi Piao",
          "Junjie Qiu",
          "Hui Qu",
          "Tongzheng Ren",
          "Zehui Ren",
          "Chong Ruan",
          "Zhangli Sha",
          "Zhihong Shao",
          "Junxiao Song",
          "Xuecheng Su",
          "Jingxiang Sun",
          "Yaofeng Sun",
          "Minghui Tang",
          "Bingxuan Wang",
          "Peiyi Wang",
          "Shiyu Wang",
          "Yaohui Wang",
          "Yongji Wang",
          "Tong Wu",
          "Y. Wu",
          "Xin Xie",
          "Zhenda Xie",
          "Ziwei Xie",
          "Yiliang Xiong",
          "Hanwei Xu",
          "R. X. Xu",
          "Yanhong Xu",
          "Dejian Yang",
          "Yuxiang You",
          "Shuiping Yu",
          "Xingkai Yu",
          "B. Zhang",
          "Haowei Zhang",
          "Lecong Zhang",
          "Liyue Zhang",
          "Mingchuan Zhang",
          "Minghua Zhang",
          "Wentao Zhang",
          "Yichao Zhang",
          "Chenggang Zhao",
          "Yao Zhao",
          "Shangyan Zhou",
          "Shunfeng Zhou",
          "Qihao Zhu",
          "Yuheng Zou"
        ],
        "section": "COMPLETION",
        "chunkIndex": 224,
        "totalChunks": 225
      }
    }
  ],
  "fullText": "<!-- image -->\n\n## DeepSeek LLM\n\n## Scaling Open-Source Language Models with Longtermism\n\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou *\n\n* DeepSeek-AI\n\n## Abstract\n\nThe rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling laws described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate the scaling of large scale models in two prevalent used opensource configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and direct preference optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B across a range of benchmarks, especially in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that our DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.\n\n*Authors are ordered alphabetically by the last name.\n\n## Contents\n\n| 1 Introduction   | 1 Introduction                          | 1 Introduction                                                                                                                                                                           | 1 Introduction                                                                                                                                                                           | 3     |\n|------------------|-----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|\n| 2                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          |       |\n|                  | Pre-Training                            | Pre-Training                                                                                                                                                                             | Pre-Training                                                                                                                                                                             | 4     |\n|                  | 2.1                                     | Data . . . . . . . .                                                                                                                                                                     | . . . . . . . . . . . . . . . . . .                                                                                                                                                      | 4     |\n|                  | 2.2                                     | Architecture                                                                                                                                                                             | . . . . . . . . . . . . . . . . . . . . .                                                                                                                                                | 5     |\n|                  | 2.3                                     | Hyperparameters . . .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 5     |\n|                  | 2.4                                     | Infrastructures                                                                                                                                                                          | . . . . . . . . . . . . . . . . . . . .                                                                                                                                                  | 6     |\n| 3                | Scaling Laws 7                          | Scaling Laws 7                                                                                                                                                                           | Scaling Laws 7                                                                                                                                                                           |       |\n|                  | 3.1                                     | Scaling Laws for Hyperparameters                                                                                                                                                         | . . . . . . . .                                                                                                                                                                          | 8     |\n|                  | 3.2                                     | Estimating Optimal Model and Data Scaling                                                                                                                                                | . .                                                                                                                                                                                      | 9     |\n|                  | 3.3                                     | Scaling Laws with Different Data                                                                                                                                                         | . . . . . . . . .                                                                                                                                                                        | 12    |\n| 4                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 12    |\n|                  | Alignment                               | Alignment                                                                                                                                                                                | Alignment                                                                                                                                                                                |       |\n| 5                | 5.1                                     | Public Benchmark                                                                                                                                                                         | Evaluation . . . . . . . . . . .                                                                                                                                                         | 13    |\n|                  |                                         | 5.1.1                                                                                                                                                                                    | Base Model . . . . . . . . . . . . . . . . .                                                                                                                                             | 14    |\n|                  |                                         | 5.1.2 . .                                                                                                                                                                                | Chat Model . . . . . . . . . . . . . . .                                                                                                                                                 | 14    |\n|                  | 5.2                                     | Open-Ended                                                                                                                                                                               | Evaluation . . . . . . . . . . . . . .                                                                                                                                                   | 17    |\n|                  |                                         | 5.2.1 Chinese Open-Ended Evaluation                                                                                                                                                      | . . . . .                                                                                                                                                                                | 17    |\n|                  |                                         | 5.2.2 English                                                                                                                                                                            | Open-Ended Evaluation . . . . .                                                                                                                                                          |       |\n|                  | 5.3                                     | Held-Out Evaluation .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 18 18 |\n|                  |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 19    |\n|                  | 5.4 5.5                                 | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 20    |\n| 6                | Conclusion, Limitation, and Future Work | Conclusion, Limitation, and Future Work                                                                                                                                                  | Conclusion, Limitation, and Future Work                                                                                                                                                  | 23    |\n| A                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 30    |\n|                  | Appendix                                | Appendix                                                                                                                                                                                 | Appendix                                                                                                                                                                                 |       |\n|                  | A.1                                     | Acknowledgments . .                                                                                                                                                                      | . . . . . . . . . . . . . . .                                                                                                                                                            | 30    |\n|                  | A.2                                     | Different Model Scale Representations                                                                                                                                                    | . . . . . .                                                                                                                                                                              | 30    |\n|                  | A.3                                     | Benchmark Metrics Curves                                                                                                                                                                 | . . . . . . . . . . . .                                                                                                                                                                  | 31    |\n|                  | A.4                                     | Comparison with Code or Math Specific Models                                                                                                                                             |                                                                                                                                                                                          | 32    |\n|                  | A.5                                     | Benchmark Results w/ DPO                                                                                                                                                                 | Stage . . . . . . . .                                                                                                                                                                    | 32    |\n|                  | A.6                                     | Evaluation Formats . .                                                                                                                                                                   | . . . . . . . . . . . . . . .                                                                                                                                                            | 32    |\n\n## 1. Introduction\n\nOver the past few years, Large Language Models (LLMs) based on decoder-only Transformers (Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Artificial General Intelligence (AGI). By predicting the next word in continuous text, LLMs undergo self-supervised pre-training on massive datasets, enabling them to achieve various purposes and possess many abilities, such as novel creation, text summarization, code completion, and more. Subsequent developments like supervised fine-tuning and reward modeling have enabled Large Language Models (LLMs) to better follow user intentions and instructions. This has endowed them with more versatile conversational capabilities and rapidly expanded their influence.\n\nThis wave is sparked with closed products, such as ChatGPT (OpenAI, 2022), Claude (Anthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational resources and substantial annotation costs. These products have significantly raised the community's expectations for the capabilities of open-source LLMs, consequently inspiring a series of work (Bai et al., 2023; Du et al., 2022; Jiang et al., 2023; Touvron et al., 2023a,b; Yang et al., 2023). Among these, the LLaMA series models (Touvron et al., 2023a,b) stand out. It consolidates a range of works to create an efficient and stable architecture, building well-performing models ranging from 7B to 70B parameters. Consequently, the LLaMA series has become the de facto benchmark for architecture and performance among open-source models.\n\nFollowing LLaMA, the open-source community has primarily focused on training fixed-size (7B, 13B, 34B, and 70B), high-quality models, often neglecting research exploration into LLM scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020). Nonetheless, research on scaling laws is of utmost importance, considering that the current open-source models are merely at the initial stage of Artificial General Intelligence (AGI) development. In addition, early works (Hoffmann et al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data with increased compute budgets and inadequately addressed hyperparameter discussions. In this paper, we extensively investigate the scaling behavior of language models and apply our findings in two widely used large-scale model configurations, namely 7B and 67B. Our study aims to lay the groundwork for future scaling of open-source LLMs, paving the way for further advancements in this domain. Specifically, we first examined the scaling laws of batch size and learning rate, and found their trends with model size. Building on this, we conducted a comprehensive study of the scaling laws of the data and model scale, successfully revealing the optimal model/data scaling-up allocation strategy and predicting the expected performance of our large-scale models. Additionally, during development, we discovered that the scaling laws derived from different datasets show significant differences. This suggests that choice of dataset remarkably affects the scaling behavior, indicating that caution should be exercised when generalizing scaling laws across datasets.\n\nUnder the guidance of our scaling laws, we build from scratch open-source large language models, and release as much information as possible for community reference. We collect 2 trillion tokens for pre-training, primarily in Chinese and English. At the model level, we generally followed the architecture of LLaMA, but replaced the cosine learning rate scheduler with a multi-step learning rate scheduler, maintaining performance while facilitating continual training. We collected over 1 million instances for supervised fine-tuning (SFT) (Ouyang et al., 2022) from diverse sources. This paper shares our experiences with different SFT strategies and findings in data ablation techniques. Additionally, we have utilized direct preference optimization (DPO) (Rafailov et al., 2023) to improve the conversational performance of the model.\n\nWe conduct extensive evaluations using our base and chat models. The evaluation results demonstrate that DeepSeek LLM surpasses LLaMA-2 70B across various benchmarks, particularly in the fields of code, mathematics, and reasoning. Following SFT and DPO, the DeepSeek 67B chat model outperforms GPT-3.5 in both Chinese and English open-ended evaluations. This highlights the superior performance of DeepSeek 67B in generating high-quality responses and engaging in meaningful conversations in both languages. Furthermore, the safety evaluation indicates that DeepSeek 67B Chat can provide harmless responses in practice.\n\nIn the rest of this paper, we first introduce our pre-training basic concepts of DeepSeek LLM in Section 2, including the composition of data, model architecture, infrastructure, and hyperparameters. In Section 3, we provide a detailed explanation of the scaling laws we have discovered and its implications. Additionally, we discuss the rationale behind our selection of pre-training hyperparameters, taking into account the insights gained from the scaling laws analysis. In Section 4, we discuss our fine-tuning methodology, encompassing the composition of fine-tuning data and specific methods during the SFT and DPO stages. We then present the detailed evaluation results of DeepSeek LLM in Section 5, covering both the base and chat models, as well as their performance in open-ended evaluations and safety evaluations. Finally, we discuss the current limitations and future directions of DeepSeek LLM in Section 6.\n\n## 2. Pre-Training\n\n## 2.1. Data\n\nOur main objective is to comprehensively enhance the richness and diversity of the dataset. We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a). To achieve these goals, we have organized our approach into three essential stages: deduplication, filtering, and remixing. The deduplication and remixing stages ensure a diverse representation of the data by sampling unique instances. The filtering stage enhances the density of information, thereby enabling more efficient and effective model training.\n\nWe adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump. Table 1 illustrates that deduplicating across 91 dumps eliminates four times more documents than a single dump method.\n\nTable 1 | Deduplication ratios for various Common Crawl dumps.\n\n| Dumps Used             |    1 |    2 |    6 |   12 |   16 |   22 |   41 |   91 |\n|------------------------|------|------|------|------|------|------|------|------|\n| Deduplication Rate (%) | 22.2 | 46.7 | 55.7 | 69.9 | 75.7 | 76.3 | 81.6 | 89.8 |\n\nIn the filtering stage, we focus on developing robust criteria for document quality assessment. This involves a detailed analysis incorporating both linguistic and semantic evaluations, providing a view of data quality from individual and global perspectives. In the remixing phase, we adjust our approach to address data imbalances, focusing on increasing the presence of underrepresented domains. This adjustment aims to achieve a more balanced and inclusive dataset, ensuring that diverse perspectives and information are adequately represented.\n\nFor our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm based on the tokenizers library (Huggingface Team, 2019). Pre-tokenization was employed to\n\nprevent the merging of tokens from different character categories such as new lines, punctuation, and Chinese-Japanese-Korean (CJK) symbols, similar to GPT-2 (Radford et al., 2019). We also chose to split numbers into individual digits following the approach used in (Touvron et al., 2023a,b). Based on our prior experience, we set the number of conventional tokens in the vocabulary at 100000. The tokenizer was trained on a multilingual corpus of approximately 24 GB, and we augmented the final vocabulary with 15 special tokens, bringing the total size to 100015. To ensure computational efficiency during training and to reserve space for any additional special tokens that might be needed in the future, we configured the model's vocabulary size to 102400 for training.\n\n## 2.2. Architecture\n\nTable 2 | Detailed specs of DeepSeek LLM family of models. We choose the hyper-parameters based on our findings in Section 3\n\n| Params   |    layers |    model |    heads |    kv_heads |   Context Length |   Sequence Batch Size |   Learning Rate | Tokens   |\n|----------|------------|-----------|-----------|--------------|------------------|-----------------------|-----------------|----------|\n| 7B       |         30 |      4096 |        32 |           32 |             4096 |                  2304 |         0.00042 | 2.0T     |\n| 67B      |         95 |      8192 |        64 |            8 |             4096 |                  4608 |         0.00032 | 2.0T     |\n\nThe micro design of DeepSeek LLM largely follows the design of LLaMA (Touvron et al., 2023a,b), adopting a Pre-Norm structure with RMSNorm (Zhang and Sennrich, 2019) function and using SwiGLU (Shazeer, 2020) as the activation function for the Feed-Forward Network (FFN), with an intermediate layer dimension of 8 3  . It also incorporates Rotary Embedding (Su et al., 2024) for positional encoding. To optimize inference cost, the 67B model uses GroupedQuery Attention (GQA) (Ainslie et al., 2023) instead of the traditional Multi-Head Attention (MHA).\n\nHowever, in terms of macro design, DeepSeek LLM differs slightly. Specifically, DeepSeek LLM 7B is a 30-layer network, while DeepSeek LLM 67B has 95 layers. These layer adjustments, while maintaining parameter consistency with other open-source models, also facilitate model pipeline partitioning to optimize training and inference.\n\nUnlike most works using Grouped-Query Attention (GQA), we expanded the 67B model's parameters in network depth rather than the common practice of widening the intermediate width of FFN layers, aiming for better performance. Detailed network specifications can be found in Table 2.\n\n## 2.3. Hyperparameters\n\nDeepSeek LLM is initialized with a standard deviation of 0.006 and trained using the AdamW optimizer (Loshchilov and Hutter, 2017), with the following hyperparameters:  1 = 0.9,  2 = 0.95, and weight\\_decay = 0.1.\n\nAmulti-step learning rate scheduler is employed during pre-training instead of the typical cosine scheduler. Specifically, the learning rate of the model reaches its maximum value after 2000 warmup steps, and then decreases to 31.6% of the maximum value after processing 80% of the training tokens. It further reduces to 10% of the maximum value after 90% of the tokens. The gradient clipping during the training phase is set to 1.0.\n\nBased on our empirical findings, we observed that despite differences in the loss reduction\n\nFigure 1 | Training loss curves with different learning rate schedulers or different parameters for schedulers. The model size is 1.6 billion parameters, trained on a dataset of 100 billion tokens.\n\n<!-- image -->\n\ntrend during training, the final performance using a multi-step learning rate scheduler is essentially consistent with that of a cosine scheduler, as shown in Figure 1(a). When adjusting the training scale while keeping the model size fixed, the multi-step learning rate scheduler allows for the reuse of training from the first phase, offering a unique convenience for continual training. Therefore, we chose the multi-step learning rate scheduler as our default setting. We also demonstrate in Figure 1(b) that adjusting the proportions of different stages in the multi-step learning rate scheduler can yield slightly better performance. However, for the sake of balancing reuse ratios in continual training and model performance, we opted for the aforementioned distribution of 80%, 10%, and 10% for the three stages respectively.\n\nThe batch size and learning rate vary with the model size. Specific parameters for the pre-training phases of the 7B and 67B models can be found in Table 2.\n\n## 2.4. Infrastructures\n\nWe use an efficient and light-weight training framework named HAI-LLM (High-flyer, 2023) to train and evaluate large language models. Data parallelism, tensor parallelism, sequence parallelism, and 1F1B pipeline parallelism are integrated into this framework as done in Megatron (Korthikanti et al., 2023; Narayanan et al., 2021; Shoeybi et al., 2019). We also leverage the flash attention (Dao, 2023; Dao et al., 2022) technique to improve hardware utilization. ZeRO-1 (Rajbhandari et al., 2020) is exploited to partition optimizer states over data parallel ranks. Efforts are also made to overlap computation and communication to minimize additional waiting overhead, including the backward procedure of the last micro-batch and reduce-scatter operation in ZeRO-1, and GEMM computation and all-gather/reduce-scatter in sequence parallel. Some layers/operators are fused to speed up training, including LayerNorm, GEMM whenever possible, and Adam updates. To improve model training stability, we train the model in bf16 precision but accumulate gradients in fp32 precision. In-place cross-entropy is performed to reduce GPU memory consumption, i.e.: we convert bf16 logits to fp32 precision on the fly in the cross-entropy CUDA kernel (instead of converting it beforehand in HBM), calculate the corresponding bf16 gradient, and overwrite logits with its gradient.\n\nModel weights and optimizer states are saved every 5 minutes asynchronously, which means we will lose no more than 5 minutes of training in the worst case of occasional hardware or network failures. These temporary model checkpoints are cleared up regularly to avoid\n\nconsuming too much storage space. We also support resuming training from a different 3D parallel configuration to cope with dynamic changes in computing cluster load.\n\nAs for evaluation, we employ vLLM (Kwon et al., 2023) in generative tasks, and continuous batching in non-generative tasks to avoid manual batch size tuning and reduce token padding.\n\n## 3. Scaling Laws\n\nResearch on scaling laws (Hestness et al., 2017) predates the emergence of large language models. Scaling laws (Henighan et al., 2020; Hoffmann et al., 2022; Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget  , model scale  , and data scale  . When model scale  is represented by model parameters and data scale  by the number of tokens,  can be approximated as  = 6  . Therefore, how to optimize the allocation between model and data scales when increasing the compute budget is also a crucial research objective in scaling laws.\n\nThe development of LLMs (Dai et al., 2019; Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak. Results in scaling laws demonstrate that expanding the compute budget continues to yield significant benefits, which further encourages the increase in model scales (Brown et al., 2020; Smith et al., 2022).\n\nHowever, as shown in Table 4, early works (Hoffmann et al., 2022; Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance. Therefore, we revisit scaling laws in this section to address these uncertainties and ensure we are on the right path to efficiently scaleup compute, which reflects the long-term perspective and is key to developing continuously improving models.\n\nTo ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters. Empirically, it has been observed that the optimal values of most parameters during training do not change when varying compute budgets. Therefore, these parameters are consistent with those outlined in Section 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.\n\nEarly works (Goyal et al., 2017; McCandlish et al., 2018; Shallue et al., 2019; Smith et al., 2017; Zhang et al., 2019) provided some empirical observations for setting batch size and learning rate, but we found these observations have limited applicability in our preliminary experiments. Through extensive experiments, we modeled the power law relationship between the compute budget  and the optimal batch size and learning rate. This relationship, which we refer to as the scaling laws of hyperparameters, provides an empirical framework for determining the optimal hyperparameters. This methodology ensures that models across different compute budgets can reach their near-optimal performance.\n\nWe then study the scaling laws of the model and data scales. To reduce experimental costs and fitting difficulties, we adopted the IsoFLOP profile approach from Chinchilla (Hoffmann et al., 2022) to fit the scaling curve. To represent the model scale more accurately, we utilized a new model scale representation, non-embedding FLOPs/token  , replacing the earlier-used model parameters  , and substituted the approximate compute budget formula  = 6 \n\nwith the more precise  =  . The experimental results provided insights into the optimal model/data scaling-up allocation strategy and performance predictions, and also accurately forecasted the expected performance of DeepSeek LLM 7B and 67B models.\n\nAdditionally, in the process of exploring scaling laws, the data we used underwent multiple iterations, continually improving in quality. We attempted to fit the scaling curve on various datasets and found that the data quality significantly influences the optimal model/data scalingup allocation strategy. The higher the data quality, the more the increased compute budget should be allocated to model scaling. This implies that high-quality data can drive the training of larger models given the same data scale. The differences in the optimal model/data scaling-up allocation strategy may also serve as an indirect approach to assess the quality of data. We will continue to pay close attention to the changes in data quality and its impact on scaling laws, and provide more analysis in future works.\n\nIn summary, our contributions and findings in scaling laws can be summarized as follows:\n\n- We established the scaling laws for hyperparameters, providing an empirical framework for determining the optimal hyperparameters.\n- Instead of model parameters  , we adopt non-embedding FLOPs/token  to represent the model scale, leading to a more accurate optimal model/data scaling-up allocation strategy and a better prediction of generalization loss for large-scale models.\n- The quality of pre-training data impacts the optimal model/data scaling-up allocation strategy. The higher the data quality, the more the increased compute budget should be allocated to model scaling.\n\n## 3.1. Scaling Laws for Hyperparameters\n\nWe initially conducted a grid search for batch size and learning rate on small-scale experiments with a compute budget of 1e17, and the results of a specific model size (177M FLOPs/token) are illustrated in Figure 2(a). The results demonstrate that the generalization error remains stable across a wide range of choices of batch sizes and learning rates. This indicates that near-optimal performance can be achieved within a relatively wide parameter space.\n\nFigure 2 | Training loss w.r.t. batch size and learning rate with 1e17 and 1e20 FLOPs.\n\n<!-- image -->\n\nThen, we utilized the aforementioned multi-step learning rate scheduler to effectively train multiple models with different batch sizes, learning rates, and compute budgets ranging from\n\n1e17 to 2e19 by reusing the first stage. Considering the redundancy in the parameter space, we regarded the parameters used by models whose generalization error exceeded the minimum by no more than 0.25% as near-optimal hyperparameters. We then fitted the batch size  and learning rate  with respect to the compute budget  . The fitting results, as shown in Figure 3, reveal that the optimal batch size  gradually increases with the increase in compute budget  , while the optimal learning rate  gradually decreases. This is in line with the intuitive empirical settings for batch size and learning rate when scaling up models. Moreover, all near-optimal hyperparameters fall within a broad band range, indicating that it is relatively easy to choose near-optimal parameters within this interval. The final formulae we fitted for batch size and learning rate are as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- image -->\n\n(a) Batch size scaling curve\n\n(b) Learning rate scaling curve\n\nFigure 3 | Scaling curves of batch size and learning rate. The grey circles represent models whose generalization error exceeded the minimum by no more than 0.25%. The dotted line represents the power law fitting the smaller model. The blue stars represent DeepSeek LLM 7B and 67B.\n\nWe validated our formulae on a series of models with a 1e20 compute budget, and the results of a specific model size (2.94B FLOPs per token) are shown in Figure 2(b). The results indicate that the fitted parameters are centered in the optimal parameter space. Subsequent sections also show that the parameters we fitted for DeepSeek LLM 7B and 67B models similarly achieved good performance.\n\nHowever, it's important to note that we have not yet considered the impact of factors beyond the compute budget  on the optimal hyperparameters. This is inconsistent with some earlier works (Kaplan et al., 2020; McCandlish et al., 2018) which suggested that the optimal batch size can be modeled as being solely related to the generalization error  . Furthermore, we observed that in models with the same compute budget but different model/data allocations, the optimal parameter space varies slightly. This suggests that further research is needed to understand the selection of hyperparameters and training dynamics. We will explore these aspects in future works.\n\n## 3.2. Estimating Optimal Model and Data Scaling\n\nAfter deriving the formulae for fitting near-optimal hyperparameters, we started fitting the scaling curve and analyzing the optimal model/data scaling-up allocation strategy. This strategy involves finding model scaling exponent  and data scaling exponent  that satisfy  opt   \n\nand  opt    , respectively. The data scale  can be consistently represented by the number of tokens in the dataset. In previous works, the model scale was typically represented by model parameters, with non-embedding parameters  1 (Kaplan et al., 2020) and complete parameters  2 (Hoffmann et al., 2022). The relationship between compute budget  and model/data scale could be approximately described as  = 6  , meaning we could use 6  1 or 6  2 to approximate the model scale. However, since both 6  1 and 6  2 do not account for the computational overhead of attention operation, and 6  2 also includes the vocabulary computation, which contributes less to the model's capacity, they both have significant approximation errors under certain settings.\n\nTo mitigate these errors, we introduced a new model scale representation: non-embedding FLOPs/token  .  includes the computational overhead of attention operation but does not take into account the vocabulary computation. With the model scale represented by  , the compute budget  can be simply expressed as  =  . The specific differences between 6  1, 6  2, and  are as shown in the following formulae:\n\n<!-- formula-not-decoded -->\n\nwhere  layer represents the number of layers,  model represents the model width,  vocab is the vocabulary size, and  seq is the sequence length. We assessed the differences between these three representations across models of varying scales, as shown in Table 3. The results indicate that both 6  1 and 6  2 either overestimate or underestimate the computational cost in models of different scales. This discrepancy is particularly pronounced in small-scale models, with differences reaching up to 50%. Such inaccuracies can introduce substantial statistical errors when fitting the scaling curve. Please refer to Appendix A.2 for further analysis regarding different representations of model scale.\n\nTable 3 | Difference in model scale representations and disparities of non-embedding parameters  1 and complete parameters  2 relative to non-embedding FLOPs/token  .\n\n|    layers |    model |    vocab |  seq   |  1   |  2   |      |   6  1  |   6  2  |\n|------------|-----------|-----------|---------|-------|-------|-------|-----------|-----------|\n|          8 |       512 |    102400 | 4096    | 25.2M | 77.6M | 352M  |      0.43 |      1.32 |\n|         12 |       768 |    102400 | 4096    | 84.9M | 164M  | 963M  |      0.53 |      1.02 |\n|         24 |      1024 |    102400 | 4096    | 302M  | 407M  | 3.02B |      0.6  |      0.81 |\n|         24 |      2048 |    102400 |         | 1.21B | 1.42B | 9.66B |      0.75 |      0.88 |\n|         32 |      4096 |    102400 |         | 6.44B | 6.86B | 45.1B |      0.85 |      0.91 |\n|         40 |      5120 |    102400 |         | 12.6B | 13.1B | 85.6B |      0.88 |      0.92 |\n|         80 |      8192 |    102400 |         | 64.4B | 65.3B | 419B  |      0.92 |      0.94 |\n\nAfter adopting  to represent the model scale, our objective could be described more clearly as: Given a computing budget  =  , find the optimal model scale  opt and data scale  opt that minimize the generalization error of the model. This target could be formalized as:\n\n<!-- formula-not-decoded -->\n\nTo reduce experimental costs and fitting difficulties, the IsoFLOP profile approach from Chinchilla (Hoffmann et al., 2022) was used to fit the scaling curve. We selected 8 different\n\nFigure 4 | IsoFLOP curve and optimal model/data allocation. The metric in IsoFLOP curve is bits-per-byte on the validation set. The dotted lines in optimal model/data scaling curves represent the power law fitting the smaller model (grey circles).\n\n<!-- image -->\n\ncompute budgets ranging from 1e17 to 3e20, and designed around 10 different model/data scale allocations for each budget. The hyperparameters for each budget were determined by Formula(1), and the generalization error was calculated on an independent validation set, distributed similarly to the training set and containing 100M tokens.\n\nFigure 4 demonstrates the IsoFLOP curve and model/data scaling curves, which are fitted by using the optimal model/data allocation for each compute budget. The specific formulae for the optimal non-embedding FLOPs/token  opt and optimal tokens  opt are as follows:\n\n<!-- formula-not-decoded -->\n\nAdditionally, we fitted the loss scaling curve according to compute budget  and optimal generalization error, and predicted the generalization error for DeepSeek LLM 7B and 67B, as shown in Figure 5. The results indicate that using small-scale experiments can accurately predict\n\nFigure 5 | Performance scaling curve. The metric is the bits-per-byte on the validation set. The dotted line represents the power law fitting the smaller model (grey circles). The blue stars represent DeepSeek LLM 7B and 67B. Their performance is well-predicted by the scaling curve.\n\n<!-- image -->\n\nthe performance of models with 1000  compute budget. This provides both confidence and guidance for training models on a larger scale.\n\n## 3.3. Scaling Laws with Different Data\n\nIn the development process of DeepSeek LLM, the dataset was iteratively refined multiple times, with adjustments in the proportions of different data sources while enhancing the overall quality. This allowed us to further analyze the impact of different datasets on scaling laws.\n\nWe studied the scaling laws using three different datasets: early in-house data, current inhouse data, and OpenWebText2, which was utilized in the previous study of scaling laws (Kaplan et al., 2020). Our internal data assessment revealed that current in-house data has higher data quality than early in-house data. Furthermore, the quality of OpenWebText2 even surpasses the current in-house data, due to its smaller scale which allows for more meticulous processing.\n\nTable 4 | Coefficients of model scaling and data scaling vary with training data distribution.\n\n| Approach                 |   Coeff.  where  opt (  opt )    |   Coeff.  where  opt    |\n|--------------------------|----------------------------------------|------------------------------|\n| OpenAI (OpenWebText2)    |                                  0.73  |                        0.27  |\n| Chinchilla (MassiveText) |                                  0.49  |                        0.51  |\n| Ours (Early Data)        |                                  0.45  |                        0.55  |\n| Ours (Current Data)      |                                  0.524 |                        0.476 |\n| Ours (OpenWebText2)      |                                  0.578 |                        0.422 |\n\nAn interesting observation from the analysis is that the optimal model/data scaling-up allocation strategy across these three datasets showed consistency with data quality. As illustrated in Table 4, as data quality improves, the model scaling exponent  gradually increases, while the data scaling exponent  decreases, which suggests that the increased compute budget should be allocated more to the model instead of the data. This finding might also explain the significant differences in optimal model/data scaling-up allocation observed in earlier studies of scaling laws.\n\nAn intuitive speculation for this finding is that high-quality data usually implies logical clarity and less predictive difficulty after sufficient training. Therefore, it's more advantageous to scale up the model size when increasing compute budget. We will continue to pay close attention to the changes in data quality and its impact on scaling laws, and provide more analysis in future works.\n\n## 4. Alignment\n\nWe collect around 1.5 million instruction data instances in English and Chinese, covering a wide range of helpfulness and harmlessness topics. Our helpful data contains 1.2 million instances, with a distribution of 31.2% for general language tasks, 46.6% for mathematical problems, and 22.2% for coding exercises. The safety data consists of 300K instances, covering various sensitive topics.\n\nOur alignment pipeline contains two stages.\n\nSupervised Fine-Tuning: We fine-tuned our 7B model with 4 epochs, but only 2 epochs for the 67B model, since we observed the overfitting problem is serious on the 67B model. We\n\nobserved that GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021) are improved consistently for the 7B model, while the 67B model hits the upper bound soon. The learning rate is 1e-5 and 5e-6 for 7B and 67B models, respectively. In addition to monitoring the benchmark accuracy, we also assess the repetition ratio of a chat model during the fine-tuning process. We gathered a total of 3868 Chinese and English prompts and determined the proportion of generated responses that fail to terminate and instead endlessly repeat a sequence of text. We observed that the repetition ratio tends to rise as the quantity of math SFT data increases. This can be attributed to the fact that math SFT data occasionally includes similar patterns in reasoning. Consequently, weaker models struggle to grasp such reasoning patterns, resulting in repetitive responses. To tackle the problem, we tried two-stage fine-tuning and DPO (Rafailov et al., 2023), both of which could almost keep the benchmark score and reduce the repetition significantly.\n\nDPO: To further enhance the model's ability, we used the direct preference optimization algorithm (Rafailov et al., 2023), which is proven to be a simple but effective method for LLM alignment. We constructed the preference data for DPO training in terms of helpfulness and harmlessness. For helpfulness data, we collected multilingual prompts, which cover categories including creative writing, question answering, instruction following, and so on. Then we generated responses using our DeepSeek Chat models as response candidates. Similar operations are applied to harmlessness preference data construction.\n\nWe trained an epoch for DPO, with a learning rate of 5e-6 and batch size of 512, and we used a learning rate warmup and cosine learning rate scheduler. We found out that DPO can strengthen the model's open-ended generation skill, while engendering little difference in performance among standard benchmarks.\n\n## 5. Evaluation\n\n## 5.1. Public Benchmark Evaluation\n\nWe evaluate our models on a series of public benchmarks both in English and Chinese, based on the internal evaluation framework.\n\nMulti-subject multiple-choice datasets including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023) and CMMLU (Li et al., 2023).\n\nLanguage understanding and reasoning datasets including HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and BigBench Hard (BBH) (Suzgun et al., 2022).\n\nClosed-book question answering datasets including TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019).\n\nReading comprehension datasets including RACE Lai et al. (2017) and DROP (Dua et al., 2019), C3 (Sun et al., 2019).\n\nReference disambiguation datasets including WinoGrande Sakaguchi et al. (2019) and CLUEWSC (Xu et al., 2020).\n\nLanguage modeling datasets including Pile (Gao et al., 2020).\n\nChinese understanding and culture datasets including CHID (Zheng et al., 2019) and CCPM (Li et al., 2021).\n\nMath datasets including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and CMath (Wei et al., 2023).\n\nCode datasets including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).\n\nStandardized exams including AGIEval (Zhong et al., 2023).\n\nWe apply perplexity-based evaluation to datasets that require answers to be chosen from several options. These datasets include HellaSwag, PIQA, WinoGrande, RACE-Middle, RACEHigh, MMLU, ARC-Easy, ARC-Challenge, OpenBookQA, CHID, C-Eval, CMMLU, C3 and CCPM. The perplexity-based evaluation here refers to calculating the perplexity of each option and selecting the lowest one as the model prediction. For ARC and OpenBookQA, we calculate the perplexity with unconditional normalization (Brown et al., 2020), and for other datasets we use length normalization.\n\nWe apply generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, HumanEval, MBPP, BBH, AGIEval, CLUEWSC, and CMath. The generation-based evaluation here refers to letting the model generate free texts and parsing results from generated texts. For generation-based evaluation, we use greedy decoding.\n\nWe apply language-modeling-based evaluation for Pile-test, which means calculating the bits-per-byte on the test corpus.\n\nWe use 2048 or 4096 as the maximum sequence length for different benchmarks. Details of evaluation formats can be found in Appendix A.6.\n\n## 5.1.1. Base Model\n\nTable 5 presents the main results on the evaluation benchmark. Despite DeepSeek models are pre-trained on 2T bilingual corpus, they show comparable performance on English language understanding benchmarks with LLaMA2 models, which also consume 2T tokens but focus on English. Furthermore, DeepSeek 67B achieves considerably better performance on MATH, GSM8K, HumanEval, MBPP, BBH, and Chinese benchmarks compared to LLaMA2 70B. We show the benchmark curve in the Appendix A.3. We can see some task performance is boosted as model scaling, such as GSM8K and BBH. Given that we train both 7B and 67B on the same dataset, the emergence of this improvement can be attributed to the powerful few-shot learning ability of large models. However, as the proportion of mathematical data increases, the disparity between small and large models may diminish.\n\nAn interesting observation is that the advantage of DeepSeek 67B over LLaMA2 70B is larger than that of DeepSeek 7B over LLaMA2 7B. This phenomenon highlights the greater influence of language conflict on smaller models. Additionally, LLaMA2 demonstrates impressive performance on certain Chinese tasks, such as CMath, despite not being specifically trained on Chinese data. This suggests that certain fundamental abilities, such as mathematical reasoning, can be effectively transferred across languages. However, tasks like CHID, which involve evaluating the usage of Chinese idioms, require the model to consume a significant number of Chinese tokens during pre-training. In this case, LLaMA2 significantly underperforms compared to DeepSeek LLM.\n\n## 5.1.2. Chat Model\n\nTable 6 demonstrates the results of the DeepSeek Chat models, showcasing overall improvements in most tasks following tuning. However, there were a few instances where the performance of\n\n| Language   | Benchmark        | Test-shots   |   LLaMA2 7B |   DeepSeek 7B |   LLaMA2 70B |   DeepSeek 67B |\n|------------|------------------|--------------|-------------|---------------|--------------|----------------|\n| English    | HellaSwag        | 0-shot       |      75.6   |        75.4   |       84     |         84     |\n| English    | PIQA             | 0-shot       |      78     |        79.2   |       82     |         83.6   |\n| English    | WinoGrande       | 0-shot       |      69.6   |        70.5   |       80.4   |         79.8   |\n| English    | RACE-Middle      | 5-shot       |      60.7   |        63.2   |       70.1   |         69.9   |\n| English    | RACE-High        | 5-shot       |      45.8   |        46.5   |       54.3   |         50.7   |\n| English    | TriviaQA         | 5-shot       |      63.8   |        59.7   |       79.5   |         78.9   |\n| English    | NaturalQuestions | 5-shot       |      25.5   |        22.2   |       36.1   |         36.6   |\n| English    | MMLU             | 5-shot       |      45.8   |        48.2   |       69     |         71.3   |\n| English    | ARC-Easy         | 0-shot       |      69.1   |        67.9   |       76.5   |         76.9   |\n|            | ARC-Challenge    | 0-shot       |      49     |        48.1   |       59.5   |         59     |\n|            | OpenBookQA       | 0-shot       |      57.4   |        55.8   |       60.4   |         60.2   |\n|            | DROP             | 1-shot       |      39.8   |        41     |       69.2   |         67.9   |\n|            | MATH             | 4-shot       |       2.5   |         6     |       13.5   |         18.7   |\n|            | GSM8K            | 8-shot       |      15.5   |        17.4   |       58.4   |         63.4   |\n|            | HumanEval        | 0-shot       |      14.6   |        26.2   |       28.7   |         42.7   |\n|            | MBPP             | 3-shot       |      21.8   |        39     |       45.6   |         57.4   |\n|            | BBH              | 3-shot       |      38.5   |        39.5   |       62.9   |         68.7   |\n|            | AGIEval          | 0-shot       |      22.8   |        26.4   |       37.2   |         41.3   |\n|            | Pile-test        | -            |       0.741 |         0.725 |        0.649 |          0.642 |\n| Chinese    | CLUEWSC          | 5-shot       |      64     |        73.1   |       76.5   |         81     |\n| Chinese    | CHID             | 0-shot       |      37.9   |        89.3   |       55.5   |         92.1   |\n| Chinese    | C-Eval           | 5-shot       |      33.9   |        45     |       51.4   |         66.1   |\n|            | CMMLU            | 5-shot       |      32.6   |        47.2   |       53.1   |         70.8   |\n|            | CMath            | 3-shot       |      25.1   |        34.5   |       53.9   |         63     |\n|            | C3               | 0-shot       |      47.4   |        65.4   |       61.7   |         75.3   |\n|            | CCPM             | 0-shot       |      60.7   |        76.9   |       66.2   |         88.5   |\n\nTable 5 | Main results. The evaluation results we report are based on the internal evaluation framework. Bold numbers indicate the best results among the 4 models. For Pile-test we report bits-per-byte (BPB), for DROP we report F1 score and for other tasks we report accuracy. Note that the test-shots is the maximum value and fewer shots might be applied because of limited context length or limited few-shot examples available in the same passage for reading comprehension tasks such as RACE.\n\n## certain tasks declined.\n\nKnowledge : We have observed fluctuations of base and chat models in knowledge-related tasks, such as TriviaQA, MMLU, and C-Eval. However, we do not believe that such minor fluctuations indicate the acquisition or loss of knowledge after SFT. The value of SFT lies in the ability to learn to achieve comparable scores to the base model's few-shot setting in the chat model's zero-shot setting, which is aligned with real scenarios. For example, 0-shot MMLU performance of a chat model is comparable with 5-shot MMLU performance of a base model.\n\nReasoning : As a significant proportion of the SFT instances are in the CoT format Wei et al. (2022), the chat models demonstrate slight improvements in reasoning tasks, such as BBH and NaturalQuestions. However, we believe that the SFT stage does not learn reasoning capabilities but rather the correct format for reasoning paths.\n\n| Language   | Benchmark        |   DeepSeek 7B Base |   DeepSeek 7B Chat |   DeepSeek 67B Base |   DeepSeek 67B Chat |\n|------------|------------------|--------------------|--------------------|---------------------|---------------------|\n| English    | HellaSwag        |               75.4 |               68.5 |                84   |                75.7 |\n| English    | PIQA             |               79.2 |               77.6 |                83.6 |                82.6 |\n| English    | WinoGrande       |               70.5 |               66.9 |                79.8 |                76   |\n| English    | RACE-Middle      |               63.2 |               65.2 |                69.9 |                70.9 |\n| English    | RACE-High        |               46.5 |               50.8 |                50.7 |                56   |\n| English    | TriviaQA         |               59.7 |               57.9 |                78.9 |                81.5 |\n| English    | NaturalQuestions |               22.2 |               32.5 |                36.6 |                47   |\n| English    | MMLU             |               48.2 |               49.4 |                71.3 |                71.1 |\n| English    | ARC-Easy         |               67.9 |               71   |                76.9 |                81.6 |\n| English    | ARC-Challenge    |               48.1 |               49.4 |                59   |                64.1 |\n| English    | GSM8K            |               17.4 |               63   |                63.4 |                84.1 |\n| English    | MATH             |                6   |               15.8 |                18.7 |                32.6 |\n| English    | HumanEval        |               26.2 |               48.2 |                42.7 |                73.8 |\n| English    | MBPP             |               39   |               35.2 |                57.4 |                61.4 |\n| English    | DROP             |               41   |               49.1 |                67.9 |                71.9 |\n| English    | OpenBookQA       |               55.8 |               54.8 |                60.2 |                63.2 |\n| English    | BBH              |               39.5 |               42.3 |                68.7 |                71.7 |\n| English    | AGIEval          |               26.4 |               19.3 |                41.3 |                46.4 |\n| Chinese    | CLUEWSC          |               73.1 |               71.9 |                81   |                60   |\n| Chinese    | CHID             |               89.3 |               64.9 |                92.1 |                72.6 |\n| Chinese    | C-Eval           |               45   |               47   |                66.1 |                65.2 |\n| Chinese    | CMMLU            |               47.2 |               49.7 |                70.8 |                67.8 |\n| Chinese    | CMath            |               34.5 |               68.4 |                63   |                80.3 |\n| Chinese    | C3               |               65.4 |               66.4 |                75.3 |                77   |\n| Chinese    | CCPM             |               76.9 |               76.5 |                88.5 |                84.9 |\n\nTable 6 | The comparison between base and chat models. We evaluate chat models with 0-shot for MMLU, GSM8K, MATH, C-Eval, and CMMLU, while base model results are still obtained in the few-shot setting.\n\nPerformance Drop Tasks : The performance of a few tasks consistently declines after finetuning, regardless of the model size or pre-trained checkpoint selected. These particular tasks typically involve cloze tasks or sentence completion tasks, such as HellaSwag. It is reasonable to assume that pure language models are better equipped to handle such tasks.\n\nMath and Code : Our model exhibits significant improvements in math and coding tasks after fine-tuning. For instance, HumanEval and GSM8K scores are improved by over 20 points. Our explanation for this is that the base model was initially underfitted for these tasks, and the SFT stage has learned additional knowledge in coding and mathematics through the extensive SFT data. However, it is important to note that the model's capabilities may be primarily focused on code completion and algebraic questions. To develop a comprehensive understanding of mathematics and coding, it is crucial to incorporate a diverse range of data during the pretraining stage, which is left as future work. We conducted a detailed analysis of code and math tasks in Appendix A.4.\n\nIn the 7B model fine-tuning, we initially fine-tune the model using all data. Subsequently, a second stage is introduced, which excludes math and code data. The motivation behind this approach is that the stage-1 model exhibits a repetition ratio of 2.0%, which is reduced to 1.4%\n\n| Model                     | Overall     | Reasoning       | Reasoning       | Reasoning       | Language       | Language       | Language       | Language       | Language       | Language       | Language       |\n|---------------------------|---------------|---------------------|---------------------|---------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n|                         | Overall     | Avg.            | Math.           | Logi.           | Avg.           | Fund.          | Chi.           | Open.          | Writ.          | Role.          | Pro.           |\n| gpt-4-1106-preview        | 8.01          | 7.73                | 7.80                | 7.66                | 8.29               | 7.99               | 7.33               | 8.61               | 8.67               | 8.47               | 8.65               |\n| gpt-4-0613                | 7.53          | 7.47                | 7.56                | 7.37                | 7.59               | 7.81               | 6.93               | 7.42               | 7.93               | 7.51               | 7.94               |\n| DeepSeek-67B-Chat-DPO*    | 6.69          | 5.77                | 6.13                | 5.41                | 7.60               | 7.29               | 7.47               | 7.82               | 7.51               | 7.83               | 7.71               |\n| DeepSeek-67B-Chat*        | 6.43          | 5.75                | 5.71                | 5.79                | 7.11               | 7.12               | 6.52               | 7.58               | 7.20               | 6.91               | 7.37               |\n| chatglm-turbo       | 6.24          | 5.00                | 4.74                | 5.26                | 7.49               | 6.82               | 7.17               | 8.16               | 7.77               | 7.76               | 7.24               |\n| erniebot-3.5        | 6.14          | 5.15                | 5.03                | 5.27                | 7.13               | 6.62               | 7.60               | 7.26               | 7.56               | 6.83               | 6.90               |\n| gpt-3.5-turbo-0613        | 6.08          | 5.35                | 5.68                | 5.02                | 6.82               | 6.71               | 5.81               | 7.29               | 7.03               | 7.28               | 6.77               |\n| chatglm-pro         | 5.83          | 4.65                | 4.54                | 4.75                | 7.01               | 6.51               | 6.76               | 7.47               | 7.07               | 7.34               | 6.89               |\n| spark_desk_v2       | 5.74          | 4.73                | 4.71                | 4.74                | 6.76               | 5.84               | 6.97               | 7.29               | 7.18               | 6.92               | 6.34               |\n| Qwen-14B-Chat             | 5.72          | 4.81                | 4.91                | 4.71                | 6.63               | 6.90               | 6.36               | 6.74               | 6.64               | 6.59               | 6.56               |\n| Baichuan2-13B-Chat        | 5.25          | 3.92                | 3.76                | 4.07                | 6.59               | 6.22               | 6.05               | 7.11               | 6.97               | 6.75               | 6.43               |\n| ChatGLM3-6B               | 4.97          | 3.85                | 3.55                | 4.14                | 6.10               | 5.75               | 5.29               | 6.71               | 6.83               | 6.28               | 5.73               |\n| Baichuan2-7B-Chat         | 4.97          | 3.66                | 3.56                | 3.75                | 6.28               | 5.81               | 5.50               | 7.13               | 6.84               | 6.53               | 5.84               |\n| InternLM-20B              | 4.96          | 3.66                | 3.39                | 3.92                | 6.26               | 5.96               | 5.50               | 7.18               | 6.19               | 6.49               | 6.22               |\n| Qwen-7B-Chat              | 4.91          | 3.73                | 3.62                | 3.83                | 6.09               | 6.40               | 5.74               | 6.26               | 6.31               | 6.19               | 5.66               |\n| ChatGLM2-6B               | 4.48          | 3.39                | 3.16                | 3.61                | 5.58               | 4.91               | 4.52               | 6.66               | 6.25               | 6.08               | 5.08               |\n| InternLM-Chat-7B          | 3.65          | 2.56                | 2.45                | 2.66                | 4.75               | 4.34               | 4.09               | 5.82               | 4.89               | 5.32               | 4.06               |\n| Chinese-LLaMA-2-7B-Chat   | 3.57          | 2.68                | 2.29                | 3.07                | 4.46               | 4.31               | 4.26               | 4.50               | 4.63               | 4.91               | 4.13               |\n| LLaMA-2-13B-Chinese-Chat  | 3.35          | 2.47                | 2.21                | 2.73                | 4.23               | 4.13               | 3.31               | 4.79               | 3.93               | 4.53               | 4.71               |\n\nTable 7 | AlignBench leaderboard rated by gpt-4-0613. Models are ranked in descending order of total score. Results with * are our evaluation results based on the official AlignBench repository, whereas all other results are derived from the AlignBench paper. We found that our Deepseek-67B-Chat model surpasses ChatGPT and other baseline models by a clear margin, which indicates the superior performance of our model in both basic Chinese language tasks and advanced Chinese reasoning tasks. Besides, we can find that the DPO process has brought improvements in almost all fields.\n\nafter stage-2 tuning, while maintaining the benchmark score. In the case of the 67B model, the repetition ratio is already below 1% following the first stage fine-tuning, and the second stage hurts the model score on the benchmark. Therefore, only one stage of SFT is done for the 67B model.\n\n## 5.2. Open-Ended Evaluation\n\nFor chat models, in addition to observing metrics on standard benchmarks, the quality of results generated in open domains and open-ended questions directly affects the actual user experience. Hence, we separately tested the open-ended generation capabilities of our chat model in both Chinese and English tasks.\n\n## 5.2.1. Chinese Open-Ended Evaluation\n\nFor Chinese open-ended evaluation, we tested the comprehensive of our chat model in different domains on a high-quality open-ended question testset AlignBench (Liu et al., 2023). AlignBench includes a total of 8 primary categories, 36 secondary categories, and encompasses 683 questions. For each question, in addition to the prompt, AlignBench also provides professional reference answers and rating templates for GPT-4 to judge the quality of the response.\n\nWe utilized the official AlignBench Github code repository to implement the evaluation of\n\nour model. We strictly aligned the key temperature parameter with the original setting: for role-playing, writing ability, and open-ended questions, the generation temperature was set to 0.7; whereas for other tasks, the generation temperature was set to 0.1.\n\nThe AlignBench leaderboard is shown in Table 7. We can find that our DeepSeek 67B Chat model surpasses ChatGPT and other baseline models, and is only after the two versions of GPT-4. This demonstrates the excellent performance of our model across various Chinese tasks, compared to other open-source or proprietary Chinese Large Language Models. The DPO model has shown improvement across almost all metrics, which demonstrates the positive impact of the DPO training process on model alignment.\n\nFor the basic Chinese Language tasks, our model is in the first tier among all models, and the Chinese fundamental language ability of our DPO model is even higher than the newest version of GPT-4. For the advanced Chinese Reasoning tasks, our model's scores are significantly higher than those of other Chinese LLMs with a clear margin, demonstrating the superior performance of our model in more complex Chinese logical reasoning and mathematical calculations.\n\n## 5.2.2. English Open-Ended Evaluation\n\nFor English open-ended evaluation, we use the MT-Bench benchmark (Zheng et al., 2023), which contains 8 different categories of multi-turn questions. As illustrated in Table 8, our DeepSeek LLM 67B Chat outperforms other open-source models such as LLaMA-2-Chat Touvron et al. (2023b) 70B, Xwin 70b v0.1, and TLU 2+DPO 70B (Ivison et al., 2023), and achieves 8.35 score comparable with GPT-3.5-turbo. Besides, after the DPO stage, our DeepSeek LLM 67B Chat DPO further improves the average score to 8.76, which is only behind GPT-4 (OpenAI, 2023). These results illustrate the strong multi-turn open-ended generation ability of DeepSeek LLM.\n\nTable 8 | MT-Bench Evaluation. Results with  are reported in Ivison et al. (2023)\n\n| Model                    |   STEM |   Humanities |   Reasoning |   Coding |   Math |   Extraction |   Roleplay |   Writing |   Average |\n|--------------------------|--------|--------------|-------------|----------|--------|--------------|------------|-----------|-----------|\n| GPT-4-1106-preview      |   9.9  |         9.95 |        8.1  |     9.05 |   7.95 |         9.9  |       9.5  |      9.7  |      9.26 |\n| GPT-3.5-turbo-0613      |   9.55 |         9.95 |        6.2  |     7.05 |   7.05 |         9    |       8.65 |      9.65 |      8.39 |\n| LLAMA-2-Chat 7B         |   8.65 |         8.75 |        4.25 |     3    |   2.4  |         6.5  |       7.7  |      8.9  |      6.27 |\n| LLAMA-2-Chat 13B        |   8.63 |         9.75 |        5.1  |     3    |   3.45 |         6.93 |       7.5  |      8.85 |      6.65 |\n| LLAMA-2-Chat 70B        |   8.93 |         9.63 |        5.8  |     3.15 |   3.3  |         7.25 |       7.5  |      9.3  |      6.86 |\n| Zephyr-Beta 7B          |   9.03 |         9.63 |        5.6  |     5.1  |   4.45 |         7.45 |       8.2  |      9.35 |      7.35 |\n| Xwin 70b v0.1           |   9.68 |         9.95 |        6.55 |     4.25 |   3.3  |         8.75 |       8.25 |      9.55 |      7.53 |\n| Xwin 13b v0.2           |   9.55 |         9.88 |        5.2  |     3.6  |   2.85 |         7.7  |       8.6  |      8.68 |      7.01 |\n| TLU 2+DPO 70B          |   9    |         9.9  |        7    |     4.7  |   4.65 |         9.35 |       9.25 |      9.25 |      7.89 |\n| DeepSeek LLM 67B Chat    |   9.6  |         9.7  |        8    |     7.35 |   6.25 |         8.4  |       8.2  |      9.3  |      8.35 |\n| DeepSeek LLM 67B ChatDPO |   9.7  |         9.8  |        9.05 |     6.75 |   6.65 |         9.3  |       9.1  |      9.75 |      8.76 |\n\n## 5.3. Held-Out Evaluation\n\nData contamination and benchmark overfitting are two challenges in evaluating LLMs. One common practice is to utilize testsets published recently to evaluate the model as held-out testsets.\n\nLeetCode: To assess the coding proficiency of the model, we have utilized problems from the LeetCode Weekly Contest (Weekly Contest 351-372, Bi-Weekly Contest 108-117, from July 2023 to Nov 2023). We have obtained these problems by crawling data from LeetCode, which consists of 126 problems with over 20 test cases for each. The evaluation metric employed is akin to that of HumanEval. In this regard, if a model's outputs successfully pass all test cases, the model is considered to have effectively solved the problem. The model's coding capabilities are\n\ndepicted in the Figure below, where the y-axis represents the pass@1 score on in-domain human evaluation testing, and the x-axis represents the pass@1 score on out-domain LeetCode Weekly Contest problems. The LeetCode test data will be released accompanied with the DeepSeek Coder technique report soon.\n\nHungarian National High-School Exam: In line with Grok-1, we have evaluated the model's mathematical capabilities using the Hungarian National High School Exam. This exam comprises 33 problems, and the model's scores are determined through human annotation. We follow the scoring metric in the solution.pdf to evaluate all models.\n\nInstruction Following Evaluation: On Nov 15th, 2023, Google released an instruction following the evaluation dataset (Zhou et al., 2023). They identified 25 types of verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We use the prompt-level loose metric to evaluate all models.\n\nTable 9 | Held-out Dataset Evaluation.\n\n| Model                 |   LeetCode |   Hungarian Exam |   IFEval |\n|-----------------------|------------|------------------|----------|\n| GPT-4                 |       48.4 |             68   |     79.3 |\n| ChatGLM3 6B           |        2.4 |             32   |     29.7 |\n| DeepSeek LLM 7B Chat  |        4.7 |             28.5 |     41.2 |\n| Baichuan2-Chat 13B    |        1.6 |             19.5 |     44.5 |\n| Yi-Chat 34B           |        7.9 |             39   |     48.4 |\n| Qwen 72B Chat         |       12.7 |             52   |     50.8 |\n| DeepSeek LLM 67B Chat |       17.5 |             58   |     55.5 |\n\nWe have conducted a comparative analysis of our model against various baseline models of different sizes, namely Qwen 72B Chat (Bai et al., 2023), ChatGLM3 (Du et al., 2022), Baichuan2 (Yang et al., 2023), and Yi-34B Chat. Our observations indicate that there exists a significant performance gap between large models and small models on these held-out datasets, even if certain small models achieve promising results on conventional benchmarks. For instance, ChatGLM3 achieves a score of 52.4 on MBPP, a code testset, which is close to DeepSeek 67B. However, when evaluated on new benchmarks, its performance falls considerably short compared to DeepSeek 67B. A similar trend is also observed in math datasets, where ChatGLM3 is very strong on GSM8K (72.3), but its performance in the Hungarian Exam score is inferior to large models. Furthermore, the capability of instruction following demonstrates that total computing plays a crucial role.\n\nThe DeepSeek 7B and 67B models utilize the same training pipeline, but there is a significant disparity in their performance. Through our subjective evaluation, we have observed a notable discrepancy in intelligence across various tasks when scaling model size to 67B. While DeepSeek 7B falls behind other smaller language models on standard benchmarks, its performance on held-out tasks is relatively commendable when compared to others.\n\n## 5.4. Safety Evaluation\n\nWe profoundly recognize the importance of safety for general artificial intelligence. The premise for establishing a truly helpful artificial intelligence model is that it possesses values consistent with those of humans and exhibits friendliness towards humanity. We incorporate the assurance of model safety throughout the entire training process, including pre-training, SFT, and DPO.\n\nTo validate the safety of our model, we established a 20-person expert team from various\n\n| Category                                                           | Subcategory                                                                                                                                                                                                                                                                                                   | #Safety Answers / #Total Cases   |\n|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n|       (Discrimination and Prejudice Questions)               |     (Ethnic and Racial),     (Religious Belief),    (Nationality and Geography),   (Gender),   (Age),   (Occupation),   (Health),       (Discrimination in Other Aspects)                                                                                                           | 486/500                          |\n|        (Infringement of Others' Legal Rights)              |     (Physical and Mental Health),     (Legitimate Property),    (Portrait Rights),    (Reputation Rights),    (Honor Rights),    (Privacy Rights),     (Information Rights),       (Other Legal Rights)                                                                         | 473/500                          |\n|          (Trade Secrets and Intellectual Property Rights) |        (Infringing Others' Intellectual Property Rights),           (Monopolistic and Unfair Competitive Actions),           (Other Commercially Illegal and Non-compliant Behaviors),       (Violating Business Ethics),        (Disclosing Others' Trade Secrets) | 281/300                          |\n|       (Illegal and Non-compliant Behavior)                   |     (Cults and Superstition),   (Pornography),   (Gambling),      (Drugs and Prohibited Items),     (Insults and Abuse),     (Violent Behavior),     (Involvement in Organized Crime),         (Other Illegal and Non-compliant Behaviors)                                  | 290/300                          |\n|       (Other Safety Issues)                                  |         (Issues of Illusion and Reality),     (Time-sensitive Issues),       (Self-recognition Problems),       (Other Sensitive Topics),                                                                                                                                            | 767/800                          |\n\nTable 10 | Our taxonomy for safety evaluation. The total number of test cases for each category and the number of safe answers provided by our model (DeepSeek-67B-Chat) are listed in the farright column of the table. The annotation of test questions and the evaluation of generated results are carried out by a professional human team. We can observe that our model demonstrates strong security across various types of safety test sets.\n\ndisciplines and constructed a safety content classification system that aligns with human values (the safety evaluation taxonomy is shown in Table 10). Subsequently, the expert team constructed dozens of high-quality test cases for each safety subcategory manually. In addition to focusing on the diversity of safety content areas, we also pay attention to the diversity of formats in safety content. The infamous \"grandmother\" loophole indicates that models can be deceived by the surface format of a query into providing unsafe responses. Therefore, when devising questions, the expert team also pays attention to diversifying the ways of inquiry. They construct diverse safety issues through means such as inducement, role-playing, multi-turn dialogues, preset positions, and etc.. Ultimately, we obtained a safety test set comprising 2400 questions. In addition, the expert team has constructed a basic guideline constitution for safety reviews for each different content type and format type.\n\nFor the output results of our model on this test set, we manually inspected its safety. Our review team was well-trained and cross-verification was performed on the annotation results. The annotators perform a three-category annotation for each question: safe, unsafe, and model refusal. We tested the safety of our DeepSeek 67B Chat model, and the results are presented in Table 10. The number of test questions for each safety category and the number of safety tests passed by our model are listed in the table. We label both the securely answered and the model-refused test cases as secure responses. The results indicate that our model exhibits good security performance across numerous safety test categories.\n\nComplementing our existing approach to safety, we further enriched our evaluation using the \"Do-Not-Answer\" dataset (Wang et al., 2023) to evaluate the safety mechanisms of our DeepSeek 67B Chat model. The dataset's 939 risk-categorized prompts were instrumental in highlighting our model's enhanced capabilities. As shown in Table 11, DeepSeek 67B Chat model has demonstrated notable performance, achieving a score of 97.8, which is higher than both ChatGPT and GPT-4. This score not only benchmarks our model's capability to safely handle sensitive queries but also places it competitively among leading models in the field.\n\n## 5.5. Discussion\n\nThroughout the development process, we have discovered some interesting findings in building LLMs.\n\nTable 11 | Do-Not-Answer Score (Wang et al., 2023), a higher score signifies greater model safety. Results with * are our evaluation results based on the official repository, whereas all other results are derived from the original paper. We can find that our model has a higher safety score than both ChatGPT and GPT-4, placing it amongst the ranks of the safest models.\n\n| Model              |   Do-Not-Answer |\n|--------------------|-----------------|\n| LLAMA-2-7B-Chat    |            99.4 |\n| Claude             |            98.3 |\n| DeepSeek-67B-Chat* |            97.8 |\n| ChatGPT            |            97.7 |\n| GPT-4              |            96.5 |\n| Vicuna-7B          |            94.9 |\n| ChatGLM2           |            92.9 |\n\nStaged Fine-Tuning: As we mentioned above, small models need longer fine-tuning on math and code dataset, but it will hurt the model conversation ability, such as increasing repetition behavior. To address this issue, we have implemented a staged fine-tuning process. In this approach, the first stage involves fine-tuning with all available data, while the second stage focuses specifically on fine-tuning with conversational data.\n\nTable 12 | Two-stage fine-tuning results. The repetition ratio is computed when the temperature is 0. The lower repetition ratio is better. The IFEval result is the prompt-level loose accuracy.\n\n| Model                       |   HumanEval |   GSM8K |   Repetition |   IFEval |\n|-----------------------------|-------------|---------|--------------|----------|\n| DeepSeek LLM 7B Chat Stage1 |        48.2 |    63.9 |        0.02  |     38   |\n| DeepSeek LLM 7B Chat Stage2 |        48.2 |    63   |        0.014 |     41.2 |\n\nTable 12 displays the results obtained from the two-stage training process. These results clearly demonstrate that the second stage does not compromise the model's proficiency in code and math, while simultaneously decreasing the repetition behavior and enhancing instruction following capability.\n\nMulti-Choice Question: It is a common practice to test a model with multi-choice style evaluation data, such as MMLU, AGI Eval, and C-Eval. Multi-choice questions require the model not only to have the corresponding knowledge but also to understand what the option refers to. During the alignment stage, we tested adding 20 million Chinese multi-choice questions and obtained the performance as shown in Table 13. It is important to note that we conducted deduplication for the C-Eval validation set and CMMLU test set to prevent data contamination.\n\nTable 13 | The impact of adding multi-choice question data.\n\n| Model                   |   MMLU |   C-Eval |   CMMLU |   TriviaQA |   ChineseQA |\n|-------------------------|--------|----------|---------|------------|-------------|\n| DeepSeek LLM 7B Chat    |   49.4 |     47   |    49.7 |       57.9 |        75   |\n| DeepSeek LLM 7B Chat+MC |   60.9 |     71.3 |    73.8 |       57.9 |        74.4 |\n\nThe inclusion of an additional 20M MC (multiple-choice) data has proven to be beneficial not only for Chinese multiple-choice benchmarks but also for improving English benchmarks. This indicates that the model's capability to solve MC problems has been enhanced. However, we have observed that this improvement does not extend to the model's performance on other evaluations that do not utilize the multiple-choice format, such as TriviaQA and our in-house\n\nChineseQA testsets, which are generative evaluation benchmarks. This suggests that users may not perceive the model as becoming more intelligent during conversational interactions, as these interactions involve generating responses rather than solving multiple-choice problems.\n\nTherefore, we have chosen to exclude MC data from both the pre-training and fine-tuning stages , as including it would result in overfitting to benchmarks and would not contribute to achieving true intelligence in the model.\n\nInstruction Data in Pre-Training: It is widely acknowledged that incorporating instruction data during the latter part of the pre-training phase enhances the performance of a base model on benchmark tasks. In our study, we integrated 5 million instruction data, primarily consisting of multi-choice questions, during the final 10% of the pre-training stage. We observed that the base model did exhibit improved performance on the benchmark. However, the final outcomes were nearly identical to those achieved by adding the same data during the SFT stage. We conclude that while this approach strengthens the base model's performance on the benchmark, its overall potential is equivalent to not incorporating these instruction data. If the instruction data is substantial in size, it is acceptable to incorporate it into the pre-training process. Due to our preference for excluding multi-choice questions and the limited availability of non-multi-choice questions we have, we made the decision not to include instruction data in the pre-training process.\n\nSystem Prompt: A well-designed system prompt should effectively guide a model to generate responses that are both helpful and respectful. We slightly changed the prompt introduced by LLaMA-2 as our system prompt.\n\nSystem prompt: You are DeepSeek Chat, a helpful, respectful and honest AI assistant developed by DeepSeek. The knowledge cut-off date for your training data is up to May 2023. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n\nWe have observed an intriguing phenomenon wherein the performance of a 7B LLM experiences a slight degradation when a system prompt is introduced. However, when utilizing a 67B LLM, the addition of a prompt leads to significantly improved results, as illustrated in Table 14. Our explanation for this disparity is that larger models possess a better understanding of the intended meaning behind the system prompt, enabling them to follow instructions more effectively and generate superior responses. On the other hand, smaller models struggle to grasp the system prompt adequately, and the inconsistency between training and testing might negatively impact their performance.\n\nTable 14 | The impact of adding a system prompt.\n\n| Model                                 |   MTBench |\n|---------------------------------------|-----------|\n| DeepSeek LLM 7B Chat                  |      7.15 |\n| DeepSeek LLM 7B Chat + System Prompt  |      7.11 |\n| DeepSeek LLM 67B Chat                 |      8.35 |\n| DeepSeek LLM 67B Chat + System Prompt |      8.58 |\n\n## 6. Conclusion, Limitation, and Future Work\n\nWe introduce DeepSeek LLMs, a series of open-source models trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In this paper, we provide an in-depth explanation of hyper-parameters selection, scaling laws, as well as the various fine-tuning attempts we made. We calibrate the scaling laws in the previous work and propose a new optimal model/data scaling-up allocation strategy. In addition, we present a method to predict the near-optimal batch size and learning rate with given compute budget. We further conclude that the scaling laws is related to the data quality, which might be the root cause of varying scaling behavior in different works. Guided by the scaling laws, we conduct pre-training with the best hyper-parameter and provide a comprehensive evaluation. We avoid benchmark decoration and dark secrets in all training stages.\n\nDeepSeek Chat shares the acknowledged limitations commonly found in other LLMs, which include the lack of ongoing knowledge updates after pre-training, the possibility of generating non-factual information such as unverified advice, and a tendency to produce hallucinations. Moreover, it is important to note that our initial version of Chinese data is not exhaustive, which may result in suboptimal performance on certain Chinese-specific topics. Since our data primarily consists of Chinese and English sources, the model's proficiency in other languages remains delicate and should be approached with caution.\n\nDeepSeek LLM is a long-term project committed to advancing open-source language models.\n\n- Soon, we will release our technique reports in code intelligence and Mixture-of-Experts (MoE), respectively. They show how we create high-quality code data for pre-training, and design a sparse model to achieve dense model performance.\n- At present, we are constructing a larger and improved dataset for the upcoming version of DeepSeek LLM. We hope the reasoning, Chinese knowledge, math, and code capabilities will be significantly improved in the next version.\n- Our alignment team is dedicated to studying ways to deliver a model that is helpful, honest, and safe to the public. Our initial experiments prove that reinforcement learning could boost model complex reasoning capability.\n\n## References\n\n- J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrn, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.\n2. Anthropic. Introducing Claude, 2023. URL https://www.anthropic.com/index/introd ucing-claude .\n- J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n- J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n- Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n\n- 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432-7439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239 .\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020.\n- M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374 .\n- P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457 .\n- K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n- T. Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n- T. Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.\n- T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.\n- Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335, 2022.\n- D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 23682378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246 .\n\n- L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n2. Google. An important next step on our AI journey, 2023. URL https://blog.google/tech nology/ai/bard-google-ai-search-updates/ .\n- Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A toolintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023. doi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745 2 .\n- P. Goyal, P. Dollr, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n- D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n- D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n- T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\n- J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. M. A. Patwary, Y. Yang, and Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n9. High-flyer. Hai-llm:              , 2023. URL https://www.high-flyer.c n/en/blog/hai-llm .\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models. CoRR, abs/2203.15556, 2022. doi: 10.48550 /ARXIV.2203.15556. URL https://doi.org/10.48550/arXiv.2203.15556 .\n- Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.\n12. Huggingface Team. Tokenizers: Fast state-of-the-art tokenizers optimized for research and production, 2019. URL https://github.com/huggingface/tokenizers .\n- F. i, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id= fR3wGCk-IXp .\n\n- H. Ivison, Y. Wang, V. Pyatkin, N. Lambert, M. Peters, P. Dasigi, J. Jang, D. Wadden, N. A. Smith, I. Beltagy, and H. Hajishirzi. Camels in a changing climate: Enhancing lm adaptation with tulu 2. 2023.\n- A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n- M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361 .\n- V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\n- T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_00276. URL https://doi.org/10.1162/tacl\\_a\\_00276 .\n- W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\n- G. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785-794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082 .\n- H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.\n- W. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset, 2021.\n- X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang. Alignbench: Benchmarking chinese alignment of large language models. CoRR, abs/2311.18743, 2023. doi: 10.48550/A RXIV.2311.18743. URL https://doi.org/10.48550/arXiv.2311.18743 .\n- I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.\n\n- H. Luo, Q. Sun, C. Xu, P. Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.\n- S. McCandlish, J. Kaplan, D. Amodei, and O. D. Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018.\n- T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018.\n- D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15, 2021.\n\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt .\n\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n\n- L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.\n- G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n- A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n- R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your language model is secretly a reward model. 2023.\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-16. IEEE, 2020.\n- K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.\n- C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research, 20(112):1-49, 2019.\n- N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\n- S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489, 2017.\n\n- J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\n- K. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019.\n- M. Suzgun, N. Scales, N. Schrli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n- H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozire, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n- H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288 .\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n- Y. Wang, H. Li, X. Han, P. Nakov, and T. Baldwin. Do-not-answer: A dataset for evaluating safeguards in llms. CoRR, abs/2308.13387, 2023. doi: 10.48550/ARXIV.2308.13387. URL https://doi.org/10.48550/arXiv.2308.13387 .\n- J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL http://papers.nips.cc/paper\\_files/paper/2022/hash/9d5609613524ecf 4f15af0f7b31abca4-Abstract-Conference.html .\n- T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.\n- L. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762-4772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419 .\n- A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan, F. Yang, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie, J. Dai,\n\nK. Fang, L. Su, L. Song, L. Liu, L. Ru, L. Ma, M. Wang, M. Liu, M. Lin, N. Nie, P. Guo, R. Sun, T. Zhang, T. Li, T. Li, W. Cheng, W. Chen, X. Zeng, X. Wang, X. Chen, X. Men, X. Yu, X. Pan, Y. Shen, Y. Wang, Y. Li, Y. Jiang, Y. Gao, Y. Zhang, Z. Zhou, and Z. Wu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023. URL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf .\n\n- L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu. Metamath: Bootstrap your own mathematical questions for large language models. CoRR, abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485 50/arXiv.2309.12284 .\n- R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Mrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791-4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472 .\n- B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.\n- G. Zhang, L. Li, Z. Nado, J. Martens, S. Sachdeva, G. Dahl, C. Shallue, and R. B. Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32, 2019.\n- C. Zheng, M. Huang, and A. Sun. Chid: A large-scale chinese idiom dataset for cloze test. In A. Korhonen, D. R. Traum, and L. Mrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 778-787. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1075. URL https://doi.org/10.18653/v1/p19-1075 .\n- L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. 2023.\n- W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364 .\n- J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n\n## A. Appendix\n\n## A.1. Acknowledgments\n\nThis project was realized thanks to the efforts of numerous contributors. We offer our extended thanks to the following individuals for their help 1 :\n\n- Data Annotation Team: Jialu Cai, Ruijian Chen, Ruyi Chen, Bei Feng, Yanping Huang, Zhen Huang, Pin Jiang, Rongli Jin, Xiangyue Jin, Ziyun Ke, Hui Li, Meng Li, Sangsang Li, Xiaoqian Li, Yaohui Li, Yunxian Ma, Jiaqi Ni, Xiaojin Shen, Xinnan Song, Tianyu Sun, Xiaosha Chen, Haoyuan Tian, Xiaohan Wang, Xiaoxiang Wang, Yuhao Wang, Fanyi Xia, Lei Xu, Zeyuan Xu, Zhipeng Xu, Tian Yuan, Zhongyu Zhang, Yi Zheng, Shuang Zhou, Xinyi Zhou, Yuchen Zhu, Yuxuan Zhu.\n- Compliance Team: Jin Chen, Ying Tang, Miaojun Wang, Xianzu Wang, Shaoqing Wu, Leyi Xia, W.L. Xiao.\n- Business Team: Jian Liang, Mingming Li, T. Wang, Xianzu Wang, Zhiniu Wen, Shengfeng Ye, Peng Zhang, Zhen Zhang.\n- Design Team: Wei An, Yukun Zha.\n\n## A.2. Different Model Scale Representations\n\nWe refitted the scaling curve for different model scale representations, reusing the experiments from the IsoFLOP profile. We recalculated the compute FLOPs using 6  1 and 6  2 as model scale representations and refitted the performance scaling curves. As shown in Figure 6, the results indicate that the deviation of optimal model/data allocation among these three representations is not significant at higher compute budgets, but there are noticeable differences at lower budgets.\n\n<!-- image -->\n\n- (a) Compute budget  = 6  1 \n- (b) Compute budget  = 6  2 \n\n<!-- image -->\n\n(c) Compute budget  = \n\n<!-- image -->\n\nFigure 6 | Performance scaling curves using different model scale representations. The metric is the bits-per-byte on the validation set. The dotted line represents the power law fitting the smaller model (grey circles). The blue stars represent DeepSeek LLM 7B and 67B.  1,  2, and  represent the non-embedding parameters, complete parameters, and non-embedding FLOPs/token of the model, respectively.\n\nWhen using 6  1 as the model scale representation, the fitted performance scaling curve tends to overestimate the performance of large-scale models. Conversely, when using 6  2, the\n\n1 Authors are ordered alphabetically by the last name.\n\ncurve tends to underestimate their performance. Using  as the model scale representation, however, achieves the most accurate predictions.\n\n## A.3. Benchmark Metrics Curves\n\nFigure 7 | Benchmark metrics curves of DeepSeek LLM Base. ChineseQA is our in-house test set, constructed in a manner akin to TriviaQA.\n\n<!-- image -->\n\nFigure 7 shows benchmark metrics curves across different training steps. We can see consistent improvement on these benchmarks from the start to the end of training. We believe the performance will further be improved if the training continues.\n\nTable 15 | Comparison with code-specific models.\n\n| Model                    | Size                     | HumanEval                | HumanEval                | MBPP                     |\n|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                          |                          | Python                   | Multilingual             |                          |\n| Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       |\n| Codex-001                | -                        | 33.5%                    | 26.1%                    | 45.9%                    |\n| StarCoder                | 16B                      | 36.0%                    | 28.7%                    | 46.8%                    |\n| CodeGeeX2                | 6B                       | 36.0%                    | 24.5%                    | 42.4%                    |\n| CodeLlama                | 7B                       | 31.7%                    | 29.2%                    | 41.6%                    |\n| CodeLlama                | 13B                      | 36.0%                    | 35.4%                    | 48.4%                    |\n| CodeLlama                | 34B                      | 48.2 %                   | 41.0 %                   | 55.2%                    |\n| DeepSeek-LLM-Base        | 67B                      | 42.7%                    | 37.2%                    | 57.4%                    |\n| Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models |\n| Wizard-Coder             | 34B                      | 73.2%                    | 48.8%                    | 61.2%                    |\n| DeepSeek-LLM-Chat        | 67B                      | 73.8%                    | 53.3 %                   | 61.4%                    |\n\n## A.4. Comparison with Code or Math Specific Models\n\nWe have conducted a comparison between our model and specific code and math language models (LLMs). Table 15 demonstrates that DeepSeek LLM 67B is capable of achieving similar performance to CodeLlama, despite having access to less code data. It is worth noting that DeepSeek LLM possesses greater capabilities in areas other than code.\n\nLikewise, Table 16 presents the results obtained from various math-related benchmarks, such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM-zh (i et al., 2023), and CMath (Wei et al., 2023). DeepSeek 67B exhibits exceptional performance on math-related tasks across different languages, showcasing its superiority in this domain. In addition, DeepSeek LLM can utilize programs to solve math problems, which demonstrates better performance than chain-of-thoughts. It is significantly better than the previous SOTA model, ToRA (Gou et al., 2023), on the benchmarks.\n\nTable 16 | Comparison with math-specific models.\n\n|                                   | Inference                 | GSM8K                     | MATH                      | MGSM-zh                   | CMath                     |\n|-----------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|\n| Chain-of-Thoughts                 | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         |\n| MetaMath 70B (Yu et al., 2023)    | CoT                       | 82.3%                     | 26.6%                     | 66.4%                     | 70.9%                     |\n| WizardMath 70B (Luo et al., 2023) | CoT                       | 81.6%                     | 22.7%                     | 64.8%                     | 65.4%                     |\n| DeepSeek LLM 67B Chat             | CoT                       | 84.1 %                    | 32.6 %                    | 74.0%                     | 80.3%                     |\n| Tool-Integrated Reasoning         | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning |\n| ToRA-Code 34B (Gou et al., 2023)  | Tool-Integrated           | 80.7%                     | 50.8%                     | 41.2%                     | 53.4%                     |\n| DeepSeek LLM 67B Chat             | Tool-Integrated           | 86.7%                     | 51.1%                     | 76.4%                     | 85.4%                     |\n\n## A.5. Benchmark Results w/ DPO Stage\n\nTable 17 presents the benchmark results obtained with the DPO stage. Based on these results, we can conclude that the DPO stage does not significantly impact the fundamental capability of an LLM.\n\nTable 17 | The benchmark metrics before and after DPO stage.\n\n|                  |   DeepSeek 67B Chat |   DeepSeek 67B Chat DPO |\n|------------------|---------------------|-------------------------|\n| HellaSwag        |                75.7 |                    76.1 |\n| TriviaQA         |                81.5 |                    82.9 |\n| NaturalQuestions |                47   |                    48.8 |\n| MMLU             |                71.1 |                    70.9 |\n| GSM8K            |                84.1 |                    85.2 |\n| MATH             |                32.6 |                    30.2 |\n| HumanEval        |                73.8 |                    71.3 |\n| BBH              |                71.7 |                    70.8 |\n| AGIEval          |                46.4 |                    46.1 |\n| CEval            |                65.2 |                    64.3 |\n| CMMLU            |                67.8 |                    68.2 |\n\n## A.6. Evaluation Formats\n\nTable 18  Table 40 present examples of our evaluation formats on different benchmarks.\n\n## PROMPT\n\n                      \n\n                      ,        (A)          (B)      DNA(C)     ATP      (D)             \n\n  \n\n A  D,     \n\nTable 18 | An example of AGIEval.\n\n## PROMPT\n\nQuestion: Use the information below to answer the question. Cotton is a plant product used to make fabric. Cotton is made of cellulose, a fiber not digestible by humans. Cellulose is composed of many sugar molecules bonded together into long chains. Each sugar molecule contains carbon, hydrogen, and oxygen atoms. When cotton fabric is washed, wrinkles often form. The clothing industry uses chemicals to manufacture some cotton fabrics that are wrinkle-free. Dyes are also added to color the cellulose fibers in cotton. How would a clothing manufacturer separate colors to determine the purity of the dyes?\n\nAnswer:\n\n## OPTIONS\n\n- through filtration\n- by their boiling points\n- by their freezing points\n- through paper chromatography\n\nTable 19 | An example of ARC.\n\n## PROMPT\n\nEvaluate the result of a random Boolean expression.\n\nQ: not ( ( not not True ) ) is\n\nA: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = not ( ( not not True ) ) = not ( ( A ) )\" where \"A = not not True\". Let's evaluate A: A=not not True = not (not True) = not False = True. Plugging in A, we get: Z = not ( ( A ) ) = not ( ( True ) ) = not True = False. So the answer is False.\n\n## Q: True and False and not True and True is\n\nA: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = True and False and not True and True = A and B\" where \"A = True and False\" and \"B = not True and True\". Let's evaluate A: A = True and False = False. Let's evaluate B: B = not True and True = not (True and True) = not (True) = False. Plugging in A and B, we get: Z = A and B = False and False = False. So the answer is False.\n\nQ: not not ( not ( False ) ) is\n\nA: Let's think step by step.\n\nRemember that (i) expressions inside brackets are always evaluated first and that (ii) the order of operations from highest priority to lowest priority is \"not\", \"and\", \"or\", respectively. We first simplify this expression \"Z\" as follows: \"Z = not not ( not ( False ) ) = not not ( A )\" where \"A = not ( False )\". Let's evaluate A: A = not ( False ) = not False = True. Plugging in A, we get: Z = not not ( A ) = not not (True) = not not False = True. So the answer is True.\n\nQ: False and False and False or not False is A: Let's think step by step.\n\nTable 20 | An example of BBH.\n\n## PROMPT\n\n                           \n\n                           \\_\\_\\_\\_ \n\n- A.     \n\n- B.     \n\n- C.       \n\n- D.     \n\n  \n\nC\n\n                          \\_\\_\\_\\_ \n\n- A.     \n\n- B.     \n\n- C.     \n\n- D.     \n\n  \n\nA\n\n-         \\_\\_\\_\\_ \n\n- A.           \n\n- B.           \n\n- C.           \n\n- D.           \n\n\n\n\n\n\n\nB\n\n-                        \\_\\_\\_\\_ \n\n- A.         \n\n- B.              \n\n- C.          \n\n- D.             \n\n  \n\nB\n\n                                                 \\_\\_\\_\\_ \n\n- A.      \n\n- B.   \n\n- C.    \n\n- D.     \n\n  \n\nB\n\n                             \\_\\_\\_\\_\n\n- A.           \n\n- B.            \n\n- C.          \n\n- D.            \n\n-   \n\n## OPTIONS\n\n- A\n\n- B\n\n- C\n\n- D\n\nTable 21 | An example of C-Eval.\n\n## PROMPT\n\n        ?\n\n           \n\n         \n\n    ?   \n\n## OPTIONS\n\n- - \n- - \n- - \n- - \n\n## PROMPT\n\n                                                                                               \n\n           \n\n## OPTIONS\n\n- -      \n- -      \n- -     \n- -      \n\nTable 22 | An example of C3.\n\nTable 23 | An example of CCPM.\n\n## PROMPT\n\nQ:     '    -        '             8000        1500           200       1600             3  5         \n\nA:     1500           200         1500+200=1700               8000              =     -              8000-15001700-1600=3200                 3  5        3200/(3+5)*3=1200         1200 \n\nQ:                         800                    100              \n\nA:                                    -       =100       100+800=900         900 \n\nQ: A   B                5                       3      A       135    B       165              \n\nA:   A      x       B      y          A  B    A     5    A    3         135    B    3          165             =5x+5y=135+8x=165+8y       10(x+y)=300+8(x+y)    x+y=150       5(x+y)=750          750 \n\nQ:        10             10                4     6                         \n\nA:\n\nTable 24 | An example of CMATH.\n\n## PROMPT\n\n                          \n\n           \n\nA.  \n\nB.  \n\nC.   \n\nD.   \n\n   \n\nB\n\n           \n\nA.   \n\nB.  \n\nC.   \n\nD.    \n\n   \n\nB\n\n           \n\nA.  \n\nB.   \n\nC.   \n\nD.   \n\n   \n\nD\n\n      \n\nA.   \n\nB.   \n\nC.   \n\nD.   \n\n   \n\nC\n\n         \n\nA.  \n\nB.  \n\nC.  \n\nD.  \n\n   \n\nB\n\n              \n\nA.   \n\nB.    \n\nC.    \n\nD.    \n\n   \n\n## OPTIONS\n\n- A\n\n- B\n\n- C\n\n- D\n\nTable 25 | An example of CMMLU.\n\n## PROMPT\n\nPassage: The median age in the city was 22.1 years. 10.1% of residents were under the age of 18; 56.2% were between the ages of 18 and 24; 16.1% were from 25 to 44; 10.5% were from 45 to 64; and 7% were 65 years of age or older. The gender makeup of the city was 64.3% male and 35.7% female.\n\nAnswer the following questions based on the above passage, please calculate carefully if calculation is necessary.\n\nQ: How many percent were not from 25 to 44?\n\nA: The answer type is number. So according to above Passage, the answer is 83.9.\n\nQ: How many in percent weren't 25 to 44?\n\nA: The answer type is number. So according to above Passage, the answer is\n\nTable 26 | An example of DROP.\n\n## PROMPT\n\n   12  7       6    ,         ,                   (Joseph Varon)       260  ,        2              ,            ,        ' \n\n## OPTIONS\n\n- -    ' \n- -   ' \n- -    ' \n- -    ' \n- -    ' \n- -    ' \n- -   ' \n\nTable 27 | An example of CHID.\n\n## PROMPT\n\n                                                       \n\n       \"  \"   \n\n  \n\n                                \n\n       \"   \"     \n\n'       JulesTellier                                                                   '\n\n         \n\n     \n\n                                                                  \"  \"   \n\n    \n\n                                                                    \"   \"   \n\n \n\n                             1962         '\n\n       \"  \"   \n\nTable 28 | An example of CLUEWSC.\n\n\" \" \n\n## PROMPT\n\nQ: Max can mow the lawn in 40 minutes. If it takes him twice that long to fertilize the lawn, how long will it take him to both mow and fertilize the lawn?\n\nA: Let's think step by step. It takes Max 2 * 40 minutes = 80 minutes to fertilize the lawn. In total, Max takes 80 minutes + 40 minutes = 120 minutes to both mow and fertilize the lawn. The answer is 120.\n\nQ: The bagels cost $2.25 each, or a dozen for $24. How much is saved, per bagel, in cents, by buying a dozen at a time?\n\nA: Let's think step by step. They cost 2.25*100=225 cents each. At the bulk rate, they are 24/12=2 dollar each. They cost 2*100=200 cents each. 225-200=25 cents are saved per bagel. The answer is 25.\n\nQ: Tim is 5 years old. His cousin, Rommel, is thrice as old as he is. His other cousin, Jenny, is 2 years older than Rommel. How many years younger is Tim than Jenny? A: Let's think step by step. Rommel is 5 x 3 = 15 years old. Jenny is 15 + 2 = 17 years old. So, Tim is 17 - 5 = 12 years younger than Jenny. The answer is 12.\n\nQ: The school has 14 boys and 10 girls. If 4 boys and 3 girls drop out, how many boys and girls are left?\n\nA: Let's think step by step. There are 14 boys - 4 boys = 10 boys left. There are 10 girls - 3 girls = 7 girls left. In total there are 10 boys + 7 girls = 17 boys and girls left. The answer is 17.\n\nQ: Building one birdhouse requires 7 planks and 20 nails. If 1 nail costs 0.05, and one plank costs 3, what is the cost, in dollars, to build 4 birdhouses?\n\nA: Let's think step by step. The cost of the planks for one birdhouse is 7 * 3 = 21. And the nails are a cost of 20 * 0.05 = 1 for each birdhouse. So to build one birdhouse one will need 21 + 1 = 22. So the cost of building 4 birdhouses is at 4 * 22 = 88. The answer is 88.\n\nQ: Danny brings 3 watermelons to his family picnic. He cuts each watermelon into 10 slices. His sister brings 1 watermelon to the family picnic, and she cuts the watermelon into 15 slices. How many watermelon slices are there in total at the picnic?\n\nA: Let's think step by step. From Danny, there are 3 * 10 = 30 watermelon slices. From his sister, there are 1 * 15 = 15 watermelon slices. There are a total of 30 + 15 = 45 watermelon slices. The answer is 45.\n\nQ: Angela is a bike messenger in New York. She needs to deliver 8 times as many packages as meals. If she needs to deliver 27 meals and packages combined, how many meals does she deliver?\n\nA: Let's think step by step. Let p be the number of packages Angela delivers and mbe the number of meals. We know that p + m = 27 and p = 8m. Substituting the second equation into the first equation, we get 8m + m = 27. Combining like terms, we get 9m = 27. Dividing both sides by 9, we get m = 3. The answer is 3.\n\nQ: Cori is 3 years old today. In 5 years, she will be one-third the age of her aunt. How old is her aunt today?\n\nA: Let's think step by step. In 5 years, Cori will be 3 + 5 = 8 years old. In 5 years, Cori's aunt will be 8 x 3 = 24 years old. Today, her aunt is 24 - 5 = 19 years old. The answer is 19.\n\nQ: Indras has 6 letters in her name. Her sister's name has 4 more letters than half of the letters in Indras' name. How many letters are in Indras and her sister's names? A: Let's think step by step.\n\n## PROMPT\n\nPlaying piano: A man is seated at a piano. He\n\n## OPTIONS\n\n- is playing the piano with his hands and his face.\n- bigins to play a song by timbaland on the piano.\n- plays slowly, and pauses to snap his fingers.\n- is playing a song in front of him.\n\nTable 30 | An example of HellaSwag.\n\n## PROMPT\n\ndef starts\\_one\\_ends(n):\n\n\"\"\"\n\nGiven a positive integer n, return the count of the numbers of n-digit positive integers that start or end with 1. \"\"\"\n\nTable 31 | An example of HumanEval.\n\n```\nPROMPT Problem: Find the domain of the expression $\\frac{\\sqrt{x-2}}{\\sqrt{5-x}}$.} Solution: The expressions inside each square root must be non-negative. Therefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge 0$, so $x \\le 5$. Also, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$. Therefore, the domain of the expression is $\\boxed{[2,5)}$. Final Answer: The final answer is $[2,5)$. I hope it is correct. Problem: If $\\det \\mathbf{A} = 2$ and $\\det \\mathbf{B} = 12,$ then find $\\det (\\mathbf{A} \\mathbf{B}).$ Solution: We have that $\\det (\\mathbf{A} \\mathbf{B}) = (\\det \\mathbf{A})(\\det \\mathbf{B}) = (2)(12) = \\boxed{24}.$ Final Answer: The final answer is $24$. I hope it is correct. Problem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight? Solution: If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight. If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight. Equating this to 480 pounds, we can solve for $n$: \\begin{align*} 30n&=480\\\\ \\Rightarrow\\qquad n&=480/30=\\boxed{16} \\end{align*} Final Answer: The final answer is $16$. I hope it is correct. Problem: If the system of equations \\begin{align*} 6x-4y&=a,\\\\ 6y-9x &=b. \\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac{a}{b},$ assuming $b$ is nonzero. Solution: If we multiply the first equation by $-\\frac{3}{2}$, we obtain $$6y-9x=-\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have $$-\\frac{3}{2}a=b\\Rightarrow\\frac{a}{b}=\\boxed{-\\frac{2}{3}}.$$ Final Answer: The final answer is $-\\frac{2}{3}$. I hope it is correct. Problem: Evaluate $\\log_21$. Solution:\n```\n\n## PROMPT\n\nYou are an expert Python programmer, and here is your task: Write a function\n\n```\nto find the similar elements from the given two tuple lists. Your code should pass these tests: assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5) assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4) assert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14) [BEGIN] def similar_elements(test_tup1, test_tup2): res = tuple(set(test_tup1) & set(test_tup2)) return (res) [DONE] You are an expert Python programmer, and here is your task: Write a python function to identify non-prime numbers. Your code should pass these tests: assert is_not_prime(2) == False assert is_not_prime(10) == True assert is_not_prime(35) == True [BEGIN] import math def is_not_prime(n): result = False for i in range(2,int(math.sqrt(n)) + 1): if n % i == 0: result = True return result [DONE] You are an expert Python programmer, and here is your task: Write a function to find the largest integers from a given list of numbers using heap queue algorithm. Your code should pass these tests: assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65] assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75] assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35] [BEGIN] import heapq as hq def heap_queue_largest(nums,n): largest_nums = hq.nlargest(n, nums) return largest_nums [DONE] You are an expert Python programmer, and here is your task: Write a function to return the sum of all divisors of a number. Your code should pass these tests: assert sum_div(8)==7 assert sum_div(12)==16 assert sum_div(7)==1 [BEGIN]\n```\n\nTable 33 | An example of MBPP.\n\n## PROMPT\n\nThe following are multiple choice questions (with answers) about miscellaneous.\n\nHow many axles does a standard automobile have?\n\nA. one\n\nB. two\n\nC. four\n\nD. eight\n\nAnswer: B\n\nWhat place is named in the title of the 1979 live album by rock legends Cheap\n\nTrick?\n\nA. Budapest\n\nB. Budokan\n\nC. Bhutan\n\nD. Britain\n\nAnswer: B\n\nWho is the shortest man to ever win an NBA slam dunk competition?\n\nA. Anthony 'Spud' Webb\n\nB. Michael 'Air' Jordan\n\nC. Tyrone 'Muggsy' Bogues\n\nD. Julius 'Dr J' Erving\n\nAnswer: A\n\nWhat is produced during photosynthesis?\n\nA. hydrogen\n\nB. nylon\n\nC. oxygen\n\nD. light\n\nAnswer: C\n\nWhich of these songs was a Top 10 hit for the rock band The Police?\n\nA. 'Radio Ga-Ga'\n\nB. 'Ob-la-di Ob-la-da'\n\nC. 'De Do Do Do De Da Da Da'\n\nD. 'In-a-Gadda-Da-Vida'\n\nAnswer: C\n\nWhich of the Three Stooges was not related to the others?\n\nA. Moe\n\nB. Larry\n\nC. Curly\n\nD. Shemp\n\nAnswer:\n\n## OPTIONS\n\n- A\n\n- B\n\n- C\n\n- D\n\nTable 34 | An example of MMLU.\n\n## PROMPT\n\nAnswer these questions:\n\n- Q: Who is hosting the fifa world cup in 2022?\n- A: Qatar\n- Q: Who won the first women 's fifa world cup?\n- A: United States\n- Q: When did miami vice go off the air?\n- A: 1989\n- Q: Who wrote the song shout to the lord?\n- A: Darlene Zschech\n- Q: Who was thrown in the lion 's den?\n- A: Daniel\n- Q: What is the meaning of the name habib?\n- A:\n\n## PROMPT\n\nA woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites\n\n## OPTIONS\n\n- flowers blooming\n- grass turning brown\n- trees growing\n- blossoms blooming\n\nTable 36 | An example of OpenBookQA.\n\n## PROMPT\n\nTo make it easier to push the reset button of the garbage disposable machine which is located underneath the machine,\n\n## OPTIONS\n\n- place a wall mirror on the floor of the cabinet\n- hold a hand mirror under the garbage disposable machine\n\nTable 37 | An example of PIQA.\n\nTable 35 | An example of NaturalQuestions.\n\n## PROMPT\n\nArticle:\n\nWhen you read an article you will understand and remember it better if you can work out how the writer has put the ideas together. Sometimes a writer puts ideas together by asking questions and then answering them. For example, if the article is about groundhogs, the set of questions in the writer's head might be:\n\nWhat does a groundhog look like?\n\nWhere do groundhogs live?\n\nWhat do they eat?...\n\nIn the article,the author might answer those questions.\n\nSometimes an author writes out her questions in the article. These questions give you signals. They tell you what the author is going to write next.Often an author has a question in her head but she doesn't write it out for you. You have to work out her question for yourself. Here's a sample reading for you to practice this method.\n\nEarthworms\n\nDo you know how many kinds of earthworms there are? There are about 1800 kinds in the world! They can be brown, purple, green. They can be as small as 3 cm long and as large as 3 m long.\n\nThe best time to see earthworms is at night, especially a cool, damp night. That's when they come up from their burrows to hunt for food. Earthworms don't like to be in the sun. That's because they breathe through their skin,and they can't breathe if their skin gets too dry. Earthworms must come out of the earth if it rains a lot, because they can't breathe in their flooded burrows. What a dangerous life!\n\nEarthworms don't have eyes, so how can they tell when it's dark? They have special places on their skin that are sensitive to light. These spots tell whether it's light or dark. If you shine a flashlight on an earthworm at night,it will quickly disappear into the ground.\n\nEarthworms don't have ears either, but they can hear by feeling movements in the earth.If you want to hear like an earthworm, lie on the ground with your fingers in your ears. Then have a friend stamp his or her feet near you. This is how earthworms feel birds and people walking, and moles digging, near them.\n\nEarthworms are useful. Farmers and gardeners like having lots of earthworms in their land because the worms help to make better soil when they dig. That digging keeps the soil loose and airy. In one year earthworms can pile up as much as 23,000 kg of castings in an area about the size of a football field.\n\n- Q: What's the purpose of reading Earthworms?\n- A: To put the writer's idea into real use.\n- Q: Which question CANNOT be answered in the passage?\n- A: Why can human listen like earthworms?\n- Q: How can you understand Earthworms better according to this passage?\n- A: Read to work out all the questions in the writer's head while reading.\n- Q: What's the best title for the passage?\n\nA:\n\n## OPTIONS\n\n- One way to help with understanding\n- One way to practice with a new idea\n- One way to learn to be a wise writer\n- One way to be clearer about worms\n\n## PROMPT\n\nAnswer these questions:\n\nQ: A Jayhawker was a term applied to anti-slavery militant bands from a certain US state that clashed with pro-slavery factions from Missouri. Which state is this, sometimes referred to as the Jayhawk State?\n\nA: Kans.\n\nQ: Which Swedish DJ and record producer had a UK Number One single in 2013 with 'Wake Me Up'?\n\nA: Tim Bergling\n\nQ: Who is the MP for Sheffield Hallam?\n\nA: Nick clegg\n\nQ: A case that riveted the nation, the case of The State of Tennessee v. John Thomas Scopes concluded on July 21, 1925, with the jury finding Mr. Scopes guilty of teaching what?\n\nA: Survival of species\n\nQ: What cartoon series featured a character called Little My?\n\nA: Muumi\n\nQ: \"What English model, with her short-haired androgynous look, born Lesley Hornby, was discovered in 1966 by Nigel Davies when she was 16 and weighed 6 stone (41 kg, 91 lbs), and became \"\"The Face of '66\"\" with her high fashion mod look created by Mary Quant?\"\n\nA:\n\n## PREFIXES\n\n- So Monica\n\n- So Jessica\n\n## COMPLETION\n\navoids eating carrots for their eye health because Emily needs good eyesight while Monica doesn't.\n\nTable 40 | An example of WinoGrande. Note that there are multiple prefixes and only one completion for WinoGrande, and we choose the predicted prefix with the lowest perplexity of the completion.\n\nTable 39 | An example of TriviaQA.",
  "tables": [
    {
      "index": 0,
      "markdown": "| 1 Introduction   | 1 Introduction                          | 1 Introduction                                                                                                                                                                           | 1 Introduction                                                                                                                                                                           | 3     |\n|------------------|-----------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|\n| 2                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          |       |\n|                  | Pre-Training                            | Pre-Training                                                                                                                                                                             | Pre-Training                                                                                                                                                                             | 4     |\n|                  | 2.1                                     | Data . . . . . . . .                                                                                                                                                                     | . . . . . . . . . . . . . . . . . .                                                                                                                                                      | 4     |\n|                  | 2.2                                     | Architecture                                                                                                                                                                             | . . . . . . . . . . . . . . . . . . . . .                                                                                                                                                | 5     |\n|                  | 2.3                                     | Hyperparameters . . .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 5     |\n|                  | 2.4                                     | Infrastructures                                                                                                                                                                          | . . . . . . . . . . . . . . . . . . . .                                                                                                                                                  | 6     |\n| 3                | Scaling Laws 7                          | Scaling Laws 7                                                                                                                                                                           | Scaling Laws 7                                                                                                                                                                           |       |\n|                  | 3.1                                     | Scaling Laws for Hyperparameters                                                                                                                                                         | . . . . . . . .                                                                                                                                                                          | 8     |\n|                  | 3.2                                     | Estimating Optimal Model and Data Scaling                                                                                                                                                | . .                                                                                                                                                                                      | 9     |\n|                  | 3.3                                     | Scaling Laws with Different Data                                                                                                                                                         | . . . . . . . . .                                                                                                                                                                        | 12    |\n| 4                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 12    |\n|                  | Alignment                               | Alignment                                                                                                                                                                                | Alignment                                                                                                                                                                                |       |\n| 5                | 5.1                                     | Public Benchmark                                                                                                                                                                         | Evaluation . . . . . . . . . . .                                                                                                                                                         | 13    |\n|                  |                                         | 5.1.1                                                                                                                                                                                    | Base Model . . . . . . . . . . . . . . . . .                                                                                                                                             | 14    |\n|                  |                                         | 5.1.2 . .                                                                                                                                                                                | Chat Model . . . . . . . . . . . . . . .                                                                                                                                                 | 14    |\n|                  | 5.2                                     | Open-Ended                                                                                                                                                                               | Evaluation . . . . . . . . . . . . . .                                                                                                                                                   | 17    |\n|                  |                                         | 5.2.1 Chinese Open-Ended Evaluation                                                                                                                                                      | . . . . .                                                                                                                                                                                | 17    |\n|                  |                                         | 5.2.2 English                                                                                                                                                                            | Open-Ended Evaluation . . . . .                                                                                                                                                          |       |\n|                  | 5.3                                     | Held-Out Evaluation .                                                                                                                                                                    | . . . . . . . . . . . . . . .                                                                                                                                                            | 18 18 |\n|                  |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 19    |\n|                  | 5.4 5.5                                 | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | Safety Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 20    |\n| 6                | Conclusion, Limitation, and Future Work | Conclusion, Limitation, and Future Work                                                                                                                                                  | Conclusion, Limitation, and Future Work                                                                                                                                                  | 23    |\n| A                |                                         |                                                                                                                                                                                          |                                                                                                                                                                                          | 30    |\n|                  | Appendix                                | Appendix                                                                                                                                                                                 | Appendix                                                                                                                                                                                 |       |\n|                  | A.1                                     | Acknowledgments . .                                                                                                                                                                      | . . . . . . . . . . . . . . .                                                                                                                                                            | 30    |\n|                  | A.2                                     | Different Model Scale Representations                                                                                                                                                    | . . . . . .                                                                                                                                                                              | 30    |\n|                  | A.3                                     | Benchmark Metrics Curves                                                                                                                                                                 | . . . . . . . . . . . .                                                                                                                                                                  | 31    |\n|                  | A.4                                     | Comparison with Code or Math Specific Models                                                                                                                                             |                                                                                                                                                                                          | 32    |\n|                  | A.5                                     | Benchmark Results w/ DPO                                                                                                                                                                 | Stage . . . . . . . .                                                                                                                                                                    | 32    |\n|                  | A.6                                     | Evaluation Formats . .                                                                                                                                                                   | . . . . . . . . . . . . . . .                                                                                                                                                            | 32    |"
    },
    {
      "index": 1,
      "markdown": "| Dumps Used             |    1 |    2 |    6 |   12 |   16 |   22 |   41 |   91 |\n|------------------------|------|------|------|------|------|------|------|------|\n| Deduplication Rate (%) | 22.2 | 46.7 | 55.7 | 69.9 | 75.7 | 76.3 | 81.6 | 89.8 |"
    },
    {
      "index": 2,
      "markdown": "| Params   |    layers |    model |    heads |    kv_heads |   Context Length |   Sequence Batch Size |   Learning Rate | Tokens   |\n|----------|------------|-----------|-----------|--------------|------------------|-----------------------|-----------------|----------|\n| 7B       |         30 |      4096 |        32 |           32 |             4096 |                  2304 |         0.00042 | 2.0T     |\n| 67B      |         95 |      8192 |        64 |            8 |             4096 |                  4608 |         0.00032 | 2.0T     |"
    },
    {
      "index": 3,
      "markdown": "|    layers |    model |    vocab |  seq   |  1   |  2   |      |   6  1  |   6  2  |\n|------------|-----------|-----------|---------|-------|-------|-------|-----------|-----------|\n|          8 |       512 |    102400 | 4096    | 25.2M | 77.6M | 352M  |      0.43 |      1.32 |\n|         12 |       768 |    102400 | 4096    | 84.9M | 164M  | 963M  |      0.53 |      1.02 |\n|         24 |      1024 |    102400 | 4096    | 302M  | 407M  | 3.02B |      0.6  |      0.81 |\n|         24 |      2048 |    102400 |         | 1.21B | 1.42B | 9.66B |      0.75 |      0.88 |\n|         32 |      4096 |    102400 |         | 6.44B | 6.86B | 45.1B |      0.85 |      0.91 |\n|         40 |      5120 |    102400 |         | 12.6B | 13.1B | 85.6B |      0.88 |      0.92 |\n|         80 |      8192 |    102400 |         | 64.4B | 65.3B | 419B  |      0.92 |      0.94 |"
    },
    {
      "index": 4,
      "markdown": "| Approach                 |   Coeff.  where  opt (  opt )    |   Coeff.  where  opt    |\n|--------------------------|----------------------------------------|------------------------------|\n| OpenAI (OpenWebText2)    |                                  0.73  |                        0.27  |\n| Chinchilla (MassiveText) |                                  0.49  |                        0.51  |\n| Ours (Early Data)        |                                  0.45  |                        0.55  |\n| Ours (Current Data)      |                                  0.524 |                        0.476 |\n| Ours (OpenWebText2)      |                                  0.578 |                        0.422 |"
    },
    {
      "index": 5,
      "markdown": "| Language   | Benchmark        | Test-shots   |   LLaMA2 7B |   DeepSeek 7B |   LLaMA2 70B |   DeepSeek 67B |\n|------------|------------------|--------------|-------------|---------------|--------------|----------------|\n| English    | HellaSwag        | 0-shot       |      75.6   |        75.4   |       84     |         84     |\n| English    | PIQA             | 0-shot       |      78     |        79.2   |       82     |         83.6   |\n| English    | WinoGrande       | 0-shot       |      69.6   |        70.5   |       80.4   |         79.8   |\n| English    | RACE-Middle      | 5-shot       |      60.7   |        63.2   |       70.1   |         69.9   |\n| English    | RACE-High        | 5-shot       |      45.8   |        46.5   |       54.3   |         50.7   |\n| English    | TriviaQA         | 5-shot       |      63.8   |        59.7   |       79.5   |         78.9   |\n| English    | NaturalQuestions | 5-shot       |      25.5   |        22.2   |       36.1   |         36.6   |\n| English    | MMLU             | 5-shot       |      45.8   |        48.2   |       69     |         71.3   |\n| English    | ARC-Easy         | 0-shot       |      69.1   |        67.9   |       76.5   |         76.9   |\n|            | ARC-Challenge    | 0-shot       |      49     |        48.1   |       59.5   |         59     |\n|            | OpenBookQA       | 0-shot       |      57.4   |        55.8   |       60.4   |         60.2   |\n|            | DROP             | 1-shot       |      39.8   |        41     |       69.2   |         67.9   |\n|            | MATH             | 4-shot       |       2.5   |         6     |       13.5   |         18.7   |\n|            | GSM8K            | 8-shot       |      15.5   |        17.4   |       58.4   |         63.4   |\n|            | HumanEval        | 0-shot       |      14.6   |        26.2   |       28.7   |         42.7   |\n|            | MBPP             | 3-shot       |      21.8   |        39     |       45.6   |         57.4   |\n|            | BBH              | 3-shot       |      38.5   |        39.5   |       62.9   |         68.7   |\n|            | AGIEval          | 0-shot       |      22.8   |        26.4   |       37.2   |         41.3   |\n|            | Pile-test        | -            |       0.741 |         0.725 |        0.649 |          0.642 |\n| Chinese    | CLUEWSC          | 5-shot       |      64     |        73.1   |       76.5   |         81     |\n| Chinese    | CHID             | 0-shot       |      37.9   |        89.3   |       55.5   |         92.1   |\n| Chinese    | C-Eval           | 5-shot       |      33.9   |        45     |       51.4   |         66.1   |\n|            | CMMLU            | 5-shot       |      32.6   |        47.2   |       53.1   |         70.8   |\n|            | CMath            | 3-shot       |      25.1   |        34.5   |       53.9   |         63     |\n|            | C3               | 0-shot       |      47.4   |        65.4   |       61.7   |         75.3   |\n|            | CCPM             | 0-shot       |      60.7   |        76.9   |       66.2   |         88.5   |"
    },
    {
      "index": 6,
      "markdown": "| Language   | Benchmark        |   DeepSeek 7B Base |   DeepSeek 7B Chat |   DeepSeek 67B Base |   DeepSeek 67B Chat |\n|------------|------------------|--------------------|--------------------|---------------------|---------------------|\n| English    | HellaSwag        |               75.4 |               68.5 |                84   |                75.7 |\n| English    | PIQA             |               79.2 |               77.6 |                83.6 |                82.6 |\n| English    | WinoGrande       |               70.5 |               66.9 |                79.8 |                76   |\n| English    | RACE-Middle      |               63.2 |               65.2 |                69.9 |                70.9 |\n| English    | RACE-High        |               46.5 |               50.8 |                50.7 |                56   |\n| English    | TriviaQA         |               59.7 |               57.9 |                78.9 |                81.5 |\n| English    | NaturalQuestions |               22.2 |               32.5 |                36.6 |                47   |\n| English    | MMLU             |               48.2 |               49.4 |                71.3 |                71.1 |\n| English    | ARC-Easy         |               67.9 |               71   |                76.9 |                81.6 |\n| English    | ARC-Challenge    |               48.1 |               49.4 |                59   |                64.1 |\n| English    | GSM8K            |               17.4 |               63   |                63.4 |                84.1 |\n| English    | MATH             |                6   |               15.8 |                18.7 |                32.6 |\n| English    | HumanEval        |               26.2 |               48.2 |                42.7 |                73.8 |\n| English    | MBPP             |               39   |               35.2 |                57.4 |                61.4 |\n| English    | DROP             |               41   |               49.1 |                67.9 |                71.9 |\n| English    | OpenBookQA       |               55.8 |               54.8 |                60.2 |                63.2 |\n| English    | BBH              |               39.5 |               42.3 |                68.7 |                71.7 |\n| English    | AGIEval          |               26.4 |               19.3 |                41.3 |                46.4 |\n| Chinese    | CLUEWSC          |               73.1 |               71.9 |                81   |                60   |\n| Chinese    | CHID             |               89.3 |               64.9 |                92.1 |                72.6 |\n| Chinese    | C-Eval           |               45   |               47   |                66.1 |                65.2 |\n| Chinese    | CMMLU            |               47.2 |               49.7 |                70.8 |                67.8 |\n| Chinese    | CMath            |               34.5 |               68.4 |                63   |                80.3 |\n| Chinese    | C3               |               65.4 |               66.4 |                75.3 |                77   |\n| Chinese    | CCPM             |               76.9 |               76.5 |                88.5 |                84.9 |"
    },
    {
      "index": 7,
      "markdown": "| Model                     | Overall     | Reasoning       | Reasoning       | Reasoning       | Language       | Language       | Language       | Language       | Language       | Language       | Language       |\n|---------------------------|---------------|---------------------|---------------------|---------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|\n|                         | Overall     | Avg.            | Math.           | Logi.           | Avg.           | Fund.          | Chi.           | Open.          | Writ.          | Role.          | Pro.           |\n| gpt-4-1106-preview        | 8.01          | 7.73                | 7.80                | 7.66                | 8.29               | 7.99               | 7.33               | 8.61               | 8.67               | 8.47               | 8.65               |\n| gpt-4-0613                | 7.53          | 7.47                | 7.56                | 7.37                | 7.59               | 7.81               | 6.93               | 7.42               | 7.93               | 7.51               | 7.94               |\n| DeepSeek-67B-Chat-DPO*    | 6.69          | 5.77                | 6.13                | 5.41                | 7.60               | 7.29               | 7.47               | 7.82               | 7.51               | 7.83               | 7.71               |\n| DeepSeek-67B-Chat*        | 6.43          | 5.75                | 5.71                | 5.79                | 7.11               | 7.12               | 6.52               | 7.58               | 7.20               | 6.91               | 7.37               |\n| chatglm-turbo       | 6.24          | 5.00                | 4.74                | 5.26                | 7.49               | 6.82               | 7.17               | 8.16               | 7.77               | 7.76               | 7.24               |\n| erniebot-3.5        | 6.14          | 5.15                | 5.03                | 5.27                | 7.13               | 6.62               | 7.60               | 7.26               | 7.56               | 6.83               | 6.90               |\n| gpt-3.5-turbo-0613        | 6.08          | 5.35                | 5.68                | 5.02                | 6.82               | 6.71               | 5.81               | 7.29               | 7.03               | 7.28               | 6.77               |\n| chatglm-pro         | 5.83          | 4.65                | 4.54                | 4.75                | 7.01               | 6.51               | 6.76               | 7.47               | 7.07               | 7.34               | 6.89               |\n| spark_desk_v2       | 5.74          | 4.73                | 4.71                | 4.74                | 6.76               | 5.84               | 6.97               | 7.29               | 7.18               | 6.92               | 6.34               |\n| Qwen-14B-Chat             | 5.72          | 4.81                | 4.91                | 4.71                | 6.63               | 6.90               | 6.36               | 6.74               | 6.64               | 6.59               | 6.56               |\n| Baichuan2-13B-Chat        | 5.25          | 3.92                | 3.76                | 4.07                | 6.59               | 6.22               | 6.05               | 7.11               | 6.97               | 6.75               | 6.43               |\n| ChatGLM3-6B               | 4.97          | 3.85                | 3.55                | 4.14                | 6.10               | 5.75               | 5.29               | 6.71               | 6.83               | 6.28               | 5.73               |\n| Baichuan2-7B-Chat         | 4.97          | 3.66                | 3.56                | 3.75                | 6.28               | 5.81               | 5.50               | 7.13               | 6.84               | 6.53               | 5.84               |\n| InternLM-20B              | 4.96          | 3.66                | 3.39                | 3.92                | 6.26               | 5.96               | 5.50               | 7.18               | 6.19               | 6.49               | 6.22               |\n| Qwen-7B-Chat              | 4.91          | 3.73                | 3.62                | 3.83                | 6.09               | 6.40               | 5.74               | 6.26               | 6.31               | 6.19               | 5.66               |\n| ChatGLM2-6B               | 4.48          | 3.39                | 3.16                | 3.61                | 5.58               | 4.91               | 4.52               | 6.66               | 6.25               | 6.08               | 5.08               |\n| InternLM-Chat-7B          | 3.65          | 2.56                | 2.45                | 2.66                | 4.75               | 4.34               | 4.09               | 5.82               | 4.89               | 5.32               | 4.06               |\n| Chinese-LLaMA-2-7B-Chat   | 3.57          | 2.68                | 2.29                | 3.07                | 4.46               | 4.31               | 4.26               | 4.50               | 4.63               | 4.91               | 4.13               |\n| LLaMA-2-13B-Chinese-Chat  | 3.35          | 2.47                | 2.21                | 2.73                | 4.23               | 4.13               | 3.31               | 4.79               | 3.93               | 4.53               | 4.71               |"
    },
    {
      "index": 8,
      "markdown": "| Model                    |   STEM |   Humanities |   Reasoning |   Coding |   Math |   Extraction |   Roleplay |   Writing |   Average |\n|--------------------------|--------|--------------|-------------|----------|--------|--------------|------------|-----------|-----------|\n| GPT-4-1106-preview      |   9.9  |         9.95 |        8.1  |     9.05 |   7.95 |         9.9  |       9.5  |      9.7  |      9.26 |\n| GPT-3.5-turbo-0613      |   9.55 |         9.95 |        6.2  |     7.05 |   7.05 |         9    |       8.65 |      9.65 |      8.39 |\n| LLAMA-2-Chat 7B         |   8.65 |         8.75 |        4.25 |     3    |   2.4  |         6.5  |       7.7  |      8.9  |      6.27 |\n| LLAMA-2-Chat 13B        |   8.63 |         9.75 |        5.1  |     3    |   3.45 |         6.93 |       7.5  |      8.85 |      6.65 |\n| LLAMA-2-Chat 70B        |   8.93 |         9.63 |        5.8  |     3.15 |   3.3  |         7.25 |       7.5  |      9.3  |      6.86 |\n| Zephyr-Beta 7B          |   9.03 |         9.63 |        5.6  |     5.1  |   4.45 |         7.45 |       8.2  |      9.35 |      7.35 |\n| Xwin 70b v0.1           |   9.68 |         9.95 |        6.55 |     4.25 |   3.3  |         8.75 |       8.25 |      9.55 |      7.53 |\n| Xwin 13b v0.2           |   9.55 |         9.88 |        5.2  |     3.6  |   2.85 |         7.7  |       8.6  |      8.68 |      7.01 |\n| TLU 2+DPO 70B          |   9    |         9.9  |        7    |     4.7  |   4.65 |         9.35 |       9.25 |      9.25 |      7.89 |\n| DeepSeek LLM 67B Chat    |   9.6  |         9.7  |        8    |     7.35 |   6.25 |         8.4  |       8.2  |      9.3  |      8.35 |\n| DeepSeek LLM 67B ChatDPO |   9.7  |         9.8  |        9.05 |     6.75 |   6.65 |         9.3  |       9.1  |      9.75 |      8.76 |"
    },
    {
      "index": 9,
      "markdown": "| Model                 |   LeetCode |   Hungarian Exam |   IFEval |\n|-----------------------|------------|------------------|----------|\n| GPT-4                 |       48.4 |             68   |     79.3 |\n| ChatGLM3 6B           |        2.4 |             32   |     29.7 |\n| DeepSeek LLM 7B Chat  |        4.7 |             28.5 |     41.2 |\n| Baichuan2-Chat 13B    |        1.6 |             19.5 |     44.5 |\n| Yi-Chat 34B           |        7.9 |             39   |     48.4 |\n| Qwen 72B Chat         |       12.7 |             52   |     50.8 |\n| DeepSeek LLM 67B Chat |       17.5 |             58   |     55.5 |"
    },
    {
      "index": 10,
      "markdown": "| Category                                                           | Subcategory                                                                                                                                                                                                                                                                                                   | #Safety Answers / #Total Cases   |\n|--------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n|       (Discrimination and Prejudice Questions)               |     (Ethnic and Racial),     (Religious Belief),    (Nationality and Geography),   (Gender),   (Age),   (Occupation),   (Health),       (Discrimination in Other Aspects)                                                                                                           | 486/500                          |\n|        (Infringement of Others' Legal Rights)              |     (Physical and Mental Health),     (Legitimate Property),    (Portrait Rights),    (Reputation Rights),    (Honor Rights),    (Privacy Rights),     (Information Rights),       (Other Legal Rights)                                                                         | 473/500                          |\n|          (Trade Secrets and Intellectual Property Rights) |        (Infringing Others' Intellectual Property Rights),           (Monopolistic and Unfair Competitive Actions),           (Other Commercially Illegal and Non-compliant Behaviors),       (Violating Business Ethics),        (Disclosing Others' Trade Secrets) | 281/300                          |\n|       (Illegal and Non-compliant Behavior)                   |     (Cults and Superstition),   (Pornography),   (Gambling),      (Drugs and Prohibited Items),     (Insults and Abuse),     (Violent Behavior),     (Involvement in Organized Crime),         (Other Illegal and Non-compliant Behaviors)                                  | 290/300                          |\n|       (Other Safety Issues)                                  |         (Issues of Illusion and Reality),     (Time-sensitive Issues),       (Self-recognition Problems),       (Other Sensitive Topics),                                                                                                                                            | 767/800                          |"
    },
    {
      "index": 11,
      "markdown": "| Model              |   Do-Not-Answer |\n|--------------------|-----------------|\n| LLAMA-2-7B-Chat    |            99.4 |\n| Claude             |            98.3 |\n| DeepSeek-67B-Chat* |            97.8 |\n| ChatGPT            |            97.7 |\n| GPT-4              |            96.5 |\n| Vicuna-7B          |            94.9 |\n| ChatGLM2           |            92.9 |"
    },
    {
      "index": 12,
      "markdown": "| Model                       |   HumanEval |   GSM8K |   Repetition |   IFEval |\n|-----------------------------|-------------|---------|--------------|----------|\n| DeepSeek LLM 7B Chat Stage1 |        48.2 |    63.9 |        0.02  |     38   |\n| DeepSeek LLM 7B Chat Stage2 |        48.2 |    63   |        0.014 |     41.2 |"
    },
    {
      "index": 13,
      "markdown": "| Model                   |   MMLU |   C-Eval |   CMMLU |   TriviaQA |   ChineseQA |\n|-------------------------|--------|----------|---------|------------|-------------|\n| DeepSeek LLM 7B Chat    |   49.4 |     47   |    49.7 |       57.9 |        75   |\n| DeepSeek LLM 7B Chat+MC |   60.9 |     71.3 |    73.8 |       57.9 |        74.4 |"
    },
    {
      "index": 14,
      "markdown": "| Model                                 |   MTBench |\n|---------------------------------------|-----------|\n| DeepSeek LLM 7B Chat                  |      7.15 |\n| DeepSeek LLM 7B Chat + System Prompt  |      7.11 |\n| DeepSeek LLM 67B Chat                 |      8.35 |\n| DeepSeek LLM 67B Chat + System Prompt |      8.58 |"
    },
    {
      "index": 15,
      "markdown": "| Model                    | Size                     | HumanEval                | HumanEval                | MBPP                     |\n|--------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                          |                          | Python                   | Multilingual             |                          |\n| Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       | Pre-Trained Models       |\n| Codex-001                | -                        | 33.5%                    | 26.1%                    | 45.9%                    |\n| StarCoder                | 16B                      | 36.0%                    | 28.7%                    | 46.8%                    |\n| CodeGeeX2                | 6B                       | 36.0%                    | 24.5%                    | 42.4%                    |\n| CodeLlama                | 7B                       | 31.7%                    | 29.2%                    | 41.6%                    |\n| CodeLlama                | 13B                      | 36.0%                    | 35.4%                    | 48.4%                    |\n| CodeLlama                | 34B                      | 48.2 %                   | 41.0 %                   | 55.2%                    |\n| DeepSeek-LLM-Base        | 67B                      | 42.7%                    | 37.2%                    | 57.4%                    |\n| Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models | Instruction-Tuned Models |\n| Wizard-Coder             | 34B                      | 73.2%                    | 48.8%                    | 61.2%                    |\n| DeepSeek-LLM-Chat        | 67B                      | 73.8%                    | 53.3 %                   | 61.4%                    |"
    },
    {
      "index": 16,
      "markdown": "|                                   | Inference                 | GSM8K                     | MATH                      | MGSM-zh                   | CMath                     |\n|-----------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|\n| Chain-of-Thoughts                 | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         | Chain-of-Thoughts         |\n| MetaMath 70B (Yu et al., 2023)    | CoT                       | 82.3%                     | 26.6%                     | 66.4%                     | 70.9%                     |\n| WizardMath 70B (Luo et al., 2023) | CoT                       | 81.6%                     | 22.7%                     | 64.8%                     | 65.4%                     |\n| DeepSeek LLM 67B Chat             | CoT                       | 84.1 %                    | 32.6 %                    | 74.0%                     | 80.3%                     |\n| Tool-Integrated Reasoning         | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning | Tool-Integrated Reasoning |\n| ToRA-Code 34B (Gou et al., 2023)  | Tool-Integrated           | 80.7%                     | 50.8%                     | 41.2%                     | 53.4%                     |\n| DeepSeek LLM 67B Chat             | Tool-Integrated           | 86.7%                     | 51.1%                     | 76.4%                     | 85.4%                     |"
    },
    {
      "index": 17,
      "markdown": "|                  |   DeepSeek 67B Chat |   DeepSeek 67B Chat DPO |\n|------------------|---------------------|-------------------------|\n| HellaSwag        |                75.7 |                    76.1 |\n| TriviaQA         |                81.5 |                    82.9 |\n| NaturalQuestions |                47   |                    48.8 |\n| MMLU             |                71.1 |                    70.9 |\n| GSM8K            |                84.1 |                    85.2 |\n| MATH             |                32.6 |                    30.2 |\n| HumanEval        |                73.8 |                    71.3 |\n| BBH              |                71.7 |                    70.8 |\n| AGIEval          |                46.4 |                    46.1 |\n| CEval            |                65.2 |                    64.3 |\n| CMMLU            |                67.8 |                    68.2 |"
    }
  ],
  "stats": {
    "pages": 48,
    "chunksCreated": 225,
    "totalCharacters": 144465,
    "totalWords": 21683,
    "numTables": 18,
    "processingTimeMs": 63792
  }
}