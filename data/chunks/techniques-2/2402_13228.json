{
  "paper": {
    "id": "2402.13228v2",
    "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
    "abstract": "Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.",
    "authors": [
      "Arka Pal",
      "Deep Karkhanis",
      "Samuel Dooley",
      "Manley Roberts",
      "Siddartha Naidu",
      "Colin White"
    ],
    "published": "2024-02-20T18:42:34.000Z",
    "updated": "2024-07-03T13:46:33.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2402.13228v2",
    "absUrl": "https://arxiv.org/abs/2402.13228v2"
  },
  "chunks": [
    {
      "id": "2402.13228v2-chunk-0",
      "content": "Arka Pal * , Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "chunkIndex": 0,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-1",
      "content": "Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-2",
      "content": "datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-3",
      "content": "Aligning large language models (LLMs) with preference data is important for their fluency and applicability to many tasks, with the natural language processing literature using many techniques to incorporate either human or ground-truth feedback [Christiano et al., 2017, Stiennon et al., 2020, Ouyang et al., 2022]. Typically in LLM alignment, we first collect large amounts of preference data, consisting of a context and two potential completions; one of these is labelled as the preferred completion, and the other as the dispreferred. We use this data to learn a general policy for generating completions in a given context. Direct Preference Optimisation (DPO) [Rafailov et al., 2023] is a popular method for learning from preference data, and it has shown to be effective at improving the performance of pretrained LLMs on downstream tasks such as reasoning, summarisation, and alignment [Wang et al., 2023, Tunstall et al., 2023].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-4",
      "content": "e data, and it has shown to be effective at improving the performance of pretrained LLMs on downstream tasks such as reasoning, summarisation, and alignment [Wang et al., 2023, Tunstall et al., 2023]. The theoretical motivation for DPO is based on a preference-ranking model with an implicit reward function that models the relative probability of picking the preferred completion over the dispreferred.\n\n* Correspondence to: arka.pal@gmail.com\n\nFigure 1: DPOP avoids a failure mode of DPO. When preference pairs differ on only a few tokens, DPO receives no loss incentive for the early tokens, and a loss incentive that in some cases can lead to degradation of the log-probs of later tokens (Section 3). We introduce DPOP, which adds a new term to the loss which leads every token to be incentivised toward the preferred completion (Section 4).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-5",
      "content": "the log-probs of later tokens (Section 3). We introduce DPOP, which adds a new term to the loss which leads every token to be incentivised toward the preferred completion (Section 4).\n\n<!-- image -->\n\nIn this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred completions, as long as the relative probability between the preferred and dispreferred classes increases. Our theoretical analysis suggests that the problem occurs most frequently in preference datasets with small edit distances between each pair of completions. Specifically, we show that in the case of preferred and dispreferred examples that differ by few tokens, DPO increases the probability of the token(s) that differ, yet decreases the probability of subsequent tokens (see Figure 1 for an overview). We also present an empirical token-level analysis that matches our theoretical findings on common datasets.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-6",
      "content": "t differ, yet decreases the probability of subsequent tokens (see Figure 1 for an overview). We also present an empirical token-level analysis that matches our theoretical findings on common datasets.\n\nUsing these insights, we design a new loss function: DPO-Positive (DPOP), which adds a new term to the loss function that penalises reducing the probability of the positive completions. We also create new preference datasets based on ARC [Clark et al., 2018], HellaSwag [Zellers et al., 2019], and MetaMath [Yu et al., 2023] and use them along with DPOP to create new models.\n\nWe introduce the Smaug class of models which use DPOP to achieve state-of-the-art open-source performance. We fine-tune 7B, 34B, and 72B models on our new datasets and show that DPOP far outperforms DPO. We evaluate our resulting models on multiple benchmarks including the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021], which aggregates six popular benchmarks such as MMLU [Hendrycks et al.,",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-7",
      "content": "esulting models on multiple benchmarks including the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021], which aggregates six popular benchmarks such as MMLU [Hendrycks et al., 2021] and GSM8K [Cobbe et al., 2021]. On the HuggingFace Open LLM Leaderboard, Smaug-72B achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model.\n\nIn order to address potential concerns about pervasive contamination on the HuggingFace Open LLM Leaderboard, we use an open-source contamination checker, finding that our model scores similarly to popular existing models. Then, we show that DPOP outperforms DPO in an apples-to-apples comparison on an LLM-judged benchmark that is independent of the fine-tuning data: MT-Bench [Zheng et al., 2023], a challenging benchmark representing eight different categories.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-8",
      "content": "n an apples-to-apples comparison on an LLM-judged benchmark that is independent of the fine-tuning data: MT-Bench [Zheng et al., 2023], a challenging benchmark representing eight different categories. We release our code, models, datasets, and documentation at https://github.com/abacusai/smaug .\n\nOur contributions. We describe our main contributions below.\n\n- We theoretically and empirically show a surprising failure mode of DPO: running DPO on preference datasets with small edit distances between completions can result in a catastrophic decrease in accuracy.\n- We introduce DPO-Positive (DPOP) which we theoretically and empirically show ameliorates the performance degradation. In particular, DPOP often outperforms DPO, even on preference datasets with high edit distances between completions.\n- We create new preference-based versions of ARC, HellaSwag, and MetaMath.\n- Using DPOP and our new datasets, we create the Smaug series of models, with Smaug-72B becoming the first open-source mod",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-9",
      "content": "s.\n- We create new preference-based versions of ARC, HellaSwag, and MetaMath.\n- Using DPOP and our new datasets, we create the Smaug series of models, with Smaug-72B becoming the first open-source model to achieve an average accuracy of 80% on the HuggingFace Open LLM Leaderboard. We open-source our trained models, datasets, and code.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-10",
      "content": "Large language models (LLMs) have shown impressive zero-shot and few-shot performance [Radford et al., 2019, Brown et al., 2020, Bubeck et al., 2023]. Recently, researchers have fine-tuned pretrained LLMs on downstream tasks by using human-written completions [Chung et al., 2022, Mishra et al., 2021] or by using datasets labelled with human-preferred completions relative to other completions [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020]. These techniques have been used to improve performance on a variety of downstream tasks such as translation [Kreutzer et al., 2018] and summarisation [Stiennon et al., 2020], as well as to create general-purpose models such as Zephyr [Tunstall et al., 2023]. Two of the most popular techniques for learning from preference data are reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020] and direct preference optimisation (DPO) [Rafailov et al., 2023].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 10,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-11",
      "content": "om preference data are reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020] and direct preference optimisation (DPO) [Rafailov et al., 2023]. We summarise these approaches below.\n\nRLHF Consider a dataset of pairwise-preference ranked data D = { x ( i ) , y ( i ) w , y ( i ) l } N i =1 where x ( i ) are prompts and y ( i ) w and y ( i ) l are respectively the preferred and dispreferred completions conditioned on that prompt. We have an initial LLM π ref that parameterises a distribution π ref ( y | x ) . Often, we initialise π ref as an LLM that has undergone supervised fine-tuning (SFT) to improve performance on downstream task(s). RLHF begins by modelling the probability of preferring y w to y l using the Bradley-Terry model [Bradley and Terry, 1952] which posits the following probabilistic form:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 11,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-12",
      "content": ". RLHF begins by modelling the probability of preferring y w to y l using the Bradley-Terry model [Bradley and Terry, 1952] which posits the following probabilistic form:\n\n<!-- formula-not-decoded -->\n\nwhere σ is the logistic function and r ( x, y ) corresponds to some latent reward function that is assumed to exist for the completion y given the prompt x . Given D , we can learn a parameterised estimate of r by minimising the negative log-likelihood of the dataset:\n\n<!-- formula-not-decoded -->\n\nFor RLHF, we use reinforcement learning to optimise based on this learned reward function r ϕ (with a regularising KL-constraint to prevent model collapse), and obtain a new LLM distribution π θ .\n\nDPO Rafailov et al. [2023] showed that it is possible to optimise the same KL-constrained reward function as in RLHF without having to learn an explicit reward function. Instead, the problem is cast as a maximum likelihood optimisation of the distribution π θ directly, with the objective:",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 12,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-13",
      "content": "reward function as in RLHF without having to learn an explicit reward function. Instead, the problem is cast as a maximum likelihood optimisation of the distribution π θ directly, with the objective:\n\n<!-- formula-not-decoded -->\n\nwhere β is a regularisation term corresponding to the strength of KL-regularisation in RLHF. In this case, the implicit reward parameterisation is r ( x, y ) = β log π θ ( y | x ) π ref ( y | x ) , and Rafailov et al. [2023] further showed that all reward classes under the Plackett-Luce model [Plackett, 1975, Luce, 2005] (such as Bradley-Terry) are representable under this parameterisation. For an abbreviation, we define π ratio ( y | x ) = π θ ( y | x ) π ref ( y | x ) .\n\nAlternatives to DPO to align models to preference data in the offline, differentiable setting have been proposed. We discuss the most relevant to our work below and further in Appendix A.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 13,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-14",
      "content": "( y | x ) .\n\nAlternatives to DPO to align models to preference data in the offline, differentiable setting have been proposed. We discuss the most relevant to our work below and further in Appendix A.\n\nSLiC Zhao et al. [2022, 2023] provides an alternative formulation for learning from pairwise preference data, either directly or on preference pairs sampled from the SFT policy. Their loss function is:\n\n<!-- formula-not-decoded -->\n\nwhere y ref is a reference target sequence and λ is a scalar. The second term in the above is a cross-entropy loss and plays the role of regularisation towards the original model, but without needing an extra copy of the weights.\n\nIPO Azar et al. [2023] aims to understand the theoretical underpinnings of RLHF and DPO. They identify that DPO may be prone to overfitting in situations where the preference probability of the preferred over the dispreferred examples is close to 1. They propose an alternative form of pairwise preference loss-'IdentityPO (IPO)'.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 14,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-15",
      "content": "erfitting in situations where the preference probability of the preferred over the dispreferred examples is close to 1. They propose an alternative form of pairwise preference loss-'IdentityPO (IPO)'. IPO tries to prevent overfitting to the preference dataset by penalising exceeding the preference margin beyond this regularised value. Conversely, we identify that DPO can lead to underfitting as well-even complete performance degradation.\n\nSubsequent work Very recently, subsequent work has verified our main findings from Section 3 and Section 4 on the existence of a failure mode of DPO and how to fix it [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]. Pang et al. find that when fine-tuning on chain-of-thought reasoning tasks, adding a term similar to our Equation (3) is crucial to achieve strong performance [Pang et al., 2024].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 15,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-16",
      "content": "ailov et al., 2024]. Pang et al. find that when fine-tuning on chain-of-thought reasoning tasks, adding a term similar to our Equation (3) is crucial to achieve strong performance [Pang et al., 2024]. Other work builds off of our theoretical insights by applying additional mathematical analysis, verifying our claims in Section 3 that DPO can decrease the probability of preferred completions [Feng et al., 2024]. Finally, Rafailov et al. analysed the same DPO failure mode phenomenon, showing that the likelihood of the preferred response should decrease during DPO training, when starting from a model that has undergone SFT [Rafailov et al., 2024]. We give additional discussion on related work in Appendix A.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "2 Background and Related Work",
        "chunkIndex": 16,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-17",
      "content": "In this section, we take a step back and examine the DPO loss in Equation (1), specifically with an eye towards how it can reduce the probability of the preferred completion. The loss is a function only of the difference in the log-ratios, which means that we can achieve a low loss value even if π ratio ( y w | x ) is lowered below 1, as long as π ratio ( y l | x ) is also lowered sufficiently. This implies that the log-likelihood of the preferred completions is reduced below the original log-likelihood from the reference model.\n\nWhy is this an issue? The original use-case of RLHF did not explicitly denote the preferred completions as being also satisfactory completions (rather than just the preferred completion out of the two choices y w and y l ), and hence the DPO objective is a good modelling choice. However, since then, a large body of work has focused on distilling the knowledge of powerful models into smaller or weaker models, while also",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 17,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-18",
      "content": ", and hence the DPO objective is a good modelling choice. However, since then, a large body of work has focused on distilling the knowledge of powerful models into smaller or weaker models, while also\n\nshowing that doing so with RLHF/DPO outperforms SFT [Taori et al., 2023, Tunstall et al., 2023, Xu et al., 2023, Chiang et al., 2023]. In this paradigm, it is often the case that in each pair of completions, the better of the two is indeed also a satisfactory completion. Furthermore, a new technique is to transform a standard labelled dataset into a pairwise preference dataset [Ivison et al., 2023, Tunstall et al., 2023], which also has the property that for each pair of completions, one is a satisfactory completion.\n\nEdit Distance 1 While the above illustrates a hypothetical situation, now we provide a specific case in which DPO may cause a decrease in the probability of the better completion.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 18,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-19",
      "content": "factory completion.\n\nEdit Distance 1 While the above illustrates a hypothetical situation, now we provide a specific case in which DPO may cause a decrease in the probability of the better completion. Consider the case of trying to improve a model's math or reasoning abilities by comparing a completion of '2+2=4' to '2+2=5.' This process creates a pair of preferred and dispreferred completions which have an edit (Hamming) distance of 1, i.e., all tokens in the completion are the same except for one. In the following, we will explore how the location of the differing token impacts the computation of the DPO loss. For sake of argument, we will examine what happens when the differing token is the first token, though the argument also follows if it appears elsewhere.\n\nFor preliminaries, consider two completions with an edit distance of 1 which differ at token m with 1 ≤ m ≤ K , i.e., consider y w = ( t 1 , . . . , t K ) and y l = ( t 1 , . . . , t m -1 , t ′ m , t m +1 , . . . , t K ) .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 19,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-20",
      "content": "sider two completions with an edit distance of 1 which differ at token m with 1 ≤ m ≤ K , i.e., consider y w = ( t 1 , . . . , t K ) and y l = ( t 1 , . . . , t m -1 , t ′ m , t m +1 , . . . , t K ) . Denote y &lt;r = ( t 1 , . . . , t r -1 ) and y ≥ r = ( t r , . . . , t K ) . Assume that the vocabulary length of the LLM is L . Let s { x } i represent the probability of the i -th token in the model's vocabulary conditioned on the input x . While the LLM model parameters θ are numerous, we restrict our attention to the logits, θ j with j ∈ [ L ] .\n\nThe gradient of Equation (1) with respect to θ is given by:\n\n<!-- formula-not-decoded -->\n\nWe note first that for any m&gt; 1 , all tokens in positions from 1 to m -1 have no effect on the gradient, as for all 1 ≤ i ≤ m -1 , π θ ( t i | y &lt;k w , x ) = π θ ( t i | y &lt;k l , x ) , causing these tokens' contribution to the gradient to cancel out.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 20,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-21",
      "content": "s from 1 to m -1 have no effect on the gradient, as for all 1 ≤ i ≤ m -1 , π θ ( t i | y &lt;k w , x ) = π θ ( t i | y &lt;k l , x ) , causing these tokens' contribution to the gradient to cancel out. Therefore, without loss of generality, assume m = 1 , i.e., y w and y l differ only at the first token. Without loss of generality, we also assume that t k takes vocabulary position 1. Then we have the following for each k &gt; 1 (derivation in Appendix B.1):\n\n<!-- formula-not-decoded -->\n\n̸\n\nAs we typically run DPO after SFT, the model is likely to be reasonably well optimised, so we should have s { y &lt;k w ,x } j ≤ s { y &lt;k l ,x } j for j = 1 and s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 . Therefore, while this analysis only extends to gradients with respect to the logits, we see that the gradient decreases logits corresponding to the correct token and increases logits corresponding to the incorrect tokens.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 21,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-22",
      "content": "analysis only extends to gradients with respect to the logits, we see that the gradient decreases logits corresponding to the correct token and increases logits corresponding to the incorrect tokens. Surprisingly, this suggests that under DPO, all locations in the sequence after the differing token will have reduced probability of emitting the correct token when compared to π ref. We give empirical evidence for this in Section 5 and Figure 4.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "3 Failure Mode of DPO",
        "chunkIndex": 22,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-23",
      "content": "Now, we introduce DPO-Positive (DPOP), our proposed method to address the failure mode described in the previous section. DPOP adds the penalty term max ( 0 , log π ref ( y w | x ) π θ ( y w | x ) ) to the loss within the log-sigmoid to incentivise maintaining a high log-likelihood of the preferred completions. This penalty term is 0 when π ratio ( y w | x ) ≥ 1 and increases as the ratio goes below 1.\n\nThe full DPOP loss function is thus:\n\n<!-- formula-not-decoded -->\n\nwhere λ &gt; 0 is a hyperparameter that can be tuned. This form of loss retains the property that we are fitting parameters on the preference data under the Bradley-Terry model. The implicit reward parameterisation is\n\n<!-- formula-not-decoded -->\n\nBy applying this optimisation pressure, the model can no longer minimise the loss by significantly reducing the log-likelihood of the dispreferred examples more than it reduces the log-likelihood of the preferred examples; it must also ensure that the log-likelihood of the pr",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "4 DPOP",
        "chunkIndex": 23,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-24",
      "content": "loss by significantly reducing the log-likelihood of the dispreferred examples more than it reduces the log-likelihood of the preferred examples; it must also ensure that the log-likelihood of the preferred examples remains high relative to the log-likelihood under the reference model.\n\nNow we show that Equation (3) mitigates the failure mode from the previous section. Recall from Section 3 that we focused on two completions, y w and y l , which differ by one token at location m = 1 . We showed in Equation (2) that for standard DPO, the gradient of the k -th token in the completions with respect to the j -th logit is s { y &lt;k l ,x } j -s { y &lt;k w ,x } j . However, for DPOP, if π ratio &lt; 1 , the gradients become (derivation in Appendix B.2):\n\n̸\n\n<!-- formula-not-decoded -->\n\n̸\n\nwhere i is the vocabulary index of token t k . Since s { y &lt;k w ,x } j ≤ 1 , for the case i = j , the gradient is guaranteed to be positive for a large enough choice of λ .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "4 DPOP",
        "chunkIndex": 24,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-25",
      "content": "mula-not-decoded -->\n\n̸\n\nwhere i is the vocabulary index of token t k . Since s { y &lt;k w ,x } j ≤ 1 , for the case i = j , the gradient is guaranteed to be positive for a large enough choice of λ . Similarly, for the case i = j , the gradient is guaranteed to be negative for a large enough λ (as long as s { y &lt;k w ,x } j &gt; 0 ). This therefore fixes the issue we identified in DPO in Section 3.\n\nConnection to Contrastive Loss While the main motivation for DPOP is to avoid the failure mode described in Section 3, we also note its connection to contrastive loss . Contrastive learning is a popular technique in areas such as computer vision for datasets of similar and dissimilar pairs [Oord et al., 2018, Chen et al., 2020, He et al., 2020], and the loss function often uses a margin factor. Equation (3) can be viewed as similar to contrastive loss with margin m = log 1 π ref ( y w | x ) . We give further details in Appendix C.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "4 DPOP",
        "chunkIndex": 25,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-26",
      "content": "or. Equation (3) can be viewed as similar to contrastive loss with margin m = log 1 π ref ( y w | x ) . We give further details in Appendix C.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "4 DPOP",
        "chunkIndex": 26,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-27",
      "content": "In this section, we empirically validate that the failure mode does arise in practice and that DPOP is able to mitigate the failure. We also show that even when the edit distance is large and DPO does not show degradation in performance, DPOP can still outperform DPO on downstream task evaluation.\n\nFigure 2: Preference optimization comparisons on low (MetaMath) and high (ARC) edit distance datasets . DPOP outperforms DPO, IPO, and SLiC on both MetaMath (left) and ARC (right), whose normalized edit distances are 6.5% and 90%, respectively. Evaluation is performed on the test set of the datasets using the LLM Evaluation Harness.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5 DPOP Datasets &amp; Experiments",
        "chunkIndex": 27,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-28",
      "content": "For our empirical analysis, we focus on the downstream tasks of GSM8K , ARC , and HellaSwag , and we introduce and release associated paired preference-ranked datasets.\n\nGSM8K [Cobbe et al., 2021], a dataset of diverse grade school math word problems, has been adopted as a measure of the math and reasoning skills of LLMs [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021]. We create a paired preference-ranked version of MetaMath [Yu et al., 2023], an extended version of the GSM8K training data [An et al., 2023, Yu et al., 2023]. The correct completions in the MetaMath dataset consist of a series of steps which lead to the final answer. To create a dispreferred version, we randomly corrupt one of the results of an intermediate calculation. This dataset has a low (normalised) edit distance of 6.5%.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.1 Dataset Creation",
        "chunkIndex": 28,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-29",
      "content": "teps which lead to the final answer. To create a dispreferred version, we randomly corrupt one of the results of an intermediate calculation. This dataset has a low (normalised) edit distance of 6.5%.\n\nARC [Clark et al., 2018] is a dataset that tests the level of understanding of science at grade-school level. We focus specifically on ARC-Challenge, the more difficult of the two subsections of ARC, which has been widely adopted as a measure of LLM reasoning and world understanding [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Cobbe et al., 2021]. The ARC-Challenge dataset consists of four choices of responses to each question, one of which is correct. To create a paired preference-ranked dataset, for each correct response in the training split, we create three pairs using each incorrect response. Due to the differences in the responses, this dataset has a high normalised edit distance of 90%.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.1 Dataset Creation",
        "chunkIndex": 29,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-30",
      "content": "for each correct response in the training split, we create three pairs using each incorrect response. Due to the differences in the responses, this dataset has a high normalised edit distance of 90%.\n\nHellaSwag [Zellers et al., 2019] is a dataset containing commonsense inference questions known to be hard for LLMs. Similar to ARC, each question has one correct completion and three incorrect completions, and so we create a paired preference-ranked dataset by creating three pairs for each correct response in the training split. See Appendix D for further details and documentation about our new datasets.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.1 Dataset Creation",
        "chunkIndex": 30,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-31",
      "content": "In this section, we compare training DPO, IPO, SLiC and DPOP on the train splits of the datasets mentioned above and evaluate them on the corresponding tasks. We apply each preference-training method to the base\n\nFigure 3: Ablation studies: Evaluation of DPO vs. DPOP for different values of β , on the MetaMath dataset (left), as well as for different values of λ , on the MetaMath (middle) and ARC (right) datasets.\n\n<!-- image -->\n\nmodel of Mistral7B [Jiang et al., 2023]. We evaluate on the test sets of GSM8K and ARC with the LLM Evaluation Harness [Gao et al., 2021]. Unless specified otherwise, we use values of β = 0 . 3 and λ = 50 .\n\nLoss function comparison First, we compare DPO, IPO, SLiC and DPOP when training on both MetaMath and ARC; see Figure 2. We find that when training on MetaMath, both DPO and SLiC catastrophically fail, while IPO does not improve performance. DPOP is the only model to improve performance over the base model.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 31,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-32",
      "content": "ee Figure 2. We find that when training on MetaMath, both DPO and SLiC catastrophically fail, while IPO does not improve performance. DPOP is the only model to improve performance over the base model. When training on ARC, which has a higher edit distance as described in the previous section, both DPO, SLiC and DPOP are able to improve on the base model significantly; however, DPOP performs best.\n\nAblation studies for β and λ One potential hypothesis for how degradation of DPO on MetaMath could be prevented is by modifying the strength of the regularisation parameter, β . We test β ∈ { 0 . 1 , 0 . 3 , 1 . 0 } , and although a larger β does induce a slower decrease, the performance with DPO still plummets, while DPOP shows strong and consistent performance with different values of β (see Figure 3). Furthermore, we conduct an ablation study over the value of λ (a hyperparameter unique to DPOP) to determine the sensitivity of performance to tuning this value precisely.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 32,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-33",
      "content": "values of β (see Figure 3). Furthermore, we conduct an ablation study over the value of λ (a hyperparameter unique to DPOP) to determine the sensitivity of performance to tuning this value precisely. We test λ ∈ { 5 , 50 , 500 } and find that performance on MetaMath and ARC is relatively unaffected by the choice of λ . See Figure 3.\n\nToken-level analysis Recall that in Section 3, we gave theoretical motivations for why DPO is likely to perform poorly on low-edit distance datasets. We now analyse the log-probabilities of the trained models at the token level on the MetaMath dataset over 900 samples to empirically support our arguments. Let us denote the index of the first token that is different between the preferred and dispreferred completion by m .\n\nWe suggested that π θ ( y ≥ r | x, y &lt;r ) for r &gt; m will have 'wrong-way' gradient updates and therefore decrease. We find this is indeed the case-the average log-prob after training of tokens after m is -0 .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 33,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-34",
      "content": "hat π θ ( y ≥ r | x, y &lt;r ) for r &gt; m will have 'wrong-way' gradient updates and therefore decrease. We find this is indeed the case-the average log-prob after training of tokens after m is -0 . 37 for the reference model and -0 . 26 for DPOP, but -1 . 82 for DPO on the preferred completions (see (Figure 4) (left)). Perhaps most instructively, for both the reference model and DPOP, in Figure 4 (right), we see that tokens after the edit indices show higher log-likelihood than those before the edit indices-this is indicative of well-behaved language modelling, with lower perplexity as more tokens are added to the context. By contrast, DPO shows the opposite pattern-with log-likelihood actually reducing after the edit indices. This is indicative of a deeper breakdown in language modelling, which we believe is facilitated by the wrong-way gradient we outlined in Section 3.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 34,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-35",
      "content": "likelihood actually reducing after the edit indices. This is indicative of a deeper breakdown in language modelling, which we believe is facilitated by the wrong-way gradient we outlined in Section 3. Finally, we are also able to substantiate our assumption from Section 3 that s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 ; we find from our analysis that for the baseline model, the tokens after the edit have\n\nFigure 4: DPO fails to train on low edit-distance pairs, yet DPOP performs well. Left: average log-probs for 900 randomly-sampled preferred train set completions on MetaMath across training steps. For DPO, the log-probs decrease throughout training. Right: average log-prob difference for preferred completions on MetaMath by location around differing tokens, after 1000 training steps. Log-prob 'difference' signifies that each model's plot has been adjusted to have 0 log-prob at location -1; all other log-probs are shown relative to this value.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 35,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-36",
      "content": "ing tokens, after 1000 training steps. Log-prob 'difference' signifies that each model's plot has been adjusted to have 0 log-prob at location -1; all other log-probs are shown relative to this value. For DPO, there is a significant decrease after the differing tokens, while DPOP avoids this issue. The log-probs of the LLM prior to application of DPOP or DPO is also included for reference.\n\n<!-- image -->\n\nan average log-likelihood of -0 . 37 on the preferred completion, but this drops to -0 . 86 on the dispreferred completion.\n\nIn Appendix E, we present additional results on ARC comparing the averaged log-probs of DPO and DPOP on the preferred completion during training; see Appendix Figure 6. DPOP once again demonstrates higher log-probs than DPO.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "5.2 Experiments",
        "chunkIndex": 36,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-37",
      "content": "In this section, we introduce the Smaug series of models. We train models for 7B, 34B and 72B parameter sizes using DPOP. We use the 7B class for a direct comparison of DPOP vs. DPO, including testing the generalisability of the improvement in model performance on benchmarks very different from the training data, such as MT-Bench [Zheng et al., 2023]. Due to the computational resource requirements involved in training the larger model sizes, we only perform DPOP on 34B and 72B and compare to other models on the HuggingFace Open LLM Leaderboard. Also due to computational expense, we do not perform any further hyperparameter tuning.\n\nDatasets In this section, unless otherwise noted, we train on a mix of six pair preference datasets: our MetaMath DPO, ARC DPO, and HellaSwag DPO datasets described in Section 5, the ORCA DPO dataset [Intel, 2024], the Truthy DPO dataset [Durbin, 2024b], and the UltraFeedback Binarized dataset [AllenAI, 2024].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6 Smaug",
        "chunkIndex": 37,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-38",
      "content": "th DPO, ARC DPO, and HellaSwag DPO datasets described in Section 5, the ORCA DPO dataset [Intel, 2024], the Truthy DPO dataset [Durbin, 2024b], and the UltraFeedback Binarized dataset [AllenAI, 2024]. We run these experiments with 8 H100 GPUs (each with 80GB), using the transformers repository [Wolf et al., 2020] for general model loading and infrastructure, and the TRL repository [von Werra et al., 2020] for running DPOP. We set β = 0 . 3 , λ = 50 , a learning rate of 5 × 10 -5 , and the AdamW optimizer [Loshchilov and Hutter, 2017], and we run 1000 steps for all DPO and DPOP routines.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6 Smaug",
        "chunkIndex": 38,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-39",
      "content": "First we apply the above recipe on Llama-2-Chat [Touvron et al., 2023b]. Since this model has already undergone instruction fine-tuning, we perform DPO and DPOP directly using the datasets described in the previous section. We evaluate the Llama-2 finetunes on MT-Bench [Zheng et al., 2023] This benchmark tests across multiple categories of LLM performance (for example: writing, roleplay, coding and math).\n\nMT-Bench We evaluate MT-Bench on Llama-2-7B-Chat finetuned with DPO and DPOP. We run MTBench with the Llama-2 conversation template. We perform 10 trials due to the inherent stochasticity of the benchmark, computing mean and standard deviation. Across the 10 trials, DPO achieves a first turn score of 7 . 032 ± 0 . 043 whilst DPOP scores 7 . 292 ± 0 . 037 , a significant improvement.\n\nMT-Bench tests across eight diverse categories and is significantly different from our fine-tuning data.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.1 DPOP vs. DPO Ablation on 7B",
        "chunkIndex": 39,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-40",
      "content": "n score of 7 . 032 ± 0 . 043 whilst DPOP scores 7 . 292 ± 0 . 037 , a significant improvement.\n\nMT-Bench tests across eight diverse categories and is significantly different from our fine-tuning data. Furthermore, MT-Bench has been shown to have performance correlated to human preferences of LLM rankings [Zheng et al., 2023, Rafailov et al., 2023, Li et al., 2024]. The outperformance of DPOP vs. DPO in this controlled like-for-like setting is therefore an indication of the improvement gained by utilising DPOP over DPO on new tasks not in the training set, and also an indication of general model quality.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.1 DPOP vs. DPO Ablation on 7B",
        "chunkIndex": 40,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-41",
      "content": "Smaug-34B is a modified version of the base model Bagel-34B-v0.2 [Durbin, 2024a], which itself is a SFT version of Yi-34B-200k [01.AI, 2024]. We first take Bagel-34B-v0.2 and perform a SFT fine-tune using a combination of three datasets: MetaMath [Yu et al., 2023], ORCA-Chat [Es, 2024], and the ShareGPT dataset [Z., 2024]. We then apply DPOP using the methodology and datasets described above.\n\nFor 72B, we start from MoMo-72b-lora-1.8.7-DPO [Moreh, 2024], which itself is a fine-tune of Qwen72B [Bai et al., 2023]. MoMo-72b-lora-1.8.7-DPO has already undergone SFT, so we simply run the DPOP routines as in Smaug-34B. The total training time is 144 hours.\n\nHuggingFace Open LLM Leaderboard We evaluate using the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021], a widely-used benchmark suite that aggregates six popular benchmarks: ARC [Clark et al., 2018], GSM8K [Cobbe et al., 2021], HellaSwag [Zellers et al., 2019], MMLU [Hendrycks et al., 2021], TruthfulQA [Lin et a",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.2 Smaug-34B and Smaug-72B",
        "chunkIndex": 41,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-42",
      "content": "used benchmark suite that aggregates six popular benchmarks: ARC [Clark et al., 2018], GSM8K [Cobbe et al., 2021], HellaSwag [Zellers et al., 2019], MMLU [Hendrycks et al., 2021], TruthfulQA [Lin et al., 2022], and WinoGrande [Sakaguchi et al., 2020]. We evaluate directly in HuggingFace, which uses the LLM Evaluation Harness [Gao et al., 2021]. It performs few-shot prompting on the test sets of these tasks. We compare Smaug-72B to the evaluation scores of the top five open-weight LLMs according to the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021] as of March 2024; see Table 1. Smaug-72B achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model. Smaug-34B also achieves best-in-its-class performance compared to other models of similar or smaller size (see Appendix E).",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.2 Smaug-34B and Smaug-72B",
        "chunkIndex": 42,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-43",
      "content": "cy of 80% and improving by nearly 2% over the second-best open-source model. Smaug-34B also achieves best-in-its-class performance compared to other models of similar or smaller size (see Appendix E).\n\nMT-Bench Next, we evaluate again using MT-Bench [Zheng et al., 2023]. We run MT-Bench with the Llama-2 conversation template [Touvron et al., 2023b]. See Appendix Table 2 for a comparison with stateof-the-art LLMs according to Arena Elo as of March 2024. Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models. In Appendix E, we give examples of Smaug-72B completions to MT-Bench questions.\n\nTable 1: Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard as of March 2024. See Table 4 for an extended comparison.\n\n| Model                                           | Size   |   Avg.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.2 Smaug-34B and Smaug-72B",
        "chunkIndex": 43,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-44",
      "content": "ion of the top open-weight models on the HuggingFace Open LLM Leaderboard as of March 2024. See Table 4 for an extended comparison.\n\n| Model                                           | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                | 72B+   |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         | 72B+   |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         | 72B+   |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO | 72B+   |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.2 Smaug-34B and Smaug-72B",
        "chunkIndex": 44,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-45",
      "content": "72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO | 72B+   |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      | 72B+   |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.2 Smaug-34B and Smaug-72B",
        "chunkIndex": 45,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-46",
      "content": "Since LLMs train on wide swaths of the internet, data contamination, i.e., evaluating on examples that are included in the training data, is a growing concern. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities [Roberts et al., 2024, Jain et al., 2024, bench authors, 2023].\n\nWhile there is no perfect tool to check for data contamination, we use an open-source contamination checker [Shi, 2023] to compare the contamination of our model to other open-source models on training datasets of the Huggingface Open LLM Leaderboard. On ARC, TruthfulQA, and GSM8K, we find that Smaug-72B achieves scores that are similar to MoMo-72B-lora-1.8.7-DPO (the starting point of Smaug-72B), as well as Llama-2-70B. See the full details in Appendix E.3.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "6.3 Contamination check",
        "chunkIndex": 46,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-47",
      "content": "In this work, we presented new theoretical findings on a severe failure mode of DPO, in which fine-tuning causes the probability of the preferred examples to be reduced. We then presented an empirical token-level analysis that matches our theoretical findings on popular datasets. In order to mitigate this issue, we devised a new technique, DPOP, which we show overcomes the failure mode of DPO - and can outperform DPO even outside this failure mode. By fine tuning with DPOP on our new pairwise preference versions of ARC, HellaSwag, and MetaMath, we create a new LLM that achieves state-of-the-art performance. In particular, we present the first open-weights model to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard, and we also show that DPOP significantly outperforms DPO in an apples-to-apples comparison on MT-Bench, which is independent of the fine-tuning data.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "7 Conclusions and Limitations",
        "chunkIndex": 47,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-48",
      "content": "cy of 80% on the HuggingFace Open LLM Leaderboard, and we also show that DPOP significantly outperforms DPO in an apples-to-apples comparison on MT-Bench, which is independent of the fine-tuning data.\n\nIn the future, creating pairwise preference-based versions of other datasets, and running DPOP with these datasets, could push the abilities of open-source LLMs closer to the performance of proprietary models such as GPT-4 [OpenAI, 2023], especially when tuned for specific downstream tasks. Furthermore, using DPOP on additional mathematical datasets is an exciting area for future work, as it has the potential to further advance LLMs' abilites in mathematical reasoning.\n\nLimitations While our work gives theoretical and empirical evidence on a failure mode of DPO and a proposed solution, it still has limitations. First, we were unable to run a full ablation study on our 72B model.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "7 Conclusions and Limitations",
        "chunkIndex": 48,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-49",
      "content": "While our work gives theoretical and empirical evidence on a failure mode of DPO and a proposed solution, it still has limitations. First, we were unable to run a full ablation study on our 72B model. Running multiple fine-tuning experiments on a 72B model is infeasible, as each one can take over five days to complete. Therefore, we assume that our ablations on smaller models still hold up at scale. Furthermore, while we expect DPOP to achieve strong performance on any preference dataset, especially those with small edit distance, we have only demonstrated its performance on six English-language datasets. We hope that future work can verify its effectiveness on more datasets, in particular on non-English datasets.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "7 Conclusions and Limitations",
        "chunkIndex": 49,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-50",
      "content": "01.AI. Yi-34b-200k, 2024. URL https://huggingface.co/01-ai/Yi-34B-200K .\n\n- AllenAI. Ultrafeedback binarized clean, 2024. URL https://huggingface.co/datasets/allenai/ ultrafeedback\\_binarized\\_cleaned .\n- Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689 , 2023.\n- Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.\n- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Ha",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 50,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-51",
      "content": ", Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.\n- Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open\\_llm\\_leaderboard , 2023.\n- BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.\n- R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 51,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-52",
      "content": "mitation game: Quantifying and extrapolating the capabilities of language models, 2023.\n- R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika , 39(3/4):324-345, 1952. doi: 10.2307/2334029.\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\n- Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 52,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-53",
      "content": "eriments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning , pages 1597-1607. PMLR, 2020.\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/ .\n\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1-113, 2023.\n- Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-54",
      "content": "t al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1-113, 2023.\n- Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017.\n- Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-55",
      "content": "ge, 2018.\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.\n- Jon Durbin. Bagel-34b-v0.2, 2024a. URL https://huggingface.co/jondurbin/bagel-34b-v0.2 .\n- Jon Durbin. Truthy dpo, 2024b. URL https://huggingface.co/datasets/jondurbin/truthy-dpo-v0. 1 .\n- Shahul Es. Orca-chat, 2024. URL https://huggingface.co/datasets/shahules786/orca-chat .\n- Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. Human-centered loss functions (halos). Technical report, Contextual AI, 2023.\n- Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: A theoretical perspective.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-56",
      "content": "los). Technical report, Contextual AI, 2023.\n- Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: A theoretical perspective. arXiv preprint arXiv:2404.04626 , 2024.\n- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n- R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) , volume 2, pages 1735-1742, 2006. doi: 10.1109/CVPR.2006.100.\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-57",
      "content": "06) , volume 2, pages 1735-1742, 2006. doi: 10.1109/CVPR.2006.100.\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9729-9738, 2020.\n- Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938 , 2021.\n- Intel. Orca dpo pairs, 2024. URL https://huggingface.co/datasets/Intel/orca\\_dpo\\_pairs .\n\n- Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-58",
      "content": "ng Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702 , 2023.\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n- Julia Kreutzer, Joshua Uyheng, and Stefan Riezler.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-59",
      "content": "ard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n- Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. arXiv preprint arXiv:1805.10627 , 2018.\n- Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys. org/blog/2024-04-19-arena-hard/ .\n- Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2022.\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.\n- R Duncan Luce. Individual choice behavior: A theoretical analysis .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-60",
      "content": "Papers) , 2022.\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.\n- R Duncan Luce. Individual choice behavior: A theoretical analysis . Courier Corporation, 2005.\n- Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.\n- Moreh. Momo-72b-lora-1.8.7-dpo, 2024. URL https://huggingface.co/moreh/MoMo-72B-lora-1.8. 7-DPO .\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 , 2018.\n- OpenAI. Gpt-4 technical report. Technical Report , 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leik",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-61",
      "content": "amela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733 , 2024.\n- Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics , 24(2):193-202, 1975.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-62",
      "content": ", 1(8):9, 2019.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2023.\n- Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q* : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358 , 2024.\n- Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. To the cutoff... and beyond? a longitudinal perspective on llm data contamination. In Proceedings of the International Conference on Learning Representations (ICLR) , 2024.\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-63",
      "content": "ernational Conference on Learning Representations (ICLR) , 2024.\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence , 34, 2020.\n- Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning , pages 5628-5637. PMLR, 2019.\n- Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 815-823, 2015. doi: 10.1109/CVPR.2015.7298682.\n- Weijia Shi. Detect pretrain code contamination.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-64",
      "content": "ition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 815-823, 2015. doi: 10.1109/CVPR.2015.7298682.\n- Weijia Shi. Detect pretrain code contamination. https://github.com/swj0419/ detect-pretrain-code-contamination , 2023.\n- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2020.\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-65",
      "content": "rin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7, 2023.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023a.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\n- Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-66",
      "content": "Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.\n- Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty , 5(4):297-323, 1992.\n\n- Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl , 2020.\n- Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 2495-2504, 2021.\n- Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-67",
      "content": "recognition , pages 2495-2504, 2021.\n- Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144 , 2023.\n- Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning , pages 9929-9939. PMLR, 2020.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-68",
      "content": ", Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/2020.emnlp-demos.6 .\n- Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 , 2023.\n- Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-69",
      "content": "af, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417 , 2024.\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.\n- Z. Sharegpt\\_vicuna\\_unfiltered, 2024. URL https://huggingface.co/datasets/anon8231489123/ ShareGPT\\_Vicuna\\_unfiltered .\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-70",
      "content": "eedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://www.aclweb.org/anthology/P19-1472 .\n- Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J. Liu. Calibrating sequence likelihood improves conditional language generation, 2022.\n- Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023.\n\n- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\n- Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-71",
      "content": "5685 , 2023.\n- Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-72",
      "content": "In this section, we continue our discussion of related work from Section 2.\n\nAFT Wang et al. [2023] seek to align LLMs to correctly 'score' (in terms of perplexity) their own generations. They do so by generating multiple chain-of-thought [Wei et al., 2023] responses to each prompt, which they categorise as preferred or dispreferred according to whether they answer the question correctly. Their proposed 'Alignment Fine-Tuning (AFT)' paradigm adds an alignment objective L ∗ A to the standard fine tuning loss, defined as\n\n<!-- formula-not-decoded -->\n\nwhere G p is the set of preferred examples and G n is the set of dispreferred examples. By minimising L ∗ A , the log-likelihoods of preferred examples are encouraged to be larger than the log-likelihoods of dispreferred examples, akin to DPO. However, Wang et al. [2023] takes an opposing motivation to us: they are particularly concerned with the issue of the log-likelihoods of dispreferred examples being pushed down too significantly",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 72,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-73",
      "content": "DPO. However, Wang et al. [2023] takes an opposing motivation to us: they are particularly concerned with the issue of the log-likelihoods of dispreferred examples being pushed down too significantly\n\nOur work differs from AFT in three key points. First, although Wang et al. [2023] discusses DPO in the appendix, they do not show how their approach would extend to a reformulation of its objective; they also focus their experiments solely on supervised fine-tuning. Next, we use a different constraint mechanism-theirs is a soft margin constraint on the log-probability distance of the dispreferred example from the preferred example, while ours is a soft penalty for deviating from a reference model. Finally, they are focused specifically on the case of self-generated LLM CoT responses and calibrating the LLM's perplexity of its own responses.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 73,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-74",
      "content": "s a soft penalty for deviating from a reference model. Finally, they are focused specifically on the case of self-generated LLM CoT responses and calibrating the LLM's perplexity of its own responses.\n\nHALO Ethayarajh et al. [2023] seek to understand alignment methods, including DPO, in the context of 'Human-Centred Loss Functions (HALOs)'. By drawing an equivalence between the alignment methods and the work of Tversky and Kahneman [1992] in prospect theory, they adapt the 'human value function' in that paper to the LLM setting:\n\n<!-- formula-not-decoded -->\n\nwhere they define g ( x, y ; β ) as and\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe major difference of this approach with DPO is that it does not require paired preference data. The above loss function can be used for any dataset as long as the labels are individually marked as positive or negative.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 74,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-75",
      "content": "of this approach with DPO is that it does not require paired preference data. The above loss function can be used for any dataset as long as the labels are individually marked as positive or negative.\n\nCPO Very recently, concurrent work [Xu et al., 2024] proposes adding a new term to the DPO loss function in order to allow DPO to become better at rejecting 'worse' completions that are good quality but not perfect. Specifically, they include the term\n\n<!-- formula-not-decoded -->\n\nWhile similar, their work uses a different loss function with different motivation, and furthermore they only considered machine translation models up to 13B parameters.\n\nSubsequent work, extended discussion As described in Section 2, very recently, subsequent work has verified our main findings from Section 3 and Section 4 on the existence of a failure mode of DPO and how to fix it [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]. We give additional discussion here.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 75,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-76",
      "content": "ain findings from Section 3 and Section 4 on the existence of a failure mode of DPO and how to fix it [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]. We give additional discussion here.\n\nPang et al. consider fine-tuning on chain-of-thought reasoning tasks using pairwise preference data. They find that including an additional positive log-likelihood term is crucial for their reasoning tasks. Their final loss is similar to our Equation (3), but the additional term is outside the log sigmoid [Pang et al., 2024].\n\nFeng et al. give additional mathematical analyses of the gradient vector fields of the DPO loss. They come to the conclusion that the DPO loss function decreases the probability of producing dispreferred completions at a faster rate than it increases the probability of producing preferred completions [Feng et al., 2024].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 76,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-77",
      "content": "on that the DPO loss function decreases the probability of producing dispreferred completions at a faster rate than it increases the probability of producing preferred completions [Feng et al., 2024].\n\nFinally, Rafailov et al. also analysed the 'phenomena in which the likelihood of the chosen responses actually decrease over time [in DPO]' by examining the expected log ratio of a completion under a model being optimised to a reference model. By showing that this is equivalent to the KL divergence, which is necessarily non-negative, they find that 'the likelihood of the chosen response should decrease in the process of DPO training' when starting from a model that has undergone SFT on the preferred completions [Rafailov et al., 2024].",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "A Related Work Continued",
        "chunkIndex": 77,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-78",
      "content": "Consider two completions of length K with edit (Hamming) distance of 1 which differ at token m with 1 ≤ m ≤ K . Put y w = ( t 1 , . . . , t K ) and y l = ( t 1 , . . . , t m -1 , t ′ m , t m +1 , . . . , t K ) . Put y &lt;r = ( t 1 , . . . , t r -1 ) and y ≥ r = ( t r , . . . , t K ) . Note that the derivative of Equation (1) with respect to θ is proportional to:\n\n<!-- formula-not-decoded -->\n\nBy the chain rule, we have and by using logarithm identities\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhen we substitute this into ∇ θ L DPO , we have:\n\n<!-- formula-not-decoded -->\n\nAssume that the vocabulary length of the LLM is L . While the LLM model parameters θ are numerous, let us restrict our attention to just the logits, which we denote as θ j with j ∈ [ L ] and are the input to the softmax. We further denote the softmax probabilities by s { x } , so that s { x } i represents the probability of the i -th token in the model's vocabulary given the input context x .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "B.1 Derivation for DPO",
        "chunkIndex": 78,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-79",
      "content": "the input to the softmax. We further denote the softmax probabilities by s { x } , so that s { x } i represents the probability of the i -th token in the model's vocabulary given the input context x .\n\nIt is then a standard result of the softmax function that:\n\n<!-- formula-not-decoded -->\n\nConsider the case where y w and y l differ only at the first token, i.e., m = 1 , and without loss of generality, assume that t k takes vocabulary position 1. In this instance, for each term of Equation (4) where k &gt; 1 we have 1 :\n\n<!-- formula-not-decoded -->\n\n̸\n\nAs we typically run DPO after SFT the model is likely to be reasonably well optimised, so we should have s { y &lt;k w ,x } j ≤ s { y &lt;k l ,x } j for j = 1 and s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 . Therefore, while this analysis only extends to gradients with respect to the logits, we see that the gradient vector is decreasing in the correct logit dimension and increasing in the wrong logit dimensions.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "B.1 Derivation for DPO",
        "chunkIndex": 79,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-80",
      "content": "re, while this analysis only extends to gradients with respect to the logits, we see that the gradient vector is decreasing in the correct logit dimension and increasing in the wrong logit dimensions. In particular, this derivation suggests that under DPO, all tokens that follow a difference at m at any point should have reduced probability of emitting the correct token when compared to π ref .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "B.1 Derivation for DPO",
        "chunkIndex": 80,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-81",
      "content": "We can follow a similar line of reasoning for calculating ∇ θ L DPOP with respect to its logits which we denote again by θ j .\n\nAgain taking token position k for illustrative purposes and assuming that token t k has vocabulary position i , in the case when π ratio ( y | x ) &lt; 1 ,\n\n̸\n\n<!-- formula-not-decoded -->\n\n1 Note that the preceding negative sign is dropped as it is the gradient of a loss function and so during optimisation we take steps in the opposite direction, so as to minimise the loss.\n\n̸\n\nSince s { y &lt;k w ,x } j ≤ 1 , for the case i = j , the gradient is guaranteed to be positive for a large enough choice of λ . Similarly, for the case i = j , the gradient is guaranteed to be negative for a large enough λ (as long as s { y &lt;k w ,x } j &gt; 0 ). This therefore fixes the issue in DPO from Section 3.\n\nIn the case when π ratio ( y | x ) ≥ 1 , then we have the standard gradient from L DPO.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "B.2 Derivation for DPOP",
        "chunkIndex": 81,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-82",
      "content": "While the main motivation for DPOP is to avoid the failure mode described in Section 3, we also note its connection to contrastive loss . Contrastive learning is widely used [Wang and Liu, 2021, Wang and Isola, 2020, Saunshi et al., 2019, Oord et al., 2018, Chen et al., 2020, He et al., 2020], often for embedding learning applications. The contrastive loss formulation typically includes two main terms: one encouraging the proximity of analogous inputs, the other encouraging the divergence of distinct classifiable data.\n\nMoreover, the introduction of a margin appended to one of these terms often ensures a more stable training process. This margin serves as an indicator of indifference to point displacement once a specific value threshold is exceeded. The margin, when attached to the similar points term, establishes a minimum threshold beyond which we do not care about pulling the points closer. Alternatively, if added on the dissimilar points term, the margin sets a maximum threshold.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "C Motivation: Contrastive Loss",
        "chunkIndex": 82,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-83",
      "content": "ar points term, establishes a minimum threshold beyond which we do not care about pulling the points closer. Alternatively, if added on the dissimilar points term, the margin sets a maximum threshold.\n\nWe show that the DPO loss is structured such that learning the probabilities during DPO training are equivalent to learning the embeddings in a contrastive loss formulation. However, the standard DPO only uses the term computing distance between dissimilar points, and does not include the similar points term or the margin. Consequently, it is predictable that traditional DPO's inefficiencies mirror the known shortcomings of contrastive training when one constituent term is absent. DPOP, our refined DPO formulation, fixes this by adding the absent term and the margin.\n\nContrastive loss is defined in Hadsell et al. [2006]. If we keep the margin in the similar points terms, it can be written as follows:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "C Motivation: Contrastive Loss",
        "chunkIndex": 83,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-84",
      "content": "ng the absent term and the margin.\n\nContrastive loss is defined in Hadsell et al. [2006]. If we keep the margin in the similar points terms, it can be written as follows:\n\n<!-- formula-not-decoded -->\n\nRecall that the standard DPO loss (Equation (1)) is as follows:\n\n<!-- formula-not-decoded -->\n\nSay we designate an embedding function H :\n\n<!-- formula-not-decoded -->\n\nAnd we define a distance function D as follows:\n\n<!-- formula-not-decoded -->\n\nThe standard DPO only has the dissimilar points term under the analogy of the contrastive loss formulation. For more robust training we accommodate for the similar embeddings term. We use the concept of anchor points or embeddings for both positive and negative points as in triplet loss Schroff et al. [2015]. These points are known ideal embeddings we want our points to achieve. They carry probabilities of 1 and 0 respectively in our equivalence depending on whether they are preferred or dispreferred samples.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "C Motivation: Contrastive Loss",
        "chunkIndex": 84,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-85",
      "content": "ddings we want our points to achieve. They carry probabilities of 1 and 0 respectively in our equivalence depending on whether they are preferred or dispreferred samples.\n\n<!-- formula-not-decoded -->\n\nThe DPOP loss can thus be formulated as:\n\n<!-- formula-not-decoded -->\n\nIf we set the margin m = log 1 π ref ( y w | x ) , the second term is:\n\n<!-- formula-not-decoded -->\n\nThis choice of margin is mathematically equivalent to choosing a threshold which ensures the similarity term only contributes to the loss when the learned model performs worse on the preferred response than the base model.\n\nWe can ignore the third term during training for two primary reasons. First, it is trying to push the log probability of negative samples to negative infinity which may be unstable during training. Second, in essence, it negatively impacts the likelihood of the negative samples.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "C Motivation: Contrastive Loss",
        "chunkIndex": 85,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-86",
      "content": "is trying to push the log probability of negative samples to negative infinity which may be unstable during training. Second, in essence, it negatively impacts the likelihood of the negative samples. However, given our objective of extending the distance between positive and negative samples without diminishing the likelihood of positives, sacrificing this signal is acceptable. In the worst-case scenario, while the probability of negatives may be increased, the consequent increase in the probability of positives is ensured by the first two terms, thus rendering the sacrifice acceptable.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "C Motivation: Contrastive Loss",
        "chunkIndex": 86,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-87",
      "content": "In this section, we give more details and examples of the three paired preference versions of datasets we release. We release all datasets under the Apache 2.0 License.\n\nMetaMath GSM8K [Cobbe et al., 2021] is a dataset of diverse grade school maths word problems, which has been commonly adopted as a measure of the math and reasoning skills of LLMs [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021]. The MetaMath dataset is partitioned into queries and responses, where the query is a question involving mathematical calculation or reasoning, and the response is a logical series of steps and calculations that culminate in a final answer. To construct our paired-preference version of MetaMath, we take the queries as prompts x and the responses as the preferred completions y w .",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 87,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-88",
      "content": "f steps and calculations that culminate in a final answer. To construct our paired-preference version of MetaMath, we take the queries as prompts x and the responses as the preferred completions y w . We create y l by modifying the response by randomly corrupting one of the results of an intermediate calculation so that it is incorrect; however, we leave the answer untouched.\n\nAn example is as follows: x : 'What is the total cost of purchasing equipment for all sixteen players on the football team, considering that each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80?\" y w : 'Each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80. So the total cost for each player is $25 + $15.20 + $6.80 = $ 47 . Since there are sixteen players on the football team, the total cost for all of them is 16 * $47 = $752.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 88,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-89",
      "content": "ir of socks priced at $6.80. So the total cost for each player is $25 + $15.20 + $6.80 = $ 47 . Since there are sixteen players on the football team, the total cost for all of them is 16 * $47 = $752. #### 752 The answer is: 752\" y l : 'Each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80. So the total cost for each player is $25 + $15.20 + $6.80 = $ 52 . Since there are sixteen players on the football team, the total cost for all of them is 16 * $47 = $752. #### 752 The answer is: 752\"\n\nThe dataset contains 393 999 training examples and 1 000 evaluation examples. Our motivation in building this dataset is to align models towards being precise in intermediate calculations. This dataset has low edit distance - the normalised edit distance is approximately 6.5%.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 89,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-90",
      "content": "Our motivation in building this dataset is to align models towards being precise in intermediate calculations. This dataset has low edit distance - the normalised edit distance is approximately 6.5%.\n\nARC ARC [Clark et al., 2018] is a dataset that tests the level of understanding of science at approximately grade-school level. We focus specifically on the 'Challenge' subsection of ARC, the more difficult of the two subsections, which has been widely adopted as a measure of LLM reasoning and world understanding [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021, Cobbe et al., 2021]. We create a paired preference-ranked dataset from the train split of ARC-Challenge. The dataset is partitioned into questions which we take as our prompts x , and four choices of responses to each question of which only one is the correct answer.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 90,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-91",
      "content": "rom the train split of ARC-Challenge. The dataset is partitioned into questions which we take as our prompts x , and four choices of responses to each question of which only one is the correct answer. The correct response is taken as y w and the incorrect responses are taken to be y l ; as there are three incorrect responses for every prompt, we repeat y w multiple times for each prompt. The dataset contains 3357 training examples and 895 evaluation examples. This dataset has a high normalised edit distance of approximately 90%.\n\nHellaSwag Finally, we consider the HellaSwag dataset [Zellers et al., 2019], a dataset containing commonsense inference questions known to be hard for LLMs. An example prompt is 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 91,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-92",
      "content": "containing commonsense inference questions known to be hard for LLMs. An example prompt is 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then' And the potential completions are [ \", the man adds wax to the windshield and cuts it.\", \", a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.\", \", the man puts on a christmas coat, knitted with netting.\", \", the man continues removing the snow on his car.\" ] The dataset contains 119 715 training and 30 126 evaluation examples.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "D Details of Datasets",
        "chunkIndex": 92,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-93",
      "content": "No hyperparameter tuning was done when creating Smaug-34B or Smaug-72B. DPOP has two hyperparameters, β and λ . We chose β = 0 . 3 , similar to prior work [Rafailov et al., 2023], and we chose λ = 50 without trying other values. It is possible that even better performance can be achieved, e.g., with a different value of λ .\n\nHere, we give the licenses of all models used to train our Smaug-series of models.\n\nSmaug-7B started from Llama 2-chat [Touvron et al., 2023b]. Therefore, we release it under the Llama 2 license ( https://ai.meta.com/llama/license/ ).\n\nSmaug-34B started from Bagel-34B-v0.2 [Durbin, 2024a], which itself is a SFT version of Yi-34B-200k [01.AI, 2024]. Therefore, we release Smaug-34B under the Yi Series Models Community License Agreement ( https://github.com/01-ai/Yi/blob/main/MODEL\\_LICENSE\\_AGREEMENT.txt ).\n\nFigure 5: Average log-probs for preferred completions of DPOP and DPO on the MetaMath dataset showing the failure mode of DPO.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.1 Additional Training Details",
        "chunkIndex": 93,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-94",
      "content": ".com/01-ai/Yi/blob/main/MODEL\\_LICENSE\\_AGREEMENT.txt ).\n\nFigure 5: Average log-probs for preferred completions of DPOP and DPO on the MetaMath dataset showing the failure mode of DPO.\n\n<!-- image -->\n\nFigure 6: Average log-probs for preferred completions of DPOP and DPO on the ARC dataset showing the failure mode of DPO.\n\n<!-- image -->\n\nSmaug-72B started from MoMo-72b-lora-1.8.7-DPO [Moreh, 2024], which itself is a fine-tune of Qwen72B [Bai et al., 2023]. Therefore, we release Smaug-72B under the Qwen license ( https://github.com/ QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT ).",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.1 Additional Training Details",
        "chunkIndex": 94,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-95",
      "content": "Log-probabilities of preferred completions In Figure 5, we show the log-probabilities of the preferred completion of the train and eval sets during training on MetaMath. We plot the log-probabilities in more granularity than in Figure 4. We confirm our theoretical insights from Section 4 - the log-probabilities of the preferred completion drop substantially in DPO, whereas they increase for DPOP - across both the train and eval sets. For ARC, we see in Figure 6 that DPOP maintains high train-set log-probs, while both the train and eval set log-probs decrease for DPO. Notably, even though eval set log-probs do decrease for DPOP, they are still higher than the train set log-probs of DPO.\n\nTable 2: Evaluation of the top models according to MT-Bench and MMLU.\n\n| Model                      | MT-bench   | MMLU   | Organization   | License                          |\n|----------------------------|------------|--------|----------------|----------------------------------|\n| GPT-4-1106-preview",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.2 Additional Results",
        "chunkIndex": 95,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-96",
      "content": "-bench   | MMLU   | Organization   | License                          |\n|----------------------------|------------|--------|----------------|----------------------------------|\n| GPT-4-1106-preview         | 9.32       |        | OpenAI         | Proprietary                      |\n| GPT-4-0314                 | 8.96       | 86.4   | OpenAI         | Proprietary                      |\n| GPT-4-0613                 | 9.18       |        | OpenAI         | Proprietary                      |\n| Mistral Medium             | 8.61       | 75.3   | Mistral        | Proprietary                      |\n| Claude-1                   | 7.9        | 77     | Anthropic      | Proprietary                      |\n| Claude-2.0                 | 8.06       | 78.5   | Anthropic      | Proprietary                      |\n| Gemini Pro (Dev API)       |            | 71.8   | Google         | Proprietary                      |\n| Claude-2.1                 | 8.18       |        | Anthropic      | Proprietary",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.2 Additional Results",
        "chunkIndex": 96,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-97",
      "content": "|\n| Gemini Pro (Dev API)       |            | 71.8   | Google         | Proprietary                      |\n| Claude-2.1                 | 8.18       |        | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0613         | 8.39       |        | OpenAI         | Proprietary                      |\n| Mixtral-8x7b-Instruct-v0.1 | 8.3        | 70.6   | Mistral        | Apache 2.0                       |\n| Yi-34B-Chat                |            | 73.5   | 01 AI          | Yi License                       |\n| Gemini Pro                 |            | 71.8   | Google         | Proprietary                      |\n| Claude-Instant-1           | 7.85       | 73.4   | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0314         | 7.94       | 70     | OpenAI         | Proprietary                      |\n| WizardLM-70B-v1.0          | 7.71       | 63.7   | Microsoft      | Llama 2 Community                |\n| Tulu-2-DPO-70B             | 7.89       |",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.2 Additional Results",
        "chunkIndex": 97,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-98",
      "content": "I         | Proprietary                      |\n| WizardLM-70B-v1.0          | 7.71       | 63.7   | Microsoft      | Llama 2 Community                |\n| Tulu-2-DPO-70B             | 7.89       |        | AllenAI/UW     | AI2 ImpACT Low-risk              |\n| Vicuna-33B                 | 7.12       | 59.2   | LMSYS          | Non-commercial                   |\n| Starling-LM-7B-alpha       | 8.09       | 63.9   | UC Berkeley    | CC-BY-NC-4.0                     |\n| Smaug-72B                  | 7.76       | 77.15  | Abacus.AI      | tongyi-qianwen-license-agreement |\n\nAdditional tables In Table 4, we give an extension of Table 1 (whose experimental details are in Section 6). In Table 5, we give the same table, except for models of size 34B or lower.\n\nMT-Bench Next, we evaluate using MT-Bench [Zheng et al., 2023], a challenging benchmark that uses GPT-4 [OpenAI, 2023] to score candidate model responses across eight different categories of performance.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.2 Additional Results",
        "chunkIndex": 98,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-99",
      "content": "Bench Next, we evaluate using MT-Bench [Zheng et al., 2023], a challenging benchmark that uses GPT-4 [OpenAI, 2023] to score candidate model responses across eight different categories of performance. As shown in other works [Zheng et al., 2023, Rafailov et al., 2023], strong LLMs such as GPT-4 show good agreement with human preferences. We note that MT-Bench questions were not included in our fine-tuning datasets in any way. We run MT-Bench with the Llama-2 conversation template [Touvron et al., 2023b]. See Appendix Table 2 for a comparison with state-of-the-art LLMs according to Arena Elo as of March 2024. Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models. In Appendix E, we give examples of Smaug-72B completions to MT-Bench questions.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.2 Additional Results",
        "chunkIndex": 99,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-100",
      "content": "Since LLMs train on wide swaths of the internet, data contamination, i.e., evaluating on examples that are included in the training data, is a growing concern. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities [Roberts et al., 2024, Jain et al., 2024, bench authors, 2023].\n\nWhile there is no perfect tool to check for data contamination, we use an open-source contamination checker ( https://github.com/swj0419/detect-pretrain-code-contamination ) to compare the contamination of our model to other open-source models - see Table 3. On ARC, TruthfulQA, and GSM8K, we\n\nTable 3: Comparison of contamination values across models, according to https://github.com/swj0419/ detect-pretrain-code-contamination . A score above 0.85 indicates likely contamination.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 100,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-101",
      "content": "d GSM8K, we\n\nTable 3: Comparison of contamination values across models, according to https://github.com/swj0419/ detect-pretrain-code-contamination . A score above 0.85 indicates likely contamination.\n\n| Model      |   Smaug-72B |   MoMo-72B-lora-1.8.7-DPO |   Llama-2-70B |\n|------------|-------------|---------------------------|---------------|\n| ARC        |        0.2  |                      0.2  |          0.22 |\n| TruthfulQA |        0.45 |                      0.39 |          0.51 |\n| GSM8K      |        1    |                      1    |          0.89 |\n\nTable 4: (Extension of Table 1.) Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard as of March 2024. Our 72B model achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model (other than a fine-tune of our own model).",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 101,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-102",
      "content": "ccuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model (other than a fine-tune of our own model).\n\n| Model                                           |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Trut",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 102,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-103",
      "content": "86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |\n| CCK_Asura_v1                                    |  77.43 | 73.89 |       89.07 |  75.44 |        71.75 |        86.35 |   68.08 |\n| FusionNet_34Bx2_MoE_v0.1                        |  77.38 | 73.72 |       86.46 |  76.72 |        71.01 |        83.35 |   73.01 |\n| MoMo-72B-lora-1.8.6-DPO                         |  77.29 | 70.14 |       86.03 |  77.4  |        69    |        84.37 |   76.8  |\n| Smaug-34B-v0.1 (Ours)                           |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| Truthful_DPO_TomGrc_FusionNet_34Bx2_MoE         |  77.28 | 72.87 |       86.52 |  76.96 |        73.28 |        83.19 |   70.89 |\n| DARE_TIES_13",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 103,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-104",
      "content": "76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| Truthful_DPO_TomGrc_FusionNet_34Bx2_MoE         |  77.28 | 72.87 |       86.52 |  76.96 |        73.28 |        83.19 |   70.89 |\n| DARE_TIES_13B                                   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                    |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| FusionNet_34Bx2_MoE                             |  77.07 | 72.95 |       86.22 |  77.05 |        71.31 |        83.98 |   70.89 |\n| MoE_13B_DPO                                     |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO  |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |\n| MixTAO-7Bx2-MoE-Instruct-v7.0                   |  76.55 | 74.23 |       89.37 |  64.54 |        74.26 |        87.77 |   69.14 |\n| Truthful_DPO_cloudyu",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 104,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-105",
      "content": ".44 |        72.78 |        82.95 |   71.19 |\n| MixTAO-7Bx2-MoE-Instruct-v7.0                   |  76.55 | 74.23 |       89.37 |  64.54 |        74.26 |        87.77 |   69.14 |\n| Truthful_DPO_cloudyu_Mixtral_34Bx2_MoE_60B0     |  76.48 | 71.25 |       85.24 |  77.28 |        66.74 |        84.29 |   74.07 |\n| MoMo-72B-lora-1.8.4-DPO                         |  76.23 | 69.62 |       85.35 |  77.33 |        64.64 |        84.14 |   76.27 |\n| FusionNet_7Bx2_MoE_v0.1                         |  76.16 | 74.06 |       88.9  |  65    |        71.2  |        87.53 |   70.28 |\n| MBX-7B-v3-DPO                                   |  76.13 | 73.55 |       89.11 |  64.91 |        74    |        85.56 |   69.67 |\n\nfind that Smaug-72B achieves scores that are similar to MoMo-72B-lora-1.8.7-DPO (the starting point of Smaug-72B), as well as Llama-2-70B.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "E.3 Contamination Checker",
        "chunkIndex": 105,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-106",
      "content": "In this section, we give example completions by Smaug-72B for questions in MT-Bench [Zheng et al., 2023]. Note that these are not cherry-picked - they include examples of both good and bad completions.\n\nTable 5: Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard for models less than 35B parameters as of March 2024. Our 34B model achieves best-in-its-class performance compared to other models of similar or smaller size.\n\n| Model                                          | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-34B-v0.1 (Ours)                          | <35B   |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| DARE_TIES_13B                                  | <35B   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.0",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "F Example Completions",
        "chunkIndex": 106,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-107",
      "content": "| 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| DARE_TIES_13B                                  | <35B   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                   | <35B   |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| MoE_13B_DPO                                    | <35B   |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO | <35B   |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "F Example Completions",
        "chunkIndex": 107,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-108",
      "content": "Question: Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n\nOutline:\n\n1. Introduction to the two smartphone models being compared (mention brand names and models)\n2. Design and Build Quality\n- Comparison of dimensions, weight, materials used, and overall aesthetic appeal\n- Water and dust resistance, if applicable\n3. Display and Screen Quality\n- Comparison of screen technology (OLED, IPS LCD, etc.)\n- Display resolution, refresh rate, and brightness levels\n4. Performance and Hardware\n- Processor, RAM, and storage configurations\n- Comparison of benchmark scores (e.g., AnTuTu, Geekbench)\n5.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 1",
        "chunkIndex": 108,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-109",
      "content": ", etc.)\n- Display resolution, refresh rate, and brightness levels\n4. Performance and Hardware\n- Processor, RAM, and storage configurations\n- Comparison of benchmark scores (e.g., AnTuTu, Geekbench)\n5. Camera System and Image Quality\n- Comparison of primary, ultra-wide, telephoto, and macro lenses (if available)\n- Comparison of low-light performance and image processing capabilities\n6. Battery Life and Charging\n- Battery capacity and estimated usage time\n- Charging technologies (wired, wireless, fast charging, reverse charging)\n7. Software and User Experience\n- Comparison of operating systems (iOS vs. Android) and their features\n- User interface, customization options, and app ecosystem\n8. Additional Features and Extras\n- Comparison of biometric authentication (fingerprint, face unlock, etc.)\n- Additional features like NFC, 5G connectivity, and audio quality\n9.",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 1",
        "chunkIndex": 109,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-110",
      "content": "ons, and app ecosystem\n8. Additional Features and Extras\n- Comparison of biometric authentication (fingerprint, face unlock, etc.)\n- Additional features like NFC, 5G connectivity, and audio quality\n9. Pricing and Value for Money\n- Comparison of retail prices and current offers\n- Assessment of overall value considering the features and performance\n10. Conclusion and recommendation for potential buyers based on their preferences",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 1",
        "chunkIndex": 110,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-111",
      "content": "Question: In a survey conducted at a local high school, preferences for a new school color were measured: 58% of students liked the color blue, 45% preferred green, and 22% liked both colors. If we randomly pick a student from the school, what's the probability that they would like neither blue nor green?\n\nTo find the probability that a randomly picked student would like neither blue nor green, we need to find the percentage of students who like only one color or neither color.\n\nFrom the given information, we know:\n\n- 58% like blue\n- 45% prefer green\n- 22% like both colors\n\nTo find the percentage of students who like only one color, we can subtract the percentage of students who like both colors from the sum of students who like blue and green:\n\n- 58% (blue) + 45% (green) - 22% (both) = 83%\n\nNow, to find the probability of a student liking neither blue nor green, we subtract the percentage of students who like at least one color from 100%:\n\n- 100% - 83% = 17%",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 2",
        "chunkIndex": 111,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-112",
      "content": "% (green) - 22% (both) = 83%\n\nNow, to find the probability of a student liking neither blue nor green, we subtract the percentage of students who like at least one color from 100%:\n\n- 100% - 83% = 17%\n\nSo, the probability that a randomly picked student would like neither blue nor green is 17%.\n\nQuestion: Implement a program to find the common elements in two arrays without using any",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 2",
        "chunkIndex": 112,
        "totalChunks": 114
      }
    },
    {
      "id": "2402.13228v2-chunk-113",
      "content": "Finding common elements in two arrays without using extra data structures can be achieved by using a simple algorithm. Here's a Python implementation of this problem:\n\n```\ndef find_common_elements(arr1, arr2): common_elements = [] for num1 in arr1: if num1 in arr2 and num1 not in common_elements: common_elements.append(num1) return common_elements # Example usage: array1 = [1, 2, 3, 4, 5] array2 = [4, 5, 6, 7, 8] print (find_common_elements(array1, array2)) # Output: [4, 5]\n```\n\nThis program iterates through the elements of the first array (arr1) and checks if each element is present in the second array (arr2). If it is, and the element hasn't been added to the common\\_elements list yet, it is appended. The function then returns the list of common elements.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2402.13228v2",
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
          "Arka Pal",
          "Deep Karkhanis",
          "Samuel Dooley",
          "Manley Roberts",
          "Siddartha Naidu",
          "Colin White"
        ],
        "section": "Example 3 extra data structures.",
        "chunkIndex": 113,
        "totalChunks": 114
      }
    }
  ],
  "fullText": "## Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive\n\nArka Pal * , Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White\n\n## Abacus.AI\n\n## Abstract\n\nDirect Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.\n\n## 1 Introduction\n\nAligning large language models (LLMs) with preference data is important for their fluency and applicability to many tasks, with the natural language processing literature using many techniques to incorporate either human or ground-truth feedback [Christiano et al., 2017, Stiennon et al., 2020, Ouyang et al., 2022]. Typically in LLM alignment, we first collect large amounts of preference data, consisting of a context and two potential completions; one of these is labelled as the preferred completion, and the other as the dispreferred. We use this data to learn a general policy for generating completions in a given context. Direct Preference Optimisation (DPO) [Rafailov et al., 2023] is a popular method for learning from preference data, and it has shown to be effective at improving the performance of pretrained LLMs on downstream tasks such as reasoning, summarisation, and alignment [Wang et al., 2023, Tunstall et al., 2023]. The theoretical motivation for DPO is based on a preference-ranking model with an implicit reward function that models the relative probability of picking the preferred completion over the dispreferred.\n\n* Correspondence to: arka.pal@gmail.com\n\nFigure 1: DPOP avoids a failure mode of DPO. When preference pairs differ on only a few tokens, DPO receives no loss incentive for the early tokens, and a loss incentive that in some cases can lead to degradation of the log-probs of later tokens (Section 3). We introduce DPOP, which adds a new term to the loss which leads every token to be incentivised toward the preferred completion (Section 4).\n\n<!-- image -->\n\nIn this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred completions, as long as the relative probability between the preferred and dispreferred classes increases. Our theoretical analysis suggests that the problem occurs most frequently in preference datasets with small edit distances between each pair of completions. Specifically, we show that in the case of preferred and dispreferred examples that differ by few tokens, DPO increases the probability of the token(s) that differ, yet decreases the probability of subsequent tokens (see Figure 1 for an overview). We also present an empirical token-level analysis that matches our theoretical findings on common datasets.\n\nUsing these insights, we design a new loss function: DPO-Positive (DPOP), which adds a new term to the loss function that penalises reducing the probability of the positive completions. We also create new preference datasets based on ARC [Clark et al., 2018], HellaSwag [Zellers et al., 2019], and MetaMath [Yu et al., 2023] and use them along with DPOP to create new models.\n\nWe introduce the Smaug class of models which use DPOP to achieve state-of-the-art open-source performance. We fine-tune 7B, 34B, and 72B models on our new datasets and show that DPOP far outperforms DPO. We evaluate our resulting models on multiple benchmarks including the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021], which aggregates six popular benchmarks such as MMLU [Hendrycks et al., 2021] and GSM8K [Cobbe et al., 2021]. On the HuggingFace Open LLM Leaderboard, Smaug-72B achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model.\n\nIn order to address potential concerns about pervasive contamination on the HuggingFace Open LLM Leaderboard, we use an open-source contamination checker, finding that our model scores similarly to popular existing models. Then, we show that DPOP outperforms DPO in an apples-to-apples comparison on an LLM-judged benchmark that is independent of the fine-tuning data: MT-Bench [Zheng et al., 2023], a challenging benchmark representing eight different categories. We release our code, models, datasets, and documentation at https://github.com/abacusai/smaug .\n\nOur contributions. We describe our main contributions below.\n\n- We theoretically and empirically show a surprising failure mode of DPO: running DPO on preference datasets with small edit distances between completions can result in a catastrophic decrease in accuracy.\n- We introduce DPO-Positive (DPOP) which we theoretically and empirically show ameliorates the performance degradation. In particular, DPOP often outperforms DPO, even on preference datasets with high edit distances between completions.\n- We create new preference-based versions of ARC, HellaSwag, and MetaMath.\n- Using DPOP and our new datasets, we create the Smaug series of models, with Smaug-72B becoming the first open-source model to achieve an average accuracy of 80% on the HuggingFace Open LLM Leaderboard. We open-source our trained models, datasets, and code.\n\n## 2 Background and Related Work\n\nLarge language models (LLMs) have shown impressive zero-shot and few-shot performance [Radford et al., 2019, Brown et al., 2020, Bubeck et al., 2023]. Recently, researchers have fine-tuned pretrained LLMs on downstream tasks by using human-written completions [Chung et al., 2022, Mishra et al., 2021] or by using datasets labelled with human-preferred completions relative to other completions [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020]. These techniques have been used to improve performance on a variety of downstream tasks such as translation [Kreutzer et al., 2018] and summarisation [Stiennon et al., 2020], as well as to create general-purpose models such as Zephyr [Tunstall et al., 2023]. Two of the most popular techniques for learning from preference data are reinforcement learning from human feedback (RLHF) [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020] and direct preference optimisation (DPO) [Rafailov et al., 2023]. We summarise these approaches below.\n\nRLHF Consider a dataset of pairwise-preference ranked data D = { x ( i ) , y ( i ) w , y ( i ) l } N i =1 where x ( i ) are prompts and y ( i ) w and y ( i ) l are respectively the preferred and dispreferred completions conditioned on that prompt. We have an initial LLM π ref that parameterises a distribution π ref ( y | x ) . Often, we initialise π ref as an LLM that has undergone supervised fine-tuning (SFT) to improve performance on downstream task(s). RLHF begins by modelling the probability of preferring y w to y l using the Bradley-Terry model [Bradley and Terry, 1952] which posits the following probabilistic form:\n\n<!-- formula-not-decoded -->\n\nwhere σ is the logistic function and r ( x, y ) corresponds to some latent reward function that is assumed to exist for the completion y given the prompt x . Given D , we can learn a parameterised estimate of r by minimising the negative log-likelihood of the dataset:\n\n<!-- formula-not-decoded -->\n\nFor RLHF, we use reinforcement learning to optimise based on this learned reward function r ϕ (with a regularising KL-constraint to prevent model collapse), and obtain a new LLM distribution π θ .\n\nDPO Rafailov et al. [2023] showed that it is possible to optimise the same KL-constrained reward function as in RLHF without having to learn an explicit reward function. Instead, the problem is cast as a maximum likelihood optimisation of the distribution π θ directly, with the objective:\n\n<!-- formula-not-decoded -->\n\nwhere β is a regularisation term corresponding to the strength of KL-regularisation in RLHF. In this case, the implicit reward parameterisation is r ( x, y ) = β log π θ ( y | x ) π ref ( y | x ) , and Rafailov et al. [2023] further showed that all reward classes under the Plackett-Luce model [Plackett, 1975, Luce, 2005] (such as Bradley-Terry) are representable under this parameterisation. For an abbreviation, we define π ratio ( y | x ) = π θ ( y | x ) π ref ( y | x ) .\n\nAlternatives to DPO to align models to preference data in the offline, differentiable setting have been proposed. We discuss the most relevant to our work below and further in Appendix A.\n\nSLiC Zhao et al. [2022, 2023] provides an alternative formulation for learning from pairwise preference data, either directly or on preference pairs sampled from the SFT policy. Their loss function is:\n\n<!-- formula-not-decoded -->\n\nwhere y ref is a reference target sequence and λ is a scalar. The second term in the above is a cross-entropy loss and plays the role of regularisation towards the original model, but without needing an extra copy of the weights.\n\nIPO Azar et al. [2023] aims to understand the theoretical underpinnings of RLHF and DPO. They identify that DPO may be prone to overfitting in situations where the preference probability of the preferred over the dispreferred examples is close to 1. They propose an alternative form of pairwise preference loss-'IdentityPO (IPO)'. IPO tries to prevent overfitting to the preference dataset by penalising exceeding the preference margin beyond this regularised value. Conversely, we identify that DPO can lead to underfitting as well-even complete performance degradation.\n\nSubsequent work Very recently, subsequent work has verified our main findings from Section 3 and Section 4 on the existence of a failure mode of DPO and how to fix it [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]. Pang et al. find that when fine-tuning on chain-of-thought reasoning tasks, adding a term similar to our Equation (3) is crucial to achieve strong performance [Pang et al., 2024]. Other work builds off of our theoretical insights by applying additional mathematical analysis, verifying our claims in Section 3 that DPO can decrease the probability of preferred completions [Feng et al., 2024]. Finally, Rafailov et al. analysed the same DPO failure mode phenomenon, showing that the likelihood of the preferred response should decrease during DPO training, when starting from a model that has undergone SFT [Rafailov et al., 2024]. We give additional discussion on related work in Appendix A.\n\n## 3 Failure Mode of DPO\n\nIn this section, we take a step back and examine the DPO loss in Equation (1), specifically with an eye towards how it can reduce the probability of the preferred completion. The loss is a function only of the difference in the log-ratios, which means that we can achieve a low loss value even if π ratio ( y w | x ) is lowered below 1, as long as π ratio ( y l | x ) is also lowered sufficiently. This implies that the log-likelihood of the preferred completions is reduced below the original log-likelihood from the reference model.\n\nWhy is this an issue? The original use-case of RLHF did not explicitly denote the preferred completions as being also satisfactory completions (rather than just the preferred completion out of the two choices y w and y l ), and hence the DPO objective is a good modelling choice. However, since then, a large body of work has focused on distilling the knowledge of powerful models into smaller or weaker models, while also\n\nshowing that doing so with RLHF/DPO outperforms SFT [Taori et al., 2023, Tunstall et al., 2023, Xu et al., 2023, Chiang et al., 2023]. In this paradigm, it is often the case that in each pair of completions, the better of the two is indeed also a satisfactory completion. Furthermore, a new technique is to transform a standard labelled dataset into a pairwise preference dataset [Ivison et al., 2023, Tunstall et al., 2023], which also has the property that for each pair of completions, one is a satisfactory completion.\n\nEdit Distance 1 While the above illustrates a hypothetical situation, now we provide a specific case in which DPO may cause a decrease in the probability of the better completion. Consider the case of trying to improve a model's math or reasoning abilities by comparing a completion of '2+2=4' to '2+2=5.' This process creates a pair of preferred and dispreferred completions which have an edit (Hamming) distance of 1, i.e., all tokens in the completion are the same except for one. In the following, we will explore how the location of the differing token impacts the computation of the DPO loss. For sake of argument, we will examine what happens when the differing token is the first token, though the argument also follows if it appears elsewhere.\n\nFor preliminaries, consider two completions with an edit distance of 1 which differ at token m with 1 ≤ m ≤ K , i.e., consider y w = ( t 1 , . . . , t K ) and y l = ( t 1 , . . . , t m -1 , t ′ m , t m +1 , . . . , t K ) . Denote y &lt;r = ( t 1 , . . . , t r -1 ) and y ≥ r = ( t r , . . . , t K ) . Assume that the vocabulary length of the LLM is L . Let s { x } i represent the probability of the i -th token in the model's vocabulary conditioned on the input x . While the LLM model parameters θ are numerous, we restrict our attention to the logits, θ j with j ∈ [ L ] .\n\nThe gradient of Equation (1) with respect to θ is given by:\n\n<!-- formula-not-decoded -->\n\nWe note first that for any m&gt; 1 , all tokens in positions from 1 to m -1 have no effect on the gradient, as for all 1 ≤ i ≤ m -1 , π θ ( t i | y &lt;k w , x ) = π θ ( t i | y &lt;k l , x ) , causing these tokens' contribution to the gradient to cancel out. Therefore, without loss of generality, assume m = 1 , i.e., y w and y l differ only at the first token. Without loss of generality, we also assume that t k takes vocabulary position 1. Then we have the following for each k &gt; 1 (derivation in Appendix B.1):\n\n<!-- formula-not-decoded -->\n\n̸\n\nAs we typically run DPO after SFT, the model is likely to be reasonably well optimised, so we should have s { y &lt;k w ,x } j ≤ s { y &lt;k l ,x } j for j = 1 and s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 . Therefore, while this analysis only extends to gradients with respect to the logits, we see that the gradient decreases logits corresponding to the correct token and increases logits corresponding to the incorrect tokens. Surprisingly, this suggests that under DPO, all locations in the sequence after the differing token will have reduced probability of emitting the correct token when compared to π ref. We give empirical evidence for this in Section 5 and Figure 4.\n\n## 4 DPOP\n\nNow, we introduce DPO-Positive (DPOP), our proposed method to address the failure mode described in the previous section. DPOP adds the penalty term max ( 0 , log π ref ( y w | x ) π θ ( y w | x ) ) to the loss within the log-sigmoid to incentivise maintaining a high log-likelihood of the preferred completions. This penalty term is 0 when π ratio ( y w | x ) ≥ 1 and increases as the ratio goes below 1.\n\nThe full DPOP loss function is thus:\n\n<!-- formula-not-decoded -->\n\nwhere λ &gt; 0 is a hyperparameter that can be tuned. This form of loss retains the property that we are fitting parameters on the preference data under the Bradley-Terry model. The implicit reward parameterisation is\n\n<!-- formula-not-decoded -->\n\nBy applying this optimisation pressure, the model can no longer minimise the loss by significantly reducing the log-likelihood of the dispreferred examples more than it reduces the log-likelihood of the preferred examples; it must also ensure that the log-likelihood of the preferred examples remains high relative to the log-likelihood under the reference model.\n\nNow we show that Equation (3) mitigates the failure mode from the previous section. Recall from Section 3 that we focused on two completions, y w and y l , which differ by one token at location m = 1 . We showed in Equation (2) that for standard DPO, the gradient of the k -th token in the completions with respect to the j -th logit is s { y &lt;k l ,x } j -s { y &lt;k w ,x } j . However, for DPOP, if π ratio &lt; 1 , the gradients become (derivation in Appendix B.2):\n\n̸\n\n<!-- formula-not-decoded -->\n\n̸\n\nwhere i is the vocabulary index of token t k . Since s { y &lt;k w ,x } j ≤ 1 , for the case i = j , the gradient is guaranteed to be positive for a large enough choice of λ . Similarly, for the case i = j , the gradient is guaranteed to be negative for a large enough λ (as long as s { y &lt;k w ,x } j &gt; 0 ). This therefore fixes the issue we identified in DPO in Section 3.\n\nConnection to Contrastive Loss While the main motivation for DPOP is to avoid the failure mode described in Section 3, we also note its connection to contrastive loss . Contrastive learning is a popular technique in areas such as computer vision for datasets of similar and dissimilar pairs [Oord et al., 2018, Chen et al., 2020, He et al., 2020], and the loss function often uses a margin factor. Equation (3) can be viewed as similar to contrastive loss with margin m = log 1 π ref ( y w | x ) . We give further details in Appendix C.\n\n## 5 DPOP Datasets &amp; Experiments\n\nIn this section, we empirically validate that the failure mode does arise in practice and that DPOP is able to mitigate the failure. We also show that even when the edit distance is large and DPO does not show degradation in performance, DPOP can still outperform DPO on downstream task evaluation.\n\nFigure 2: Preference optimization comparisons on low (MetaMath) and high (ARC) edit distance datasets . DPOP outperforms DPO, IPO, and SLiC on both MetaMath (left) and ARC (right), whose normalized edit distances are 6.5% and 90%, respectively. Evaluation is performed on the test set of the datasets using the LLM Evaluation Harness.\n\n<!-- image -->\n\n## 5.1 Dataset Creation\n\nFor our empirical analysis, we focus on the downstream tasks of GSM8K , ARC , and HellaSwag , and we introduce and release associated paired preference-ranked datasets.\n\nGSM8K [Cobbe et al., 2021], a dataset of diverse grade school math word problems, has been adopted as a measure of the math and reasoning skills of LLMs [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021]. We create a paired preference-ranked version of MetaMath [Yu et al., 2023], an extended version of the GSM8K training data [An et al., 2023, Yu et al., 2023]. The correct completions in the MetaMath dataset consist of a series of steps which lead to the final answer. To create a dispreferred version, we randomly corrupt one of the results of an intermediate calculation. This dataset has a low (normalised) edit distance of 6.5%.\n\nARC [Clark et al., 2018] is a dataset that tests the level of understanding of science at grade-school level. We focus specifically on ARC-Challenge, the more difficult of the two subsections of ARC, which has been widely adopted as a measure of LLM reasoning and world understanding [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Cobbe et al., 2021]. The ARC-Challenge dataset consists of four choices of responses to each question, one of which is correct. To create a paired preference-ranked dataset, for each correct response in the training split, we create three pairs using each incorrect response. Due to the differences in the responses, this dataset has a high normalised edit distance of 90%.\n\nHellaSwag [Zellers et al., 2019] is a dataset containing commonsense inference questions known to be hard for LLMs. Similar to ARC, each question has one correct completion and three incorrect completions, and so we create a paired preference-ranked dataset by creating three pairs for each correct response in the training split. See Appendix D for further details and documentation about our new datasets.\n\n## 5.2 Experiments\n\nIn this section, we compare training DPO, IPO, SLiC and DPOP on the train splits of the datasets mentioned above and evaluate them on the corresponding tasks. We apply each preference-training method to the base\n\nFigure 3: Ablation studies: Evaluation of DPO vs. DPOP for different values of β , on the MetaMath dataset (left), as well as for different values of λ , on the MetaMath (middle) and ARC (right) datasets.\n\n<!-- image -->\n\nmodel of Mistral7B [Jiang et al., 2023]. We evaluate on the test sets of GSM8K and ARC with the LLM Evaluation Harness [Gao et al., 2021]. Unless specified otherwise, we use values of β = 0 . 3 and λ = 50 .\n\nLoss function comparison First, we compare DPO, IPO, SLiC and DPOP when training on both MetaMath and ARC; see Figure 2. We find that when training on MetaMath, both DPO and SLiC catastrophically fail, while IPO does not improve performance. DPOP is the only model to improve performance over the base model. When training on ARC, which has a higher edit distance as described in the previous section, both DPO, SLiC and DPOP are able to improve on the base model significantly; however, DPOP performs best.\n\nAblation studies for β and λ One potential hypothesis for how degradation of DPO on MetaMath could be prevented is by modifying the strength of the regularisation parameter, β . We test β ∈ { 0 . 1 , 0 . 3 , 1 . 0 } , and although a larger β does induce a slower decrease, the performance with DPO still plummets, while DPOP shows strong and consistent performance with different values of β (see Figure 3). Furthermore, we conduct an ablation study over the value of λ (a hyperparameter unique to DPOP) to determine the sensitivity of performance to tuning this value precisely. We test λ ∈ { 5 , 50 , 500 } and find that performance on MetaMath and ARC is relatively unaffected by the choice of λ . See Figure 3.\n\nToken-level analysis Recall that in Section 3, we gave theoretical motivations for why DPO is likely to perform poorly on low-edit distance datasets. We now analyse the log-probabilities of the trained models at the token level on the MetaMath dataset over 900 samples to empirically support our arguments. Let us denote the index of the first token that is different between the preferred and dispreferred completion by m .\n\nWe suggested that π θ ( y ≥ r | x, y &lt;r ) for r &gt; m will have 'wrong-way' gradient updates and therefore decrease. We find this is indeed the case-the average log-prob after training of tokens after m is -0 . 37 for the reference model and -0 . 26 for DPOP, but -1 . 82 for DPO on the preferred completions (see (Figure 4) (left)). Perhaps most instructively, for both the reference model and DPOP, in Figure 4 (right), we see that tokens after the edit indices show higher log-likelihood than those before the edit indices-this is indicative of well-behaved language modelling, with lower perplexity as more tokens are added to the context. By contrast, DPO shows the opposite pattern-with log-likelihood actually reducing after the edit indices. This is indicative of a deeper breakdown in language modelling, which we believe is facilitated by the wrong-way gradient we outlined in Section 3. Finally, we are also able to substantiate our assumption from Section 3 that s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 ; we find from our analysis that for the baseline model, the tokens after the edit have\n\nFigure 4: DPO fails to train on low edit-distance pairs, yet DPOP performs well. Left: average log-probs for 900 randomly-sampled preferred train set completions on MetaMath across training steps. For DPO, the log-probs decrease throughout training. Right: average log-prob difference for preferred completions on MetaMath by location around differing tokens, after 1000 training steps. Log-prob 'difference' signifies that each model's plot has been adjusted to have 0 log-prob at location -1; all other log-probs are shown relative to this value. For DPO, there is a significant decrease after the differing tokens, while DPOP avoids this issue. The log-probs of the LLM prior to application of DPOP or DPO is also included for reference.\n\n<!-- image -->\n\nan average log-likelihood of -0 . 37 on the preferred completion, but this drops to -0 . 86 on the dispreferred completion.\n\nIn Appendix E, we present additional results on ARC comparing the averaged log-probs of DPO and DPOP on the preferred completion during training; see Appendix Figure 6. DPOP once again demonstrates higher log-probs than DPO.\n\n## 6 Smaug\n\nIn this section, we introduce the Smaug series of models. We train models for 7B, 34B and 72B parameter sizes using DPOP. We use the 7B class for a direct comparison of DPOP vs. DPO, including testing the generalisability of the improvement in model performance on benchmarks very different from the training data, such as MT-Bench [Zheng et al., 2023]. Due to the computational resource requirements involved in training the larger model sizes, we only perform DPOP on 34B and 72B and compare to other models on the HuggingFace Open LLM Leaderboard. Also due to computational expense, we do not perform any further hyperparameter tuning.\n\nDatasets In this section, unless otherwise noted, we train on a mix of six pair preference datasets: our MetaMath DPO, ARC DPO, and HellaSwag DPO datasets described in Section 5, the ORCA DPO dataset [Intel, 2024], the Truthy DPO dataset [Durbin, 2024b], and the UltraFeedback Binarized dataset [AllenAI, 2024]. We run these experiments with 8 H100 GPUs (each with 80GB), using the transformers repository [Wolf et al., 2020] for general model loading and infrastructure, and the TRL repository [von Werra et al., 2020] for running DPOP. We set β = 0 . 3 , λ = 50 , a learning rate of 5 × 10 -5 , and the AdamW optimizer [Loshchilov and Hutter, 2017], and we run 1000 steps for all DPO and DPOP routines.\n\n## 6.1 DPOP vs. DPO Ablation on 7B\n\nFirst we apply the above recipe on Llama-2-Chat [Touvron et al., 2023b]. Since this model has already undergone instruction fine-tuning, we perform DPO and DPOP directly using the datasets described in the previous section. We evaluate the Llama-2 finetunes on MT-Bench [Zheng et al., 2023] This benchmark tests across multiple categories of LLM performance (for example: writing, roleplay, coding and math).\n\nMT-Bench We evaluate MT-Bench on Llama-2-7B-Chat finetuned with DPO and DPOP. We run MTBench with the Llama-2 conversation template. We perform 10 trials due to the inherent stochasticity of the benchmark, computing mean and standard deviation. Across the 10 trials, DPO achieves a first turn score of 7 . 032 ± 0 . 043 whilst DPOP scores 7 . 292 ± 0 . 037 , a significant improvement.\n\nMT-Bench tests across eight diverse categories and is significantly different from our fine-tuning data. Furthermore, MT-Bench has been shown to have performance correlated to human preferences of LLM rankings [Zheng et al., 2023, Rafailov et al., 2023, Li et al., 2024]. The outperformance of DPOP vs. DPO in this controlled like-for-like setting is therefore an indication of the improvement gained by utilising DPOP over DPO on new tasks not in the training set, and also an indication of general model quality.\n\n## 6.2 Smaug-34B and Smaug-72B\n\nSmaug-34B is a modified version of the base model Bagel-34B-v0.2 [Durbin, 2024a], which itself is a SFT version of Yi-34B-200k [01.AI, 2024]. We first take Bagel-34B-v0.2 and perform a SFT fine-tune using a combination of three datasets: MetaMath [Yu et al., 2023], ORCA-Chat [Es, 2024], and the ShareGPT dataset [Z., 2024]. We then apply DPOP using the methodology and datasets described above.\n\nFor 72B, we start from MoMo-72b-lora-1.8.7-DPO [Moreh, 2024], which itself is a fine-tune of Qwen72B [Bai et al., 2023]. MoMo-72b-lora-1.8.7-DPO has already undergone SFT, so we simply run the DPOP routines as in Smaug-34B. The total training time is 144 hours.\n\nHuggingFace Open LLM Leaderboard We evaluate using the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021], a widely-used benchmark suite that aggregates six popular benchmarks: ARC [Clark et al., 2018], GSM8K [Cobbe et al., 2021], HellaSwag [Zellers et al., 2019], MMLU [Hendrycks et al., 2021], TruthfulQA [Lin et al., 2022], and WinoGrande [Sakaguchi et al., 2020]. We evaluate directly in HuggingFace, which uses the LLM Evaluation Harness [Gao et al., 2021]. It performs few-shot prompting on the test sets of these tasks. We compare Smaug-72B to the evaluation scores of the top five open-weight LLMs according to the HuggingFace Open LLM Leaderboard [Beeching et al., 2023, Gao et al., 2021] as of March 2024; see Table 1. Smaug-72B achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model. Smaug-34B also achieves best-in-its-class performance compared to other models of similar or smaller size (see Appendix E).\n\nMT-Bench Next, we evaluate again using MT-Bench [Zheng et al., 2023]. We run MT-Bench with the Llama-2 conversation template [Touvron et al., 2023b]. See Appendix Table 2 for a comparison with stateof-the-art LLMs according to Arena Elo as of March 2024. Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models. In Appendix E, we give examples of Smaug-72B completions to MT-Bench questions.\n\nTable 1: Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard as of March 2024. See Table 4 for an extended comparison.\n\n| Model                                           | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                | 72B+   |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         | 72B+   |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         | 72B+   |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO | 72B+   |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      | 72B+   |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |\n\n## 6.3 Contamination check\n\nSince LLMs train on wide swaths of the internet, data contamination, i.e., evaluating on examples that are included in the training data, is a growing concern. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities [Roberts et al., 2024, Jain et al., 2024, bench authors, 2023].\n\nWhile there is no perfect tool to check for data contamination, we use an open-source contamination checker [Shi, 2023] to compare the contamination of our model to other open-source models on training datasets of the Huggingface Open LLM Leaderboard. On ARC, TruthfulQA, and GSM8K, we find that Smaug-72B achieves scores that are similar to MoMo-72B-lora-1.8.7-DPO (the starting point of Smaug-72B), as well as Llama-2-70B. See the full details in Appendix E.3.\n\n## 7 Conclusions and Limitations\n\nIn this work, we presented new theoretical findings on a severe failure mode of DPO, in which fine-tuning causes the probability of the preferred examples to be reduced. We then presented an empirical token-level analysis that matches our theoretical findings on popular datasets. In order to mitigate this issue, we devised a new technique, DPOP, which we show overcomes the failure mode of DPO - and can outperform DPO even outside this failure mode. By fine tuning with DPOP on our new pairwise preference versions of ARC, HellaSwag, and MetaMath, we create a new LLM that achieves state-of-the-art performance. In particular, we present the first open-weights model to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard, and we also show that DPOP significantly outperforms DPO in an apples-to-apples comparison on MT-Bench, which is independent of the fine-tuning data.\n\nIn the future, creating pairwise preference-based versions of other datasets, and running DPOP with these datasets, could push the abilities of open-source LLMs closer to the performance of proprietary models such as GPT-4 [OpenAI, 2023], especially when tuned for specific downstream tasks. Furthermore, using DPOP on additional mathematical datasets is an exciting area for future work, as it has the potential to further advance LLMs' abilites in mathematical reasoning.\n\nLimitations While our work gives theoretical and empirical evidence on a failure mode of DPO and a proposed solution, it still has limitations. First, we were unable to run a full ablation study on our 72B model. Running multiple fine-tuning experiments on a 72B model is infeasible, as each one can take over five days to complete. Therefore, we assume that our ablations on smaller models still hold up at scale. Furthermore, while we expect DPOP to achieve strong performance on any preference dataset, especially those with small edit distance, we have only demonstrated its performance on six English-language datasets. We hope that future work can verify its effectiveness on more datasets, in particular on non-English datasets.\n\n## References\n\n01.AI. Yi-34b-200k, 2024. URL https://huggingface.co/01-ai/Yi-34B-200K .\n\n- AllenAI. Ultrafeedback binarized clean, 2024. URL https://huggingface.co/datasets/allenai/ ultrafeedback\\_binarized\\_cleaned .\n- Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes llm better reasoner. arXiv preprint arXiv:2310.20689 , 2023.\n- Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical paradigm to understand learning from human preferences, 2023.\n- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback, 2022.\n- Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/ spaces/HuggingFaceH4/open\\_llm\\_leaderboard , 2023.\n- BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.\n- R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika , 39(3/4):324-345, 1952. doi: 10.2307/2334029.\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901, 2020.\n- Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n- Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning , pages 1597-1607. PMLR, 2020.\n- Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/ .\n\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research , 24(240):1-113, 2023.\n- Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017.\n- Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n- Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018.\n- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.\n- Jon Durbin. Bagel-34b-v0.2, 2024a. URL https://huggingface.co/jondurbin/bagel-34b-v0.2 .\n- Jon Durbin. Truthy dpo, 2024b. URL https://huggingface.co/datasets/jondurbin/truthy-dpo-v0. 1 .\n- Shahul Es. Orca-chat, 2024. URL https://huggingface.co/datasets/shahules786/orca-chat .\n- Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe Kiela. Human-centered loss functions (halos). Technical report, Contextual AI, 2023.\n- Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards analyzing and understanding the limitations of dpo: A theoretical perspective. arXiv preprint arXiv:2404.04626 , 2024.\n- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\n- R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06) , volume 2, pages 1735-1742, 2006. doi: 10.1109/CVPR.2006.100.\n- Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 9729-9738, 2020.\n- Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938 , 2021.\n- Intel. Orca dpo pairs, 2024. URL https://huggingface.co/datasets/Intel/orca\\_dpo\\_pairs .\n\n- Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint arXiv:2311.10702 , 2023.\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\n- Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.\n- Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning. arXiv preprint arXiv:1805.10627 , 2018.\n- Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL https://lmsys. org/blog/2024-04-19-arena-hard/ .\n- Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2022.\n- Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.\n- R Duncan Luce. Individual choice behavior: A theoretical analysis . Courier Corporation, 2005.\n- Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.\n- Moreh. Momo-72b-lora-1.8.7-dpo, 2024. URL https://huggingface.co/moreh/MoMo-72B-lora-1.8. 7-DPO .\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 , 2018.\n- OpenAI. Gpt-4 technical report. Technical Report , 2023.\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733 , 2024.\n- Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics , 24(2):193-202, 1975.\n\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2023.\n- Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q* : Your language model is secretly a q-function. arXiv preprint arXiv:2404.12358 , 2024.\n- Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. To the cutoff... and beyond? a longitudinal perspective on llm data contamination. In Proceedings of the International Conference on Learning Representations (ICLR) , 2024.\n- Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Proceedings of the AAAI Conference on Artificial Intelligence , 34, 2020.\n- Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning , pages 5628-5637. PMLR, 2019.\n- Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 815-823, 2015. doi: 10.1109/CVPR.2015.7298682.\n- Weijia Shi. Detect pretrain code contamination. https://github.com/swj0419/ detect-pretrain-code-contamination , 2023.\n- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2020.\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html , 3(6):7, 2023.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023a.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023b.\n- Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.\n- Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. Journal of Risk and Uncertainty , 5(4):297-323, 1992.\n\n- Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl , 2020.\n- Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 2495-2504, 2021.\n- Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144 , 2023.\n- Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning , pages 9929-9939. PMLR, 2020.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/2020.emnlp-demos.6 .\n- Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 , 2023.\n- Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417 , 2024.\n- Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.\n- Z. Sharegpt\\_vicuna\\_unfiltered, 2024. URL https://huggingface.co/datasets/anon8231489123/ ShareGPT\\_Vicuna\\_unfiltered .\n- Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://www.aclweb.org/anthology/P19-1472 .\n- Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter J. Liu. Calibrating sequence likelihood improves conditional language generation, 2022.\n- Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback, 2023.\n\n- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.\n- Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020.\n\n## A Related Work Continued\n\nIn this section, we continue our discussion of related work from Section 2.\n\nAFT Wang et al. [2023] seek to align LLMs to correctly 'score' (in terms of perplexity) their own generations. They do so by generating multiple chain-of-thought [Wei et al., 2023] responses to each prompt, which they categorise as preferred or dispreferred according to whether they answer the question correctly. Their proposed 'Alignment Fine-Tuning (AFT)' paradigm adds an alignment objective L ∗ A to the standard fine tuning loss, defined as\n\n<!-- formula-not-decoded -->\n\nwhere G p is the set of preferred examples and G n is the set of dispreferred examples. By minimising L ∗ A , the log-likelihoods of preferred examples are encouraged to be larger than the log-likelihoods of dispreferred examples, akin to DPO. However, Wang et al. [2023] takes an opposing motivation to us: they are particularly concerned with the issue of the log-likelihoods of dispreferred examples being pushed down too significantly\n\nOur work differs from AFT in three key points. First, although Wang et al. [2023] discusses DPO in the appendix, they do not show how their approach would extend to a reformulation of its objective; they also focus their experiments solely on supervised fine-tuning. Next, we use a different constraint mechanism-theirs is a soft margin constraint on the log-probability distance of the dispreferred example from the preferred example, while ours is a soft penalty for deviating from a reference model. Finally, they are focused specifically on the case of self-generated LLM CoT responses and calibrating the LLM's perplexity of its own responses.\n\nHALO Ethayarajh et al. [2023] seek to understand alignment methods, including DPO, in the context of 'Human-Centred Loss Functions (HALOs)'. By drawing an equivalence between the alignment methods and the work of Tversky and Kahneman [1992] in prospect theory, they adapt the 'human value function' in that paper to the LLM setting:\n\n<!-- formula-not-decoded -->\n\nwhere they define g ( x, y ; β ) as and\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe major difference of this approach with DPO is that it does not require paired preference data. The above loss function can be used for any dataset as long as the labels are individually marked as positive or negative.\n\nCPO Very recently, concurrent work [Xu et al., 2024] proposes adding a new term to the DPO loss function in order to allow DPO to become better at rejecting 'worse' completions that are good quality but not perfect. Specifically, they include the term\n\n<!-- formula-not-decoded -->\n\nWhile similar, their work uses a different loss function with different motivation, and furthermore they only considered machine translation models up to 13B parameters.\n\nSubsequent work, extended discussion As described in Section 2, very recently, subsequent work has verified our main findings from Section 3 and Section 4 on the existence of a failure mode of DPO and how to fix it [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]. We give additional discussion here.\n\nPang et al. consider fine-tuning on chain-of-thought reasoning tasks using pairwise preference data. They find that including an additional positive log-likelihood term is crucial for their reasoning tasks. Their final loss is similar to our Equation (3), but the additional term is outside the log sigmoid [Pang et al., 2024].\n\nFeng et al. give additional mathematical analyses of the gradient vector fields of the DPO loss. They come to the conclusion that the DPO loss function decreases the probability of producing dispreferred completions at a faster rate than it increases the probability of producing preferred completions [Feng et al., 2024].\n\nFinally, Rafailov et al. also analysed the 'phenomena in which the likelihood of the chosen responses actually decrease over time [in DPO]' by examining the expected log ratio of a completion under a model being optimised to a reference model. By showing that this is equivalent to the KL divergence, which is necessarily non-negative, they find that 'the likelihood of the chosen response should decrease in the process of DPO training' when starting from a model that has undergone SFT on the preferred completions [Rafailov et al., 2024].\n\n## B Derivation of logit gradients\n\n## B.1 Derivation for DPO\n\nConsider two completions of length K with edit (Hamming) distance of 1 which differ at token m with 1 ≤ m ≤ K . Put y w = ( t 1 , . . . , t K ) and y l = ( t 1 , . . . , t m -1 , t ′ m , t m +1 , . . . , t K ) . Put y &lt;r = ( t 1 , . . . , t r -1 ) and y ≥ r = ( t r , . . . , t K ) . Note that the derivative of Equation (1) with respect to θ is proportional to:\n\n<!-- formula-not-decoded -->\n\nBy the chain rule, we have and by using logarithm identities\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWhen we substitute this into ∇ θ L DPO , we have:\n\n<!-- formula-not-decoded -->\n\nAssume that the vocabulary length of the LLM is L . While the LLM model parameters θ are numerous, let us restrict our attention to just the logits, which we denote as θ j with j ∈ [ L ] and are the input to the softmax. We further denote the softmax probabilities by s { x } , so that s { x } i represents the probability of the i -th token in the model's vocabulary given the input context x .\n\nIt is then a standard result of the softmax function that:\n\n<!-- formula-not-decoded -->\n\nConsider the case where y w and y l differ only at the first token, i.e., m = 1 , and without loss of generality, assume that t k takes vocabulary position 1. In this instance, for each term of Equation (4) where k &gt; 1 we have 1 :\n\n<!-- formula-not-decoded -->\n\n̸\n\nAs we typically run DPO after SFT the model is likely to be reasonably well optimised, so we should have s { y &lt;k w ,x } j ≤ s { y &lt;k l ,x } j for j = 1 and s { y &lt;k w ,x } 1 ≥ s { y &lt;k l ,x } 1 . Therefore, while this analysis only extends to gradients with respect to the logits, we see that the gradient vector is decreasing in the correct logit dimension and increasing in the wrong logit dimensions. In particular, this derivation suggests that under DPO, all tokens that follow a difference at m at any point should have reduced probability of emitting the correct token when compared to π ref .\n\n## B.2 Derivation for DPOP\n\nWe can follow a similar line of reasoning for calculating ∇ θ L DPOP with respect to its logits which we denote again by θ j .\n\nAgain taking token position k for illustrative purposes and assuming that token t k has vocabulary position i , in the case when π ratio ( y | x ) &lt; 1 ,\n\n̸\n\n<!-- formula-not-decoded -->\n\n1 Note that the preceding negative sign is dropped as it is the gradient of a loss function and so during optimisation we take steps in the opposite direction, so as to minimise the loss.\n\n̸\n\nSince s { y &lt;k w ,x } j ≤ 1 , for the case i = j , the gradient is guaranteed to be positive for a large enough choice of λ . Similarly, for the case i = j , the gradient is guaranteed to be negative for a large enough λ (as long as s { y &lt;k w ,x } j &gt; 0 ). This therefore fixes the issue in DPO from Section 3.\n\nIn the case when π ratio ( y | x ) ≥ 1 , then we have the standard gradient from L DPO.\n\n## C Motivation: Contrastive Loss\n\nWhile the main motivation for DPOP is to avoid the failure mode described in Section 3, we also note its connection to contrastive loss . Contrastive learning is widely used [Wang and Liu, 2021, Wang and Isola, 2020, Saunshi et al., 2019, Oord et al., 2018, Chen et al., 2020, He et al., 2020], often for embedding learning applications. The contrastive loss formulation typically includes two main terms: one encouraging the proximity of analogous inputs, the other encouraging the divergence of distinct classifiable data.\n\nMoreover, the introduction of a margin appended to one of these terms often ensures a more stable training process. This margin serves as an indicator of indifference to point displacement once a specific value threshold is exceeded. The margin, when attached to the similar points term, establishes a minimum threshold beyond which we do not care about pulling the points closer. Alternatively, if added on the dissimilar points term, the margin sets a maximum threshold.\n\nWe show that the DPO loss is structured such that learning the probabilities during DPO training are equivalent to learning the embeddings in a contrastive loss formulation. However, the standard DPO only uses the term computing distance between dissimilar points, and does not include the similar points term or the margin. Consequently, it is predictable that traditional DPO's inefficiencies mirror the known shortcomings of contrastive training when one constituent term is absent. DPOP, our refined DPO formulation, fixes this by adding the absent term and the margin.\n\nContrastive loss is defined in Hadsell et al. [2006]. If we keep the margin in the similar points terms, it can be written as follows:\n\n<!-- formula-not-decoded -->\n\nRecall that the standard DPO loss (Equation (1)) is as follows:\n\n<!-- formula-not-decoded -->\n\nSay we designate an embedding function H :\n\n<!-- formula-not-decoded -->\n\nAnd we define a distance function D as follows:\n\n<!-- formula-not-decoded -->\n\nThe standard DPO only has the dissimilar points term under the analogy of the contrastive loss formulation. For more robust training we accommodate for the similar embeddings term. We use the concept of anchor points or embeddings for both positive and negative points as in triplet loss Schroff et al. [2015]. These points are known ideal embeddings we want our points to achieve. They carry probabilities of 1 and 0 respectively in our equivalence depending on whether they are preferred or dispreferred samples.\n\n<!-- formula-not-decoded -->\n\nThe DPOP loss can thus be formulated as:\n\n<!-- formula-not-decoded -->\n\nIf we set the margin m = log 1 π ref ( y w | x ) , the second term is:\n\n<!-- formula-not-decoded -->\n\nThis choice of margin is mathematically equivalent to choosing a threshold which ensures the similarity term only contributes to the loss when the learned model performs worse on the preferred response than the base model.\n\nWe can ignore the third term during training for two primary reasons. First, it is trying to push the log probability of negative samples to negative infinity which may be unstable during training. Second, in essence, it negatively impacts the likelihood of the negative samples. However, given our objective of extending the distance between positive and negative samples without diminishing the likelihood of positives, sacrificing this signal is acceptable. In the worst-case scenario, while the probability of negatives may be increased, the consequent increase in the probability of positives is ensured by the first two terms, thus rendering the sacrifice acceptable.\n\n## D Details of Datasets\n\nIn this section, we give more details and examples of the three paired preference versions of datasets we release. We release all datasets under the Apache 2.0 License.\n\nMetaMath GSM8K [Cobbe et al., 2021] is a dataset of diverse grade school maths word problems, which has been commonly adopted as a measure of the math and reasoning skills of LLMs [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021]. The MetaMath dataset is partitioned into queries and responses, where the query is a question involving mathematical calculation or reasoning, and the response is a logical series of steps and calculations that culminate in a final answer. To construct our paired-preference version of MetaMath, we take the queries as prompts x and the responses as the preferred completions y w . We create y l by modifying the response by randomly corrupting one of the results of an intermediate calculation so that it is incorrect; however, we leave the answer untouched.\n\nAn example is as follows: x : 'What is the total cost of purchasing equipment for all sixteen players on the football team, considering that each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80?\" y w : 'Each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80. So the total cost for each player is $25 + $15.20 + $6.80 = $ 47 . Since there are sixteen players on the football team, the total cost for all of them is 16 * $47 = $752. #### 752 The answer is: 752\" y l : 'Each player requires a $25 jersey, a $15.20 pair of shorts, and a pair of socks priced at $6.80. So the total cost for each player is $25 + $15.20 + $6.80 = $ 52 . Since there are sixteen players on the football team, the total cost for all of them is 16 * $47 = $752. #### 752 The answer is: 752\"\n\nThe dataset contains 393 999 training examples and 1 000 evaluation examples. Our motivation in building this dataset is to align models towards being precise in intermediate calculations. This dataset has low edit distance - the normalised edit distance is approximately 6.5%.\n\nARC ARC [Clark et al., 2018] is a dataset that tests the level of understanding of science at approximately grade-school level. We focus specifically on the 'Challenge' subsection of ARC, the more difficult of the two subsections, which has been widely adopted as a measure of LLM reasoning and world understanding [Chowdhery et al., 2023, Touvron et al., 2023b,a, Beeching et al., 2023, Gao et al., 2021, Cobbe et al., 2021]. We create a paired preference-ranked dataset from the train split of ARC-Challenge. The dataset is partitioned into questions which we take as our prompts x , and four choices of responses to each question of which only one is the correct answer. The correct response is taken as y w and the incorrect responses are taken to be y l ; as there are three incorrect responses for every prompt, we repeat y w multiple times for each prompt. The dataset contains 3357 training examples and 895 evaluation examples. This dataset has a high normalised edit distance of approximately 90%.\n\nHellaSwag Finally, we consider the HellaSwag dataset [Zellers et al., 2019], a dataset containing commonsense inference questions known to be hard for LLMs. An example prompt is 'Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. then' And the potential completions are [ \", the man adds wax to the windshield and cuts it.\", \", a person board a ski lift, while two men supporting the head of the person wearing winter clothes snow as the we girls sled.\", \", the man puts on a christmas coat, knitted with netting.\", \", the man continues removing the snow on his car.\" ] The dataset contains 119 715 training and 30 126 evaluation examples.\n\n## E Additional Experiments and Details\n\n## E.1 Additional Training Details\n\nNo hyperparameter tuning was done when creating Smaug-34B or Smaug-72B. DPOP has two hyperparameters, β and λ . We chose β = 0 . 3 , similar to prior work [Rafailov et al., 2023], and we chose λ = 50 without trying other values. It is possible that even better performance can be achieved, e.g., with a different value of λ .\n\nHere, we give the licenses of all models used to train our Smaug-series of models.\n\nSmaug-7B started from Llama 2-chat [Touvron et al., 2023b]. Therefore, we release it under the Llama 2 license ( https://ai.meta.com/llama/license/ ).\n\nSmaug-34B started from Bagel-34B-v0.2 [Durbin, 2024a], which itself is a SFT version of Yi-34B-200k [01.AI, 2024]. Therefore, we release Smaug-34B under the Yi Series Models Community License Agreement ( https://github.com/01-ai/Yi/blob/main/MODEL\\_LICENSE\\_AGREEMENT.txt ).\n\nFigure 5: Average log-probs for preferred completions of DPOP and DPO on the MetaMath dataset showing the failure mode of DPO.\n\n<!-- image -->\n\nFigure 6: Average log-probs for preferred completions of DPOP and DPO on the ARC dataset showing the failure mode of DPO.\n\n<!-- image -->\n\nSmaug-72B started from MoMo-72b-lora-1.8.7-DPO [Moreh, 2024], which itself is a fine-tune of Qwen72B [Bai et al., 2023]. Therefore, we release Smaug-72B under the Qwen license ( https://github.com/ QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT ).\n\n## E.2 Additional Results\n\nLog-probabilities of preferred completions In Figure 5, we show the log-probabilities of the preferred completion of the train and eval sets during training on MetaMath. We plot the log-probabilities in more granularity than in Figure 4. We confirm our theoretical insights from Section 4 - the log-probabilities of the preferred completion drop substantially in DPO, whereas they increase for DPOP - across both the train and eval sets. For ARC, we see in Figure 6 that DPOP maintains high train-set log-probs, while both the train and eval set log-probs decrease for DPO. Notably, even though eval set log-probs do decrease for DPOP, they are still higher than the train set log-probs of DPO.\n\nTable 2: Evaluation of the top models according to MT-Bench and MMLU.\n\n| Model                      | MT-bench   | MMLU   | Organization   | License                          |\n|----------------------------|------------|--------|----------------|----------------------------------|\n| GPT-4-1106-preview         | 9.32       |        | OpenAI         | Proprietary                      |\n| GPT-4-0314                 | 8.96       | 86.4   | OpenAI         | Proprietary                      |\n| GPT-4-0613                 | 9.18       |        | OpenAI         | Proprietary                      |\n| Mistral Medium             | 8.61       | 75.3   | Mistral        | Proprietary                      |\n| Claude-1                   | 7.9        | 77     | Anthropic      | Proprietary                      |\n| Claude-2.0                 | 8.06       | 78.5   | Anthropic      | Proprietary                      |\n| Gemini Pro (Dev API)       |            | 71.8   | Google         | Proprietary                      |\n| Claude-2.1                 | 8.18       |        | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0613         | 8.39       |        | OpenAI         | Proprietary                      |\n| Mixtral-8x7b-Instruct-v0.1 | 8.3        | 70.6   | Mistral        | Apache 2.0                       |\n| Yi-34B-Chat                |            | 73.5   | 01 AI          | Yi License                       |\n| Gemini Pro                 |            | 71.8   | Google         | Proprietary                      |\n| Claude-Instant-1           | 7.85       | 73.4   | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0314         | 7.94       | 70     | OpenAI         | Proprietary                      |\n| WizardLM-70B-v1.0          | 7.71       | 63.7   | Microsoft      | Llama 2 Community                |\n| Tulu-2-DPO-70B             | 7.89       |        | AllenAI/UW     | AI2 ImpACT Low-risk              |\n| Vicuna-33B                 | 7.12       | 59.2   | LMSYS          | Non-commercial                   |\n| Starling-LM-7B-alpha       | 8.09       | 63.9   | UC Berkeley    | CC-BY-NC-4.0                     |\n| Smaug-72B                  | 7.76       | 77.15  | Abacus.AI      | tongyi-qianwen-license-agreement |\n\nAdditional tables In Table 4, we give an extension of Table 1 (whose experimental details are in Section 6). In Table 5, we give the same table, except for models of size 34B or lower.\n\nMT-Bench Next, we evaluate using MT-Bench [Zheng et al., 2023], a challenging benchmark that uses GPT-4 [OpenAI, 2023] to score candidate model responses across eight different categories of performance. As shown in other works [Zheng et al., 2023, Rafailov et al., 2023], strong LLMs such as GPT-4 show good agreement with human preferences. We note that MT-Bench questions were not included in our fine-tuning datasets in any way. We run MT-Bench with the Llama-2 conversation template [Touvron et al., 2023b]. See Appendix Table 2 for a comparison with state-of-the-art LLMs according to Arena Elo as of March 2024. Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models. In Appendix E, we give examples of Smaug-72B completions to MT-Bench questions.\n\n## E.3 Contamination Checker\n\nSince LLMs train on wide swaths of the internet, data contamination, i.e., evaluating on examples that are included in the training data, is a growing concern. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities [Roberts et al., 2024, Jain et al., 2024, bench authors, 2023].\n\nWhile there is no perfect tool to check for data contamination, we use an open-source contamination checker ( https://github.com/swj0419/detect-pretrain-code-contamination ) to compare the contamination of our model to other open-source models - see Table 3. On ARC, TruthfulQA, and GSM8K, we\n\nTable 3: Comparison of contamination values across models, according to https://github.com/swj0419/ detect-pretrain-code-contamination . A score above 0.85 indicates likely contamination.\n\n| Model      |   Smaug-72B |   MoMo-72B-lora-1.8.7-DPO |   Llama-2-70B |\n|------------|-------------|---------------------------|---------------|\n| ARC        |        0.2  |                      0.2  |          0.22 |\n| TruthfulQA |        0.45 |                      0.39 |          0.51 |\n| GSM8K      |        1    |                      1    |          0.89 |\n\nTable 4: (Extension of Table 1.) Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard as of March 2024. Our 72B model achieves an average accuracy of 80.48%, becoming the first open-source LLM to surpass an average accuracy of 80% and improving by nearly 2% over the second-best open-source model (other than a fine-tune of our own model).\n\n| Model                                           |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |\n| CCK_Asura_v1                                    |  77.43 | 73.89 |       89.07 |  75.44 |        71.75 |        86.35 |   68.08 |\n| FusionNet_34Bx2_MoE_v0.1                        |  77.38 | 73.72 |       86.46 |  76.72 |        71.01 |        83.35 |   73.01 |\n| MoMo-72B-lora-1.8.6-DPO                         |  77.29 | 70.14 |       86.03 |  77.4  |        69    |        84.37 |   76.8  |\n| Smaug-34B-v0.1 (Ours)                           |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| Truthful_DPO_TomGrc_FusionNet_34Bx2_MoE         |  77.28 | 72.87 |       86.52 |  76.96 |        73.28 |        83.19 |   70.89 |\n| DARE_TIES_13B                                   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                    |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| FusionNet_34Bx2_MoE                             |  77.07 | 72.95 |       86.22 |  77.05 |        71.31 |        83.98 |   70.89 |\n| MoE_13B_DPO                                     |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO  |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |\n| MixTAO-7Bx2-MoE-Instruct-v7.0                   |  76.55 | 74.23 |       89.37 |  64.54 |        74.26 |        87.77 |   69.14 |\n| Truthful_DPO_cloudyu_Mixtral_34Bx2_MoE_60B0     |  76.48 | 71.25 |       85.24 |  77.28 |        66.74 |        84.29 |   74.07 |\n| MoMo-72B-lora-1.8.4-DPO                         |  76.23 | 69.62 |       85.35 |  77.33 |        64.64 |        84.14 |   76.27 |\n| FusionNet_7Bx2_MoE_v0.1                         |  76.16 | 74.06 |       88.9  |  65    |        71.2  |        87.53 |   70.28 |\n| MBX-7B-v3-DPO                                   |  76.13 | 73.55 |       89.11 |  64.91 |        74    |        85.56 |   69.67 |\n\nfind that Smaug-72B achieves scores that are similar to MoMo-72B-lora-1.8.7-DPO (the starting point of Smaug-72B), as well as Llama-2-70B.\n\n## F Example Completions\n\nIn this section, we give example completions by Smaug-72B for questions in MT-Bench [Zheng et al., 2023]. Note that these are not cherry-picked - they include examples of both good and bad completions.\n\nTable 5: Evaluation of the top open-weight models on the HuggingFace Open LLM Leaderboard for models less than 35B parameters as of March 2024. Our 34B model achieves best-in-its-class performance compared to other models of similar or smaller size.\n\n| Model                                          | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-34B-v0.1 (Ours)                          | <35B   |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| DARE_TIES_13B                                  | <35B   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                   | <35B   |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| MoE_13B_DPO                                    | <35B   |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO | <35B   |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |\n\n## Example 1\n\nQuestion: Imagine you are writing a blog post comparing two popular smartphone models. Develop an outline for the blog post, including key points and subheadings to effectively compare and contrast the features, performance, and user experience of the two models. Please answer in fewer than 200 words.\n\nOutline:\n\n1. Introduction to the two smartphone models being compared (mention brand names and models)\n2. Design and Build Quality\n- Comparison of dimensions, weight, materials used, and overall aesthetic appeal\n- Water and dust resistance, if applicable\n3. Display and Screen Quality\n- Comparison of screen technology (OLED, IPS LCD, etc.)\n- Display resolution, refresh rate, and brightness levels\n4. Performance and Hardware\n- Processor, RAM, and storage configurations\n- Comparison of benchmark scores (e.g., AnTuTu, Geekbench)\n5. Camera System and Image Quality\n- Comparison of primary, ultra-wide, telephoto, and macro lenses (if available)\n- Comparison of low-light performance and image processing capabilities\n6. Battery Life and Charging\n- Battery capacity and estimated usage time\n- Charging technologies (wired, wireless, fast charging, reverse charging)\n7. Software and User Experience\n- Comparison of operating systems (iOS vs. Android) and their features\n- User interface, customization options, and app ecosystem\n8. Additional Features and Extras\n- Comparison of biometric authentication (fingerprint, face unlock, etc.)\n- Additional features like NFC, 5G connectivity, and audio quality\n9. Pricing and Value for Money\n- Comparison of retail prices and current offers\n- Assessment of overall value considering the features and performance\n10. Conclusion and recommendation for potential buyers based on their preferences\n\n## Example 2\n\nQuestion: In a survey conducted at a local high school, preferences for a new school color were measured: 58% of students liked the color blue, 45% preferred green, and 22% liked both colors. If we randomly pick a student from the school, what's the probability that they would like neither blue nor green?\n\nTo find the probability that a randomly picked student would like neither blue nor green, we need to find the percentage of students who like only one color or neither color.\n\nFrom the given information, we know:\n\n- 58% like blue\n- 45% prefer green\n- 22% like both colors\n\nTo find the percentage of students who like only one color, we can subtract the percentage of students who like both colors from the sum of students who like blue and green:\n\n- 58% (blue) + 45% (green) - 22% (both) = 83%\n\nNow, to find the probability of a student liking neither blue nor green, we subtract the percentage of students who like at least one color from 100%:\n\n- 100% - 83% = 17%\n\nSo, the probability that a randomly picked student would like neither blue nor green is 17%.\n\nQuestion: Implement a program to find the common elements in two arrays without using any\n\n## Example 3 extra data structures.\n\nFinding common elements in two arrays without using extra data structures can be achieved by using a simple algorithm. Here's a Python implementation of this problem:\n\n```\ndef find_common_elements(arr1, arr2): common_elements = [] for num1 in arr1: if num1 in arr2 and num1 not in common_elements: common_elements.append(num1) return common_elements # Example usage: array1 = [1, 2, 3, 4, 5] array2 = [4, 5, 6, 7, 8] print (find_common_elements(array1, array2)) # Output: [4, 5]\n```\n\nThis program iterates through the elements of the first array (arr1) and checks if each element is present in the second array (arr2). If it is, and the element hasn't been added to the common\\_elements list yet, it is appended. The function then returns the list of common elements.\n\n<!-- image -->",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model                                           | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                | 72B+   |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         | 72B+   |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         | 72B+   |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO | 72B+   |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      | 72B+   |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |"
    },
    {
      "index": 1,
      "markdown": "| Model                      | MT-bench   | MMLU   | Organization   | License                          |\n|----------------------------|------------|--------|----------------|----------------------------------|\n| GPT-4-1106-preview         | 9.32       |        | OpenAI         | Proprietary                      |\n| GPT-4-0314                 | 8.96       | 86.4   | OpenAI         | Proprietary                      |\n| GPT-4-0613                 | 9.18       |        | OpenAI         | Proprietary                      |\n| Mistral Medium             | 8.61       | 75.3   | Mistral        | Proprietary                      |\n| Claude-1                   | 7.9        | 77     | Anthropic      | Proprietary                      |\n| Claude-2.0                 | 8.06       | 78.5   | Anthropic      | Proprietary                      |\n| Gemini Pro (Dev API)       |            | 71.8   | Google         | Proprietary                      |\n| Claude-2.1                 | 8.18       |        | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0613         | 8.39       |        | OpenAI         | Proprietary                      |\n| Mixtral-8x7b-Instruct-v0.1 | 8.3        | 70.6   | Mistral        | Apache 2.0                       |\n| Yi-34B-Chat                |            | 73.5   | 01 AI          | Yi License                       |\n| Gemini Pro                 |            | 71.8   | Google         | Proprietary                      |\n| Claude-Instant-1           | 7.85       | 73.4   | Anthropic      | Proprietary                      |\n| GPT-3.5-Turbo-0314         | 7.94       | 70     | OpenAI         | Proprietary                      |\n| WizardLM-70B-v1.0          | 7.71       | 63.7   | Microsoft      | Llama 2 Community                |\n| Tulu-2-DPO-70B             | 7.89       |        | AllenAI/UW     | AI2 ImpACT Low-risk              |\n| Vicuna-33B                 | 7.12       | 59.2   | LMSYS          | Non-commercial                   |\n| Starling-LM-7B-alpha       | 8.09       | 63.9   | UC Berkeley    | CC-BY-NC-4.0                     |\n| Smaug-72B                  | 7.76       | 77.15  | Abacus.AI      | tongyi-qianwen-license-agreement |"
    },
    {
      "index": 2,
      "markdown": "| Model      |   Smaug-72B |   MoMo-72B-lora-1.8.7-DPO |   Llama-2-70B |\n|------------|-------------|---------------------------|---------------|\n| ARC        |        0.2  |                      0.2  |          0.22 |\n| TruthfulQA |        0.45 |                      0.39 |          0.51 |\n| GSM8K      |        1    |                      1    |          0.89 |"
    },
    {
      "index": 3,
      "markdown": "| Model                                           |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|-------------------------------------------------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-72B (Ours)                                |  80.48 | 76.02 |       89.27 |  77.15 |        76.67 |        85.08 |   78.7  |\n| MoMo-72B-lora-1.8.7-DPO                         |  78.55 | 70.82 |       85.96 |  77.13 |        74.71 |        84.06 |   78.62 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO_f16         |  77.91 | 74.06 |       86.74 |  76.65 |        72.24 |        83.35 |   74.45 |\n| TomGrc_FusionNet_34Bx2_MoE_v0.1_full_linear_DPO |  77.52 | 74.06 |       86.67 |  76.69 |        71.32 |        83.43 |   72.93 |\n| Truthful_DPO_TomGrc_FusionNet_7Bx2_MoE_13B      |  77.44 | 74.91 |       89.3  |  64.67 |        78.02 |        88.24 |   69.52 |\n| CCK_Asura_v1                                    |  77.43 | 73.89 |       89.07 |  75.44 |        71.75 |        86.35 |   68.08 |\n| FusionNet_34Bx2_MoE_v0.1                        |  77.38 | 73.72 |       86.46 |  76.72 |        71.01 |        83.35 |   73.01 |\n| MoMo-72B-lora-1.8.6-DPO                         |  77.29 | 70.14 |       86.03 |  77.4  |        69    |        84.37 |   76.8  |\n| Smaug-34B-v0.1 (Ours)                           |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| Truthful_DPO_TomGrc_FusionNet_34Bx2_MoE         |  77.28 | 72.87 |       86.52 |  76.96 |        73.28 |        83.19 |   70.89 |\n| DARE_TIES_13B                                   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                    |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| FusionNet_34Bx2_MoE                             |  77.07 | 72.95 |       86.22 |  77.05 |        71.31 |        83.98 |   70.89 |\n| MoE_13B_DPO                                     |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO  |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |\n| MixTAO-7Bx2-MoE-Instruct-v7.0                   |  76.55 | 74.23 |       89.37 |  64.54 |        74.26 |        87.77 |   69.14 |\n| Truthful_DPO_cloudyu_Mixtral_34Bx2_MoE_60B0     |  76.48 | 71.25 |       85.24 |  77.28 |        66.74 |        84.29 |   74.07 |\n| MoMo-72B-lora-1.8.4-DPO                         |  76.23 | 69.62 |       85.35 |  77.33 |        64.64 |        84.14 |   76.27 |\n| FusionNet_7Bx2_MoE_v0.1                         |  76.16 | 74.06 |       88.9  |  65    |        71.2  |        87.53 |   70.28 |\n| MBX-7B-v3-DPO                                   |  76.13 | 73.55 |       89.11 |  64.91 |        74    |        85.56 |   69.67 |"
    },
    {
      "index": 4,
      "markdown": "| Model                                          | Size   |   Avg. |   ARC |   HellaSwag |   MMLU |   TruthfulQA |   Winogrande |   GSM8K |\n|------------------------------------------------|--------|--------|-------|-------------|--------|--------------|--------------|---------|\n| Smaug-34B-v0.1 (Ours)                          | <35B   |  77.29 | 74.23 |       86.76 |  76.66 |        70.22 |        83.66 |   72.18 |\n| DARE_TIES_13B                                  | <35B   |  77.1  | 74.32 |       89.5  |  64.47 |        78.66 |        88.08 |   67.55 |\n| 13B_MATH_DPO                                   | <35B   |  77.08 | 74.66 |       89.51 |  64.53 |        78.63 |        88.08 |   67.1  |\n| MoE_13B_DPO                                    | <35B   |  77.05 | 74.32 |       89.39 |  64.48 |        78.47 |        88    |   67.63 |\n| 4bit_quant_TomGrc_FusionNet_34Bx2_MoE_v0.1_DPO | <35B   |  76.95 | 73.21 |       86.11 |  75.44 |        72.78 |        82.95 |   71.19 |"
    }
  ],
  "stats": {
    "pages": 30,
    "chunksCreated": 114,
    "totalCharacters": 80477,
    "totalWords": 12667,
    "numTables": 5,
    "processingTimeMs": 34441
  }
}