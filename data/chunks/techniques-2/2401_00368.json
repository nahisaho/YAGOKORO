{
  "paper": {
    "id": "2401.00368v3",
    "title": "Improving Text Embeddings with Large Language Models",
    "abstract": "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps. Unlike existing methods that often depend on multi-stage intermediate pre-training with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.",
    "authors": [
      "Liang Wang",
      "Nan Yang",
      "Xiaolong Huang",
      "Linjun Yang",
      "Rangan Majumder",
      "Furu Wei"
    ],
    "published": "2023-12-31T02:13:18.000Z",
    "updated": "2024-05-31T07:22:01.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2401.00368v3",
    "absUrl": "https://arxiv.org/abs/2401.00368v3"
  },
  "chunks": [
    {
      "id": "2401.00368v3-chunk-0",
      "content": "Microsoft Corporation\n\n{wangliang,nanya,xiaolhu,yang.linjun,ranganm,fuwei}@microsoft.com",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang , Rangan Majumder , Furu Wei",
        "chunkIndex": 0,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-1",
      "content": "In this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1 k training steps. Unlike existing methods that often depend on multi-stage intermediate pretraining with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-2",
      "content": "the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-3",
      "content": "Text embeddings are vector representations of natural language that encode its semantic information. They are widely used in various natural language processing (NLP) tasks, such as information retrieval (IR), question answering, semantic textual similarity, bitext mining, item recommendation, etc. In the field of IR, the first-stage retrieval often relies on text embeddings to efficiently recall a small set of candidate documents from a large-scale corpus using approximate nearest neighbor search techniques. Embedding-based retrieval is also a crucial component of retrieval-augmented generation (RAG) (Lewis et al., 2020), which is an emerging paradigm that enables large language models (LLMs) to access dynamic external knowledge without modifying the model parameters. Source attribution of generated text is another important application of text embeddings (Gao et al., 2023) that can improve the interpretability and trustworthiness of LLMs.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-4",
      "content": "ing the model parameters. Source attribution of generated text is another important application of text embeddings (Gao et al., 2023) that can improve the interpretability and trustworthiness of LLMs.\n\nPrevious studies have demonstrated that weighted average of pre-trained word embeddings (Pennington et al., 2014; Arora et al., 2017) is a strong baseline for measuring semantic similarity. However, these methods fail to capture the rich contextual information of natural language. With the advent of pre-trained language models (Devlin et al., 2019), Sentence-BERT (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021) have been proposed to learn text embeddings by fine-tuning BERT on natural language inference (NLI) datasets. To further enhance the performance and robustness of text embeddings, state-of-the-art methods like E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023) employ a more complex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-5",
      "content": "dings, state-of-the-art methods like E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023) employ a more complex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text pairs, and then fine-tunes on several high-quality labeled datasets.\n\nExisting multi-stage approaches suffer from several drawbacks. Firstly, they entail a complex multistage training pipeline that demands substantial engineering efforts to curate large amounts of relevance pairs. Secondly, they rely on manually collected datasets that are often constrained by the diversity of tasks and the coverage of languages. For instance, Instructor (Su et al., 2023) is only trained on instructions from 330 English datasets, whereas BGE (Xiao et al., 2023) only focuses on high-resource languages such as English and Chinese.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-6",
      "content": "r instance, Instructor (Su et al., 2023) is only trained on instructions from 330 English datasets, whereas BGE (Xiao et al., 2023) only focuses on high-resource languages such as English and Chinese. Moreover, most existing methods employ BERT-style encoders as the backbone, neglecting the recent advances of training better LLMs and related techniques such as context length extension (Rozière et al., 2023).\n\nIn this paper, we propose a novel method for text embeddings that leverages LLMs to overcome the limitations of existing approaches. We use proprietary LLMs to generate synthetic data for a diverse range of text embedding tasks in 93 languages, covering hundreds of thousands of embedding tasks. Specifically, we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the pool.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-7",
      "content": "fically, we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the pool. To cover various application scenarios, we design multiple prompt templates for each task type and combine the generated data from different templates to boost diversity. For the text embedding models, we opt for fine-tuning powerful open-source LLMs rather than small BERT-style models. Since LLMs such as Mistral (Jiang et al., 2023) have been extensively pre-trained on webscale data, contrastive pre-training that proves to be important for BERT models (Wang et al., 2022b) offers little additional benefit.\n\nWe demonstrate that Mistral-7B, when finetuned solely on synthetic data, attains competitive performance on the BEIR (Thakur et al., 2021) and MTEB (Muennighoff et al., 2023) benchmarks. This is particularly intriguing considering that this setting does not involve any labeled data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-8",
      "content": "mpetitive performance on the BEIR (Thakur et al., 2021) and MTEB (Muennighoff et al., 2023) benchmarks. This is particularly intriguing considering that this setting does not involve any labeled data. When fine-tuned on a mixture of synthetic and labeled data, our model achieves new state-of-the-art results, surpassing previous methods by a significant margin (+ 2% ). The entire training process requires less than 1 k steps.\n\nMoreover, we empirically validate that our model can effectively perform personalized passkey retrieval for inputs up to 32 k tokens by altering the rotation base of the position embeddings, extending the context length beyond the conventional 512 token limit. Regarding its multilinguality, our model excels on high-resource languages. However, for low-resource languages, there is still room for improvement as current opensource LLMs are not adequately pre-trained on them.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-9",
      "content": "es, there is still room for improvement as current opensource LLMs are not adequately pre-trained on them.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-10",
      "content": "Text Embeddings are continuous lowdimensional representations of text and have been extensively applied to various downstream tasks such as information retrieval, question answering, and retrieval-augmented generation (RAG). Early work on text embeddings includes latent semantic indexing (Deerwester et al., 1990) and weighted average of word embeddings (Mikolov et al.,\n\n2013). More recent methods exploit supervision from natural language inference (Bowman et al., 2015) and labeled query-document pairs, such as the MS-MARCO passage ranking dataset (Campos et al., 2016), to train text embeddings (Reimers and Gurevych, 2019; Conneau et al., 2017; Gao et al., 2021). However, labeled data are often limited in terms of task diversity and language coverage. To address this challenge, methods like Contriever (Izacard et al., 2021), OpenAI Embeddings (Neelakantan et al., 2022), E5 (Wang et al., 2022b), and BGE (Xiao et al., 2023) adopt a multi-stage training paradigm.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 10,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-11",
      "content": "his challenge, methods like Contriever (Izacard et al., 2021), OpenAI Embeddings (Neelakantan et al., 2022), E5 (Wang et al., 2022b), and BGE (Xiao et al., 2023) adopt a multi-stage training paradigm. They first pre-train on large-scale weakly-supervised text pairs using contrastive loss and then fine-tune on small-scale but high-quality datasets. In this paper, we demonstrate that it is possible to obtain state-of-the-art text embeddings with single-stage training.\n\nSynthetic Data Synthetic data generation is a widely studied topic in information retrieval research, with various methods proposed to enhance retrieval systems with artificially created data. For instance, Doc2query (Nogueira et al., 2019), InPars (Bonifacio et al., 2022), and Promptagator (Dai et al., 2022) generate synthetic queries for unlabeled documents, which are then leveraged for document expansion or model training.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 11,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-12",
      "content": "al., 2019), InPars (Bonifacio et al., 2022), and Promptagator (Dai et al., 2022) generate synthetic queries for unlabeled documents, which are then leveraged for document expansion or model training. GPL (Wang et al., 2022a) employs a cross-encoder to produce pseudo-labels for query-document pairs. Similarly, Query2doc (Wang et al., 2023) generates pseudo-documents for query expansion by fewshot prompting LLMs. Unlike these methods, our approach does not rely on any unlabeled documents or queries and thus can generate more diverse synthetic data.\n\nAnother related line of work focuses on knowledge distillation from black-box LLMs by training on synthetic data generated from them. DINO (Schick and Schütze, 2021) generates synthetic text pairs for semantic textual similarity. Unnatural Instructions (Honovich et al., 2022) is a synthetic instruction following dataset by prompting existing LLMs.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 12,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-13",
      "content": "d Schütze, 2021) generates synthetic text pairs for semantic textual similarity. Unnatural Instructions (Honovich et al., 2022) is a synthetic instruction following dataset by prompting existing LLMs. Orca (Mukherjee et al., 2023) and Phi (Gunasekar et al., 2023) propose to train better small language models by using high-quality synthetic data from GPT-3.5/4 (OpenAI, 2023).\n\nLarge Language Models With the popularization of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities in in-\n\nBrainstorm a list of potentially useful text retrieval tasks. Here are a few examples for your reference:\n\n- Search for documents that answers a FAQ-style query on children's nutrition.\n- Provided a scientific claim as query, retrieve documents that help verify or refute the claim.\n\nPlease adhere to the following guidelines:",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 13,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-14",
      "content": "nts that answers a FAQ-style query on children's nutrition.\n- Provided a scientific claim as query, retrieve documents that help verify or refute the claim.\n\nPlease adhere to the following guidelines:\n\n- Each retrieval task should cover a wide range of queries, and should not be too specific.\n- Specify what the query is, and what the desired documents are.\n\nYour output should always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\n\n[\"Retrieve company's financial reports for a given stock ticker symbol.\", \"Given a book name as a query, retrieve reviews, ratings and summaries of that book.\", \"Search for scientific research papers supporting a medical diagnosis for a specified disease.' … (omitted for space) ]\n\nnew session",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 14,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-15",
      "content": "as a query, retrieve reviews, ratings and summaries of that book.\", \"Search for scientific research papers supporting a medical diagnosis for a specified disease.' … (omitted for space) ]\n\nnew session\n\nYou have been assigned a retrieval task: {task} Your mission is to write one text retrieval example for this task in JSON format. The JSON object must\n\n- -\"user\\_query\" : a string, a random user search query specified by the retrieval task.\n\ncontain the following keys:\n\n- -\"positive\\_document\" : a string, a relevant document for the user query.\n\nPlease adhere to the following guidelines:\n\n- -\"hard\\_negative\\_document\" : a string, a hard negative document that only  appears relevant to the query.\n- The \"user\\_query\" should be {query\\_type} , {query\\_length} , {clarity} , and diverse in topic.\n- Both the query and documents should be in {language} .\n- All documents should be at least {num\\_words} words long.\n- … (omitted some for space)",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 15,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-16",
      "content": ", {query\\_length} , {clarity} , and diverse in topic.\n- Both the query and documents should be in {language} .\n- All documents should be at least {num\\_words} words long.\n- … (omitted some for space)\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\n{ \"user\\_query\": \"How to use Microsoft Power BI for data analysis\", \"positive\\_document\": \"Microsoft Power BI is a sophisticated tool that requires time and practice to master. In this tutorial, we'll show you how to navigate Power BI … (omitted) \" , ' hard\\_negative\\_document ': 'Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our tutorial series focuses on how you …(omitted) ' }\n\nFigure 1: An example two-step prompt template for generating synthetic data with GPT-4. We first prompt GPT-4 to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard negative) triplets for each task.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 16,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-17",
      "content": "t template for generating synthetic data with GPT-4. We first prompt GPT-4 to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard negative) triplets for each task. ' {...} ' denotes a placeholder that will be replaced by sampling from a predefined set of values. Full prompts are available in Appendix C.\n\nstruction following and few-shot in-context learning (Brown et al., 2020). However, the most advanced LLMs such as GPT-4 (OpenAI, 2023) are proprietary and have little technical details disclosed. To bridge the gap between proprietary and open-source LLMs, several notable efforts have been made, such as LLaMA-2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023) models. A major limitation of LLMs is that they lack awareness of recent events and private knowledge. This issue can be partly mitigated by augmenting LLMs with information retrieved from external sources, a technique known as retrieval-augmented generation (RAG).",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 17,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-18",
      "content": "recent events and private knowledge. This issue can be partly mitigated by augmenting LLMs with information retrieved from external sources, a technique known as retrieval-augmented generation (RAG). On the other hand, LLMs can also serve as foundation models to enhance text embeddings. RepLLaMA (Ma et al., 2023) proposes to fine-tune LLaMA-2 with bi-encoder architecture for ad-hoc retrieval. SGPT (Muennighoff, 2022), GTR (Ni et al., 2022b), and Udever (Zhang et al., 2023a) demonstrate the scaling law of text embeddings empirically, but their performance still falls behind small bidirectional encoders such as E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023). In this paper, we present a novel approach to train state-of-the-art text embeddings by exploiting the latest advances of LLMs and synthetic data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "2 Related Work",
        "chunkIndex": 18,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-19",
      "content": "Utilizing synthetic data generated by advanced LLMs such as GPT-4 presents a compelling opportunity, especially in terms of enhancing diversity across a multitude of tasks and languages. Such diversity is essential for developing robust text embeddings that can perform well across different tasks, be it semantic retrieval, textual similarity, or clustering.\n\nTo generate diverse synthetic data, we propose a simple taxonomy that categorizes embedding tasks into several groups, and then apply different prompt templates to each group.\n\nAsymmetric Tasks This category comprises tasks where the query and document are semantically related but are not paraphrases of each other. Depending on the length of the query and document, we further divide asymmetric tasks into four subgroups: short-long match, long-short match, short-short match, and long-long match. For instance, short-long match tasks involve a short query and a long document, which is a typical scenario in commercial search engines.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.1 Synthetic Data Generation",
        "chunkIndex": 19,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-20",
      "content": "tch, long-short match, short-short match, and long-long match. For instance, short-long match tasks involve a short query and a long document, which is a typical scenario in commercial search engines. For each subgroup, we design a two-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a concrete example conditioned on the task definition. In Figure 1, we show an example prompt for the short-long match subgroup. The full output is available in Table 16. The outputs from GPT-4 are mostly coherent and of high quality. In our preliminary experiments, we also attempted to generate the task definition and query-document pairs using a single prompt, but the data diversity was not as satisfactory as the proposed two-step approach.\n\nSymmetric Tasks Symmetric tasks involve queries and documents that have similar semantic meanings but different surface forms.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.1 Synthetic Data Generation",
        "chunkIndex": 20,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-21",
      "content": "ata diversity was not as satisfactory as the proposed two-step approach.\n\nSymmetric Tasks Symmetric tasks involve queries and documents that have similar semantic meanings but different surface forms. We examine two application scenarios: monolingual semantic textual similarity (STS) and bitext retrieval. We design two distinct prompt templates for each scenario, tailored to their specific objectives. Since the task definition is straightforward, we omit the brainstorming step for symmetric tasks.\n\nTo further boost the diversity of the prompts and thus the synthetic data, we incorporate several placeholders in each prompt template, whose values are randomly sampled at runtime. For example, in Figure 1, the value of ' {query\\_length} ' is sampled from the set ' {less than 5 words, 5-10 words, at least 10 words} '.\n\nTo generate multilingual data, we sample the value of ' {language} ' from the language list of",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.1 Synthetic Data Generation",
        "chunkIndex": 21,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-22",
      "content": "e of ' {query\\_length} ' is sampled from the set ' {less than 5 words, 5-10 words, at least 10 words} '.\n\nTo generate multilingual data, we sample the value of ' {language} ' from the language list of\n\nXLM-R (Conneau et al., 2020), giving more weight to high-resource languages. Any generated data that does not conform to the predefined JSON format are discarded during the parsing process. We also remove duplicates based on exact string matching.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.1 Synthetic Data Generation",
        "chunkIndex": 22,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-23",
      "content": "Given a relevant query-document pair ( q + , d + ), we first apply the following instruction template to the original query q + to generate a new one q + inst :\n\n<!-- formula-not-decoded -->\n\nwhere ' {task\\_definition} ' is a placeholder for a one-sentence description of the embedding task. For generated synthetic data, we use the outputs from the brainstorming step. For other datasets, such as MS-MARCO, we manually craft the task definitions and apply them to all the queries in the dataset. We do not modify the document side with any instruction prefix. In this way, the document index can be prebuilt, and we can customize the task to perform by changing only the query side.\n\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings ( h q + inst , h d + ) by taking the last layer [EOS] vector.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.2 Training",
        "chunkIndex": 23,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-24",
      "content": "we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings ( h q + inst , h d + ) by taking the last layer [EOS] vector. To train the embedding model, we adopt the standard InfoNCE loss L over the in-batch negatives and hard negatives:\n\n<!-- formula-not-decoded -->\n\nwhere N denotes the set of all negatives, and ϕ ( q, d ) is a function that computes the matching score between query q and document d . In this paper, we adopt the temperature-scaled cosine similarity function as follows:\n\n<!-- formula-not-decoded -->\n\nτ is a temperature hyper-parameter, which is fixed to 0 . 02 in our experiments.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "3.2 Training",
        "chunkIndex": 24,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-25",
      "content": "Figure 2 presents the statistics of our generated synthetic data. We manage to generate 500 k examples with 150 k unique instructions using Azure\n\nFigure 2: Task type and language statistics of the generated synthetic data (see Section 3.1 for task type definitions). The 'Others' category contains the remaining languages from the XLM-R language list.\n\n<!-- image -->\n\nTable 1: Results on the MTEB benchmark (Muennighoff et al., 2023) (56 datasets in the English subset). The numbers are averaged for each category. Please refer to Table 17 for the scores per dataset.\n\n| # of datasets →                                  | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|--------------------------------------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised Models                              |             |             |                |            |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Statistics of the Synthetic Data",
        "chunkIndex": 25,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-26",
      "content": "--------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised Models                              |             |             |                |            |            |          |           |          |\n| Glove (Pennington et al., 2014)                  | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| SimCSE bert-unsup (Gao et al., 2021)             | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| Supervised Models                                |             |             |                |            |            |          |           |          |\n| SimCSE bert-sup (Gao et al., 2021)               | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| Contriever (Izacard et al., 2021)                | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Statistics of the Synthetic Data",
        "chunkIndex": 26,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-27",
      "content": "73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| Contriever (Izacard et al., 2021)                | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR xxl (Ni et al., 2022b)                       | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl (Ni et al., 2022a)               | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |\n| E5 large-v2 (Wang et al., 2022b)                 | 75.2        | 44.5        | 86.0           | 56.6       | 50.6       | 82.1     | 30.2      | 62.3     |\n| GTE large (Li et al., 2023)                      | 73.3        | 46.8        | 85.0           | 59.1       | 52.2       | 83.4     | 31.7      | 63.1     |\n| BGE large-en-v1.5 (Xiao et al., 2023)            | 76.0        | 46.1        | 87.1           | 60.0       | 54.3       | 83.1",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Statistics of the Synthetic Data",
        "chunkIndex": 27,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-28",
      "content": "| 59.1       | 52.2       | 83.4     | 31.7      | 63.1     |\n| BGE large-en-v1.5 (Xiao et al., 2023)            | 76.0        | 46.1        | 87.1           | 60.0       | 54.3       | 83.1     | 31.6      | 64.2     |\n| Ours                                             |             |             |                |            |            |          |           |          |\n| E5 mistral-7b + full data w/ synthetic data only | 78.5        | 50.3        | 88.3           | 60.2       | 56.9       | 84.6     | 31.4      | 66.6     |\n|                                                  | 78.2        | 50.5        | 86.0           | 59.0       | 46.9       | 81.2     | 31.9      | 63.1     |\n| w/ synthetic + msmarco                           | 78.3        | 49.9        | 87.1           | 59.5       | 52.2       | 81.2     | 32.7      | 64.5     |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Statistics of the Synthetic Data",
        "chunkIndex": 28,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-29",
      "content": "| 81.2     | 31.9      | 63.1     |\n| w/ synthetic + msmarco                           | 78.3        | 49.9        | 87.1           | 59.5       | 52.2       | 81.2     | 32.7      | 64.5     |\n\nOpenAI Service 1 , among which 25% are generated by GPT-35-Turbo and others are generated by GPT-4 . The total token consumption is about 180 M. The predominant language is English, with coverage extending to a total of 93 languages. For the bottom 75 low-resource languages, there are about 1 k examples per language on average. Please see Table 16 in the appendix for examples of synthetic data.\n\nIn terms of data quality, we find that a portion of GPT-35-Turbo outputs do not strictly follow the guidelines specified in the prompt templates. Nevertheless, the overall quality remains acceptable, and preliminary experiments have demonstrated the benefits of incorporating this data subset.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.1 Statistics of the Synthetic Data",
        "chunkIndex": 29,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-30",
      "content": "The pretrained Mistral-7b (Jiang et al., 2023) checkpoint is fine-tuned for 1 epoch using the loss in Equation 2. We follow the training recipe from RankLLaMA (Ma et al., 2023) and utilize LoRA (Hu et al., 2022) with rank 16 . To further reduce GPU memory requirement, techniques including gradient checkpointing, mixed precision training, and DeepSpeed ZeRO-3 are applied.\n\nFor the training data, we utilize both the generated synthetic data and a collection of 13 public datasets, yielding approximately 1 . 8 M examples after sampling. More details are available in Appendix A. To provide a fair comparison with some previous work, we also report results when the only labeled supervision is the MS-MARCO passage\n\nTable 2: nDCG@10 on the dev set of the MIRACL dataset for both high-resource and low-resource languages. We select the 4 high-resource languages and the 4 low-resource languages according to the number of candidate documents. The numbers for BM25 and mDPR come from Zhang et al.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Model Fine-tuning and Evaluation",
        "chunkIndex": 30,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-31",
      "content": "nd low-resource languages. We select the 4 high-resource languages and the 4 low-resource languages according to the number of candidate documents. The numbers for BM25 and mDPR come from Zhang et al. (2023b). For the complete results on all 16 languages, please see Table 6.\n\n|                               | High-resource Languages   | High-resource Languages   | High-resource Languages   | High-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   |\n|-------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                               | en                        | fr                        | es                        | ru                        | te                       | hi                       | bn",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Model Fine-tuning and Evaluation",
        "chunkIndex": 31,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-32",
      "content": "| en                        | fr                        | es                        | ru                        | te                       | hi                       | bn                       | sw                       |\n| BM25 (Zhang et al., 2023b)    | 35.1                      | 18.3                      | 31.9                      | 33.4                      | 49.4                     | 45.8                     | 50.8                     | 38.3                     |\n| mDPR (Zhang et al., 2023b)    | 39.4                      | 43.5                      | 47.8                      | 40.7                      | 35.6                     | 38.3                     | 44.3                     | 29.9                     |\n| mE5 base (Wang et al., 2024)  | 51.2                      | 49.7                      | 51.5                      | 61.5                      | 75.2                     | 58.4                     | 70.2                     | 71.1                     |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Model Fine-tuning and Evaluation",
        "chunkIndex": 32,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-33",
      "content": "| 49.7                      | 51.5                      | 61.5                      | 75.2                     | 58.4                     | 70.2                     | 71.1                     |\n| mE5 large (Wang et al., 2024) | 52.9                      | 54.5                      | 52.9                      | 67.4                      | 84.6                     | 62.0                     | 75.9                     | 74.9                     |\n| E5 mistral-7b + full data     | 57.3                      | 55.2                      | 52.2                      | 67.7                      | 73.9                     | 52.1                     | 70.3                     | 68.4                     |\n\nFigure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 7.\n\n<!-- image -->\n\n<!-- image -->\n\nranking (Campos et al., 2016) dataset.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Model Fine-tuning and Evaluation",
        "chunkIndex": 33,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-34",
      "content": "| 68.4                     |\n\nFigure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 7.\n\n<!-- image -->\n\n<!-- image -->\n\nranking (Campos et al., 2016) dataset.\n\nWe evaluate the trained model on the MTEB benchmark (Muennighoff et al., 2023). Note that the retrieval category in MTEB corresponds to the 15 publicly available datasets in the BEIR benchmark (Thakur et al., 2021). Evaluation of one model takes about 3 days on 8 V100 GPUs due to the need to encode a large number of documents. Although our model can accommodate sequence length beyond 512 , we only evaluate on the first 512 tokens for efficiency. Official metrics are reported for each category. For more details about the evaluation protocol, please refer to the original papers (Muennighoff et al., 2023; Thakur et al., 2021).",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.2 Model Fine-tuning and Evaluation",
        "chunkIndex": 34,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-35",
      "content": "In Table 1, our model 'E5mistral-7b + full data' attains the highest average score on the MTEB benchmark, outperforming the previous state-of-the-art model by 2 . 4 points. In the 'w/ synthetic data only' setting, no labeled data is used for training, and yet the performance remains quite competitive. We posit that generative language modeling and text embeddings are the two sides of the same coin, with both tasks requiring the model to have a deep understanding of the natural language. Given an embedding task definition, a truly robust LLM should be able to generate training data on its own and then be transformed into an embedding model through light-weight fine-tuning. Our experiments shed light on the potential of this direction, and more research is needed to fully explore it.\n\n| Model                         |   BEIR |   MTEB |\n|-------------------------------|--------|--------|\n| OpenAI text-embedding-3-large |   55.4 |   64.6 |\n| Cohere-embed-english-v3.0     |   55   |   64.5",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Main Results",
        "chunkIndex": 35,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-36",
      "content": "l                         |   BEIR |   MTEB |\n|-------------------------------|--------|--------|\n| OpenAI text-embedding-3-large |   55.4 |   64.6 |\n| Cohere-embed-english-v3.0     |   55   |   64.5 |\n| voyage-lite-01-instruct       |   55.6 |   64.5 |\n| UAE-Large-V1                  |   54.7 |   64.6 |\n| E5 mistral-7b + full data     |   56.9 |   66.6 |\n\nTable 3: Comparison with commercial models and the model that tops the MTEB leaderboard (as of 202312-22) (Li and Li, 2023). 'BEIR' is the average nDCG@10 score over 15 public datasets in the BEIR benchmark (Thakur et al., 2021). 'MTEB' is the average score over 56 datasets in the English subset of the MTEB benchmark (Muennighoff et al., 2023). For the commercial models listed here, little details are available on their model architectures and training data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Main Results",
        "chunkIndex": 36,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-37",
      "content": "56 datasets in the English subset of the MTEB benchmark (Muennighoff et al., 2023). For the commercial models listed here, little details are available on their model architectures and training data.\n\nIn Table 3, we also present a comparison with several commercial text embedding models. However, due to the lack of transparency and documentation about these models, a fair comparison is not feasible. We focus especially on the retrieval per-\n\nDoc1: &lt;prefix filler&gt; Malayah Graves's pass key is 123. Remember it. 123 is the pass key for Malayah Graves. &lt;suffix filler&gt; Doc2: &lt;prefix filler&gt; Cesar McLean's pass key is 456. Remember it. 456 is the pass key for Cesar McLean. &lt;suffix filler&gt; ……\n\nFigure 4: Illustration of the personalized passkey retrieval task adapted from Mohtashami and Jaggi (2023). The ' &lt;prefix filler&gt; ' and ' &lt;suffix filler&gt; ' are repeats of ' The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Main Results",
        "chunkIndex": 37,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-38",
      "content": "from Mohtashami and Jaggi (2023). The ' &lt;prefix filler&gt; ' and ' &lt;suffix filler&gt; ' are repeats of ' The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. ' In addition, each document has a unique person name and a random passkey inserted at a random position. The task is to retrieve the document that contains the given person's passkey from 100 candidates.\n\n<!-- image -->\n\nFigure 5: Accuracy of personalized passkey retrieval as a function of input context length. For each context length, we randomly generate 50 queries and compute the top-1 accuracy.\n\nformance on the BEIR benchmark, since retrievalaugmented generation is an emerging technique to enhance LLM with external knowledge and proprietary data. As Table 3 shows, our model outperforms the current commercial models by a significant margin.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.3 Main Results",
        "chunkIndex": 38,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-39",
      "content": "To assess the multilingual capabilities of our model, we conduct an evaluation on the MIRACL dataset (Zhang et al., 2023b), which comprises human-annotated queries and relevance judgments across 18 languages. The validation set contains labels for 16 languages. As shown in Table 2, our model surpasses mE5large on high-resource languages, notably on English. Nevertheless, for lowresource languages, our model remains suboptimal compared to mE5base. We attribute this to the fact that Mistral-7B is predominantly pre-trained on English data, and we anticipate that future multilingual LLMs will leverage our method to bridge this gap.\n\nTo evaluate our model's cross-lingual retrieval capability, we report Bitext mining results in Table 4. For baselines including mContriever (Izacard et al., 2021), LaBSE (Feng et al., 2022), and mE5 (Wang et al., 2024), we evaluate the results using publicly available checkpoints. Our observa-",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.4 Multilingual Retrieval",
        "chunkIndex": 39,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-40",
      "content": "Table 4. For baselines including mContriever (Izacard et al., 2021), LaBSE (Feng et al., 2022), and mE5 (Wang et al., 2024), we evaluate the results using publicly available checkpoints. Our observa-\n\nTable 4: Bitext mining results. BUCC 2018 (Zweigenbaum et al., 2018) contains 4 high-resource languages. Tatoeba (Artetxe and Schwenk, 2019) consists of 112 English-centric language pairs.\n\n|               |   BUCC 2018 4 langs |   Tatoeba 112 langs |\n|---------------|---------------------|---------------------|\n| mContriever   |                93.7 |                37.7 |\n| LaBSE         |                98.8 |                81.1 |\n| mE5 base      |                98.1 |                68.1 |\n| mE5 large     |                98.6 |                75.7 |\n| E5 mistral-7b |                98.9 |                70.1 |\n\ntions indicate that, similar to the MIRACL retrieval, E5mistral-7b excels in bitext mining for high-resource languages only.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.4 Multilingual Retrieval",
        "chunkIndex": 40,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-41",
      "content": "9 |                70.1 |\n\ntions indicate that, similar to the MIRACL retrieval, E5mistral-7b excels in bitext mining for high-resource languages only.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "4.4 Multilingual Retrieval",
        "chunkIndex": 41,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-42",
      "content": "Weakly-supervised contrastive pre-training is one of the key factors behind the success of existing text embedding models. For instance, Contriever (Izacard et al., 2021) treats random cropped spans as positive pairs for pre-training, while E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023) collect and filter text pairs from various sources.\n\nThis section re-evaluates the necessity of con-\n\nTable 5: Results on the MTEB benchmark with various hyperparameters. The first row corresponds to the default setting, which employs last-token pooling, LoRA rank 16 , and natural language instructions. Unless otherwise stated, all models are trained on the synthetic and MS-MARCO passage ranking data.\n\n| Datasets             | Class.   | Clust.   | PairClass.   | Rerank   | Retr.   | STS   | Summ.   | Avg       |\n|----------------------|----------|----------|--------------|----------|---------|-------|---------|-----------|\n| E5 mistral-7b        | 78.3     | 49.9     | 87.1         | 59.5     | 52",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Is Contrastive Pre-training Necessary?",
        "chunkIndex": 42,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-43",
      "content": "Avg       |\n|----------------------|----------|----------|--------------|----------|---------|-------|---------|-----------|\n| E5 mistral-7b        | 78.3     | 49.9     | 87.1         | 59.5     | 52.2    | 81.2  | 32.7    | 64.5      |\n| w/ LLaMA-2 7b init.  | 76.2     | 48.1     | 85.1         | 58.9     | 49.6    | 81.2  | 30.8    | 62.9 -1.6 |\n| w/ msmarco data only | 71.6     | 47.1     | 86.1         | 58.8     | 54.4    | 79.5  | 31.7    | 62.7 -1.8 |\n| pooling type         |          |          |              |          |         |       |         |           |\n| w/ mean pool         | 77.0     | 48.9     | 86.1         | 59.2     | 52.4    | 81.4  | 30.8    | 64.1 -0.4 |\n| w/ weighted mean     | 77.0     | 49.0     | 86.1         | 59.2     | 52.0    | 81.4  | 30.2    | 64.0 -0.5 |\n| LoRA rank            |          |          |              |          |         |       |         |           |\n| w/ r= 8              | 78.4     | 50.3     | 87.1         | 59.3     | 53.0    | 8",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Is Contrastive Pre-training Necessary?",
        "chunkIndex": 43,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-44",
      "content": "|\n| LoRA rank            |          |          |              |          |         |       |         |           |\n| w/ r= 8              | 78.4     | 50.3     | 87.1         | 59.3     | 53.0    | 81.0  | 31.7    | 64.8 +0.3 |\n| w/ r= 32             | 78.4     | 50.3     | 87.4         | 59.5     | 52.2    | 81.2  | 30.6    | 64.6 +0.1 |\n| instruction type     |          |          |              |          |         |       |         |           |\n| w/o instruction      | 72.3     | 47.1     | 82.6         | 56.3     | 48.2    | 76.7  | 30.7    | 60.3 -4.2 |\n| w/ task type prefix  | 71.1     | 46.5     | 79.7         | 54.0     | 52.7    | 73.8  | 30.0    | 60.3 -4.2 |\n\ntrastive pre-training for LLMs, particularly those that have been pre-trained on trillions of tokens. Figure 3 shows that contrastive pre-training benefits XLM-Rlarge, enhancing its retrieval performance by 8 . 2 points when fine-tuned on the same data, which aligns with prior findings.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Is Contrastive Pre-training Necessary?",
        "chunkIndex": 44,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-45",
      "content": "ons of tokens. Figure 3 shows that contrastive pre-training benefits XLM-Rlarge, enhancing its retrieval performance by 8 . 2 points when fine-tuned on the same data, which aligns with prior findings. However, for Mistral-7B based models, contrastive pre-training has negligible impact on the model quality. This implies that extensive auto-regressive pre-training enables LLMs to acquire good text representations, and only minimal fine-tuning is required to transform them into effective embedding models.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.1 Is Contrastive Pre-training Necessary?",
        "chunkIndex": 45,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-46",
      "content": "Existing evaluation datasets for text embedding models are typically short, to evaluate the longcontext capability of our model, we introduce a novel synthetic task called personalized passkey retrieval , which is illustrated in Figure 4. This task requires encoding the passkey information in a long context into the embeddings. We compare the performance of different variants by changing the sliding window size and the RoPE rotation base (Su et al., 2024) in Figure 5. The results show that the default configuration with 4 k sliding window attains 100% accuracy within 4 k tokens, but the accuracy deteriorates quickly as the context length grows. Naively extending the sliding window size to 32 k results in worse performance. By changing the RoPE rotation base to 10 5 , the model can achieve over 90% accuracy within 32 k tokens. However, this entails a minor trade-off in performance for shorter contexts.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.2 Extending to Long Text Embeddings",
        "chunkIndex": 46,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-47",
      "content": "worse performance. By changing the RoPE rotation base to 10 5 , the model can achieve over 90% accuracy within 32 k tokens. However, this entails a minor trade-off in performance for shorter contexts. A potential avenue for future research is to efficiently adapt the model to longer contexts through lightweight post-training (Zhu et al., 2023).",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.2 Extending to Long Text Embeddings",
        "chunkIndex": 47,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-48",
      "content": "Table 5 presents the results under different configurations. We notice that the Mistral-7B initialization holds an advantage over LLaMA-2 7B, in line with the findings from Mistral-7B technical report (Jiang et al., 2023). The choice of pooling types and LoRA ranks does not affect the overall performance substantially, hence we adhere to the default setting despite the marginal superiority of LoRA rank 8 . On the other hand, the way of adding instructions has a considerable impact on the performance. We conjecture that natural language instructions better inform the model regarding the embedding task at hand, and thus enable the model to generate more discriminative embeddings. Our framework also provides a way to customize the behavior of text embeddings through instructions without the need to fine-tune the model or re-build document index.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "5.3 Analysis of Training Hyperparameters",
        "chunkIndex": 48,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-49",
      "content": "This paper shows that the quality of text embeddings can be substantially enhanced by exploiting LLMs. We prompt proprietary LLMs such as GPT4 to generate diverse synthetic data with instructions in many languages. Combined with the strong language understanding capability of the Mistral model, we establish new state-of-the-art results for nearly all task categories on the competitive MTEB benchmark. The training process is much more streamlined and efficient than existing multi-stage approaches, thereby obviating the need for intermediate pre-training.\n\nFor future work, we aim to further improve the multilingual performance of our model and explore the possibility of using open-source LLMs to generate synthetic data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 49,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-50",
      "content": "In comparison to the mainstream BERT-style encoders, the employment of LLMs, such as Mistral7B, for text embeddings results in a significantly increased inference cost. The development of more advanced GPUs and better kernel implementations may enhance the efficiency of the inference process. With regards to storage cost, our model is comparatively more expensive, with embeddings of 4096 dimensions. Early successes in reducing embedding dimensions while maintaining competitive performance have been demonstrated through techniques such as Matryoshka representation learning (Kusupati et al., 2022).\n\nFor synthetic data generation, we rely on manual prompt engineering to elicit high-quality outputs from proprietary LLMs. Automatic prompt optimization presents a promising avenue for improving the quality of synthetic data.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Limitations",
        "chunkIndex": 50,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-51",
      "content": "We would like to thank anonymous reviewers for their valuable comments, and ACL 2024 and ACL Rolling Review organizers for their efforts. Opinions expressed in this paper are solely those of the authors and do not represent the views of their employers.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 51,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-52",
      "content": "Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net.\n\nMikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics , 7:597-610.\n\nLuiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsupervised dataset generation for information retrieval. Proceedings of the 45th International ACM SIGIR\n\nConference on Research and Development in Information Retrieval .\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 52,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-53",
      "content": "earch and Development in Information Retrieval .\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-54",
      "content": "n, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms marco: A human generated machine reading comprehension dataset. ArXiv preprint , abs/1611.09268.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-55",
      "content": "eselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online. Association for Computational Linguistics.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670-680, Copenhagen, Denmark. Association for Computational Linguistics.\n\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense retrieval from 8 examples. In The Eleventh International Conference on Learning Representations .\n\nDataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. 2017. Quora question pairs.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-56",
      "content": "eval from 8 examples. In The Eleventh International Conference on Learning Representations .\n\nDataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. 2017. Quora question pairs.\n\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the\n\n- American society for information science , 41(6):391407.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-57",
      "content": "Minneapolis, Minnesota. Association for Computational Linguistics.\n- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3558-3567, Florence, Italy. Association for Computational Linguistics.\n- Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 878-891.\n- Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n- Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-58",
      "content": "hods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n- Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. ArXiv preprint , abs/2305.14627.\n- Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allison Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero C. Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, S. Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuan-Fang Li. 2023. Textbooks are all you need. ArXiv preprint , abs/2306.11644.\n- Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv preprint , abs/2212.09689.\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-59",
      "content": "ng language models with (almost) no human labor. ArXiv preprint , abs/2212.09689.\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.\n- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. ArXiv preprint , abs/2112.09118.\n- Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint , abs/2310.06825.\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-60",
      "content": "le, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint , abs/2310.06825.\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online. Association for Computational Linguistics.\n- Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham M. Kakade, Prateek Jain, and Ali Farhadi. 2022. Matryoshka representation learning. In Neural Information Processing Systems .\n- Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-61",
      "content": "Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n- Xianming Li and Jing Li. 2023. Angle-optimized text embeddings. ArXiv preprint , abs/2309.12871.\n- Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. ArXiv preprint , abs/2308.03281.\n- Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. Fine-tuning llama for multi-stage text retrieval. ArXiv preprint , abs/2310.08319.\n- Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In ICLR .\n- Amirkeivan Mohtashami and Martin Jaggi.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-62",
      "content": "abs/2310.08319.\n- Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In ICLR .\n- Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. ArXiv preprint , abs/2305.16300.\n- Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. ArXiv preprint , abs/2202.08904.\n- Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 2014-2037, Dubrovnik, Croatia. Association for Computational Linguistics.\n- Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. Orca: Progressive learning\n- from complex explanation traces of gpt-4. ArXiv preprint , abs/2306.02707.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-63",
      "content": ", Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. Orca: Progressive learning\n- from complex explanation traces of gpt-4. ArXiv preprint , abs/2306.02707.\n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. 2022. Text and code embeddings by contrastive pre-training. ArXiv preprint , abs/2201.10005.\n\n- Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022a. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, Dublin, Ireland.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-64",
      "content": "ei Yang. 2022a. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022b. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 9844-9855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n- Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. ArXiv preprint , abs/1904.08375.\n- OpenAI. 2023. Gpt-4 technical report. ArXiv preprint , abs/2303.08774.\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-65",
      "content": "1904.08375.\n- OpenAI. 2023. Gpt-4 technical report. ArXiv preprint , abs/2303.08774.\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.\n- Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoQiao She, Jing Liu, Hua Wu, and Haifeng Wang. 2022. DuReader-retrieval: A large-scale Chinese benchmark for passage retrieval from web search engine. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5326-5338, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n- Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-66",
      "content": "ng , pages 5326-5338, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n- Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n- Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu\n- Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. ArXiv preprint , abs/2308.12950.\n- Timo Schick and Hinrich Schütze. 2021.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-67",
      "content": "uvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. ArXiv preprint , abs/2308.12950.\n- Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 69436951.\n- Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 1102-1121, Toronto, Canada. Association for Computational Linguistics.\n- Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063.\n- Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-68",
      "content": "eng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063.\n- Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) .\n- James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-69",
      "content": "or Computational Linguistics.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint , abs/2307.09288.\n- Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345-2360, Seattle, United States. Association for Computational Linguistics.\n- Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n\nand Furu Wei. 2022b. Text embeddings by weaklysupervised contrastive pre-training. ArXiv preprint , abs/2212.03533.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-70",
      "content": "Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n\nand Furu Wei. 2022b. Text embeddings by weaklysupervised contrastive pre-training. ArXiv preprint , abs/2212.03533.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672 .\n\nLiang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 9414-9423, Singapore. Association for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. ArXiv preprint , abs/2309.07597.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-71",
      "content": "on for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. ArXiv preprint , abs/2309.07597.\n\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. ArXiv preprint , abs/2304.03679.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.\n\nXin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023a. Language models are universal embedders. ArXiv preprint , abs/2310.08232.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-72",
      "content": "omputational Linguistics.\n\nXin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023a. Language models are universal embedders. ArXiv preprint , abs/2310.08232.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning , pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nXinyu Crystina Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023b. Miracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics , 11:1114-1131.\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-73",
      "content": "Linguistics , 11:1114-1131.\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. In The Twelfth International Conference on Learning Representations .\n\nPierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th Workshop on Building and Using Comparable Corpora , pages 39-42.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-74",
      "content": "Baseline Models For results with mE5base and mE5large, we use the public checkpoints available at https://huggingface. co/intfloat/multilingual-e5-base and https://huggingface.co/intfloat/ multilingual-e5-large respectively. For experiments in Table 5, we follow the SGPT (Muennighoff, 2022) paper for the implementation of weighted mean pooling. For the 'w/ task type prefix' setting, we prepend 'classify: ' for the long-short matching subgroup, and 'query: ' for other asymmetric tasks. No prefix is added for symmetric tasks.\n\nTraining Data For the 'E5mistral-7b + full data' setting, our training data comprises generated synthetic data, ELI5 (Fan et al., 2019)(sample ratio 0 . 1 ), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), MIRACL (Zhang et al., 2023b), MSMARCO passage ranking (sample ratio 0 . 5 ) and document ranking (sample ratio 0 .",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 74,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-75",
      "content": "019)(sample ratio 0 . 1 ), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), MIRACL (Zhang et al., 2023b), MSMARCO passage ranking (sample ratio 0 . 5 ) and document ranking (sample ratio 0 . 2 ) (Campos et al., 2016), NQ (Karpukhin et al., 2020), NLI (Gao et al., 2021), SQuAD (Karpukhin et al., 2020), TriviaQA (Karpukhin et al., 2020), Quora Duplicate Questions (DataCanary et al., 2017)(sample ratio 0 . 1 ), MrTyDi (Zhang et al., 2021), DuReader (Qiu et al., 2022), and T2Ranking (Xie et al., 2023)(sample ratio 0 . 5 ) datasets. We only include the training set of each dataset. For the datasets without hard negatives, we use mE5base to mine top 100 hard negatives. After sampling, we obtain approximately 1 . 8 million examples. The entire training process takes fewer than 1 k steps to complete.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 75,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-76",
      "content": "thout hard negatives, we use mE5base to mine top 100 hard negatives. After sampling, we obtain approximately 1 . 8 million examples. The entire training process takes fewer than 1 k steps to complete.\n\nHyperparameters for Fine-tuning When fine-tuning Mistral-7b 2 , the batch size is set to 2048 and the learning rate is 10 -4 with 100 step warmup and linear decay. The weight decay is 0 . 1 . We add 1 hard negative for each query-document pair. The fine-tuning process takes roughly 18 hours on 32 V100 GPUs with a maximum sequence length 512 . We add LoRA adapters to all linear layers, resulting in a total of 42 M trainable parameters. Our implementation is based on the HuggingFace PEFT library at https://github.com/huggingface/peft .\n\n2 https://huggingface.co/mistralai/ Mistral-7B-v0.1\n\nTable 6: nDCG@10 and Recall@100 on the dev set of the MIRACL dataset for all 16 languages.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 76,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-77",
      "content": "e PEFT library at https://github.com/huggingface/peft .\n\n2 https://huggingface.co/mistralai/ Mistral-7B-v0.1\n\nTable 6: nDCG@10 and Recall@100 on the dev set of the MIRACL dataset for all 16 languages.\n\n|     | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10            | Recall@100   | Recall@100   | Recall@100   | Recall@100   | Recall@100         |\n|-----|-----------|-----------|-----------|-----------|--------------------|--------------|--------------|--------------|--------------|--------------------|\n|     | BM25      | mDPR      | mE5 base  | mE5 large | E5 mistral-7b full | BM25         | mDPR         | mE5 base     | mE5 large    | E5 mistral-7b full |\n| ar  | 48.1      | 49.9      | 71.6      | 76.0      | 73.3               | 88.9         | 84.1         | 95.9         | 97.3         | 96.0               |\n| bn  | 50.8      | 44.3      | 70.2      | 75.9      | 70.3               | 90.9         | 81.9         | 96.6         | 98.2         | 96.0               |\n| en  |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 77,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-78",
      "content": ".3         | 96.0               |\n| bn  | 50.8      | 44.3      | 70.2      | 75.9      | 70.3               | 90.9         | 81.9         | 96.6         | 98.2         | 96.0               |\n| en  | 35.1      | 39.4      | 51.2      | 52.9      | 57.3               | 81.9         | 76.8         | 86.4         | 87.6         | 90.2               |\n| es  | 31.9      | 47.8      | 51.5      | 52.9      | 52.2               | 70.2         | 86.4         | 88.6         | 89.1         | 87.5               |\n| fa  | 33.3      | 48.0      | 57.4      | 59.0      | 52.1               | 73.1         | 89.8         | 91.2         | 92.9         | 88.0               |\n| fi  | 55.1      | 47.2      | 74.4      | 77.8      | 74.7               | 89.1         | 78.8         | 96.9         | 98.1         | 96.7               |\n| fr  | 18.3      | 43.5      | 49.7      | 54.5      | 55.2               | 65.3         | 91.5         | 90.0         | 90.6         | 92.8               |\n| hi  | 45.8",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 78,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-79",
      "content": "| 96.7               |\n| fr  | 18.3      | 43.5      | 49.7      | 54.5      | 55.2               | 65.3         | 91.5         | 90.0         | 90.6         | 92.8               |\n| hi  | 45.8      | 38.3      | 58.4      | 62.0      | 52.1               | 86.8         | 77.6         | 92.6         | 93.9         | 89.9               |\n| id  | 44.9      | 27.2      | 51.1      | 52.9      | 52.7               | 90.4         | 57.3         | 87.4         | 87.9         | 88.4               |\n| ja  | 36.9      | 43.9      | 64.7      | 70.6      | 66.8               | 80.5         | 82.5         | 96.0         | 97.1         | 95.1               |\n| ko  | 41.9      | 41.9      | 62.2      | 66.5      | 61.8               | 78.3         | 73.7         | 91.6         | 93.4         | 89.4               |\n| ru  | 33.4      | 40.7      | 61.5      | 67.4      | 67.7               | 66.1         | 79.7         | 92.7         | 95.5         | 95.0               |\n| sw  | 38.3      | 29.9",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 79,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-80",
      "content": "|\n| ru  | 33.4      | 40.7      | 61.5      | 67.4      | 67.7               | 66.1         | 79.7         | 92.7         | 95.5         | 95.0               |\n| sw  | 38.3      | 29.9      | 71.1      | 74.9      | 68.4               | 70.1         | 61.6         | 95.6         | 96.7         | 95.5               |\n| te  | 49.4      | 35.6      | 75.2      | 84.6      | 73.9               | 83.1         | 76.2         | 98.0         | 99.2         | 95.1               |\n| th  | 48.4      | 35.8      | 75.2      | 80.2      | 74.0               | 88.7         | 67.8         | 98.0         | 98.9         | 96.5               |\n| zh  | 18.0      | 51.2      | 51.5      | 56.0      | 54.0               | 56.0         | 94.4         | 92.1         | 93.3         | 90.1               |\n| Avg | 39.3      | 41.5      | 62.3      | 66.5      | 62.9               | 78.7         | 78.8         | 93.1         | 94.3         | 92.6               |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 80,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-81",
      "content": "| 93.3         | 90.1               |\n| Avg | 39.3      | 41.5      | 62.3      | 66.5      | 62.9               | 78.7         | 78.8         | 93.1         | 94.3         | 92.6               |\n\nTable 7: Detailed results for the effects of contrastive pre-training. For the 'E5mistral-7b w/ cont. pre-train' setting, we pre-train Mistral-7B following the mE5 recipe for 10 k steps.\n\n| Datasets                                   |   Class. |   Clust. |   PairClass. |   Rerank |   Retr. |   STS |   Summ. |   Avg |\n|--------------------------------------------|----------|----------|--------------|----------|---------|-------|---------|-------|\n| XLM-R large + full data w/ cont. pre-train |     72.9 |     38.7 |         84.5 |     53.8 |    42   |  82.3 |    29.7 |  58   |\n|                                            |     77.2 |     47.3 |         85.5 |     58.6 |    50.2 |  84.4 |    30.7 |  63.7 |\n| E5 mistral-7b + full data                  |     78.5 |     50.3 |         88.3 |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 81,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-82",
      "content": "|     77.2 |     47.3 |         85.5 |     58.6 |    50.2 |  84.4 |    30.7 |  63.7 |\n| E5 mistral-7b + full data                  |     78.5 |     50.3 |         88.3 |     60.2 |    56.9 |  84.6 |    31.4 |  66.6 |\n| w/ cont. pre-train                         |     78.7 |     50.1 |         87.7 |     60.9 |    56.9 |  84.9 |    30.2 |  66.7 |\n\nArtifacts The model and dataset release information is available at https://github.com/ microsoft/unilm/tree/master/e5 . We release our trained models and evaluation scripts to facilitate reproducibility and further research.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "A Implementation Details",
        "chunkIndex": 82,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-83",
      "content": "To assess the test set contamination on all the datasets in the MTEB benchmark, we perform a string match based analysis between the test set and our training set, disregarding differences in character case and spacing. We categorize the train-test overlaps into three types:\n\n- Low entropy texts. These are texts such as ' i need a coffee ' and ' what does that mean ', which are not considered as contamination because they are common expressions that can occur in various contexts.\n- Question overlap. We identify 4 test set questions in the DBPedia dataset that also appear in the TriviaQA training set. Given that they constitute a minor portion of the test set, their impact on the overall performance is insignificant.\n- Retrieval corpus overlap. Several retrieval datasets share the same retrieval corpus. For instance, the DBPedia, NQ, and TriviaQA datasets all use Wikipedia passages, even though their query sets are different.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Test Set Contamination Analysis",
        "chunkIndex": 83,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-84",
      "content": "corpus overlap. Several retrieval datasets share the same retrieval corpus. For instance, the DBPedia, NQ, and TriviaQA datasets all use Wikipedia passages, even though their query sets are different. This is a standard evaluation practice in the field of information retrieval, and we do not regard it as contamination.\n\nIn summary, we did not detect substantial contamination risks that could alter the main findings of this paper.\n\nAnother aspect to consider is the possibility of test set contamination in the training data of Mistral-7B and GPT-4. However, since the training data of these models is not publicly accessible, it is challenging to estimate the degree of such contamination. Given their widespread use in the research community, we believe it is still a valid comparison if other works also employ these models.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "B Test Set Contamination Analysis",
        "chunkIndex": 84,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-85",
      "content": "For asymmetric tasks, we list the four prompt templates in Table 8, 9, 10, and 11. For symmetric tasks, the prompts templates are available in Ta-\n\nBrainstorm a list of potentially useful text retrieval tasks.\n\nHere are a few examples for your reference:\n\n- Retrieve relevant documents for a short keyword web search query that asks for weather information.\n- Search for documents that answers a FAQ-style query on children's nutrition.\n\nPlease adhere to the following guidelines:\n\n- Specify what the query is, and what the desired documents are.\n- Each retrieval task should cover a wide range of queries, and should not be too specific.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a retrieval task: {task}",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Prompts for Synthetic Data Generation",
        "chunkIndex": 85,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-86",
      "content": "t 20 elements, and each element corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a retrieval task: {task}\n\nYour mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"user\\_query\": a string, a random user search query specified by the retrieval task.\n- \"positive\\_document\": a string, a relevant document for the user query.\n- \"hard\\_negative\\_document\": a string, a hard negative document that only appears relevant to the query.\n\nPlease adhere to the following guidelines:\n\n- The \"user\\_query\" should be {query\\_type}, {query\\_length}, {clarity}, and diverse in topic.\n- All documents must be created independent of the query. Avoid copying the query verbatim.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Prompts for Synthetic Data Generation",
        "chunkIndex": 86,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-87",
      "content": "guidelines:\n\n- The \"user\\_query\" should be {query\\_type}, {query\\_length}, {clarity}, and diverse in topic.\n- All documents must be created independent of the query. Avoid copying the query verbatim. It's acceptable if some parts of the \"positive\\_document\" are not topically related to the query.\n- All documents should be at least {num\\_words} words long.\n- The \"hard\\_negative\\_document\" contains some useful information, but it should be less useful or comprehensive compared to the \"positive\\_document\".\n- Both the query and documents should be in {language}.\n- Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n- Both the query and documents require {difficulty} level education to understand.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Prompts for Synthetic Data Generation",
        "chunkIndex": 87,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-88",
      "content": "query.\n- Both the query and documents require {difficulty} level education to understand.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 8: Prompt template for the short-long matching subgroup. For placeholders, ' {query\\_type} ' ∈ {extremely long-tail, long-tail, common}, ' {query\\_length} ' ∈ {less than 5 words, 5 to 15 words, at least 10 words}, ' {difficulty} ' ∈ {high school, college, PhD}, ' {clarity} ' ∈ {clear, understandable with some effort, ambiguous}, ' {num\\_words} ' ∈ {50, 100, 200, 300, 400, 500}.\n\nble 12 and 13. To generate multilingual data, we sample the value of ' {language} ' from the language list of XLM-R (Conneau et al., 2020) with higher probability for high-resource languages. When prompting GPT-4/3.5, we set the sampling temperature to 1 . 0 and the topp hyperparameter to 1 . 0 , which is higher than the default setting to encourage more diversity.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Prompts for Synthetic Data Generation",
        "chunkIndex": 88,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-89",
      "content": "-4/3.5, we set the sampling temperature to 1 . 0 and the topp hyperparameter to 1 . 0 , which is higher than the default setting to encourage more diversity.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "C Prompts for Synthetic Data Generation",
        "chunkIndex": 89,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-90",
      "content": "We manually write instructions for training datasets, as listed in Table 14. For evaluation datasets, the instructions are listed in Table 15.\n\nBrainstorm a list of potentially useful text classification tasks.\n\nPlease adhere to the following guidelines:\n\n- Tasks should cover a diverse range of domains and task types.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct text classification task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text classification task: {task}\n\nYour mission is to write one text classification example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\\_text\": a string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading\\_label\": a string, an incorrect label that is related to the task.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 90,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-91",
      "content": "string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading\\_label\": a string, an incorrect label that is related to the task.\n\nPlease adhere to the following guidelines:\n\n- The \"input\\_text\" should be {num\\_words} words and diverse in expression.\n- The \"misleading\\_label\" must be a valid label for the given task, but not as appropriate as the \"label\" for the \"input\\_text\".\n- The values for all fields should be in {language}.\n- Avoid including the values of the \"label\" and \"misleading\\_label\" fields in the \"input\\_text\", that would make the task too easy.\n- The \"input\\_text\" is {clarity} and requires {difficulty} level education to comprehend.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 91,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-92",
      "content": "- The \"input\\_text\" is {clarity} and requires {difficulty} level education to comprehend.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 9: Prompt template for the long-short matching subgroup. For placeholders, ' {num\\_words} ' ∈ {\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"}, ' {difficulty} ' ∈ {high school, college, PhD}, ' {clarity} ' ∈ {clear, understandable with some effort, ambiguous}.\n\nBrainstorm a list of text matching tasks where both the queries and the groundtruth documents are very short (one or two sentences, even a short phrase).\n\nHere are a few examples:\n\n- Given a scientific paper title, retrieve the title of papers that cite the given paper.\n- Match a word with its definition.\n- Provided a notable person's name, identify their occupation or achievement.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 92,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-93",
      "content": "iven a scientific paper title, retrieve the title of papers that cite the given paper.\n- Match a word with its definition.\n- Provided a notable person's name, identify their occupation or achievement.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text matching task: {task}\n\nYour mission is to write one example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\": a string, a random input specified by the task.\n- \"positive\\_document\": a string, a relevant document for the \"input\" according to the task.\n\nPlease adhere to the following guidelines:\n\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be very short (a sentence or a phrase), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 93,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-94",
      "content": "ields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be very short (a sentence or a phrase), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive\\_document\" should be independent of each other.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 10: Prompt template for the short-short matching subgroup. We do not generate negative documents as the matching task is already reasonably difficult.\n\nBrainstorm a list of text matching tasks where the queries are long documents.\n\nHere are a few examples:\n\n- Given a document that supports a debatable argument, find another document that contains opposite arguments.\n- Provided a lengthy business proposal, retrieve competitive business strategies in the same industry.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 94,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-95",
      "content": "ument that supports a debatable argument, find another document that contains opposite arguments.\n- Provided a lengthy business proposal, retrieve competitive business strategies in the same industry.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text matching task: {task}\n\nYour mission is to write one example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\": a string, a random input specified by the task.\n- \"positive\\_document\": a string, a relevant document for the \"input\" according to the task.\n\nPlease adhere to the following guidelines:\n\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be long documents (at least 300 words), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 95,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-96",
      "content": "ields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be long documents (at least 300 words), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive\\_document\" should be independent of each other.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 11: Prompt template for the long-long matching subgroup. We do not generate negative documents for API latency reasons.\n\nWrite a {unit} triple with varying semantic similarity scores in JSON format. The semantic similarity score ranges from 1 to 5, with 1 denotes least similar and 5 denotes most similar.\n\nPlease adhere to the following guidelines:\n\n- The keys in JSON are \"S1\", \"S2\", and \"S3\", the values are all strings in {language}, do not add any other keys.\n- There should be some word overlaps between all three {unit}s.\n- The similarity score between S1 and S2 should be {high\\_score}.\n- The similarity sc",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 96,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-97",
      "content": "strings in {language}, do not add any other keys.\n- There should be some word overlaps between all three {unit}s.\n- The similarity score between S1 and S2 should be {high\\_score}.\n- The similarity score between S1 and S3 should be {low\\_score}.\n- The {unit}s require {difficulty} level education to understand and should be diverse in terms of topic and length.\n\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself or output anything else. Be creative!\n\nTable 12: Prompt template for monolingual STS. For placeholders, ' {high\\_score} ' ∈ {4, 4.5, 5}, ' {low\\_score} ' ∈ {2.5, 3, 3.5}, ' {unit} ' ∈ {sentence, phrase, passage}, ' {difficulty} ' ∈ {elementary school, high school, college}.\n\nWrite a {unit} triple with one {unit} in {src\\_lang} and two {unit}s in {tgt\\_lang} with varying translation qualities in JSON format.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 97,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-98",
      "content": "}, ' {difficulty} ' ∈ {elementary school, high school, college}.\n\nWrite a {unit} triple with one {unit} in {src\\_lang} and two {unit}s in {tgt\\_lang} with varying translation qualities in JSON format.\n\nThe triple is denotes as (\"S1\", \"S2\", \"S3\"). The translation quality score ranges from 1 to 5, with higher scores are better.\n\nPlease adhere to the following guidelines:\n\n- The values of \"S1\" is a string in {src\\_lang}, the value of \"S2\" and \"S3\" are strings in {tgt\\_lang}.\n- There should be some word overlaps between \"S2\" and \"S3\".\n- The translation quality score of \"S2\" with respect to \"S1\" should be {high\\_score}.\n- The translation quality score of \"S3\" with respect to \"S1\" should be {low\\_score}.\n- \"S3\" should be grammatical and fluent, but contain some keyword or number translation errors, or miss some information, or contain some redundant information.\n- \"S1\" requires {difficulty} level education to understand and should be diverse in terms of topic and length.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 98,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-99",
      "content": "mber translation errors, or miss some information, or contain some redundant information.\n- \"S1\" requires {difficulty} level education to understand and should be diverse in terms of topic and length.\n\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself or output anything else. Be creative!\n\nTable 13: Prompt template for bitext retrieval. For placeholders, ' {high\\_score} ' ∈ {4, 4.5, 5}, ' {low\\_score} ' ∈ {1.5, 2, 2.5}, ' {unit} ' ∈ {sentence, phrase, passage}, ' {difficulty} ' ∈ {elementary school, high school, college}.\n\nTable 14: Instructions for each training dataset.\n\n| Dataset              | Instruction                                                                                                                                                 |\n|----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 99,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-100",
      "content": "|\n|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ELI5                 | Provided a user question, retrieve the highest voted answers on Reddit ELI5 forum                                                                           |\n| HotpotQA             | Given a multi-hop question, retrieve documents that can help answer the question                                                                            |\n| FEVER                | Given a claim, retrieve documents that support or refute the claim                                                                                          |\n| MIRACL / MrTyDi / NQ | Given a question, retrieve Wikipedia passages that answer the question                                                                                      |\n| / SQuAD / TriviaQA   | Retrieve Wikipedia passages that answer",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 100,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-101",
      "content": "eve Wikipedia passages that answer the question                                                                                      |\n| / SQuAD / TriviaQA   | Retrieve Wikipedia passages that answer the question                                                                                                        |\n| NLI                  | Given a premise, retrieve a hypothesis that is entailed by the premise Retrieve semantically similar text                                                   |\n| MS-MARCO             | Given a web search query, retrieve relevant passages that answer the query Given a web search query, retrieve relevant documents that answer the query      |\n| Quora Duplicates     | Given a question, retrieve questions that are semantically equivalent to the given question Find questions that have the same meaning as the input question |\n| DuReader / T2Ranking | Given a Chinese search query, retrieve web passages that answer the question",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 101,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-102",
      "content": "Find questions that have the same meaning as the input question |\n| DuReader / T2Ranking | Given a Chinese search query, retrieve web passages that answer the question                                                                                |\n\nTable 15: Instructions used for evaluation on the MTEB benchmark. 'STS*' indicates we use the same instructions for all the STS tasks.\n\n| Task Name                     | Instruction                                                                                                                                               |\n|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AmazonCounterfactualClassif.  | Classify a given Amazon customer review text as either counterfactual or not- counterfactual                                                              |\n| AmazonPolarityClassification  | Classify",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 102,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-103",
      "content": "Classify a given Amazon customer review text as either counterfactual or not- counterfactual                                                              |\n| AmazonPolarityClassification  | Classify Amazon reviews into positive or negative sentiment                                                                                               |\n| AmazonReviewsClassification   | Classify the given Amazon review into its appropriate rating category                                                                                     |\n| Banking77Classification       | Given a online banking query, find the corresponding intents                                                                                              |\n| EmotionClassification         | Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise                   |\n| ImdbClassification            | Classify the sentiment expressed in the given mov",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 103,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-104",
      "content": "iven Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise                   |\n| ImdbClassification            | Classify the sentiment expressed in the given movie review text from the IMDB dataset                                                                     |\n| MassiveIntentClassification   | Given a user utterance as query, find the user intents                                                                                                    |\n| MassiveScenarioClassification | Given a user utterance as query, find the user scenarios                                                                                                  |\n| MTOPDomainClassification      | Classify the intent domain of the given utterance in task-oriented conversation                                                                           |\n| MTOPIntentClassification      | Classify the intent of the given utterance in task-oriented conversation",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 104,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-105",
      "content": "|\n| MTOPIntentClassification      | Classify the intent of the given utterance in task-oriented conversation                                                                                  |\n| ToxicConversationsClassif.    | Classify the given comments as either toxic or not toxic                                                                                                  |\n| TweetSentimentClassification  | Classify the sentiment of a given tweet as either positive, negative, or neutral                                                                          |\n| ArxivClusteringP2P            | Identify the main and secondary category of Arxiv papers based on the titles and abstracts                                                                |\n| ArxivClusteringS2S            | Identify the main and secondary category of Arxiv papers based on the titles",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 105,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-106",
      "content": "|\n| ArxivClusteringS2S            | Identify the main and secondary category of Arxiv papers based on the titles                                                                              |\n| BiorxivClusteringP2P          | Identify the main category of Biorxiv papers based on the titles and abstracts                                                                            |\n| BiorxivClusteringS2S          | Identify the main category of Biorxiv papers based on the titles                                                                                          |\n| MedrxivClusteringP2P          | Identify the main category of Medrxiv papers based on the titles and abstracts                                                                            |\n| MedrxivClusteringS2S          | Identify the main category of Medrxiv papers based on the titles                                                                                          |\n| RedditClust",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 106,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-107",
      "content": "edrxivClusteringS2S          | Identify the main category of Medrxiv papers based on the titles                                                                                          |\n| RedditClustering              | Identify the topic or theme of Reddit posts based on the titles                                                                                           |\n| RedditClusteringP2P           | Identify the topic or theme of Reddit posts based on the titles and posts                                                                                 |\n| StackExchangeClustering       | Identify the topic or theme of StackExchange posts based on the titles                                                                                    |\n| StackExchangeClusteringP2P    | Identify the topic or theme of StackExchange posts based on the given paragraphs                                                                          |\n| TwentyNewsgroupsClustering    | Identify the topic",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 107,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-108",
      "content": "the topic or theme of StackExchange posts based on the given paragraphs                                                                          |\n| TwentyNewsgroupsClustering    | Identify the topic or theme of the given news articles                                                                                                    |\n| SprintDuplicateQuestions      | Retrieve duplicate questions from Sprint forum                                                                                                            |\n| TwitterSemEval2015            | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| TwitterURLCorpus              | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| AskUbuntuDupQuestions         | Retrieve duplicate questions from AskUbuntu forum",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 108,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-109",
      "content": "the given tweet                                                                                          |\n| AskUbuntuDupQuestions         | Retrieve duplicate questions from AskUbuntu forum                                                                                                         |\n| MindSmallReranking            | Retrieve relevant news articles based on user browsing history                                                                                            |\n| SciDocsRR                     | Given a title of a scientific paper, retrieve the titles of other relevant papers                                                                         |\n| StackOverflowDupQuestions     | Retrieve duplicate questions from StackOverflow forum                                                                                                     |\n| ArguAna                       | Given a claim, find documents that refute the claim",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 109,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-110",
      "content": "|\n| ArguAna                       | Given a claim, find documents that refute the claim                                                                                                       |\n| ClimateFEVER                  | Given a claim about climate change, retrieve documents that support or refute the claim                                                                   |\n| CQADupstackRetrieval          | Given a question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question                                    |\n| DBPedia                       | Given a query, retrieve relevant entity descriptions from DBPedia                                                                                         |\n| FEVER                         | Given a claim, retrieve documents that support or refute the claim",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 110,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-111",
      "content": "|\n| FEVER                         | Given a claim, retrieve documents that support or refute the claim                                                                                        |\n| FiQA2018                      | Given a financial question, retrieve user replies that best answer the question                                                                           |\n| HotpotQA                      | Given a multi-hop question, retrieve documents that can help answer the question                                                                          |\n| MSMARCO                       | Given a web search query, retrieve relevant passages that answer the query                                                                                |\n| NFCorpus                      | Given a question, retrieve relevant documents that best answer the question                                                                               |\n| NQ",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 111,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-112",
      "content": "| Given a question, retrieve relevant documents that best answer the question                                                                               |\n| NQ                            | Given a question, retrieve Wikipedia passages that answer the question                                                                                    |\n| QuoraRetrieval                | Given a question, retrieve questions that are semantically equivalent to the given question                                                               |\n| SCIDOCS                       | Given a scientific paper title, retrieve paper abstracts that are cited by the given paper                                                                |\n| SciFact                       | Given a scientific claim, retrieve documents that support or refute the claim                                                                             |\n| Touche2020 TRECCOVID          | Given a question, retrieve de",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 112,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-113",
      "content": "claim, retrieve documents that support or refute the claim                                                                             |\n| Touche2020 TRECCOVID          | Given a question, retrieve detailed and persuasive arguments that answer the question Given a query on COVID-19, retrieve documents that answer the query |\n|                               | Retrieve semantically similar text.                                                                                                                       |\n| STS* BUCC/Tatoeba             | Retrieve parallel sentences.                                                                                                                              |\n| SummEval                      | Given a news summary, retrieve other semantically similar summaries                                                                                       |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "D Instructions for Training and Evaluation",
        "chunkIndex": 113,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-114",
      "content": "Task definition: The query is a name of a computer software or app, find documents that offer tutorials, guides or user experiences about the software or app.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Task group: short-long matching",
        "chunkIndex": 114,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-115",
      "content": "\"user\\_query\": \"How to use Microsoft Power BI for data analysis\",\n\n\"positive\\_document\": \"Microsoft Power BI is a sophisticated tool that requires time and practice to master. In this tutorial, we'll show you how to navigate Power BI and perform basic data analysis. First, launch the application and log in with a Microsoft account. Next, create a new project using the Wizard on the left-hand toolbar. Import the dataset you wish to work with by clicking on 'Get Data'. Power BI can handle a variety of file formats, including Excel and CSV. Load your data, then use a sequence of drag-and-drop operations to generate charts and (if applicable) maps. If you are confused at any point, consider using the 'Ask a Question' feature for clarification.\",\n\n\"hard\\_negative\\_document\": \"Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our tutorial series focuses on how you can achieve maximum efficiency with Excel.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data: {",
        "chunkIndex": 115,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-116",
      "content": ",\n\n\"hard\\_negative\\_document\": \"Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our tutorial series focuses on how you can achieve maximum efficiency with Excel. In this guide, we'll walk you through transferring data from your computer to an Excel spreadsheet as well as colour-coding cells to match various criteria. We'll also explore VLOOKUP, conditional formatting, and pivot tables, essentially transforming you into a spreadsheet wizard. Once you've mastered these techniques, consider joining us for our Advanced Data Analysis training to branch out even further.\"\n\n}\n\nTask group: long-short matching\n\nTask definition: Identifying severity level of customer complaints in support tickets",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data: {",
        "chunkIndex": 116,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-117",
      "content": "\"input\\_text\": \"I am writing to express my intense dissatisfaction with one of your products, a TV that has stopped functioning only a month after purchase. This situation yields less satisfaction to me and speaks voluminously about your quality control procedures in assembly lines. I hope this troubling issue etches into your improvement list for invoking earnest attention.\", \"label\": \"High Severity\",\n\n\"misleading\\_label\": \"Low Severity\"\n\n}",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data: {",
        "chunkIndex": 117,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-118",
      "content": "Task definition: Provided a movie quote, find the movie title in which it is said.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Task group: short-short matching",
        "chunkIndex": 118,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-119",
      "content": "{\n\n\"input\": \"I'm going to make him an offer he can't refuse.\",\n\n\"positive\\_document\": \"The Godfather\"\n\n}",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 119,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-120",
      "content": "long-long matching\n\nTask definition:\n\nProvided a legal brief, retrieve other legal documents presenting similar legal arguments.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Task group:",
        "chunkIndex": 120,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-121",
      "content": "{\n\n\"input\": \"In the united states district court for the eastern district of pennsylvania. Plaintiff John Doe brings this action against Defendants ABC Corporation and XYZ Inc., alleging negligence which resulted in his injuries. The Plaintiff alleges ABC Corporation negligently designed, manufactured, and marketed a specific product. The Plaintiff also asserts XYZ Inc. negligently failed to inspect the same product, leading to harmful exposure. Plaintiff respectfully requests this honorable court to hold the defendants liable and award damages. All parties are diverse for purposes of 28 U.S.C. § 1332.\", \"positive\\_document\": \"Before the Superior Court of California, County of Los Angeles. In the matter of the lawsuit initiated by the Plaintiff Jane Smith against the Defendants PQR Industries and LMN Enterprises, charging they negligently designed, produced, and advertised a product that directly caused harm to her.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 121,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-122",
      "content": "nitiated by the Plaintiff Jane Smith against the Defendants PQR Industries and LMN Enterprises, charging they negligently designed, produced, and advertised a product that directly caused harm to her. Furthermore, the Plaintiff has claimed that LMN Enterprises negligently fell short in inspecting the product, leading to unsafe usage. The plaintiff hereby requests this esteemed court to declare the defendants guilty and demands compensation for the damages suffered. According to 28 U.S.C.\n\n§ 1332, complete diversity exists between the parties to this lawsuit.\"\n\n}\n\nTask group:\n\nbitext matching\n\nTask definition:\n\nRetrieve parallel sentences.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 122,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-123",
      "content": "{\n\n- \"S1\": \"Men bugun uzumli tort yasashni rejalashtirdim.\",\n\n\"S2\": \"I have planned to make a grape cake today.\",\n\n\"S3\": \"I am going to bake a strawberry pie today.\"\n\n}\n\nTask group: monolingual STS\n\nTask definition: Retrieve semantically similar text.\n\n```\nGenerated data: { \"S1\": \"Tom loves to ride his blue bicycle in the park every morning.\", \"S2\": \"Every morning, Tom enjoys riding his blue bike in the park.\", \"S3\": \"Tom takes his blue pen to school every day.\" }\n```\n\nTable 16: Random samples for each subgroup of the synthetic data.\n\nTable 17: Results for each dataset in the MTEB benchmark. The evaluation metrics and detailed baseline results are available in the original paper (Muennighoff et al., 2023).\n\n| Dataset                          | w/ synthetic only   | w/ synthetic + msmarco   | w/o synthetic data   | full data   |\n|----------------------------------|---------------------|--------------------------|----------------------|-------------|\n| BIOSSES                          |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 123,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-124",
      "content": "| w/o synthetic data   | full data   |\n|----------------------------------|---------------------|--------------------------|----------------------|-------------|\n| BIOSSES                          | 84.2                | 81.0                     | 85.4                 | 85.5        |\n| SICK-R                           | 78.6                | 78.5                     | 81.7                 | 82.6        |\n| STS12                            | 75.8                | 74.7                     | 77.9                 | 79.7        |\n| STS13                            | 84.3                | 85.3                     | 88.0                 | 88.4        |\n| STS14                            | 80.9                | 81.2                     | 83.7                 | 84.5        |\n| STS15                            | 86.2                | 86.8                     | 89.5                 | 90.4        |\n| STS16                            | 85.0                | 85.3                     | 86.5",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 124,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-125",
      "content": "| 86.2                | 86.8                     | 89.5                 | 90.4        |\n| STS16                            | 85.0                | 85.3                     | 86.5                 | 87.7        |\n| STS17                            | 87.3                | 87.7                     | 91.0                 | 91.8        |\n| STS22                            | 66.0                | 67.1                     | 66.2                 | 67.0        |\n| STSBenchmark                     | 83.5                | 84.0                     | 87.8                 | 88.6        |\n| SummEval                         | 31.9                | 32.7                     | 31.9                 | 31.4        |\n| SprintDuplicateQuestions         | 93.5                | 95.8                     | 96.0                 | 95.7        |\n| TwitterSemEval2015               | 78.0                | 78.5                     | 81.7                 | 81.6        |\n| TwitterURLCorpus                 | 8",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 125,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-126",
      "content": "| 96.0                 | 95.7        |\n| TwitterSemEval2015               | 78.0                | 78.5                     | 81.7                 | 81.6        |\n| TwitterURLCorpus                 | 86.5                | 86.9                     | 87.7                 | 87.8        |\n| AmazonCounterfactualClass.       | 79.6                | 79.9                     | 77.2                 | 78.7        |\n| AmazonPolarityClassification     | 95.8                | 95.9                     | 93.9                 | 95.9        |\n| AmazonReviewsClassification      | 56.9                | 55.5                     | 48.2                 | 55.8        |\n| Banking77Classification          | 86.2                | 87.0                     | 88.8                 | 88.2        |\n| EmotionClassification            | 49.2                | 47.6                     | 51.0                 | 49.8        |\n| ImdbClassification               | 94.8                | 94.9                     | 89.0",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 126,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-127",
      "content": "| 49.2                | 47.6                     | 51.0                 | 49.8        |\n| ImdbClassification               | 94.8                | 94.9                     | 89.0                 | 94.8        |\n| MassiveIntentClassification      | 79.8                | 79.9                     | 79.6                 | 80.6        |\n| MassiveScenarioClassification    | 81.7                | 82.4                     | 82.3                 | 82.4        |\n| MTOPDomainClassification         | 95.6                | 95.9                     | 95.7                 | 96.1        |\n| MTOPIntentClassification         | 84.9                | 85.9                     | 83.4                 | 86.1        |\n| ToxicConversationsClassification | 70.2                | 70.8                     | 70.9                 | 69.6        |\n| TweetSentimentExtractionClass.",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 127,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-128",
      "content": "| 83.4                 | 86.1        |\n| ToxicConversationsClassification | 70.2                | 70.8                     | 70.9                 | 69.6        |\n| TweetSentimentExtractionClass.   | 63.5                | 63.4                     | 61.6                 | 63.7        |\n| AskUbuntuDupQuestions            | 64.3                | 65.3                     | 67.4                 | 67.0        |\n| MindSmallReranking               | 33.1                | 32.8                     | 32.5                 | 32.6        |\n| SciDocsRR                        | 86.0                | 86.0                     | 85.7                 | 86.3        |\n| StackOverflowDupQuestions        | 52.5                | 53.7                     | 55.9                 | 54.9        |\n| ArxivClusteringP2P               | 51.4                | 51.2                     | 47.8                 | 50.5        |\n| ArxivClusteringS2S               | 46.5                | 44.9                     | 44.6",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 128,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-129",
      "content": "P2P               | 51.4                | 51.2                     | 47.8                 | 50.5        |\n| ArxivClusteringS2S               | 46.5                | 44.9                     | 44.6                 | 45.5        |\n| BiorxivClusteringP2P             | 44.5                |                          | 36.9                 | 43.5        |\n| BiorxivClusteringS2S             | 40.9                | 43.3 40.1                | 37.0                 | 40.2        |\n| MedrxivClusteringS2S             | 38.0                | 37.9                     | 32.8                 | 37.5        |\n| RedditClustering                 | 56.3                | 55.9                     | 63.1                 | 57.7        |\n| RedditClusteringP2P              | 66.3                | 64.8                     | 66.4                 | 66.5        |\n| StackExchangeClustering          | 72.9                | 72.7                     | 74.5                 | 73.1        |\n| StackExchangeClusteringP2P",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 129,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-130",
      "content": "| 66.4                 | 66.5        |\n| StackExchangeClustering          | 72.9                | 72.7                     | 74.5                 | 73.1        |\n| StackExchangeClusteringP2P       | 46.1                | 45.6                     | 34.3                 | 45.9        |\n| TwentyNewsgroupsClustering       | 52.2                | 52.5                     | 55.6                 | 54.3        |\n| ArguAna                          | 52.2                | 42.7                     | 62.5                 | 61.9        |\n| ClimateFEVER                     | 21.1                | 28.8                     | 25.2                 | 38.4        |\n| CQADupstackAndroidRetrieval      | 40.8                | 36.0                     | 44.5                 | 43.0        |\n| DBPedia                          | 42.0                | 43.7                     | 47.7                 | 48.9        |\n| FEVER                            | 72.5                | 83.5                     | 73.1",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 130,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-131",
      "content": "| 42.0                | 43.7                     | 47.7                 | 48.9        |\n| FEVER                            | 72.5                | 83.5                     | 73.1                 | 87.8        |\n| FiQA2018                         | 38.1                | 48.4                     | 54.5                 | 56.6        |\n| HotpotQA                         | 48.1                | 64.0                     | 75.6                 | 75.7        |\n| MSMARCO                          | 25.7                | 45.0                     |                      | 43.1        |\n|                                  |                     |                          | 42.9                 |             |\n| NFCorpus NQ                      | 35.5                | 40.0                     | 35.3                 | 38.6 63.5   |\n|                                  | 53.3                | 63.5                     | 57.3                 |             |\n| QuoraRetrieval",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 131,
        "totalChunks": 133
      }
    },
    {
      "id": "2401.00368v3-chunk-132",
      "content": "| 35.3                 | 38.6 63.5   |\n|                                  | 53.3                | 63.5                     | 57.3                 |             |\n| QuoraRetrieval                   | 75.0                | 79.5                     | 89.5                 | 89.6        |\n| SCIDOCS SciFact                  | 20.6 71.5           | 15.8 71.9                | 19.0 74.7            | 16.3 76.4   |\n| Touche2020                       | 25.4                | 32.5                     | 19.1                 | 26.4        |\n| TRECCOVID                        | 82.3                | 87.3                     | 70.8                 | 87.2        |\n| Average                          | 63.1                | 64.5                     | 64.6                 | 66.6        |",
      "metadata": {
        "source": "arxiv:2401.00368v3",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
          "Liang Wang",
          "Nan Yang",
          "Xiaolong Huang",
          "Linjun Yang",
          "Rangan Majumder",
          "Furu Wei"
        ],
        "section": "Generated data:",
        "chunkIndex": 132,
        "totalChunks": 133
      }
    }
  ],
  "fullText": "## Improving Text Embeddings with Large Language Models\n\n## Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang , Rangan Majumder , Furu Wei\n\nMicrosoft Corporation\n\n{wangliang,nanya,xiaolhu,yang.linjun,ranganm,fuwei}@microsoft.com\n\n## Abstract\n\nIn this paper, we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1 k training steps. Unlike existing methods that often depend on multi-stage intermediate pretraining with billions of weakly-supervised text pairs, followed by fine-tuning with a few labeled datasets, our method does not require building complex training pipelines or relying on manually collected datasets that are often constrained by task diversity and language coverage. We leverage proprietary LLMs to generate diverse synthetic data for hundreds of thousands of text embedding tasks across 93 languages. We then fine-tune open-source decoder-only LLMs on the synthetic data using standard contrastive loss. Experiments demonstrate that our method achieves strong performance on highly competitive text embedding benchmarks without using any labeled data. Furthermore, when fine-tuned with a mixture of synthetic and labeled data, our model sets new state-of-the-art results on the BEIR and MTEB benchmarks.\n\n## 1 Introduction\n\nText embeddings are vector representations of natural language that encode its semantic information. They are widely used in various natural language processing (NLP) tasks, such as information retrieval (IR), question answering, semantic textual similarity, bitext mining, item recommendation, etc. In the field of IR, the first-stage retrieval often relies on text embeddings to efficiently recall a small set of candidate documents from a large-scale corpus using approximate nearest neighbor search techniques. Embedding-based retrieval is also a crucial component of retrieval-augmented generation (RAG) (Lewis et al., 2020), which is an emerging paradigm that enables large language models (LLMs) to access dynamic external knowledge without modifying the model parameters. Source attribution of generated text is another important application of text embeddings (Gao et al., 2023) that can improve the interpretability and trustworthiness of LLMs.\n\nPrevious studies have demonstrated that weighted average of pre-trained word embeddings (Pennington et al., 2014; Arora et al., 2017) is a strong baseline for measuring semantic similarity. However, these methods fail to capture the rich contextual information of natural language. With the advent of pre-trained language models (Devlin et al., 2019), Sentence-BERT (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021) have been proposed to learn text embeddings by fine-tuning BERT on natural language inference (NLI) datasets. To further enhance the performance and robustness of text embeddings, state-of-the-art methods like E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023) employ a more complex multi-stage training paradigm that first pre-trains on billions of weakly-supervised text pairs, and then fine-tunes on several high-quality labeled datasets.\n\nExisting multi-stage approaches suffer from several drawbacks. Firstly, they entail a complex multistage training pipeline that demands substantial engineering efforts to curate large amounts of relevance pairs. Secondly, they rely on manually collected datasets that are often constrained by the diversity of tasks and the coverage of languages. For instance, Instructor (Su et al., 2023) is only trained on instructions from 330 English datasets, whereas BGE (Xiao et al., 2023) only focuses on high-resource languages such as English and Chinese. Moreover, most existing methods employ BERT-style encoders as the backbone, neglecting the recent advances of training better LLMs and related techniques such as context length extension (Rozière et al., 2023).\n\nIn this paper, we propose a novel method for text embeddings that leverages LLMs to overcome the limitations of existing approaches. We use proprietary LLMs to generate synthetic data for a diverse range of text embedding tasks in 93 languages, covering hundreds of thousands of embedding tasks. Specifically, we use a two-step prompting strategy that first prompts the LLMs to brainstorm a pool of candidate tasks, and then prompts the LLMs to generate data conditioned on a given task from the pool. To cover various application scenarios, we design multiple prompt templates for each task type and combine the generated data from different templates to boost diversity. For the text embedding models, we opt for fine-tuning powerful open-source LLMs rather than small BERT-style models. Since LLMs such as Mistral (Jiang et al., 2023) have been extensively pre-trained on webscale data, contrastive pre-training that proves to be important for BERT models (Wang et al., 2022b) offers little additional benefit.\n\nWe demonstrate that Mistral-7B, when finetuned solely on synthetic data, attains competitive performance on the BEIR (Thakur et al., 2021) and MTEB (Muennighoff et al., 2023) benchmarks. This is particularly intriguing considering that this setting does not involve any labeled data. When fine-tuned on a mixture of synthetic and labeled data, our model achieves new state-of-the-art results, surpassing previous methods by a significant margin (+ 2% ). The entire training process requires less than 1 k steps.\n\nMoreover, we empirically validate that our model can effectively perform personalized passkey retrieval for inputs up to 32 k tokens by altering the rotation base of the position embeddings, extending the context length beyond the conventional 512 token limit. Regarding its multilinguality, our model excels on high-resource languages. However, for low-resource languages, there is still room for improvement as current opensource LLMs are not adequately pre-trained on them.\n\n## 2 Related Work\n\nText Embeddings are continuous lowdimensional representations of text and have been extensively applied to various downstream tasks such as information retrieval, question answering, and retrieval-augmented generation (RAG). Early work on text embeddings includes latent semantic indexing (Deerwester et al., 1990) and weighted average of word embeddings (Mikolov et al.,\n\n2013). More recent methods exploit supervision from natural language inference (Bowman et al., 2015) and labeled query-document pairs, such as the MS-MARCO passage ranking dataset (Campos et al., 2016), to train text embeddings (Reimers and Gurevych, 2019; Conneau et al., 2017; Gao et al., 2021). However, labeled data are often limited in terms of task diversity and language coverage. To address this challenge, methods like Contriever (Izacard et al., 2021), OpenAI Embeddings (Neelakantan et al., 2022), E5 (Wang et al., 2022b), and BGE (Xiao et al., 2023) adopt a multi-stage training paradigm. They first pre-train on large-scale weakly-supervised text pairs using contrastive loss and then fine-tune on small-scale but high-quality datasets. In this paper, we demonstrate that it is possible to obtain state-of-the-art text embeddings with single-stage training.\n\nSynthetic Data Synthetic data generation is a widely studied topic in information retrieval research, with various methods proposed to enhance retrieval systems with artificially created data. For instance, Doc2query (Nogueira et al., 2019), InPars (Bonifacio et al., 2022), and Promptagator (Dai et al., 2022) generate synthetic queries for unlabeled documents, which are then leveraged for document expansion or model training. GPL (Wang et al., 2022a) employs a cross-encoder to produce pseudo-labels for query-document pairs. Similarly, Query2doc (Wang et al., 2023) generates pseudo-documents for query expansion by fewshot prompting LLMs. Unlike these methods, our approach does not rely on any unlabeled documents or queries and thus can generate more diverse synthetic data.\n\nAnother related line of work focuses on knowledge distillation from black-box LLMs by training on synthetic data generated from them. DINO (Schick and Schütze, 2021) generates synthetic text pairs for semantic textual similarity. Unnatural Instructions (Honovich et al., 2022) is a synthetic instruction following dataset by prompting existing LLMs. Orca (Mukherjee et al., 2023) and Phi (Gunasekar et al., 2023) propose to train better small language models by using high-quality synthetic data from GPT-3.5/4 (OpenAI, 2023).\n\nLarge Language Models With the popularization of ChatGPT, large language models (LLMs) have demonstrated remarkable capabilities in in-\n\nBrainstorm a list of potentially useful text retrieval tasks. Here are a few examples for your reference:\n\n- Search for documents that answers a FAQ-style query on children's nutrition.\n- Provided a scientific claim as query, retrieve documents that help verify or refute the claim.\n\nPlease adhere to the following guidelines:\n\n- Each retrieval task should cover a wide range of queries, and should not be too specific.\n- Specify what the query is, and what the desired documents are.\n\nYour output should always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\n\n[\"Retrieve company's financial reports for a given stock ticker symbol.\", \"Given a book name as a query, retrieve reviews, ratings and summaries of that book.\", \"Search for scientific research papers supporting a medical diagnosis for a specified disease.' … (omitted for space) ]\n\nnew session\n\nYou have been assigned a retrieval task: {task} Your mission is to write one text retrieval example for this task in JSON format. The JSON object must\n\n- -\"user\\_query\" : a string, a random user search query specified by the retrieval task.\n\ncontain the following keys:\n\n- -\"positive\\_document\" : a string, a relevant document for the user query.\n\nPlease adhere to the following guidelines:\n\n- -\"hard\\_negative\\_document\" : a string, a hard negative document that only  appears relevant to the query.\n- The \"user\\_query\" should be {query\\_type} , {query\\_length} , {clarity} , and diverse in topic.\n- Both the query and documents should be in {language} .\n- All documents should be at least {num\\_words} words long.\n- … (omitted some for space)\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\n{ \"user\\_query\": \"How to use Microsoft Power BI for data analysis\", \"positive\\_document\": \"Microsoft Power BI is a sophisticated tool that requires time and practice to master. In this tutorial, we'll show you how to navigate Power BI … (omitted) \" , ' hard\\_negative\\_document ': 'Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our tutorial series focuses on how you …(omitted) ' }\n\nFigure 1: An example two-step prompt template for generating synthetic data with GPT-4. We first prompt GPT-4 to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard negative) triplets for each task. ' {...} ' denotes a placeholder that will be replaced by sampling from a predefined set of values. Full prompts are available in Appendix C.\n\nstruction following and few-shot in-context learning (Brown et al., 2020). However, the most advanced LLMs such as GPT-4 (OpenAI, 2023) are proprietary and have little technical details disclosed. To bridge the gap between proprietary and open-source LLMs, several notable efforts have been made, such as LLaMA-2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023) models. A major limitation of LLMs is that they lack awareness of recent events and private knowledge. This issue can be partly mitigated by augmenting LLMs with information retrieved from external sources, a technique known as retrieval-augmented generation (RAG). On the other hand, LLMs can also serve as foundation models to enhance text embeddings. RepLLaMA (Ma et al., 2023) proposes to fine-tune LLaMA-2 with bi-encoder architecture for ad-hoc retrieval. SGPT (Muennighoff, 2022), GTR (Ni et al., 2022b), and Udever (Zhang et al., 2023a) demonstrate the scaling law of text embeddings empirically, but their performance still falls behind small bidirectional encoders such as E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023). In this paper, we present a novel approach to train state-of-the-art text embeddings by exploiting the latest advances of LLMs and synthetic data.\n\n## 3 Method\n\n## 3.1 Synthetic Data Generation\n\nUtilizing synthetic data generated by advanced LLMs such as GPT-4 presents a compelling opportunity, especially in terms of enhancing diversity across a multitude of tasks and languages. Such diversity is essential for developing robust text embeddings that can perform well across different tasks, be it semantic retrieval, textual similarity, or clustering.\n\nTo generate diverse synthetic data, we propose a simple taxonomy that categorizes embedding tasks into several groups, and then apply different prompt templates to each group.\n\nAsymmetric Tasks This category comprises tasks where the query and document are semantically related but are not paraphrases of each other. Depending on the length of the query and document, we further divide asymmetric tasks into four subgroups: short-long match, long-short match, short-short match, and long-long match. For instance, short-long match tasks involve a short query and a long document, which is a typical scenario in commercial search engines. For each subgroup, we design a two-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a concrete example conditioned on the task definition. In Figure 1, we show an example prompt for the short-long match subgroup. The full output is available in Table 16. The outputs from GPT-4 are mostly coherent and of high quality. In our preliminary experiments, we also attempted to generate the task definition and query-document pairs using a single prompt, but the data diversity was not as satisfactory as the proposed two-step approach.\n\nSymmetric Tasks Symmetric tasks involve queries and documents that have similar semantic meanings but different surface forms. We examine two application scenarios: monolingual semantic textual similarity (STS) and bitext retrieval. We design two distinct prompt templates for each scenario, tailored to their specific objectives. Since the task definition is straightforward, we omit the brainstorming step for symmetric tasks.\n\nTo further boost the diversity of the prompts and thus the synthetic data, we incorporate several placeholders in each prompt template, whose values are randomly sampled at runtime. For example, in Figure 1, the value of ' {query\\_length} ' is sampled from the set ' {less than 5 words, 5-10 words, at least 10 words} '.\n\nTo generate multilingual data, we sample the value of ' {language} ' from the language list of\n\nXLM-R (Conneau et al., 2020), giving more weight to high-resource languages. Any generated data that does not conform to the predefined JSON format are discarded during the parsing process. We also remove duplicates based on exact string matching.\n\n## 3.2 Training\n\nGiven a relevant query-document pair ( q + , d + ), we first apply the following instruction template to the original query q + to generate a new one q + inst :\n\n<!-- formula-not-decoded -->\n\nwhere ' {task\\_definition} ' is a placeholder for a one-sentence description of the embedding task. For generated synthetic data, we use the outputs from the brainstorming step. For other datasets, such as MS-MARCO, we manually craft the task definitions and apply them to all the queries in the dataset. We do not modify the document side with any instruction prefix. In this way, the document index can be prebuilt, and we can customize the task to perform by changing only the query side.\n\nGiven a pretrained LLM, we append an [EOS] token to the end of the query and document, and then feed them into the LLM to obtain the query and document embeddings ( h q + inst , h d + ) by taking the last layer [EOS] vector. To train the embedding model, we adopt the standard InfoNCE loss L over the in-batch negatives and hard negatives:\n\n<!-- formula-not-decoded -->\n\nwhere N denotes the set of all negatives, and ϕ ( q, d ) is a function that computes the matching score between query q and document d . In this paper, we adopt the temperature-scaled cosine similarity function as follows:\n\n<!-- formula-not-decoded -->\n\nτ is a temperature hyper-parameter, which is fixed to 0 . 02 in our experiments.\n\n## 4 Experiments\n\n## 4.1 Statistics of the Synthetic Data\n\nFigure 2 presents the statistics of our generated synthetic data. We manage to generate 500 k examples with 150 k unique instructions using Azure\n\nFigure 2: Task type and language statistics of the generated synthetic data (see Section 3.1 for task type definitions). The 'Others' category contains the remaining languages from the XLM-R language list.\n\n<!-- image -->\n\nTable 1: Results on the MTEB benchmark (Muennighoff et al., 2023) (56 datasets in the English subset). The numbers are averaged for each category. Please refer to Table 17 for the scores per dataset.\n\n| # of datasets →                                  | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|--------------------------------------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised Models                              |             |             |                |            |            |          |           |          |\n| Glove (Pennington et al., 2014)                  | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| SimCSE bert-unsup (Gao et al., 2021)             | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| Supervised Models                                |             |             |                |            |            |          |           |          |\n| SimCSE bert-sup (Gao et al., 2021)               | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| Contriever (Izacard et al., 2021)                | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR xxl (Ni et al., 2022b)                       | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl (Ni et al., 2022a)               | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |\n| E5 large-v2 (Wang et al., 2022b)                 | 75.2        | 44.5        | 86.0           | 56.6       | 50.6       | 82.1     | 30.2      | 62.3     |\n| GTE large (Li et al., 2023)                      | 73.3        | 46.8        | 85.0           | 59.1       | 52.2       | 83.4     | 31.7      | 63.1     |\n| BGE large-en-v1.5 (Xiao et al., 2023)            | 76.0        | 46.1        | 87.1           | 60.0       | 54.3       | 83.1     | 31.6      | 64.2     |\n| Ours                                             |             |             |                |            |            |          |           |          |\n| E5 mistral-7b + full data w/ synthetic data only | 78.5        | 50.3        | 88.3           | 60.2       | 56.9       | 84.6     | 31.4      | 66.6     |\n|                                                  | 78.2        | 50.5        | 86.0           | 59.0       | 46.9       | 81.2     | 31.9      | 63.1     |\n| w/ synthetic + msmarco                           | 78.3        | 49.9        | 87.1           | 59.5       | 52.2       | 81.2     | 32.7      | 64.5     |\n\nOpenAI Service 1 , among which 25% are generated by GPT-35-Turbo and others are generated by GPT-4 . The total token consumption is about 180 M. The predominant language is English, with coverage extending to a total of 93 languages. For the bottom 75 low-resource languages, there are about 1 k examples per language on average. Please see Table 16 in the appendix for examples of synthetic data.\n\nIn terms of data quality, we find that a portion of GPT-35-Turbo outputs do not strictly follow the guidelines specified in the prompt templates. Nevertheless, the overall quality remains acceptable, and preliminary experiments have demonstrated the benefits of incorporating this data subset.\n\n## 4.2 Model Fine-tuning and Evaluation\n\nThe pretrained Mistral-7b (Jiang et al., 2023) checkpoint is fine-tuned for 1 epoch using the loss in Equation 2. We follow the training recipe from RankLLaMA (Ma et al., 2023) and utilize LoRA (Hu et al., 2022) with rank 16 . To further reduce GPU memory requirement, techniques including gradient checkpointing, mixed precision training, and DeepSpeed ZeRO-3 are applied.\n\nFor the training data, we utilize both the generated synthetic data and a collection of 13 public datasets, yielding approximately 1 . 8 M examples after sampling. More details are available in Appendix A. To provide a fair comparison with some previous work, we also report results when the only labeled supervision is the MS-MARCO passage\n\nTable 2: nDCG@10 on the dev set of the MIRACL dataset for both high-resource and low-resource languages. We select the 4 high-resource languages and the 4 low-resource languages according to the number of candidate documents. The numbers for BM25 and mDPR come from Zhang et al. (2023b). For the complete results on all 16 languages, please see Table 6.\n\n|                               | High-resource Languages   | High-resource Languages   | High-resource Languages   | High-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   |\n|-------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                               | en                        | fr                        | es                        | ru                        | te                       | hi                       | bn                       | sw                       |\n| BM25 (Zhang et al., 2023b)    | 35.1                      | 18.3                      | 31.9                      | 33.4                      | 49.4                     | 45.8                     | 50.8                     | 38.3                     |\n| mDPR (Zhang et al., 2023b)    | 39.4                      | 43.5                      | 47.8                      | 40.7                      | 35.6                     | 38.3                     | 44.3                     | 29.9                     |\n| mE5 base (Wang et al., 2024)  | 51.2                      | 49.7                      | 51.5                      | 61.5                      | 75.2                     | 58.4                     | 70.2                     | 71.1                     |\n| mE5 large (Wang et al., 2024) | 52.9                      | 54.5                      | 52.9                      | 67.4                      | 84.6                     | 62.0                     | 75.9                     | 74.9                     |\n| E5 mistral-7b + full data     | 57.3                      | 55.2                      | 52.2                      | 67.7                      | 73.9                     | 52.1                     | 70.3                     | 68.4                     |\n\nFigure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 7.\n\n<!-- image -->\n\n<!-- image -->\n\nranking (Campos et al., 2016) dataset.\n\nWe evaluate the trained model on the MTEB benchmark (Muennighoff et al., 2023). Note that the retrieval category in MTEB corresponds to the 15 publicly available datasets in the BEIR benchmark (Thakur et al., 2021). Evaluation of one model takes about 3 days on 8 V100 GPUs due to the need to encode a large number of documents. Although our model can accommodate sequence length beyond 512 , we only evaluate on the first 512 tokens for efficiency. Official metrics are reported for each category. For more details about the evaluation protocol, please refer to the original papers (Muennighoff et al., 2023; Thakur et al., 2021).\n\n## 4.3 Main Results\n\nIn Table 1, our model 'E5mistral-7b + full data' attains the highest average score on the MTEB benchmark, outperforming the previous state-of-the-art model by 2 . 4 points. In the 'w/ synthetic data only' setting, no labeled data is used for training, and yet the performance remains quite competitive. We posit that generative language modeling and text embeddings are the two sides of the same coin, with both tasks requiring the model to have a deep understanding of the natural language. Given an embedding task definition, a truly robust LLM should be able to generate training data on its own and then be transformed into an embedding model through light-weight fine-tuning. Our experiments shed light on the potential of this direction, and more research is needed to fully explore it.\n\n| Model                         |   BEIR |   MTEB |\n|-------------------------------|--------|--------|\n| OpenAI text-embedding-3-large |   55.4 |   64.6 |\n| Cohere-embed-english-v3.0     |   55   |   64.5 |\n| voyage-lite-01-instruct       |   55.6 |   64.5 |\n| UAE-Large-V1                  |   54.7 |   64.6 |\n| E5 mistral-7b + full data     |   56.9 |   66.6 |\n\nTable 3: Comparison with commercial models and the model that tops the MTEB leaderboard (as of 202312-22) (Li and Li, 2023). 'BEIR' is the average nDCG@10 score over 15 public datasets in the BEIR benchmark (Thakur et al., 2021). 'MTEB' is the average score over 56 datasets in the English subset of the MTEB benchmark (Muennighoff et al., 2023). For the commercial models listed here, little details are available on their model architectures and training data.\n\nIn Table 3, we also present a comparison with several commercial text embedding models. However, due to the lack of transparency and documentation about these models, a fair comparison is not feasible. We focus especially on the retrieval per-\n\nDoc1: &lt;prefix filler&gt; Malayah Graves's pass key is 123. Remember it. 123 is the pass key for Malayah Graves. &lt;suffix filler&gt; Doc2: &lt;prefix filler&gt; Cesar McLean's pass key is 456. Remember it. 456 is the pass key for Cesar McLean. &lt;suffix filler&gt; ……\n\nFigure 4: Illustration of the personalized passkey retrieval task adapted from Mohtashami and Jaggi (2023). The ' &lt;prefix filler&gt; ' and ' &lt;suffix filler&gt; ' are repeats of ' The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. ' In addition, each document has a unique person name and a random passkey inserted at a random position. The task is to retrieve the document that contains the given person's passkey from 100 candidates.\n\n<!-- image -->\n\nFigure 5: Accuracy of personalized passkey retrieval as a function of input context length. For each context length, we randomly generate 50 queries and compute the top-1 accuracy.\n\nformance on the BEIR benchmark, since retrievalaugmented generation is an emerging technique to enhance LLM with external knowledge and proprietary data. As Table 3 shows, our model outperforms the current commercial models by a significant margin.\n\n## 4.4 Multilingual Retrieval\n\nTo assess the multilingual capabilities of our model, we conduct an evaluation on the MIRACL dataset (Zhang et al., 2023b), which comprises human-annotated queries and relevance judgments across 18 languages. The validation set contains labels for 16 languages. As shown in Table 2, our model surpasses mE5large on high-resource languages, notably on English. Nevertheless, for lowresource languages, our model remains suboptimal compared to mE5base. We attribute this to the fact that Mistral-7B is predominantly pre-trained on English data, and we anticipate that future multilingual LLMs will leverage our method to bridge this gap.\n\nTo evaluate our model's cross-lingual retrieval capability, we report Bitext mining results in Table 4. For baselines including mContriever (Izacard et al., 2021), LaBSE (Feng et al., 2022), and mE5 (Wang et al., 2024), we evaluate the results using publicly available checkpoints. Our observa-\n\nTable 4: Bitext mining results. BUCC 2018 (Zweigenbaum et al., 2018) contains 4 high-resource languages. Tatoeba (Artetxe and Schwenk, 2019) consists of 112 English-centric language pairs.\n\n|               |   BUCC 2018 4 langs |   Tatoeba 112 langs |\n|---------------|---------------------|---------------------|\n| mContriever   |                93.7 |                37.7 |\n| LaBSE         |                98.8 |                81.1 |\n| mE5 base      |                98.1 |                68.1 |\n| mE5 large     |                98.6 |                75.7 |\n| E5 mistral-7b |                98.9 |                70.1 |\n\ntions indicate that, similar to the MIRACL retrieval, E5mistral-7b excels in bitext mining for high-resource languages only.\n\n## 5 Analysis\n\n## 5.1 Is Contrastive Pre-training Necessary?\n\nWeakly-supervised contrastive pre-training is one of the key factors behind the success of existing text embedding models. For instance, Contriever (Izacard et al., 2021) treats random cropped spans as positive pairs for pre-training, while E5 (Wang et al., 2022b) and BGE (Xiao et al., 2023) collect and filter text pairs from various sources.\n\nThis section re-evaluates the necessity of con-\n\nTable 5: Results on the MTEB benchmark with various hyperparameters. The first row corresponds to the default setting, which employs last-token pooling, LoRA rank 16 , and natural language instructions. Unless otherwise stated, all models are trained on the synthetic and MS-MARCO passage ranking data.\n\n| Datasets             | Class.   | Clust.   | PairClass.   | Rerank   | Retr.   | STS   | Summ.   | Avg       |\n|----------------------|----------|----------|--------------|----------|---------|-------|---------|-----------|\n| E5 mistral-7b        | 78.3     | 49.9     | 87.1         | 59.5     | 52.2    | 81.2  | 32.7    | 64.5      |\n| w/ LLaMA-2 7b init.  | 76.2     | 48.1     | 85.1         | 58.9     | 49.6    | 81.2  | 30.8    | 62.9 -1.6 |\n| w/ msmarco data only | 71.6     | 47.1     | 86.1         | 58.8     | 54.4    | 79.5  | 31.7    | 62.7 -1.8 |\n| pooling type         |          |          |              |          |         |       |         |           |\n| w/ mean pool         | 77.0     | 48.9     | 86.1         | 59.2     | 52.4    | 81.4  | 30.8    | 64.1 -0.4 |\n| w/ weighted mean     | 77.0     | 49.0     | 86.1         | 59.2     | 52.0    | 81.4  | 30.2    | 64.0 -0.5 |\n| LoRA rank            |          |          |              |          |         |       |         |           |\n| w/ r= 8              | 78.4     | 50.3     | 87.1         | 59.3     | 53.0    | 81.0  | 31.7    | 64.8 +0.3 |\n| w/ r= 32             | 78.4     | 50.3     | 87.4         | 59.5     | 52.2    | 81.2  | 30.6    | 64.6 +0.1 |\n| instruction type     |          |          |              |          |         |       |         |           |\n| w/o instruction      | 72.3     | 47.1     | 82.6         | 56.3     | 48.2    | 76.7  | 30.7    | 60.3 -4.2 |\n| w/ task type prefix  | 71.1     | 46.5     | 79.7         | 54.0     | 52.7    | 73.8  | 30.0    | 60.3 -4.2 |\n\ntrastive pre-training for LLMs, particularly those that have been pre-trained on trillions of tokens. Figure 3 shows that contrastive pre-training benefits XLM-Rlarge, enhancing its retrieval performance by 8 . 2 points when fine-tuned on the same data, which aligns with prior findings. However, for Mistral-7B based models, contrastive pre-training has negligible impact on the model quality. This implies that extensive auto-regressive pre-training enables LLMs to acquire good text representations, and only minimal fine-tuning is required to transform them into effective embedding models.\n\n## 5.2 Extending to Long Text Embeddings\n\nExisting evaluation datasets for text embedding models are typically short, to evaluate the longcontext capability of our model, we introduce a novel synthetic task called personalized passkey retrieval , which is illustrated in Figure 4. This task requires encoding the passkey information in a long context into the embeddings. We compare the performance of different variants by changing the sliding window size and the RoPE rotation base (Su et al., 2024) in Figure 5. The results show that the default configuration with 4 k sliding window attains 100% accuracy within 4 k tokens, but the accuracy deteriorates quickly as the context length grows. Naively extending the sliding window size to 32 k results in worse performance. By changing the RoPE rotation base to 10 5 , the model can achieve over 90% accuracy within 32 k tokens. However, this entails a minor trade-off in performance for shorter contexts. A potential avenue for future research is to efficiently adapt the model to longer contexts through lightweight post-training (Zhu et al., 2023).\n\n## 5.3 Analysis of Training Hyperparameters\n\nTable 5 presents the results under different configurations. We notice that the Mistral-7B initialization holds an advantage over LLaMA-2 7B, in line with the findings from Mistral-7B technical report (Jiang et al., 2023). The choice of pooling types and LoRA ranks does not affect the overall performance substantially, hence we adhere to the default setting despite the marginal superiority of LoRA rank 8 . On the other hand, the way of adding instructions has a considerable impact on the performance. We conjecture that natural language instructions better inform the model regarding the embedding task at hand, and thus enable the model to generate more discriminative embeddings. Our framework also provides a way to customize the behavior of text embeddings through instructions without the need to fine-tune the model or re-build document index.\n\n## 6 Conclusion\n\nThis paper shows that the quality of text embeddings can be substantially enhanced by exploiting LLMs. We prompt proprietary LLMs such as GPT4 to generate diverse synthetic data with instructions in many languages. Combined with the strong language understanding capability of the Mistral model, we establish new state-of-the-art results for nearly all task categories on the competitive MTEB benchmark. The training process is much more streamlined and efficient than existing multi-stage approaches, thereby obviating the need for intermediate pre-training.\n\nFor future work, we aim to further improve the multilingual performance of our model and explore the possibility of using open-source LLMs to generate synthetic data.\n\n## Limitations\n\nIn comparison to the mainstream BERT-style encoders, the employment of LLMs, such as Mistral7B, for text embeddings results in a significantly increased inference cost. The development of more advanced GPUs and better kernel implementations may enhance the efficiency of the inference process. With regards to storage cost, our model is comparatively more expensive, with embeddings of 4096 dimensions. Early successes in reducing embedding dimensions while maintaining competitive performance have been demonstrated through techniques such as Matryoshka representation learning (Kusupati et al., 2022).\n\nFor synthetic data generation, we rely on manual prompt engineering to elicit high-quality outputs from proprietary LLMs. Automatic prompt optimization presents a promising avenue for improving the quality of synthetic data.\n\n## Acknowledgements\n\nWe would like to thank anonymous reviewers for their valuable comments, and ACL 2024 and ACL Rolling Review organizers for their efforts. Opinions expressed in this paper are solely those of the authors and do not represent the views of their employers.\n\n## References\n\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net.\n\nMikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics , 7:597-610.\n\nLuiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsupervised dataset generation for information retrieval. Proceedings of the 45th International ACM SIGIR\n\nConference on Research and Development in Information Retrieval .\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms marco: A human generated machine reading comprehension dataset. ArXiv preprint , abs/1611.09268.\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 84408451, Online. Association for Computational Linguistics.\n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 670-680, Copenhagen, Denmark. Association for Computational Linguistics.\n\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fewshot dense retrieval from 8 examples. In The Eleventh International Conference on Learning Representations .\n\nDataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. 2017. Quora question pairs.\n\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the\n\n- American society for information science , 41(6):391407.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n- Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3558-3567, Florence, Italy. Association for Computational Linguistics.\n- Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 878-891.\n- Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 6894-6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n- Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. ArXiv preprint , abs/2305.14627.\n- Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allison Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero C. Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, S. Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuan-Fang Li. 2023. Textbooks are all you need. ArXiv preprint , abs/2306.11644.\n- Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. ArXiv preprint , abs/2212.09689.\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net.\n- Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. ArXiv preprint , abs/2112.09118.\n- Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. ArXiv preprint , abs/2310.06825.\n- Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online. Association for Computational Linguistics.\n- Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham M. Kakade, Prateek Jain, and Ali Farhadi. 2022. Matryoshka representation learning. In Neural Information Processing Systems .\n- Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual .\n- Xianming Li and Jing Li. 2023. Angle-optimized text embeddings. ArXiv preprint , abs/2309.12871.\n- Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. ArXiv preprint , abs/2308.03281.\n- Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. Fine-tuning llama for multi-stage text retrieval. ArXiv preprint , abs/2310.08319.\n- Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In ICLR .\n- Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. ArXiv preprint , abs/2305.16300.\n- Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. ArXiv preprint , abs/2202.08904.\n- Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 2014-2037, Dubrovnik, Croatia. Association for Computational Linguistics.\n- Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. Orca: Progressive learning\n- from complex explanation traces of gpt-4. ArXiv preprint , abs/2306.02707.\n\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. 2022. Text and code embeddings by contrastive pre-training. ArXiv preprint , abs/2201.10005.\n\n- Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022a. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022 , pages 1864-1874, Dublin, Ireland. Association for Computational Linguistics.\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022b. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 9844-9855, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\n- Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. ArXiv preprint , abs/1904.08375.\n- OpenAI. 2023. Gpt-4 technical report. ArXiv preprint , abs/2303.08774.\n- Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1532-1543, Doha, Qatar. Association for Computational Linguistics.\n- Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoQiao She, Jing Liu, Hua Wu, and Haifeng Wang. 2022. DuReader-retrieval: A large-scale Chinese benchmark for passage retrieval from web search engine. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5326-5338, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n- Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\n- Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu\n- Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P Bhatt, Cristian Cantón Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. ArXiv preprint , abs/2308.12950.\n- Timo Schick and Hinrich Schütze. 2021. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 69436951.\n- Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text embeddings. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 1102-1121, Toronto, Canada. Association for Computational Linguistics.\n- Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063.\n- Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) .\n- James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics.\n- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint , abs/2307.09288.\n- Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2345-2360, Seattle, United States. Association for Computational Linguistics.\n- Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\n\nand Furu Wei. 2022b. Text embeddings by weaklysupervised contrastive pre-training. ArXiv preprint , abs/2212.03533.\n\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: A technical report. arXiv preprint arXiv:2402.05672 .\n\nLiang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 9414-9423, Singapore. Association for Computational Linguistics.\n\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. ArXiv preprint , abs/2309.07597.\n\nXiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A large-scale chinese benchmark for passage ranking. ArXiv preprint , abs/2304.03679.\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.\n\nXin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. 2023a. Language models are universal embedders. ArXiv preprint , abs/2310.08232.\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. TyDi: A multi-lingual benchmark for dense retrieval. In Proceedings of the 1st Workshop on Multilingual Representation Learning , pages 127137, Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nXinyu Crystina Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023b. Miracl: A multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics , 11:1114-1131.\n\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. In The Twelfth International Conference on Learning Representations .\n\nPierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th Workshop on Building and Using Comparable Corpora , pages 39-42.\n\n## A Implementation Details\n\nBaseline Models For results with mE5base and mE5large, we use the public checkpoints available at https://huggingface. co/intfloat/multilingual-e5-base and https://huggingface.co/intfloat/ multilingual-e5-large respectively. For experiments in Table 5, we follow the SGPT (Muennighoff, 2022) paper for the implementation of weighted mean pooling. For the 'w/ task type prefix' setting, we prepend 'classify: ' for the long-short matching subgroup, and 'query: ' for other asymmetric tasks. No prefix is added for symmetric tasks.\n\nTraining Data For the 'E5mistral-7b + full data' setting, our training data comprises generated synthetic data, ELI5 (Fan et al., 2019)(sample ratio 0 . 1 ), HotpotQA (Yang et al., 2018), FEVER (Thorne et al., 2018), MIRACL (Zhang et al., 2023b), MSMARCO passage ranking (sample ratio 0 . 5 ) and document ranking (sample ratio 0 . 2 ) (Campos et al., 2016), NQ (Karpukhin et al., 2020), NLI (Gao et al., 2021), SQuAD (Karpukhin et al., 2020), TriviaQA (Karpukhin et al., 2020), Quora Duplicate Questions (DataCanary et al., 2017)(sample ratio 0 . 1 ), MrTyDi (Zhang et al., 2021), DuReader (Qiu et al., 2022), and T2Ranking (Xie et al., 2023)(sample ratio 0 . 5 ) datasets. We only include the training set of each dataset. For the datasets without hard negatives, we use mE5base to mine top 100 hard negatives. After sampling, we obtain approximately 1 . 8 million examples. The entire training process takes fewer than 1 k steps to complete.\n\nHyperparameters for Fine-tuning When fine-tuning Mistral-7b 2 , the batch size is set to 2048 and the learning rate is 10 -4 with 100 step warmup and linear decay. The weight decay is 0 . 1 . We add 1 hard negative for each query-document pair. The fine-tuning process takes roughly 18 hours on 32 V100 GPUs with a maximum sequence length 512 . We add LoRA adapters to all linear layers, resulting in a total of 42 M trainable parameters. Our implementation is based on the HuggingFace PEFT library at https://github.com/huggingface/peft .\n\n2 https://huggingface.co/mistralai/ Mistral-7B-v0.1\n\nTable 6: nDCG@10 and Recall@100 on the dev set of the MIRACL dataset for all 16 languages.\n\n|     | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10            | Recall@100   | Recall@100   | Recall@100   | Recall@100   | Recall@100         |\n|-----|-----------|-----------|-----------|-----------|--------------------|--------------|--------------|--------------|--------------|--------------------|\n|     | BM25      | mDPR      | mE5 base  | mE5 large | E5 mistral-7b full | BM25         | mDPR         | mE5 base     | mE5 large    | E5 mistral-7b full |\n| ar  | 48.1      | 49.9      | 71.6      | 76.0      | 73.3               | 88.9         | 84.1         | 95.9         | 97.3         | 96.0               |\n| bn  | 50.8      | 44.3      | 70.2      | 75.9      | 70.3               | 90.9         | 81.9         | 96.6         | 98.2         | 96.0               |\n| en  | 35.1      | 39.4      | 51.2      | 52.9      | 57.3               | 81.9         | 76.8         | 86.4         | 87.6         | 90.2               |\n| es  | 31.9      | 47.8      | 51.5      | 52.9      | 52.2               | 70.2         | 86.4         | 88.6         | 89.1         | 87.5               |\n| fa  | 33.3      | 48.0      | 57.4      | 59.0      | 52.1               | 73.1         | 89.8         | 91.2         | 92.9         | 88.0               |\n| fi  | 55.1      | 47.2      | 74.4      | 77.8      | 74.7               | 89.1         | 78.8         | 96.9         | 98.1         | 96.7               |\n| fr  | 18.3      | 43.5      | 49.7      | 54.5      | 55.2               | 65.3         | 91.5         | 90.0         | 90.6         | 92.8               |\n| hi  | 45.8      | 38.3      | 58.4      | 62.0      | 52.1               | 86.8         | 77.6         | 92.6         | 93.9         | 89.9               |\n| id  | 44.9      | 27.2      | 51.1      | 52.9      | 52.7               | 90.4         | 57.3         | 87.4         | 87.9         | 88.4               |\n| ja  | 36.9      | 43.9      | 64.7      | 70.6      | 66.8               | 80.5         | 82.5         | 96.0         | 97.1         | 95.1               |\n| ko  | 41.9      | 41.9      | 62.2      | 66.5      | 61.8               | 78.3         | 73.7         | 91.6         | 93.4         | 89.4               |\n| ru  | 33.4      | 40.7      | 61.5      | 67.4      | 67.7               | 66.1         | 79.7         | 92.7         | 95.5         | 95.0               |\n| sw  | 38.3      | 29.9      | 71.1      | 74.9      | 68.4               | 70.1         | 61.6         | 95.6         | 96.7         | 95.5               |\n| te  | 49.4      | 35.6      | 75.2      | 84.6      | 73.9               | 83.1         | 76.2         | 98.0         | 99.2         | 95.1               |\n| th  | 48.4      | 35.8      | 75.2      | 80.2      | 74.0               | 88.7         | 67.8         | 98.0         | 98.9         | 96.5               |\n| zh  | 18.0      | 51.2      | 51.5      | 56.0      | 54.0               | 56.0         | 94.4         | 92.1         | 93.3         | 90.1               |\n| Avg | 39.3      | 41.5      | 62.3      | 66.5      | 62.9               | 78.7         | 78.8         | 93.1         | 94.3         | 92.6               |\n\nTable 7: Detailed results for the effects of contrastive pre-training. For the 'E5mistral-7b w/ cont. pre-train' setting, we pre-train Mistral-7B following the mE5 recipe for 10 k steps.\n\n| Datasets                                   |   Class. |   Clust. |   PairClass. |   Rerank |   Retr. |   STS |   Summ. |   Avg |\n|--------------------------------------------|----------|----------|--------------|----------|---------|-------|---------|-------|\n| XLM-R large + full data w/ cont. pre-train |     72.9 |     38.7 |         84.5 |     53.8 |    42   |  82.3 |    29.7 |  58   |\n|                                            |     77.2 |     47.3 |         85.5 |     58.6 |    50.2 |  84.4 |    30.7 |  63.7 |\n| E5 mistral-7b + full data                  |     78.5 |     50.3 |         88.3 |     60.2 |    56.9 |  84.6 |    31.4 |  66.6 |\n| w/ cont. pre-train                         |     78.7 |     50.1 |         87.7 |     60.9 |    56.9 |  84.9 |    30.2 |  66.7 |\n\nArtifacts The model and dataset release information is available at https://github.com/ microsoft/unilm/tree/master/e5 . We release our trained models and evaluation scripts to facilitate reproducibility and further research.\n\n## B Test Set Contamination Analysis\n\nTo assess the test set contamination on all the datasets in the MTEB benchmark, we perform a string match based analysis between the test set and our training set, disregarding differences in character case and spacing. We categorize the train-test overlaps into three types:\n\n- Low entropy texts. These are texts such as ' i need a coffee ' and ' what does that mean ', which are not considered as contamination because they are common expressions that can occur in various contexts.\n- Question overlap. We identify 4 test set questions in the DBPedia dataset that also appear in the TriviaQA training set. Given that they constitute a minor portion of the test set, their impact on the overall performance is insignificant.\n- Retrieval corpus overlap. Several retrieval datasets share the same retrieval corpus. For instance, the DBPedia, NQ, and TriviaQA datasets all use Wikipedia passages, even though their query sets are different. This is a standard evaluation practice in the field of information retrieval, and we do not regard it as contamination.\n\nIn summary, we did not detect substantial contamination risks that could alter the main findings of this paper.\n\nAnother aspect to consider is the possibility of test set contamination in the training data of Mistral-7B and GPT-4. However, since the training data of these models is not publicly accessible, it is challenging to estimate the degree of such contamination. Given their widespread use in the research community, we believe it is still a valid comparison if other works also employ these models.\n\n## C Prompts for Synthetic Data Generation\n\nFor asymmetric tasks, we list the four prompt templates in Table 8, 9, 10, and 11. For symmetric tasks, the prompts templates are available in Ta-\n\nBrainstorm a list of potentially useful text retrieval tasks.\n\nHere are a few examples for your reference:\n\n- Retrieve relevant documents for a short keyword web search query that asks for weather information.\n- Search for documents that answers a FAQ-style query on children's nutrition.\n\nPlease adhere to the following guidelines:\n\n- Specify what the query is, and what the desired documents are.\n- Each retrieval task should cover a wide range of queries, and should not be too specific.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a retrieval task: {task}\n\nYour mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"user\\_query\": a string, a random user search query specified by the retrieval task.\n- \"positive\\_document\": a string, a relevant document for the user query.\n- \"hard\\_negative\\_document\": a string, a hard negative document that only appears relevant to the query.\n\nPlease adhere to the following guidelines:\n\n- The \"user\\_query\" should be {query\\_type}, {query\\_length}, {clarity}, and diverse in topic.\n- All documents must be created independent of the query. Avoid copying the query verbatim. It's acceptable if some parts of the \"positive\\_document\" are not topically related to the query.\n- All documents should be at least {num\\_words} words long.\n- The \"hard\\_negative\\_document\" contains some useful information, but it should be less useful or comprehensive compared to the \"positive\\_document\".\n- Both the query and documents should be in {language}.\n- Do not provide any explanation in any document on why it is relevant or not relevant to the query.\n- Both the query and documents require {difficulty} level education to understand.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 8: Prompt template for the short-long matching subgroup. For placeholders, ' {query\\_type} ' ∈ {extremely long-tail, long-tail, common}, ' {query\\_length} ' ∈ {less than 5 words, 5 to 15 words, at least 10 words}, ' {difficulty} ' ∈ {high school, college, PhD}, ' {clarity} ' ∈ {clear, understandable with some effort, ambiguous}, ' {num\\_words} ' ∈ {50, 100, 200, 300, 400, 500}.\n\nble 12 and 13. To generate multilingual data, we sample the value of ' {language} ' from the language list of XLM-R (Conneau et al., 2020) with higher probability for high-resource languages. When prompting GPT-4/3.5, we set the sampling temperature to 1 . 0 and the topp hyperparameter to 1 . 0 , which is higher than the default setting to encourage more diversity.\n\n## D Instructions for Training and Evaluation\n\nWe manually write instructions for training datasets, as listed in Table 14. For evaluation datasets, the instructions are listed in Table 15.\n\nBrainstorm a list of potentially useful text classification tasks.\n\nPlease adhere to the following guidelines:\n\n- Tasks should cover a diverse range of domains and task types.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct text classification task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text classification task: {task}\n\nYour mission is to write one text classification example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\\_text\": a string, the input text specified by the classification task.\n- \"label\": a string, the correct label of the input text.\n- \"misleading\\_label\": a string, an incorrect label that is related to the task.\n\nPlease adhere to the following guidelines:\n\n- The \"input\\_text\" should be {num\\_words} words and diverse in expression.\n- The \"misleading\\_label\" must be a valid label for the given task, but not as appropriate as the \"label\" for the \"input\\_text\".\n- The values for all fields should be in {language}.\n- Avoid including the values of the \"label\" and \"misleading\\_label\" fields in the \"input\\_text\", that would make the task too easy.\n- The \"input\\_text\" is {clarity} and requires {difficulty} level education to comprehend.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 9: Prompt template for the long-short matching subgroup. For placeholders, ' {num\\_words} ' ∈ {\"less than 10\", \"at least 10\", \"at least 50\", \"at least 100\", \"at least 200\"}, ' {difficulty} ' ∈ {high school, college, PhD}, ' {clarity} ' ∈ {clear, understandable with some effort, ambiguous}.\n\nBrainstorm a list of text matching tasks where both the queries and the groundtruth documents are very short (one or two sentences, even a short phrase).\n\nHere are a few examples:\n\n- Given a scientific paper title, retrieve the title of papers that cite the given paper.\n- Match a word with its definition.\n- Provided a notable person's name, identify their occupation or achievement.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text matching task: {task}\n\nYour mission is to write one example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\": a string, a random input specified by the task.\n- \"positive\\_document\": a string, a relevant document for the \"input\" according to the task.\n\nPlease adhere to the following guidelines:\n\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be very short (a sentence or a phrase), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive\\_document\" should be independent of each other.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 10: Prompt template for the short-short matching subgroup. We do not generate negative documents as the matching task is already reasonably difficult.\n\nBrainstorm a list of text matching tasks where the queries are long documents.\n\nHere are a few examples:\n\n- Given a document that supports a debatable argument, find another document that contains opposite arguments.\n- Provided a lengthy business proposal, retrieve competitive business strategies in the same industry.\n\nYour output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct task in one sentence. Do not explain yourself or output anything else. Be creative!\n\nYou have been assigned a text matching task: {task}\n\nYour mission is to write one example for this task in JSON format. The JSON object must contain the following keys:\n\n- \"input\": a string, a random input specified by the task.\n- \"positive\\_document\": a string, a relevant document for the \"input\" according to the task.\n\nPlease adhere to the following guidelines:\n\n- The values of all fields should be in {language}.\n- Both the \"input\" and \"positive\\_document\" should be long documents (at least 300 words), avoid substantial word overlaps, otherwise the task would be too easy.\n- The \"input\" and \"positive\\_document\" should be independent of each other.\n\nYour output must always be a JSON object only, do not explain yourself or output anything else. Be creative!\n\nTable 11: Prompt template for the long-long matching subgroup. We do not generate negative documents for API latency reasons.\n\nWrite a {unit} triple with varying semantic similarity scores in JSON format. The semantic similarity score ranges from 1 to 5, with 1 denotes least similar and 5 denotes most similar.\n\nPlease adhere to the following guidelines:\n\n- The keys in JSON are \"S1\", \"S2\", and \"S3\", the values are all strings in {language}, do not add any other keys.\n- There should be some word overlaps between all three {unit}s.\n- The similarity score between S1 and S2 should be {high\\_score}.\n- The similarity score between S1 and S3 should be {low\\_score}.\n- The {unit}s require {difficulty} level education to understand and should be diverse in terms of topic and length.\n\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself or output anything else. Be creative!\n\nTable 12: Prompt template for monolingual STS. For placeholders, ' {high\\_score} ' ∈ {4, 4.5, 5}, ' {low\\_score} ' ∈ {2.5, 3, 3.5}, ' {unit} ' ∈ {sentence, phrase, passage}, ' {difficulty} ' ∈ {elementary school, high school, college}.\n\nWrite a {unit} triple with one {unit} in {src\\_lang} and two {unit}s in {tgt\\_lang} with varying translation qualities in JSON format.\n\nThe triple is denotes as (\"S1\", \"S2\", \"S3\"). The translation quality score ranges from 1 to 5, with higher scores are better.\n\nPlease adhere to the following guidelines:\n\n- The values of \"S1\" is a string in {src\\_lang}, the value of \"S2\" and \"S3\" are strings in {tgt\\_lang}.\n- There should be some word overlaps between \"S2\" and \"S3\".\n- The translation quality score of \"S2\" with respect to \"S1\" should be {high\\_score}.\n- The translation quality score of \"S3\" with respect to \"S1\" should be {low\\_score}.\n- \"S3\" should be grammatical and fluent, but contain some keyword or number translation errors, or miss some information, or contain some redundant information.\n- \"S1\" requires {difficulty} level education to understand and should be diverse in terms of topic and length.\n\nYour output must always be a JSON object only with three keys \"S1\", \"S2\" and \"S3\", do not explain yourself or output anything else. Be creative!\n\nTable 13: Prompt template for bitext retrieval. For placeholders, ' {high\\_score} ' ∈ {4, 4.5, 5}, ' {low\\_score} ' ∈ {1.5, 2, 2.5}, ' {unit} ' ∈ {sentence, phrase, passage}, ' {difficulty} ' ∈ {elementary school, high school, college}.\n\nTable 14: Instructions for each training dataset.\n\n| Dataset              | Instruction                                                                                                                                                 |\n|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ELI5                 | Provided a user question, retrieve the highest voted answers on Reddit ELI5 forum                                                                           |\n| HotpotQA             | Given a multi-hop question, retrieve documents that can help answer the question                                                                            |\n| FEVER                | Given a claim, retrieve documents that support or refute the claim                                                                                          |\n| MIRACL / MrTyDi / NQ | Given a question, retrieve Wikipedia passages that answer the question                                                                                      |\n| / SQuAD / TriviaQA   | Retrieve Wikipedia passages that answer the question                                                                                                        |\n| NLI                  | Given a premise, retrieve a hypothesis that is entailed by the premise Retrieve semantically similar text                                                   |\n| MS-MARCO             | Given a web search query, retrieve relevant passages that answer the query Given a web search query, retrieve relevant documents that answer the query      |\n| Quora Duplicates     | Given a question, retrieve questions that are semantically equivalent to the given question Find questions that have the same meaning as the input question |\n| DuReader / T2Ranking | Given a Chinese search query, retrieve web passages that answer the question                                                                                |\n\nTable 15: Instructions used for evaluation on the MTEB benchmark. 'STS*' indicates we use the same instructions for all the STS tasks.\n\n| Task Name                     | Instruction                                                                                                                                               |\n|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AmazonCounterfactualClassif.  | Classify a given Amazon customer review text as either counterfactual or not- counterfactual                                                              |\n| AmazonPolarityClassification  | Classify Amazon reviews into positive or negative sentiment                                                                                               |\n| AmazonReviewsClassification   | Classify the given Amazon review into its appropriate rating category                                                                                     |\n| Banking77Classification       | Given a online banking query, find the corresponding intents                                                                                              |\n| EmotionClassification         | Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise                   |\n| ImdbClassification            | Classify the sentiment expressed in the given movie review text from the IMDB dataset                                                                     |\n| MassiveIntentClassification   | Given a user utterance as query, find the user intents                                                                                                    |\n| MassiveScenarioClassification | Given a user utterance as query, find the user scenarios                                                                                                  |\n| MTOPDomainClassification      | Classify the intent domain of the given utterance in task-oriented conversation                                                                           |\n| MTOPIntentClassification      | Classify the intent of the given utterance in task-oriented conversation                                                                                  |\n| ToxicConversationsClassif.    | Classify the given comments as either toxic or not toxic                                                                                                  |\n| TweetSentimentClassification  | Classify the sentiment of a given tweet as either positive, negative, or neutral                                                                          |\n| ArxivClusteringP2P            | Identify the main and secondary category of Arxiv papers based on the titles and abstracts                                                                |\n| ArxivClusteringS2S            | Identify the main and secondary category of Arxiv papers based on the titles                                                                              |\n| BiorxivClusteringP2P          | Identify the main category of Biorxiv papers based on the titles and abstracts                                                                            |\n| BiorxivClusteringS2S          | Identify the main category of Biorxiv papers based on the titles                                                                                          |\n| MedrxivClusteringP2P          | Identify the main category of Medrxiv papers based on the titles and abstracts                                                                            |\n| MedrxivClusteringS2S          | Identify the main category of Medrxiv papers based on the titles                                                                                          |\n| RedditClustering              | Identify the topic or theme of Reddit posts based on the titles                                                                                           |\n| RedditClusteringP2P           | Identify the topic or theme of Reddit posts based on the titles and posts                                                                                 |\n| StackExchangeClustering       | Identify the topic or theme of StackExchange posts based on the titles                                                                                    |\n| StackExchangeClusteringP2P    | Identify the topic or theme of StackExchange posts based on the given paragraphs                                                                          |\n| TwentyNewsgroupsClustering    | Identify the topic or theme of the given news articles                                                                                                    |\n| SprintDuplicateQuestions      | Retrieve duplicate questions from Sprint forum                                                                                                            |\n| TwitterSemEval2015            | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| TwitterURLCorpus              | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| AskUbuntuDupQuestions         | Retrieve duplicate questions from AskUbuntu forum                                                                                                         |\n| MindSmallReranking            | Retrieve relevant news articles based on user browsing history                                                                                            |\n| SciDocsRR                     | Given a title of a scientific paper, retrieve the titles of other relevant papers                                                                         |\n| StackOverflowDupQuestions     | Retrieve duplicate questions from StackOverflow forum                                                                                                     |\n| ArguAna                       | Given a claim, find documents that refute the claim                                                                                                       |\n| ClimateFEVER                  | Given a claim about climate change, retrieve documents that support or refute the claim                                                                   |\n| CQADupstackRetrieval          | Given a question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question                                    |\n| DBPedia                       | Given a query, retrieve relevant entity descriptions from DBPedia                                                                                         |\n| FEVER                         | Given a claim, retrieve documents that support or refute the claim                                                                                        |\n| FiQA2018                      | Given a financial question, retrieve user replies that best answer the question                                                                           |\n| HotpotQA                      | Given a multi-hop question, retrieve documents that can help answer the question                                                                          |\n| MSMARCO                       | Given a web search query, retrieve relevant passages that answer the query                                                                                |\n| NFCorpus                      | Given a question, retrieve relevant documents that best answer the question                                                                               |\n| NQ                            | Given a question, retrieve Wikipedia passages that answer the question                                                                                    |\n| QuoraRetrieval                | Given a question, retrieve questions that are semantically equivalent to the given question                                                               |\n| SCIDOCS                       | Given a scientific paper title, retrieve paper abstracts that are cited by the given paper                                                                |\n| SciFact                       | Given a scientific claim, retrieve documents that support or refute the claim                                                                             |\n| Touche2020 TRECCOVID          | Given a question, retrieve detailed and persuasive arguments that answer the question Given a query on COVID-19, retrieve documents that answer the query |\n|                               | Retrieve semantically similar text.                                                                                                                       |\n| STS* BUCC/Tatoeba             | Retrieve parallel sentences.                                                                                                                              |\n| SummEval                      | Given a news summary, retrieve other semantically similar summaries                                                                                       |\n\n## Task group: short-long matching\n\nTask definition: The query is a name of a computer software or app, find documents that offer tutorials, guides or user experiences about the software or app.\n\n## Generated data: {\n\n\"user\\_query\": \"How to use Microsoft Power BI for data analysis\",\n\n\"positive\\_document\": \"Microsoft Power BI is a sophisticated tool that requires time and practice to master. In this tutorial, we'll show you how to navigate Power BI and perform basic data analysis. First, launch the application and log in with a Microsoft account. Next, create a new project using the Wizard on the left-hand toolbar. Import the dataset you wish to work with by clicking on 'Get Data'. Power BI can handle a variety of file formats, including Excel and CSV. Load your data, then use a sequence of drag-and-drop operations to generate charts and (if applicable) maps. If you are confused at any point, consider using the 'Ask a Question' feature for clarification.\",\n\n\"hard\\_negative\\_document\": \"Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our tutorial series focuses on how you can achieve maximum efficiency with Excel. In this guide, we'll walk you through transferring data from your computer to an Excel spreadsheet as well as colour-coding cells to match various criteria. We'll also explore VLOOKUP, conditional formatting, and pivot tables, essentially transforming you into a spreadsheet wizard. Once you've mastered these techniques, consider joining us for our Advanced Data Analysis training to branch out even further.\"\n\n}\n\nTask group: long-short matching\n\nTask definition: Identifying severity level of customer complaints in support tickets\n\n## Generated data: {\n\n\"input\\_text\": \"I am writing to express my intense dissatisfaction with one of your products, a TV that has stopped functioning only a month after purchase. This situation yields less satisfaction to me and speaks voluminously about your quality control procedures in assembly lines. I hope this troubling issue etches into your improvement list for invoking earnest attention.\", \"label\": \"High Severity\",\n\n\"misleading\\_label\": \"Low Severity\"\n\n}\n\n## Task group: short-short matching\n\nTask definition: Provided a movie quote, find the movie title in which it is said.\n\n## Generated data:\n\n{\n\n\"input\": \"I'm going to make him an offer he can't refuse.\",\n\n\"positive\\_document\": \"The Godfather\"\n\n}\n\n## Task group:\n\nlong-long matching\n\nTask definition:\n\nProvided a legal brief, retrieve other legal documents presenting similar legal arguments.\n\n## Generated data:\n\n{\n\n\"input\": \"In the united states district court for the eastern district of pennsylvania. Plaintiff John Doe brings this action against Defendants ABC Corporation and XYZ Inc., alleging negligence which resulted in his injuries. The Plaintiff alleges ABC Corporation negligently designed, manufactured, and marketed a specific product. The Plaintiff also asserts XYZ Inc. negligently failed to inspect the same product, leading to harmful exposure. Plaintiff respectfully requests this honorable court to hold the defendants liable and award damages. All parties are diverse for purposes of 28 U.S.C. § 1332.\", \"positive\\_document\": \"Before the Superior Court of California, County of Los Angeles. In the matter of the lawsuit initiated by the Plaintiff Jane Smith against the Defendants PQR Industries and LMN Enterprises, charging they negligently designed, produced, and advertised a product that directly caused harm to her. Furthermore, the Plaintiff has claimed that LMN Enterprises negligently fell short in inspecting the product, leading to unsafe usage. The plaintiff hereby requests this esteemed court to declare the defendants guilty and demands compensation for the damages suffered. According to 28 U.S.C.\n\n§ 1332, complete diversity exists between the parties to this lawsuit.\"\n\n}\n\nTask group:\n\nbitext matching\n\nTask definition:\n\nRetrieve parallel sentences.\n\n## Generated data:\n\n{\n\n- \"S1\": \"Men bugun uzumli tort yasashni rejalashtirdim.\",\n\n\"S2\": \"I have planned to make a grape cake today.\",\n\n\"S3\": \"I am going to bake a strawberry pie today.\"\n\n}\n\nTask group: monolingual STS\n\nTask definition: Retrieve semantically similar text.\n\n```\nGenerated data: { \"S1\": \"Tom loves to ride his blue bicycle in the park every morning.\", \"S2\": \"Every morning, Tom enjoys riding his blue bike in the park.\", \"S3\": \"Tom takes his blue pen to school every day.\" }\n```\n\nTable 16: Random samples for each subgroup of the synthetic data.\n\nTable 17: Results for each dataset in the MTEB benchmark. The evaluation metrics and detailed baseline results are available in the original paper (Muennighoff et al., 2023).\n\n| Dataset                          | w/ synthetic only   | w/ synthetic + msmarco   | w/o synthetic data   | full data   |\n|----------------------------------|---------------------|--------------------------|----------------------|-------------|\n| BIOSSES                          | 84.2                | 81.0                     | 85.4                 | 85.5        |\n| SICK-R                           | 78.6                | 78.5                     | 81.7                 | 82.6        |\n| STS12                            | 75.8                | 74.7                     | 77.9                 | 79.7        |\n| STS13                            | 84.3                | 85.3                     | 88.0                 | 88.4        |\n| STS14                            | 80.9                | 81.2                     | 83.7                 | 84.5        |\n| STS15                            | 86.2                | 86.8                     | 89.5                 | 90.4        |\n| STS16                            | 85.0                | 85.3                     | 86.5                 | 87.7        |\n| STS17                            | 87.3                | 87.7                     | 91.0                 | 91.8        |\n| STS22                            | 66.0                | 67.1                     | 66.2                 | 67.0        |\n| STSBenchmark                     | 83.5                | 84.0                     | 87.8                 | 88.6        |\n| SummEval                         | 31.9                | 32.7                     | 31.9                 | 31.4        |\n| SprintDuplicateQuestions         | 93.5                | 95.8                     | 96.0                 | 95.7        |\n| TwitterSemEval2015               | 78.0                | 78.5                     | 81.7                 | 81.6        |\n| TwitterURLCorpus                 | 86.5                | 86.9                     | 87.7                 | 87.8        |\n| AmazonCounterfactualClass.       | 79.6                | 79.9                     | 77.2                 | 78.7        |\n| AmazonPolarityClassification     | 95.8                | 95.9                     | 93.9                 | 95.9        |\n| AmazonReviewsClassification      | 56.9                | 55.5                     | 48.2                 | 55.8        |\n| Banking77Classification          | 86.2                | 87.0                     | 88.8                 | 88.2        |\n| EmotionClassification            | 49.2                | 47.6                     | 51.0                 | 49.8        |\n| ImdbClassification               | 94.8                | 94.9                     | 89.0                 | 94.8        |\n| MassiveIntentClassification      | 79.8                | 79.9                     | 79.6                 | 80.6        |\n| MassiveScenarioClassification    | 81.7                | 82.4                     | 82.3                 | 82.4        |\n| MTOPDomainClassification         | 95.6                | 95.9                     | 95.7                 | 96.1        |\n| MTOPIntentClassification         | 84.9                | 85.9                     | 83.4                 | 86.1        |\n| ToxicConversationsClassification | 70.2                | 70.8                     | 70.9                 | 69.6        |\n| TweetSentimentExtractionClass.   | 63.5                | 63.4                     | 61.6                 | 63.7        |\n| AskUbuntuDupQuestions            | 64.3                | 65.3                     | 67.4                 | 67.0        |\n| MindSmallReranking               | 33.1                | 32.8                     | 32.5                 | 32.6        |\n| SciDocsRR                        | 86.0                | 86.0                     | 85.7                 | 86.3        |\n| StackOverflowDupQuestions        | 52.5                | 53.7                     | 55.9                 | 54.9        |\n| ArxivClusteringP2P               | 51.4                | 51.2                     | 47.8                 | 50.5        |\n| ArxivClusteringS2S               | 46.5                | 44.9                     | 44.6                 | 45.5        |\n| BiorxivClusteringP2P             | 44.5                |                          | 36.9                 | 43.5        |\n| BiorxivClusteringS2S             | 40.9                | 43.3 40.1                | 37.0                 | 40.2        |\n| MedrxivClusteringS2S             | 38.0                | 37.9                     | 32.8                 | 37.5        |\n| RedditClustering                 | 56.3                | 55.9                     | 63.1                 | 57.7        |\n| RedditClusteringP2P              | 66.3                | 64.8                     | 66.4                 | 66.5        |\n| StackExchangeClustering          | 72.9                | 72.7                     | 74.5                 | 73.1        |\n| StackExchangeClusteringP2P       | 46.1                | 45.6                     | 34.3                 | 45.9        |\n| TwentyNewsgroupsClustering       | 52.2                | 52.5                     | 55.6                 | 54.3        |\n| ArguAna                          | 52.2                | 42.7                     | 62.5                 | 61.9        |\n| ClimateFEVER                     | 21.1                | 28.8                     | 25.2                 | 38.4        |\n| CQADupstackAndroidRetrieval      | 40.8                | 36.0                     | 44.5                 | 43.0        |\n| DBPedia                          | 42.0                | 43.7                     | 47.7                 | 48.9        |\n| FEVER                            | 72.5                | 83.5                     | 73.1                 | 87.8        |\n| FiQA2018                         | 38.1                | 48.4                     | 54.5                 | 56.6        |\n| HotpotQA                         | 48.1                | 64.0                     | 75.6                 | 75.7        |\n| MSMARCO                          | 25.7                | 45.0                     |                      | 43.1        |\n|                                  |                     |                          | 42.9                 |             |\n| NFCorpus NQ                      | 35.5                | 40.0                     | 35.3                 | 38.6 63.5   |\n|                                  | 53.3                | 63.5                     | 57.3                 |             |\n| QuoraRetrieval                   | 75.0                | 79.5                     | 89.5                 | 89.6        |\n| SCIDOCS SciFact                  | 20.6 71.5           | 15.8 71.9                | 19.0 74.7            | 16.3 76.4   |\n| Touche2020                       | 25.4                | 32.5                     | 19.1                 | 26.4        |\n| TRECCOVID                        | 82.3                | 87.3                     | 70.8                 | 87.2        |\n| Average                          | 63.1                | 64.5                     | 64.6                 | 66.6        |",
  "tables": [
    {
      "index": 0,
      "markdown": "| # of datasets →                                  | Class. 12   | Clust. 11   | PairClass. 3   | Rerank 4   | Retr. 15   | STS 10   | Summ. 1   | Avg 56   |\n|--------------------------------------------------|-------------|-------------|----------------|------------|------------|----------|-----------|----------|\n| Unsupervised Models                              |             |             |                |            |            |          |           |          |\n| Glove (Pennington et al., 2014)                  | 57.3        | 27.7        | 70.9           | 43.3       | 21.6       | 61.9     | 28.9      | 42.0     |\n| SimCSE bert-unsup (Gao et al., 2021)             | 62.5        | 29.0        | 70.3           | 46.5       | 20.3       | 74.3     | 31.2      | 45.5     |\n| Supervised Models                                |             |             |                |            |            |          |           |          |\n| SimCSE bert-sup (Gao et al., 2021)               | 67.3        | 33.4        | 73.7           | 47.5       | 21.8       | 79.1     | 23.3      | 48.7     |\n| Contriever (Izacard et al., 2021)                | 66.7        | 41.1        | 82.5           | 53.1       | 41.9       | 76.5     | 30.4      | 56.0     |\n| GTR xxl (Ni et al., 2022b)                       | 67.4        | 42.4        | 86.1           | 56.7       | 48.5       | 78.4     | 30.6      | 59.0     |\n| Sentence-T5 xxl (Ni et al., 2022a)               | 73.4        | 43.7        | 85.1           | 56.4       | 42.2       | 82.6     | 30.1      | 59.5     |\n| E5 large-v2 (Wang et al., 2022b)                 | 75.2        | 44.5        | 86.0           | 56.6       | 50.6       | 82.1     | 30.2      | 62.3     |\n| GTE large (Li et al., 2023)                      | 73.3        | 46.8        | 85.0           | 59.1       | 52.2       | 83.4     | 31.7      | 63.1     |\n| BGE large-en-v1.5 (Xiao et al., 2023)            | 76.0        | 46.1        | 87.1           | 60.0       | 54.3       | 83.1     | 31.6      | 64.2     |\n| Ours                                             |             |             |                |            |            |          |           |          |\n| E5 mistral-7b + full data w/ synthetic data only | 78.5        | 50.3        | 88.3           | 60.2       | 56.9       | 84.6     | 31.4      | 66.6     |\n|                                                  | 78.2        | 50.5        | 86.0           | 59.0       | 46.9       | 81.2     | 31.9      | 63.1     |\n| w/ synthetic + msmarco                           | 78.3        | 49.9        | 87.1           | 59.5       | 52.2       | 81.2     | 32.7      | 64.5     |"
    },
    {
      "index": 1,
      "markdown": "|                               | High-resource Languages   | High-resource Languages   | High-resource Languages   | High-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   | Low-resource Languages   |\n|-------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|--------------------------|--------------------------|--------------------------|--------------------------|\n|                               | en                        | fr                        | es                        | ru                        | te                       | hi                       | bn                       | sw                       |\n| BM25 (Zhang et al., 2023b)    | 35.1                      | 18.3                      | 31.9                      | 33.4                      | 49.4                     | 45.8                     | 50.8                     | 38.3                     |\n| mDPR (Zhang et al., 2023b)    | 39.4                      | 43.5                      | 47.8                      | 40.7                      | 35.6                     | 38.3                     | 44.3                     | 29.9                     |\n| mE5 base (Wang et al., 2024)  | 51.2                      | 49.7                      | 51.5                      | 61.5                      | 75.2                     | 58.4                     | 70.2                     | 71.1                     |\n| mE5 large (Wang et al., 2024) | 52.9                      | 54.5                      | 52.9                      | 67.4                      | 84.6                     | 62.0                     | 75.9                     | 74.9                     |\n| E5 mistral-7b + full data     | 57.3                      | 55.2                      | 52.2                      | 67.7                      | 73.9                     | 52.1                     | 70.3                     | 68.4                     |"
    },
    {
      "index": 2,
      "markdown": "| Model                         |   BEIR |   MTEB |\n|-------------------------------|--------|--------|\n| OpenAI text-embedding-3-large |   55.4 |   64.6 |\n| Cohere-embed-english-v3.0     |   55   |   64.5 |\n| voyage-lite-01-instruct       |   55.6 |   64.5 |\n| UAE-Large-V1                  |   54.7 |   64.6 |\n| E5 mistral-7b + full data     |   56.9 |   66.6 |"
    },
    {
      "index": 3,
      "markdown": "|               |   BUCC 2018 4 langs |   Tatoeba 112 langs |\n|---------------|---------------------|---------------------|\n| mContriever   |                93.7 |                37.7 |\n| LaBSE         |                98.8 |                81.1 |\n| mE5 base      |                98.1 |                68.1 |\n| mE5 large     |                98.6 |                75.7 |\n| E5 mistral-7b |                98.9 |                70.1 |"
    },
    {
      "index": 4,
      "markdown": "| Datasets             | Class.   | Clust.   | PairClass.   | Rerank   | Retr.   | STS   | Summ.   | Avg       |\n|----------------------|----------|----------|--------------|----------|---------|-------|---------|-----------|\n| E5 mistral-7b        | 78.3     | 49.9     | 87.1         | 59.5     | 52.2    | 81.2  | 32.7    | 64.5      |\n| w/ LLaMA-2 7b init.  | 76.2     | 48.1     | 85.1         | 58.9     | 49.6    | 81.2  | 30.8    | 62.9 -1.6 |\n| w/ msmarco data only | 71.6     | 47.1     | 86.1         | 58.8     | 54.4    | 79.5  | 31.7    | 62.7 -1.8 |\n| pooling type         |          |          |              |          |         |       |         |           |\n| w/ mean pool         | 77.0     | 48.9     | 86.1         | 59.2     | 52.4    | 81.4  | 30.8    | 64.1 -0.4 |\n| w/ weighted mean     | 77.0     | 49.0     | 86.1         | 59.2     | 52.0    | 81.4  | 30.2    | 64.0 -0.5 |\n| LoRA rank            |          |          |              |          |         |       |         |           |\n| w/ r= 8              | 78.4     | 50.3     | 87.1         | 59.3     | 53.0    | 81.0  | 31.7    | 64.8 +0.3 |\n| w/ r= 32             | 78.4     | 50.3     | 87.4         | 59.5     | 52.2    | 81.2  | 30.6    | 64.6 +0.1 |\n| instruction type     |          |          |              |          |         |       |         |           |\n| w/o instruction      | 72.3     | 47.1     | 82.6         | 56.3     | 48.2    | 76.7  | 30.7    | 60.3 -4.2 |\n| w/ task type prefix  | 71.1     | 46.5     | 79.7         | 54.0     | 52.7    | 73.8  | 30.0    | 60.3 -4.2 |"
    },
    {
      "index": 5,
      "markdown": "|     | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10   | nDCG@10            | Recall@100   | Recall@100   | Recall@100   | Recall@100   | Recall@100         |\n|-----|-----------|-----------|-----------|-----------|--------------------|--------------|--------------|--------------|--------------|--------------------|\n|     | BM25      | mDPR      | mE5 base  | mE5 large | E5 mistral-7b full | BM25         | mDPR         | mE5 base     | mE5 large    | E5 mistral-7b full |\n| ar  | 48.1      | 49.9      | 71.6      | 76.0      | 73.3               | 88.9         | 84.1         | 95.9         | 97.3         | 96.0               |\n| bn  | 50.8      | 44.3      | 70.2      | 75.9      | 70.3               | 90.9         | 81.9         | 96.6         | 98.2         | 96.0               |\n| en  | 35.1      | 39.4      | 51.2      | 52.9      | 57.3               | 81.9         | 76.8         | 86.4         | 87.6         | 90.2               |\n| es  | 31.9      | 47.8      | 51.5      | 52.9      | 52.2               | 70.2         | 86.4         | 88.6         | 89.1         | 87.5               |\n| fa  | 33.3      | 48.0      | 57.4      | 59.0      | 52.1               | 73.1         | 89.8         | 91.2         | 92.9         | 88.0               |\n| fi  | 55.1      | 47.2      | 74.4      | 77.8      | 74.7               | 89.1         | 78.8         | 96.9         | 98.1         | 96.7               |\n| fr  | 18.3      | 43.5      | 49.7      | 54.5      | 55.2               | 65.3         | 91.5         | 90.0         | 90.6         | 92.8               |\n| hi  | 45.8      | 38.3      | 58.4      | 62.0      | 52.1               | 86.8         | 77.6         | 92.6         | 93.9         | 89.9               |\n| id  | 44.9      | 27.2      | 51.1      | 52.9      | 52.7               | 90.4         | 57.3         | 87.4         | 87.9         | 88.4               |\n| ja  | 36.9      | 43.9      | 64.7      | 70.6      | 66.8               | 80.5         | 82.5         | 96.0         | 97.1         | 95.1               |\n| ko  | 41.9      | 41.9      | 62.2      | 66.5      | 61.8               | 78.3         | 73.7         | 91.6         | 93.4         | 89.4               |\n| ru  | 33.4      | 40.7      | 61.5      | 67.4      | 67.7               | 66.1         | 79.7         | 92.7         | 95.5         | 95.0               |\n| sw  | 38.3      | 29.9      | 71.1      | 74.9      | 68.4               | 70.1         | 61.6         | 95.6         | 96.7         | 95.5               |\n| te  | 49.4      | 35.6      | 75.2      | 84.6      | 73.9               | 83.1         | 76.2         | 98.0         | 99.2         | 95.1               |\n| th  | 48.4      | 35.8      | 75.2      | 80.2      | 74.0               | 88.7         | 67.8         | 98.0         | 98.9         | 96.5               |\n| zh  | 18.0      | 51.2      | 51.5      | 56.0      | 54.0               | 56.0         | 94.4         | 92.1         | 93.3         | 90.1               |\n| Avg | 39.3      | 41.5      | 62.3      | 66.5      | 62.9               | 78.7         | 78.8         | 93.1         | 94.3         | 92.6               |"
    },
    {
      "index": 6,
      "markdown": "| Datasets                                   |   Class. |   Clust. |   PairClass. |   Rerank |   Retr. |   STS |   Summ. |   Avg |\n|--------------------------------------------|----------|----------|--------------|----------|---------|-------|---------|-------|\n| XLM-R large + full data w/ cont. pre-train |     72.9 |     38.7 |         84.5 |     53.8 |    42   |  82.3 |    29.7 |  58   |\n|                                            |     77.2 |     47.3 |         85.5 |     58.6 |    50.2 |  84.4 |    30.7 |  63.7 |\n| E5 mistral-7b + full data                  |     78.5 |     50.3 |         88.3 |     60.2 |    56.9 |  84.6 |    31.4 |  66.6 |\n| w/ cont. pre-train                         |     78.7 |     50.1 |         87.7 |     60.9 |    56.9 |  84.9 |    30.2 |  66.7 |"
    },
    {
      "index": 7,
      "markdown": "| Dataset              | Instruction                                                                                                                                                 |\n|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ELI5                 | Provided a user question, retrieve the highest voted answers on Reddit ELI5 forum                                                                           |\n| HotpotQA             | Given a multi-hop question, retrieve documents that can help answer the question                                                                            |\n| FEVER                | Given a claim, retrieve documents that support or refute the claim                                                                                          |\n| MIRACL / MrTyDi / NQ | Given a question, retrieve Wikipedia passages that answer the question                                                                                      |\n| / SQuAD / TriviaQA   | Retrieve Wikipedia passages that answer the question                                                                                                        |\n| NLI                  | Given a premise, retrieve a hypothesis that is entailed by the premise Retrieve semantically similar text                                                   |\n| MS-MARCO             | Given a web search query, retrieve relevant passages that answer the query Given a web search query, retrieve relevant documents that answer the query      |\n| Quora Duplicates     | Given a question, retrieve questions that are semantically equivalent to the given question Find questions that have the same meaning as the input question |\n| DuReader / T2Ranking | Given a Chinese search query, retrieve web passages that answer the question                                                                                |"
    },
    {
      "index": 8,
      "markdown": "| Task Name                     | Instruction                                                                                                                                               |\n|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AmazonCounterfactualClassif.  | Classify a given Amazon customer review text as either counterfactual or not- counterfactual                                                              |\n| AmazonPolarityClassification  | Classify Amazon reviews into positive or negative sentiment                                                                                               |\n| AmazonReviewsClassification   | Classify the given Amazon review into its appropriate rating category                                                                                     |\n| Banking77Classification       | Given a online banking query, find the corresponding intents                                                                                              |\n| EmotionClassification         | Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise                   |\n| ImdbClassification            | Classify the sentiment expressed in the given movie review text from the IMDB dataset                                                                     |\n| MassiveIntentClassification   | Given a user utterance as query, find the user intents                                                                                                    |\n| MassiveScenarioClassification | Given a user utterance as query, find the user scenarios                                                                                                  |\n| MTOPDomainClassification      | Classify the intent domain of the given utterance in task-oriented conversation                                                                           |\n| MTOPIntentClassification      | Classify the intent of the given utterance in task-oriented conversation                                                                                  |\n| ToxicConversationsClassif.    | Classify the given comments as either toxic or not toxic                                                                                                  |\n| TweetSentimentClassification  | Classify the sentiment of a given tweet as either positive, negative, or neutral                                                                          |\n| ArxivClusteringP2P            | Identify the main and secondary category of Arxiv papers based on the titles and abstracts                                                                |\n| ArxivClusteringS2S            | Identify the main and secondary category of Arxiv papers based on the titles                                                                              |\n| BiorxivClusteringP2P          | Identify the main category of Biorxiv papers based on the titles and abstracts                                                                            |\n| BiorxivClusteringS2S          | Identify the main category of Biorxiv papers based on the titles                                                                                          |\n| MedrxivClusteringP2P          | Identify the main category of Medrxiv papers based on the titles and abstracts                                                                            |\n| MedrxivClusteringS2S          | Identify the main category of Medrxiv papers based on the titles                                                                                          |\n| RedditClustering              | Identify the topic or theme of Reddit posts based on the titles                                                                                           |\n| RedditClusteringP2P           | Identify the topic or theme of Reddit posts based on the titles and posts                                                                                 |\n| StackExchangeClustering       | Identify the topic or theme of StackExchange posts based on the titles                                                                                    |\n| StackExchangeClusteringP2P    | Identify the topic or theme of StackExchange posts based on the given paragraphs                                                                          |\n| TwentyNewsgroupsClustering    | Identify the topic or theme of the given news articles                                                                                                    |\n| SprintDuplicateQuestions      | Retrieve duplicate questions from Sprint forum                                                                                                            |\n| TwitterSemEval2015            | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| TwitterURLCorpus              | Retrieve tweets that are semantically similar to the given tweet                                                                                          |\n| AskUbuntuDupQuestions         | Retrieve duplicate questions from AskUbuntu forum                                                                                                         |\n| MindSmallReranking            | Retrieve relevant news articles based on user browsing history                                                                                            |\n| SciDocsRR                     | Given a title of a scientific paper, retrieve the titles of other relevant papers                                                                         |\n| StackOverflowDupQuestions     | Retrieve duplicate questions from StackOverflow forum                                                                                                     |\n| ArguAna                       | Given a claim, find documents that refute the claim                                                                                                       |\n| ClimateFEVER                  | Given a claim about climate change, retrieve documents that support or refute the claim                                                                   |\n| CQADupstackRetrieval          | Given a question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question                                    |\n| DBPedia                       | Given a query, retrieve relevant entity descriptions from DBPedia                                                                                         |\n| FEVER                         | Given a claim, retrieve documents that support or refute the claim                                                                                        |\n| FiQA2018                      | Given a financial question, retrieve user replies that best answer the question                                                                           |\n| HotpotQA                      | Given a multi-hop question, retrieve documents that can help answer the question                                                                          |\n| MSMARCO                       | Given a web search query, retrieve relevant passages that answer the query                                                                                |\n| NFCorpus                      | Given a question, retrieve relevant documents that best answer the question                                                                               |\n| NQ                            | Given a question, retrieve Wikipedia passages that answer the question                                                                                    |\n| QuoraRetrieval                | Given a question, retrieve questions that are semantically equivalent to the given question                                                               |\n| SCIDOCS                       | Given a scientific paper title, retrieve paper abstracts that are cited by the given paper                                                                |\n| SciFact                       | Given a scientific claim, retrieve documents that support or refute the claim                                                                             |\n| Touche2020 TRECCOVID          | Given a question, retrieve detailed and persuasive arguments that answer the question Given a query on COVID-19, retrieve documents that answer the query |\n|                               | Retrieve semantically similar text.                                                                                                                       |\n| STS* BUCC/Tatoeba             | Retrieve parallel sentences.                                                                                                                              |\n| SummEval                      | Given a news summary, retrieve other semantically similar summaries                                                                                       |"
    },
    {
      "index": 9,
      "markdown": "| Dataset                          | w/ synthetic only   | w/ synthetic + msmarco   | w/o synthetic data   | full data   |\n|----------------------------------|---------------------|--------------------------|----------------------|-------------|\n| BIOSSES                          | 84.2                | 81.0                     | 85.4                 | 85.5        |\n| SICK-R                           | 78.6                | 78.5                     | 81.7                 | 82.6        |\n| STS12                            | 75.8                | 74.7                     | 77.9                 | 79.7        |\n| STS13                            | 84.3                | 85.3                     | 88.0                 | 88.4        |\n| STS14                            | 80.9                | 81.2                     | 83.7                 | 84.5        |\n| STS15                            | 86.2                | 86.8                     | 89.5                 | 90.4        |\n| STS16                            | 85.0                | 85.3                     | 86.5                 | 87.7        |\n| STS17                            | 87.3                | 87.7                     | 91.0                 | 91.8        |\n| STS22                            | 66.0                | 67.1                     | 66.2                 | 67.0        |\n| STSBenchmark                     | 83.5                | 84.0                     | 87.8                 | 88.6        |\n| SummEval                         | 31.9                | 32.7                     | 31.9                 | 31.4        |\n| SprintDuplicateQuestions         | 93.5                | 95.8                     | 96.0                 | 95.7        |\n| TwitterSemEval2015               | 78.0                | 78.5                     | 81.7                 | 81.6        |\n| TwitterURLCorpus                 | 86.5                | 86.9                     | 87.7                 | 87.8        |\n| AmazonCounterfactualClass.       | 79.6                | 79.9                     | 77.2                 | 78.7        |\n| AmazonPolarityClassification     | 95.8                | 95.9                     | 93.9                 | 95.9        |\n| AmazonReviewsClassification      | 56.9                | 55.5                     | 48.2                 | 55.8        |\n| Banking77Classification          | 86.2                | 87.0                     | 88.8                 | 88.2        |\n| EmotionClassification            | 49.2                | 47.6                     | 51.0                 | 49.8        |\n| ImdbClassification               | 94.8                | 94.9                     | 89.0                 | 94.8        |\n| MassiveIntentClassification      | 79.8                | 79.9                     | 79.6                 | 80.6        |\n| MassiveScenarioClassification    | 81.7                | 82.4                     | 82.3                 | 82.4        |\n| MTOPDomainClassification         | 95.6                | 95.9                     | 95.7                 | 96.1        |\n| MTOPIntentClassification         | 84.9                | 85.9                     | 83.4                 | 86.1        |\n| ToxicConversationsClassification | 70.2                | 70.8                     | 70.9                 | 69.6        |\n| TweetSentimentExtractionClass.   | 63.5                | 63.4                     | 61.6                 | 63.7        |\n| AskUbuntuDupQuestions            | 64.3                | 65.3                     | 67.4                 | 67.0        |\n| MindSmallReranking               | 33.1                | 32.8                     | 32.5                 | 32.6        |\n| SciDocsRR                        | 86.0                | 86.0                     | 85.7                 | 86.3        |\n| StackOverflowDupQuestions        | 52.5                | 53.7                     | 55.9                 | 54.9        |\n| ArxivClusteringP2P               | 51.4                | 51.2                     | 47.8                 | 50.5        |\n| ArxivClusteringS2S               | 46.5                | 44.9                     | 44.6                 | 45.5        |\n| BiorxivClusteringP2P             | 44.5                |                          | 36.9                 | 43.5        |\n| BiorxivClusteringS2S             | 40.9                | 43.3 40.1                | 37.0                 | 40.2        |\n| MedrxivClusteringS2S             | 38.0                | 37.9                     | 32.8                 | 37.5        |\n| RedditClustering                 | 56.3                | 55.9                     | 63.1                 | 57.7        |\n| RedditClusteringP2P              | 66.3                | 64.8                     | 66.4                 | 66.5        |\n| StackExchangeClustering          | 72.9                | 72.7                     | 74.5                 | 73.1        |\n| StackExchangeClusteringP2P       | 46.1                | 45.6                     | 34.3                 | 45.9        |\n| TwentyNewsgroupsClustering       | 52.2                | 52.5                     | 55.6                 | 54.3        |\n| ArguAna                          | 52.2                | 42.7                     | 62.5                 | 61.9        |\n| ClimateFEVER                     | 21.1                | 28.8                     | 25.2                 | 38.4        |\n| CQADupstackAndroidRetrieval      | 40.8                | 36.0                     | 44.5                 | 43.0        |\n| DBPedia                          | 42.0                | 43.7                     | 47.7                 | 48.9        |\n| FEVER                            | 72.5                | 83.5                     | 73.1                 | 87.8        |\n| FiQA2018                         | 38.1                | 48.4                     | 54.5                 | 56.6        |\n| HotpotQA                         | 48.1                | 64.0                     | 75.6                 | 75.7        |\n| MSMARCO                          | 25.7                | 45.0                     |                      | 43.1        |\n|                                  |                     |                          | 42.9                 |             |\n| NFCorpus NQ                      | 35.5                | 40.0                     | 35.3                 | 38.6 63.5   |\n|                                  | 53.3                | 63.5                     | 57.3                 |             |\n| QuoraRetrieval                   | 75.0                | 79.5                     | 89.5                 | 89.6        |\n| SCIDOCS SciFact                  | 20.6 71.5           | 15.8 71.9                | 19.0 74.7            | 16.3 76.4   |\n| Touche2020                       | 25.4                | 32.5                     | 19.1                 | 26.4        |\n| TRECCOVID                        | 82.3                | 87.3                     | 70.8                 | 87.2        |\n| Average                          | 63.1                | 64.5                     | 64.6                 | 66.6        |"
    }
  ],
  "stats": {
    "pages": 20,
    "chunksCreated": 133,
    "totalCharacters": 94230,
    "totalWords": 12725,
    "numTables": 10,
    "processingTimeMs": 46936
  }
}