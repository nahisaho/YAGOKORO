{
  "paper": {
    "id": "2306.05836v3",
    "title": "Can Large Language Models Infer Causation from Correlation?",
    "abstract": "Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.",
    "authors": [
      "Zhijing Jin",
      "Jiarui Liu",
      "Zhiheng Lyu",
      "Spencer Poff",
      "Mrinmaya Sachan",
      "Rada Mihalcea",
      "Mona Diab",
      "Bernhard Schölkopf"
    ],
    "published": "2023-06-09T12:09:15.000Z",
    "updated": "2024-04-17T04:27:10.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2306.05836v3",
    "absUrl": "https://arxiv.org/abs/2306.05836v3"
  },
  "chunks": [
    {
      "id": "2306.05836v3-chunk-0",
      "content": "Zhijing Jin 1,2, ∗ , ‡ Jiarui Liu 3, ∗ Zhiheng Lyu 4 Spencer Poff 5 Mrinmaya Sachan 2 Rada Mihalcea 6 Mona Diab 3, ‡ , † Bernhard Schölkopf 1, † 1 Max Planck Institute for Intelligent Systems, Tübingen, Germany 2 ETH Zürich 3 LTI, CMU 4 University of Hong Kong 5 Meta AI 6 University of Michigan jinzhi@ethz.ch jiarui@cmu.edu zhihenglyu.cs@gmail.com",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "CAN LARGE LANGUAGE MODELS INFER CAUSATION FROM CORRELATION?",
        "chunkIndex": 0,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-1",
      "content": "Causal inference is one of the hallmarks of human intelligence. While the field of Causal NLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task CORR2CAUSE, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-2",
      "content": "sting LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize - they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. CORR2CAUSE is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. 1",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 2,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-3",
      "content": "Causal inference, i.e., the ability to establish the correct causal relationships between variables or events, is fundamental to human intelligence. There are two distinct ways this causal inference capability can be acquired: one through empirical knowledge, e.g., we know from common sense that touching a hot stove will get us burned; the other through pure causal reasoning , as causality can be formally argued and reasoned about using known procedures and rules from causal inference (Spirtes et al., 2000; Pearl, 2009; Peters et al., 2017). One example is that we have the a priori knowledge that the correlation between A and B does not necessarily imply causality. This is a formal rule that holds true regardless of the realizations of the variables A and B.\n\nWith the rise of large language models (LLMs) (Radford et al., 2019; Devlin et al., 2019; Ouyang et al., 2022; Zhang et al., 2022; OpenAI, 2023, inter alia ), a crucial research question is whether they can do causal reasoning wel",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-4",
      "content": "e models (LLMs) (Radford et al., 2019; Devlin et al., 2019; Ouyang et al., 2022; Zhang et al., 2022; OpenAI, 2023, inter alia ), a crucial research question is whether they can do causal reasoning well. Recent studies have pointed out that LLMs are 'causal parrots,' which recite the causal knowledge in the training data (Zeˇ cevi´ c et al., 2023). Moreover, the vast majority of studies frame causal reasoning as a skill to navigate around empirical knowledge (Gordon et al., 2012; Sap et al., 2019a;b; Qin et al., 2019; Bhagavatula et al., 2020), and also treat LLMs as a knowledge base when evaluating its causal skills (Kıcıman et al., 2023; Tu et al., 2023; Xie et al., 2023). However, all the above lines of research frame causality as empirical knowledge, thus relying heavily on the quality and the coverage of the training data, overlooking the great potential of the formal causal reasoning skills to process correlational information to causal conclusions.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-5",
      "content": "relying heavily on the quality and the coverage of the training data, overlooking the great potential of the formal causal reasoning skills to process correlational information to causal conclusions.\n\n∗ Equal contribution. † Equal supervision. ‡ Work originated as a Meta AI internship project involving Zhijing, Mona, and Spencer.\n\n1 Our data is at https://huggingface.co/datasets/causalnlp/corr2cause . Our code is at https://github.com/causalNLP/corr2cause .\n\n<!-- image -->\n\n*Assumption that we explicitly mention in the samples: We suppose a close system of the given variables and correlations.\n\nFigure 1: Illustration of the motivation behind our task and dataset.\n\nDrawing inspirations from technical studies on causal discovery (Spirtes et al., 2000; Spirtes &amp; Zhang, 2016; Glymour et al., 2019), we formulate a novel task for NLP, correlation-to-causation inference (CORR2CAUSE), which is an important skill for LLMs.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-6",
      "content": "overy (Spirtes et al., 2000; Spirtes &amp; Zhang, 2016; Glymour et al., 2019), we formulate a novel task for NLP, correlation-to-causation inference (CORR2CAUSE), which is an important skill for LLMs. Imagine the scenario in Figure 1, where the training corpus does not tediously cover every causal relation, but more pervasively talk about correlations, such as which events tend to co-occur. Learning a good CORR2CAUSE skill can enable LLMs to draw causal relations behind the mere correlational information on the surface. For example, several decades ago, there might be an observation that female university students tend to perform better, but behind the correlational statistics is the causal graph that female students have to achieve extra good performance to get into universities as the first place.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-7",
      "content": "ty students tend to perform better, but behind the correlational statistics is the causal graph that female students have to achieve extra good performance to get into universities as the first place.\n\nTo this end, we collect the CORR2CAUSE dataset, the first dataset to test the pure causal reasoning abilities of LLMs. All the questions in this dataset are centered around testing when it is valid or invalid to infer causation from correlation. To systematically compose this dataset, we ground our generalization process in the formal framework of causal discovery (Spirtes et al., 1993; 2000; Glymour et al., 2016; Spirtes &amp; Zhang, 2016), which provides rules about how to deduce causal relations among variables given their statistical correlation in the observational data. We generate more than 200K data points, and label a correlation-causation statement pair as valid if and only if there is a bijective mapping between the statistical correlation and the underlying causality.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-8",
      "content": "rate more than 200K data points, and label a correlation-causation statement pair as valid if and only if there is a bijective mapping between the statistical correlation and the underlying causality.\n\nBased on our CORR2CAUSE dataset with 200K samples, we investigate two main research questions: (1) How well do existing LLMs perform on this task? (2) Can existing LLMs be re-trained or re-purposed on this task and obtain robust causal inference skills? Through extensive experiments, we show empirically that none of the 17 existing LLMs we investigate perform well on this pure causal inference task. We also show that although LLMs can demonstrate better performance after being finetuned on the data, the causal inference skills attained by them are not robust. In summary, our contributions are as follows:",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 8,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-9",
      "content": "show that although LLMs can demonstrate better performance after being finetuned on the data, the causal inference skills attained by them are not robust. In summary, our contributions are as follows:\n\n1. We propose the novel task of CORR2CAUSE, to probe an aspect of LLM's reasoning ability, pure causal inference ;\n2. We compose a dataset of over 200K samples, using insights from causal discovery;\n3. We evaluate the performance of 17 LLMs on our dataset, finding that all of them perform poorly, close to the random baseline;\n4. We further explored whether LLMs can learn the skill through finetuning, and find that LLMs fail to robustly acquire this skill in out-of-distribution settings. Finally, we suggest future work to explore more ways to enhance the pure causal inference skill in LLMs.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 9,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-10",
      "content": "A directed graphical causal model (DGCM) is a commonly used representation to express the causal relations among a set of variables. Given a set of N variables X = { X 1 , . . . , X N } , we can encode the causal relations among them using a directed graph G := ( X , E ) , where E is the set of directed edges. Each edge e i,j ∈ E represents a causal link X i → X j , meaning that X i is a direct cause of X j . In the context of this work, we take the common assumption of directed acyclic graphs (DAGs), which most causal discovery methods use (Glymour et al., 2019), as graphs with cycles can make the causal discovery process arbitrarily hard.\n\nFollowing the graph-theoretic terminology, we use an analogy of the ancestry tree to denote the relations between two variables. For example, we call X i as a parent of X j if there is a directed edge X i → X j in the graph, and, thus, X j is a child of X i .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.1 DIRECTED GRAPHICAL CAUSAL MODELS (DGCMS)",
        "chunkIndex": 10,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-11",
      "content": "y of the ancestry tree to denote the relations between two variables. For example, we call X i as a parent of X j if there is a directed edge X i → X j in the graph, and, thus, X j is a child of X i . Similarly, we denote X i as an ancestor of X j if there exists a directed path from X i to X j , and, thus, X j is a descendent of X i . Note that a parent is a special case of an ancestor where the directed path has a length of 1.\n\nFor convenience, we also introduce the notions for some special three-variable relations. Given two variables X i and X j , we call a third variable X k a confounder (i.e., common cause ) if X k is a parent of both X i and X j ; a collider (i.e., common effect ) if X k is a child of both X i and X j ; and a mediator if X k is both a child of X i , and a parent of X j .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.1 DIRECTED GRAPHICAL CAUSAL MODELS (DGCMS)",
        "chunkIndex": 11,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-12",
      "content": "D-Separation D-separation (Pearl, 1988) is a fundamental concept in graphical models used to determine whether two sets of nodes X and Y in a DAG G are conditionally independent given a third set of nodes Z , where the three sets are disjoint. We say that X and Y are d-separated by Z if all paths between any node in X and any node in Y are blocked by the conditioning set Z . A path between X and Y is blocked by Z if there exists a node A ∈ Z which satisfies one of the following conditions: A is the parent node in a fork structure on the path (i.e., ·← A →· ); A is the mediator node in a chain structure on the path (i.e., ·→ A →· ); or in any collider structure on the path (i.e., ·→ A ←· ), Z does not contain A or its descendants.\n\nMarkov Property The Markov property in a DAG G states that each node X i is conditionally independent of its non-descendants given its parents, namely X i ⊥ ⊥ NonDe ( X i ) | Pa ( X i ) , where NonDe ( X i ) denotes the non-descendants of X i excluding itself",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.2 D-SEPARATION AND MARKOV PROPERTY",
        "chunkIndex": 12,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-13",
      "content": "each node X i is conditionally independent of its non-descendants given its parents, namely X i ⊥ ⊥ NonDe ( X i ) | Pa ( X i ) , where NonDe ( X i ) denotes the non-descendants of X i excluding itself, and Pa ( X i ) denotes the parents of X i . Using the Markov property, we can factorize the joint distribution of all the nodes in the graph into P ( X 1 , . . . , X N ) = ∏ N i =1 P ( X i | PA ( X i )) . To infer the causal graph from probability distributions, a common assumption is faithfulness, namely the validity to infer all the d-separation sets in the graph from the independence relations in the probability distribution. In our work, we also take this broadly taken assumption which holds for most real-world scenarios.\n\nMarkov Equivalence of Graphs We denote two DAGs as Markov equivalent if they induce the same joint distribution P ( X ) . The set of DAGs that are Markov equivalent to each other is called a Markov equivalence class (MEC).",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.2 D-SEPARATION AND MARKOV PROPERTY",
        "chunkIndex": 13,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-14",
      "content": "Graphs We denote two DAGs as Markov equivalent if they induce the same joint distribution P ( X ) . The set of DAGs that are Markov equivalent to each other is called a Markov equivalence class (MEC). Causal graphs in the same MEC can be easily identified since they have the same skeleton (i.e., undirected edges) and V-structures (i.e., structures in the form of A → B ← C where A and C are not connected).\n\nObviously, there is a one-to-many mapping (i.e., surjection) between the causal graph and statistical distribution. Namely, each causal graph sufficiently determines a statistical distribution, but from a statistical distribution, we cannot necessarily induce a unique causal graph. This is why we say 'correlation does not necessarily mean causation'.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.2 D-SEPARATION AND MARKOV PROPERTY",
        "chunkIndex": 14,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-15",
      "content": "Causal discovery aims to learn the causal relations by analyzing statistical properties in the observational data (Spirtes et al., 1993; 2000; Glymour et al., 2016; Spirtes &amp; Zhang, 2016; Glymour et al., 2019). It can be achieved through constraint-based methods (Spirtes et al., 2000), score-based methods (Chickering, 2002), or other methods taking advantage of the functional causal models (Shimizu et al., 2006; Hoyer et al., 2008; Zhang &amp; Hyvärinen, 2009).\n\nTo fit for the spirit of this paper to infer from correlation (expressed in natural language) to causation, we base our dataset design on the widely-used Peter-Clark (PC) algorithm (Spirtes et al., 2000). The PC algorithm is based on the principles of conditional independence and the causal Markov assumption, which allows it to efficiently identify causal relationships among variables in a given dataset. The algorithm first starts with a fully connected undirected graph among all the variables.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.3 CAUSAL DISCOVERY",
        "chunkIndex": 15,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-16",
      "content": "assumption, which allows it to efficiently identify causal relationships among variables in a given dataset. The algorithm first starts with a fully connected undirected graph among all the variables. Then it removes the edge between two variables if there is an unconditional or conditional independence relationship between them. Afterwards, it orients the directed edges whenever there is a V-structure. And finally, it iteratively checks the direction of the other edges until the entire causal graph is consistent with all the statistical correlations.\n\nFigure 2: Pipeline of the data construction process.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "2.3 CAUSAL DISCOVERY",
        "chunkIndex": 16,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-17",
      "content": "We introduce the construction of our dataset in this section. We start with our task formulation for CORR2CAUSE, and then briefly give an overview of the data generation process, followed by detailed descriptions of each step. We conclude the section with the overall statistics of the dataset.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3 DATASET CONSTRUCTION",
        "chunkIndex": 17,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-18",
      "content": "Given a set of N variables X = { X 1 , . . . , X N } , we have a statement s about all the correlations among the variables, and a hypothesis h describing the causal relation r between the pair of variables X i and X j . The task is to learn a function f : ( s , h ) ↦→ v which maps the correlation statement s and the causal relation hypothesis h to their validity v ∈ { 0 , 1 } , which takes the value 0 if this inference is invalid, and the value 1 if this inference is valid.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.1 TASK FORMULATION",
        "chunkIndex": 18,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-19",
      "content": "We base the construction our dataset on several concepts of causal inference, including the DGCM, d-separation, and MECs, as introduced in Section 2.\n\nAs in the overview of our data generation process in Figure 2, we first choose the number N of variables (Step 1) and generate all the unique DGCMs with N nodes (Step 2), which we will introduce in the Section 3.3. Then we collect all the d-separation sets from these graphs to identify MECs (Step 3) in Section 3.4. Then, in Step 4, we create the formal form of data in Section 3.5. For each correspondence of the MEC to causal graphs, we compose the correlation statement based on the statistical relations in the MEC, and hypothesize a causal relation between two variables, and produce the validity v = 1 if the hypothesis is a shared property of all causal graphs in the MEC, and v = 0 if the hypothesis is not necessarily true for all the MEC graphs. Finally, we introduce the verbalization process in Section 3.6.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.2 OVERVIEW OF THE DATA GENERATION PROCESS",
        "chunkIndex": 19,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-20",
      "content": "The first step of the data generation is to compose the causal graphs, as in Step 1 and 2 of Figure 2. For a set of N variables X = { X 1 , . . . , X N } , there are N ( N -1) possible directed edges, since each node can link to any node other than itself. To remove cycles in the graph, we make the nodes in topological order, which only allows edges X i → X j , where i &lt; j . We achieve this by limiting the adjacency matrix of the graph to only having non-zero values above the diagonal, resulting in N ( N -1) / 2 possible directed edges for the DAGs.\n\nAt the first glance, for N nodes, there should be 2 N ( N -1) / 2 possible DAGs (i.e., the power set of all edges). However, there could be isomorphic graphs in this set. To avoid this, we perform a graph\n\nTable 1: Statistics about the source causal graphs in our dataset. Given the number of nodes, we report the number of unique DAGs, average number of edges per DAG, number of MECs, and average number of DAGs per MEC.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.3 CONSTRUCTING THE GRAPHS WITH ISOMORPHISM CHECKS",
        "chunkIndex": 20,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-21",
      "content": "tics about the source causal graphs in our dataset. Given the number of nodes, we report the number of unique DAGs, average number of edges per DAG, number of MECs, and average number of DAGs per MEC.\n\n| # Nodes   | # Unique DAGs     |   # Edges/DAG | # MECs   |   # DAGs/MEC |\n|-----------|-------------------|---------------|----------|--------------|\n| 2         | 2 out of 2        |          0.5  | 2        |         1    |\n| 3         | 6 out of 2 3      |          1.67 | 5        |         1.2  |\n| 4         | 31 out of 2 6     |          3.48 | 20       |         1.55 |\n| 5         | 302 out of 2 10   |          5.89 | 142      |         2.13 |\n| 6         | 5,984 out of 2 15 |          8.77 | 2,207    |         2.71 |\n| Total     | 6,325             |          8.6  | 2,376    |         2.66 |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.3 CONSTRUCTING THE GRAPHS WITH ISOMORPHISM CHECKS",
        "chunkIndex": 21,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-22",
      "content": "10   |          5.89 | 142      |         2.13 |\n| 6         | 5,984 out of 2 15 |          8.77 | 2,207    |         2.71 |\n| Total     | 6,325             |          8.6  | 2,376    |         2.66 |\n\nisomorphism check (McKay &amp; Piperno, 2014), and reduce the set so that only unique DAGs are retained, and we show their statistics in Table 1. Although we can handle large graphs, we mostly focus on smaller graphs that can still lead to a reasonably sized dataset, so we empirically set N = 6 , but future work can use our open-sourced codes to extend to more nodes.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.3 CONSTRUCTING THE GRAPHS WITH ISOMORPHISM CHECKS",
        "chunkIndex": 22,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-23",
      "content": "Based on the set of unique DAGs, we then programmatically generate the d-separation sets by graph theoretical conditions, as in Step 3 of Figure 2. To realize this step, we code an efficient graph-theoretic algorithm to check for all the chain, fork, and collider structures to automatically identify the set of nodes that d-separate each pair of nodes. Using the d-separation sets and the faithfulness assumption, we form the statistical correlations as follows. For each pair of nodes, they are conditionally independent given the variables in the d-separation set. If the d-separation set is empty, then the two nodes are unconditionally independent. If no d-separation set can be found for the two nodes, then they are directly correlated.\n\nMoreover, using the d-separation sets, we are able to cluster causal graphs to MECs.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.4 PROGRAMMATICALLY GENERATING THE D-SEPARATION SETS",
        "chunkIndex": 23,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-24",
      "content": "nditionally independent. If no d-separation set can be found for the two nodes, then they are directly correlated.\n\nMoreover, using the d-separation sets, we are able to cluster causal graphs to MECs. We achieve it by tracing the mapping between the causal graphs and the set of statistical correlations, and backtracking the graphs with the same d-separation sets to group them in the same MEC. We show in Table 1 that each MEC contains on average 2.66 DAGs.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.4 PROGRAMMATICALLY GENERATING THE D-SEPARATION SETS",
        "chunkIndex": 24,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-25",
      "content": "After generating the set of correlations based on the d-separation sets, we now generate the causal hypotheses. For the causal relation r , we focus on six common causal relations between two nodes introduced in Section 2.1: Is-Parent, Is-Child, Is-Ancestor (excluding the parents), Is-Descendant (excluding the children), Has-Confounder (i.e., there exists a confounder, or common cause, of the two nodes), and Has-Collider (i.e., there exists a collider, or common effect, of the two nodes). In this way, the set of hypotheses contains all six meaningful causal relations between every pair of variables, resulting in a total size of 6 · N ( N -1) / 2 = 3 N ( N -1) hypotheses for a graph with N variables.\n\nTo generate the ground-truth validity label, we start from the correlation sets in Step 3, then look up all the causal graphs in the same MEC corresponding to the given set of correlations, and check the necessity of the hypothesized causal relation.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.5 COMPOSING THE HYPOTHESES AND LABEL",
        "chunkIndex": 25,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-26",
      "content": "art from the correlation sets in Step 3, then look up all the causal graphs in the same MEC corresponding to the given set of correlations, and check the necessity of the hypothesized causal relation. If the causal relationship proposed in the hypothesis is valid for all causal graphs within the MEC, then we generate the validity v = 1 ; otherwise, we generate v = 0 . A special case of valid samples is that when the size of the MEC is 1, then there is a bijective mapping between the causal graph and the d-separation sets, so any hypothesis stating the causal properties of that unique causal graph is valid.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.5 COMPOSING THE HYPOTHESES AND LABEL",
        "chunkIndex": 26,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-27",
      "content": "Finally, as in the last step of Figure 2, we convert all the information above to text data for our CORR2CAUSE task. For the correlation statement, we verbalize the set of correlations in Step 3 into a natural language statement s . When two variables cannot be d-separated, i.e., A ̸⊥ ⊥ B , then we describe them as ' A correlates with B ' since they are directly correlated and cannot be independent by any condition. And if two variables have a valid d-separation set C , then we describe them as ' A is independent of B given C .' In the special case when the d-separation set is empty, we directly say ' A is independent of B .' In addition, we disambiguate the setting by starting the correlation statement with the setup of a closed system of the given variables, and no hidden variables: 'Suppose there is a closed system of N variables, A, B, . . . All the statistical relations among these N variables are as follows:'.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.6 VERBALIZING INTO LANGUAGE",
        "chunkIndex": 27,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-28",
      "content": "f a closed system of the given variables, and no hidden variables: 'Suppose there is a closed system of N variables, A, B, . . . All the statistical relations among these N variables are as follows:'. Finally, to verbalize the hypothesis, we feed the causal relation triplet ( X i , r , X j )\n\n| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.6 VERBALIZING INTO LANGUAGE",
        "chunkIndex": 28,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-29",
      "content": "|\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |\n\nTable 2: Templates for each causal relation in the hypothesis. We use {Var i} and {Var j} as placeholders for the two variables.\n\ninto their hypothesis templates in Table 2. For example, we turn the triplet ( A, Is-Parent , B ) into ' A directly causes B ', as in the example of Figure 2.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.6 VERBALIZING INTO LANGUAGE",
        "chunkIndex": 29,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-30",
      "content": "We show the statistics of our CORR2CAUSE dataset in Table 3. Overall, our dataset contains 207,972 samples, where 18.57% of the samples have the positive label (i.e., with validity=1). The average length of the premise is 424.11 tokens, and hypothesis 10.83 tokens. We split the data into 205,734 training samples, 1,076 development and 1,162 test samples. 2 Since the main purpose of the dataset is to benchmark the performance of LLMs, we prioritize the test and development sets to have a comprehensive coverage over all sizes of graphs. Specifically, we iterate through the subset of our data for each N , and split it entirely for only the test and development sets if the data is less than 1K, which is the case for N = 2 and 3 . For the other subsets that are larger, we randomly sample up to 1K or 10% of the data, whichever is smaller, to the test and development sets.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 30,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-31",
      "content": "data is less than 1K, which is the case for N = 2 and 3 . For the other subsets that are larger, we randomly sample up to 1K or 10% of the data, whichever is smaller, to the test and development sets. We set the cap to be 1K in order to form a reasonable computation budget, since many LLMs are expensive to query in the inference mode. Aside from the test and valid sets, all the rest of the data goes into the training set.\n\n|                     | Overall   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   |\n|---------------------|-----------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\n|                     |           | N = 2                                 | N = 3",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 31,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-32",
      "content": "-----------------|---------------------------------------|---------------------------------------|\n|                     |           | N = 2                                 | N = 3                                 | N = 4                                 | N = 5                                 | N = 6                                 |\n| # Samples           | 207,972   | 12                                    | 90                                    | 720                                   | 8,520                                 | 198,630                               |\n| # Test              | 1,162     | 6                                     | 48                                    | 72                                    | 514                                   | 522                                   |\n| # Dev               | 1,076     | 6                                     | 42                                    | 72                                    | 482",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 32,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-33",
      "content": "|\n| # Dev               | 1,076     | 6                                     | 42                                    | 72                                    | 482                                   | 474                                   |\n| # Train             | 205,734   | 0                                     | 0                                     | 576                                   | 7,524                                 | 197,634                               |\n| # Tokens/Premise    | 424.11    | 31.5                                  | 52.0                                  | 104.0                                 | 212.61                                | 434.54                                |\n| # Tokens/Hypothesis | 10.83     | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 |\n| %Positive Labels    | 18.57     | 0.00",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 33,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-34",
      "content": "| 10.83                                 | 10.83                                 | 10.83                                 |\n| %Positive Labels    | 18.57     | 0.00                                  | 3.33                                  | 7.50                                  | 13.01                                 | 18.85                                 |\n| Vocab Size          | 65        | 49                                    | 53                                    | 55                                    | 57                                    | 61                                    |\n\nTable 3: Statistics of our CORR2CAUSE dataset, and by subsets. We report the total number of samples (# Samples); splits of the test (# Test), developement (# Dev) and training sets (# Train); number of tokens per premise (# Tokens/Premise) and hypothesis (# Tokens/Hypothesis); percentage of the positive labels (% Positive Labels), and vocabulary size by the number of uniqu",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 34,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-35",
      "content": "sets (# Train); number of tokens per premise (# Tokens/Premise) and hypothesis (# Tokens/Hypothesis); percentage of the positive labels (% Positive Labels), and vocabulary size by the number of unique tokens (Vocab Size). Note that the number of unique graphs and MECs are in Table 1.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "3.7 STATISTICS OF THE RESULTING DATA",
        "chunkIndex": 35,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-36",
      "content": "We set up a diverse list of LLMs for the experiments on our CORR2CAUSE dataset. To test existing LLMs , we first include six commonly used BERT-based NLI models in the transformers library (Wolf et al., 2020): BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), DeBERTa (He et al., 2021), DistilBERT (Sanh et al., 2019), and DistilBART (Shleifer &amp; Rush, 2020). Apart from these BERT-based NLI models, we also evaluate the general-purpose autoregressive LLMs based on GPT (Radford et al., 2019): GPT-3 Ada, Babbage, Curie, Davinci (Brown et al., 2020); its instruction-tuned versions (Ouyang et al., 2022), text-davinci-001, text-davinci-002, and text-davinci-003; and GPT-3.5 (i.e., ChatGPT), and the latest GPT-4 (OpenAI, 2023) by April 2023,\n\n2\n\nNote for our dataset v2.0: We notice that our original data (v1.0) has duplication due to symmetric relations and verbalizations of the hypothesis.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 36,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-37",
      "content": "d the latest GPT-4 (OpenAI, 2023) by April 2023,\n\n2\n\nNote for our dataset v2.0: We notice that our original data (v1.0) has duplication due to symmetric relations and verbalizations of the hypothesis. E.g., Is-Parent(A, B) has the exact hypothesis verbalization as Is-Child(B, A). Hence, for our data v2.0, we perform a careful de-duplication, and update the data statistics in Table 3. See more version comparison details in Appendix D. Note that, due to the symmetry, the current version is a random sample half of the size of the original version, so the modeling results in the experiment section roughly hold.\n\nTable 4: Overall performance. We report F1 (main metric), precision, recall and accuracy. For the main metric, F1 score, we use the bold font to highlight the overall best performance, and underline to highlight the best performance within each category of models.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 37,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-38",
      "content": "ion, recall and accuracy. For the main metric, F1 score, we use the bold font to highlight the overall best performance, and underline to highlight the best performance within each category of models.\n\n|                                   | F1    | Precision   | Recall   | Accuracy   |\n|-----------------------------------|-------|-------------|----------|------------|\n| Random Baselines                  |       |             |          |            |\n| Always Majority                   | 0.0   | 0.0         | 0.0      | 84.77      |\n| Random (Proportional)             | 13.5  | 12.53       | 14.62    | 71.46      |\n| Random (Uniform)                  | 20.38 | 15.11       | 31.29    | 62.78      |\n| BERT-Based Models                 |       |             |          |            |\n| BERT MNLI                         | 2.82  | 7.23        | 1.75     | 81.61      |\n| RoBERTa MNLI                      | 22.79 | 34.73       | 16.96    | 82.50      |\n| DeBERTa MNLI                      | 14.5",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 38,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-39",
      "content": "I                         | 2.82  | 7.23        | 1.75     | 81.61      |\n| RoBERTa MNLI                      | 22.79 | 34.73       | 16.96    | 82.50      |\n| DeBERTa MNLI                      | 14.52 | 14.71       | 14.33    | 74.31      |\n| DistilBERT MNLI                   | 20.70 | 24.12       | 18.13    | 78.85      |\n| DistilBART MNLI                   | 26.74 | 15.92       | 83.63    | 30.23      |\n| BART MNLI                         | 33.38 | 31.59       | 35.38    | 78.50      |\n| LLaMa-Based Models                |       |             |          |            |\n| LLaMa-7B                          | 26.81 | 15.50       | 99.42    | 17.36      |\n| Alpaca-7B                         | 27.37 | 15.93       | 97.37    | 21.33      |\n| GPT-Based Models                  |       |             |          |            |\n| GPT-3 Ada                         | 0.00  | 0.00        | 0.00     | 84.77      |\n| GPT-3 Babbage                     | 27.45 | 15.96       | 97.95    | 21.15      |\n|",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 39,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-40",
      "content": "|          |            |\n| GPT-3 Ada                         | 0.00  | 0.00        | 0.00     | 84.77      |\n| GPT-3 Babbage                     | 27.45 | 15.96       | 97.95    | 21.15      |\n| GPT-3 Curie                       | 26.43 | 15.23       | 100.00   | 15.23      |\n| GPT-3 Davinci                     | 27.82 | 16.57       | 86.55    | 31.61      |\n| GPT-3 Instruct (text-davinci-001) | 17.99 | 11.84       | 37.43    | 48.04      |\n| GPT-3 Instruct (text-davinci-002) | 21.87 | 13.46       | 58.19    | 36.69      |\n| GPT-3 Instruct (text-davinci-003) | 15.72 | 13.4        | 19.01    | 68.97      |\n| GPT-3.5                           | 21.69 | 17.79       | 27.78    | 69.46      |\n| GPT-4                             | 29.08 | 20.92       | 47.66    | 64.60      |\n\nusing the OpenAI API ( https://openai.com/api/ ) with temperature 0. We also evaluate the recent, more efficient models, LLaMa (Touvron et al., 2023) and Alpaca (Taori et al., 2023).",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 40,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-41",
      "content": "| 64.60      |\n\nusing the OpenAI API ( https://openai.com/api/ ) with temperature 0. We also evaluate the recent, more efficient models, LLaMa (Touvron et al., 2023) and Alpaca (Taori et al., 2023).\n\nWhen inspecting the behavior of finetuned models , we adopt a large set of models, including GPTbased models (GPT-3 Ada, Babbage, Curie, and Davinci) using the OpenAI finetuning API for classification at https://platform.openai.com/docs/guides/fine-tuning , open-sourced decoder-only models (GPT2, GPT2-Large, GPT2-XL, LLaMA-7B, and LLaMA2-7B), BERT-based models from scratch (BERT-Base, BERT-Large, RoBERTa-Base, and RoBERTa-Large), and BERTBased NLI models (BERT-Base MNLI, BERT-Large MNLI, RoBERTa-Base MNLI, and RoBERTaLarge MNLI) using the transformers library (Wolf et al., 2020). See training details in Appendix A.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 41,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-42",
      "content": "RTa-Large), and BERTBased NLI models (BERT-Base MNLI, BERT-Large MNLI, RoBERTa-Base MNLI, and RoBERTaLarge MNLI) using the transformers library (Wolf et al., 2020). See training details in Appendix A.\n\nFor the random baselines , we provide 'always majority' to predict the majority class 100% of the time, 'random (uniform)' to uniformly sample a label (i.e., 50% for each), and 'random (proportional)' to sample a label from a Bernouli distribution proportional to the development set label distribution.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.1 EXPERIMENTAL SETUP",
        "chunkIndex": 42,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-43",
      "content": "We show the performance of seventeen LLMs in Table 4. We can see that pure causal inference is a very challenging task across all existing LLMs. Among all the LLMs, the best performance is 33.38% F1 by BART MNLI, which is even higher than the latest GPT-based model, GPT-4. Notably, many models are worse than random guess, which means that they totally fail at this pure causal inference task. The observation still holds for few-shot chain-of-thought prompts tested in Appendix G.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.2 THE CORR2CAUSE SKILL IN EXISTING LLMS",
        "chunkIndex": 43,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-44",
      "content": "Next, we address the question: Can we re-purpose LLMs to learn this task? The experimental results in Table 5a of 17 models finetuned on our CORR2CAUSE seem very strong at first sight. Most models see a substantial increase, among which the finetuned BERT-based NLI models demonstrate the strongest performance. The best-performing one, RoBERTa-Large MNLI, achieves 94.74% F1 score on this task, as well as very high precision, recall and accuracy scores.\n\n|                                             | F1                                          | Precison                                    | Recall                                      | Accuracy                                    | F1 (Paraph.)                    | F1 (Var. Ref.)                  |\n|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|------------",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 44,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-45",
      "content": "---|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------|---------------------------------|\n| Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API |                                 |                                 |\n| GPT-3 Ada                                   | 79.85                                       | 70.47                                       | 92.11                                       | 92.92                                       | 61.73                           | 41.57                           |\n| GPT-3 Babbage                               | 78.19                                       | 69.98                                       | 88.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 45,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-46",
      "content": "| 41.57                           |\n| GPT-3 Babbage                               | 78.19                                       | 69.98                                       | 88.60                                       | 92.48                                       | 62.34                           | 43.28                           |\n| GPT-3 Curie                                 | 81.23                                       | 75.00                                       | 88.60                                       | 93.77                                       | 64.93                           | 45.32                           |\n| GPT-3 Davinci                               | 85.52                                       | 80.26                                       | 91.52                                       | 95.28                                       | 65.01                           | 46.96                           |\n| Finetuned Open-Sourced Decoder-Only Model",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 46,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-47",
      "content": "52                                       | 95.28                                       | 65.01                           | 46.96                           |\n| Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  |                                 |                                 |\n| GPT2                                        | 89.18                                       | 88.03                                       | 90.35                                       | 96.66                                       | 56.76                           | 31.70                           |\n| GPT2-Large                                  | 94.29                                       | 92.18                                       | 96.49                                       | 98.22                                       | 55.95",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 47,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-48",
      "content": "| 94.29                                       | 92.18                                       | 96.49                                       | 98.22                                       | 55.95                           | 31.99                           |\n| GPT2-XL                                     | 94.30                                       | 91.94                                       | 96.78                                       | 98.22                                       | 60.32                           | 43.95                           |\n| LLaMA-7B                                    | 91.98                                       | 88.62                                       | 95.61                                       | 97.46                                       | 56.41                           | 53.92                           |\n| LLaMA2-7B                                   | 92.92                                       | 90.11                                       | 95.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 48,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-49",
      "content": "| 53.92                           |\n| LLaMA2-7B                                   | 92.92                                       | 90.11                                       | 95.91                                       | 97.77                                       | 52.24                           | 49.47                           |\n| Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models     | Finetuned BERT-Based Models     |\n| BERT-Base                                   | 69.29                                       | 54.42                                       | 95.32                                       | 87.13                                       | 61.13                           | 35.20                           |\n| BERT-Large",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 49,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-50",
      "content": "32                                       | 87.13                                       | 61.13                           | 35.20                           |\n| BERT-Large                                  | 85.26                                       | 77.51                                       | 94.74                                       | 95.01                                       | 63.64                           | 38.54                           |\n| RoBERTa-Base                                | 87.60                                       | 78.47                                       | 99.12                                       | 95.73                                       | 65.58                           | 53.12                           |\n| RoBERTa-Large                               | 89.10                                       | 82.54                                       | 96.78                                       | 96.39                                       | 65.05",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 50,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-51",
      "content": "| 89.10                                       | 82.54                                       | 96.78                                       | 96.39                                       | 65.05                           | 60.20                           |\n| Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models | Finetuned BERT-Based NLI Models |\n| BERT-Base MNLI                              | 89.88                                       | 85.49                                       | 94.74                                       | 86.51                                       | 65.56                           | 31.50                           |\n| BERT-Large MNLI                             | 90.19                                       | 84.44                                       | 96.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 51,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-52",
      "content": "| 31.50                           |\n| BERT-Large MNLI                             | 90.19                                       | 84.44                                       | 96.78                                       | 96.79                                       | 67.24                           | 52.04                           |\n| RoBERTa-Base MNLI                           | 94.27                                       | 90.35                                       | 98.54                                       | 98.17                                       | 57.42                           | 62.83                           |\n| RoBERTa-Large MNLI                          | 94.74                                       | 92.24                                       | 97.37                                       | 98.35                                       | 55.45                           | 67.87                           |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 52,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-53",
      "content": "| 97.37                                       | 98.35                                       | 55.45                           | 67.87                           |\n\n(a) Performance of finetuned models on the original test set.\n\n(b) F1 scores of finetuned models on the perturbed test sets by paraphrasing (Paraph.) and variable refactorization (Var. Ref.).\n\nTable 5: Performance of finetuned models on the original test set and perturbed test sets.\n\n| Relation Type   |    F1 |   Precision |   Recall |   Accuracy |    F1 |   Precision |   Recall |   Accuracy |\n|-----------------|-------|-------------|----------|------------|-------|-------------|----------|------------|\n| Is-Parent       | 96.18 |       95.45 |    96.92 |      98.67 | 74.8  |       79.31 |    70.77 |      91.73 |\n| Is-Ancestor     | 93.94 |       93.94 |    93.94 |      98.93 | 45.45 |       90.91 |    30.3  |      93.6  |\n| Is-Child        | 95.73 |       94.92 |    96.56 |      98.67 |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 53,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-54",
      "content": "70.77 |      91.73 |\n| Is-Ancestor     | 93.94 |       93.94 |    93.94 |      98.93 | 45.45 |       90.91 |    30.3  |      93.6  |\n| Is-Child        | 95.73 |       94.92 |    96.56 |      98.67 | 73.39 |       78.43 |    68.97 |      92.27 |\n| Is-Descendant   | 96.55 |       93.33 |   100    |      99.47 | 29.41 |       83.33 |    17.86 |      93.6  |\n| Has-Collider    | 92.19 |       87.41 |    97.52 |      94.64 | 70.7  |       75    |    66.9  |      82.04 |\n| Has-Confounder  | 98.67 |       97.37 |   100    |      99.73 | 70.42 |       73.53 |    67.57 |      94.37 |\n\n(a) Fine-grained performance of RoBERTa-Large by causal relation type on the original test set.\n\n(b) Its fine-grained performance by relation type after variable refactorization.\n\nTable 6: Fine-grained analysis of the best-performing model, RoBERTa-Large MNLI.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.3 FINETUNED PERFORMANCE",
        "chunkIndex": 54,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-55",
      "content": "In addition to the overall results mentioned above, we conduct a fine-grained analysis to check the performance of the strongest finetuned model, RoBERTa-Large MNLI, by our six causal relation types. As in Table 6a, the model is very good at judging relations such as Is-Parent, Is-Descendant and Has-Confounder, all with more than 96% F1 scores, whereas it is several points weaker on the Has-Collider relations. This could be due to that the collider relation is the most special type, requiring identification of the V-structure based on both the unconditional independence based on the two variables only and correlations whenever conditioned on a common descendant. We also conduct error analysis for non-finetuned models in Appendix F.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.4 FINE-GRAINED PERFORMANCE BY CAUSAL RELATION",
        "chunkIndex": 55,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-56",
      "content": "Looking at the very high performance of the finetuned models, we raise the next question: Did the models really robustly learn the causal inference skills?\n\nTwo Robustness Tests We design two simple robustness tests: (1) paraphrasing, and (2) variable refactorization. For (1) paraphrasing, we simply paraphrase the hypothesis by changing the text template for each causal relation to some semantically-equivalent alternatives in Appendix C. For (2) variable refactorization, we reverse the alphabet of the variable names, namely flipping A, B, C, to Z, Y, X and so on. The inspiration behind the two robustness tests comes from the spurious correlation analysis described in Appendix E.\n\nSpecifically, we adopt the common setup of text adversarial attack (Morris et al., 2020; Jin et al., 2020) to preserve the training set and keep the same saved models, but run the inference on the perturbed test set.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.5 ROBUSTNESS ANALYSIS",
        "chunkIndex": 56,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-57",
      "content": "adopt the common setup of text adversarial attack (Morris et al., 2020; Jin et al., 2020) to preserve the training set and keep the same saved models, but run the inference on the perturbed test set. In this way, we separate the possibilities of the models only overfitting on the training data vs. mastering the reasoning skills.\n\nResults after Perturbation We can see from Table 5b that all the models drop drastically, by up to 39.29 on the paraphrased test set, and up to 62.30 after variable refactorization. The best-performing model, RoBERTa-Large MNLI, is especially sensitive towards paraphrasing, demonstrating the most drop among all models; however, it is the most robust against the variable refactorization, maintaining a high F1 score of 67.87. We conduct fine-grained analysis for RoBERTa-Large MNLI under perturbation in Table 6b.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.5 ROBUSTNESS ANALYSIS",
        "chunkIndex": 57,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-58",
      "content": "dels; however, it is the most robust against the variable refactorization, maintaining a high F1 score of 67.87. We conduct fine-grained analysis for RoBERTa-Large MNLI under perturbation in Table 6b. We can see the the main source of the performance drop of the model comes from the two classes, Is-Ancestor (decreasing to 45.45%) and Is-Descendant (decreasing to 29.41%), while the other classes stay relatively robust, keeping their F1 scores over 70%.\n\nFrom this analysis, we make the following suggestions to future studies testing this CORR2CAUSE skill of LLMs. First, it is safe to use it as a test set to benchmark existing LLMs' performance, since the data we generate is out-of-distribution from the training data of the current LLMs. Then, when testing finetuned models, it is very important to accompany adversarial attack together with the i.i.d. test set. We open-source our perturbed test sets for future work to test the generalizability skill.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.5 ROBUSTNESS ANALYSIS",
        "chunkIndex": 58,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-59",
      "content": "nt to accompany adversarial attack together with the i.i.d. test set. We open-source our perturbed test sets for future work to test the generalizability skill.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.5 ROBUSTNESS ANALYSIS",
        "chunkIndex": 59,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-60",
      "content": "We envision our CORR2CAUSE dataset to be a foundation for future extensions to various settings, such as instantiating the variables with actual phenomena and situating the story in a more natural setting. For example, the correlation does not imply causation rule can be instantiated with the ice cream sales and swimming pool attendance as the two variables, and argue that ice cream sales does not necessarily affect swimming pool attendance, because their correlation could be due to a third variable, such as hot weather. We provide a case study for how to instantiate the symbolic expressions in our dataset to more natural stories, and find that LLMs such as GPT-4 can generate realistic, daily life stories that has foreseeably broad applications. See more details in Appendix B.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "4.6 EXTENSION TO NATURAL STORIES",
        "chunkIndex": 60,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-61",
      "content": "Existing Causal Reasoning Tasks A large body of existing research of causal reasoning in NLP focuses on leveraging empirical knowledge to do tasks such as inferring the cause and effect of why an agent perform certain tasks (Sap et al., 2019a), the motivation and emotional reaction in a social context (Sap et al., 2019b), how people achieve a given goal with a set of concrete steps (Zhang et al., 2020), the development of a story given a different beginning (Qin et al., 2019), and how in general LLMs serve as a knowledge base of cause and effect (Willig et al., 2023; Kıcıman et al., 2023). In contrast, our CORR2CAUSE task focuses on the pure causal inference skill of models, which is a knowledge-dependent reasoning skill based on formally correct rules from causal inference.\n\nExisting Logical and Inference Tasks Another related area of literature is logical and inference tasks, of which a well-established one is natural language inference (NLI), to identify the semantic relationship be",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 61,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-62",
      "content": "cal and Inference Tasks Another related area of literature is logical and inference tasks, of which a well-established one is natural language inference (NLI), to identify the semantic relationship between a pair of sentences (MacCartney &amp; Manning, 2008; Bowman et al., 2015). NLI datasets mainly focus on the set and paraphrase relations. For example, 'a group of boys are playing football' can entail 'some guys are playing football,' where 'boys' are a sub-concept of 'guys,' and 'a group of' and 'some' are paraphrases. Recently, there have been increasing efforts to extend the inference task to various logical inference skills such as deductive logic and propaganda techniques (Jin et al., 2022; Alhindi et al., 2022). Our CORR2CAUSE dataset is the first dataset testing the correlation-to-causation inference skill, which is unique of its type.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "5 RELATED WORK",
        "chunkIndex": 62,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-63",
      "content": "In this work, we introduced a novel task, CORR2CAUSE, to infer causation from correlation, and collected a large-scale dataset of over 200K samples. We evaluated an extensive list of LLMs on this new task, and showed that off-the-shelf LLMs perform poorly on this task. We also show that it is possible to re-purpose LLMs on this task by finetuning, but future work needs to be aware of the out-of-distribution generalization problem. To avoid the Goodhart's law, we recommend using this dataset to benchmark the pure causal inference skills for LLMs that have not seen this dataset. Given the limited reasoning abilities of current LLMs, and the difficulty of separating actual reasoning from training-corpus-derived knowledge, it is imperative that our community focus on work aiming to accurately disentangle and measure both abilities. We believe the present work is a first such step.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "6 CONCLUSION",
        "chunkIndex": 63,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-64",
      "content": "We identify several limitations of this work and open future directions: First, in the context of this work, we limit the causal graphs to two to six nodes, but future work can feel free to explore larger graphs. Another aspect is that we do not assume hidden confounders in this inference problem, so we welcome future work to generate an even more challenging dataset to infer the existence of hidden confounders, analogous to the causal discovery algorithm of fast causal inference (FCI) (Spirtes et al., 2000). And also in general, explorations of other causal discovery algorithms are welcomed too. Finally, a lot of motivation behind proposing this task is inspired by the problem of invalid reasoning patterns in our daily reasoning (Jin et al., 2022), which could fertilize the ground for more pervasive spread of fake news.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "LIMITATIONS AND FUTURE WORK",
        "chunkIndex": 64,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-65",
      "content": "behind proposing this task is inspired by the problem of invalid reasoning patterns in our daily reasoning (Jin et al., 2022), which could fertilize the ground for more pervasive spread of fake news. We believe false causal inference is a prevalent type of fallacious beliefs, and welcome future work to connect the idea of this benchmark to more real-world false beliefs based on confusing correlation with causation.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "LIMITATIONS AND FUTURE WORK",
        "chunkIndex": 65,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-66",
      "content": "We thank Riley Goodside for valuable suggestions to improve our prompts to LLMs. We thank Luigi Gresele and Amir Hossein Karimi for their suggestions to help us improve the formulation of our causal discovery questions.\n\nThis material is based in part upon work supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645; by a National Science Foundation award (#2306372); by a Swiss National Science Foundation award (#201009) and a Responsible AI grant by the Haslerstiftung. Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy. We also thank OpenAI for granting Zhijing quota to their API of GPT series through the Researcher Access Program.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "ACKNOWLEDGMENT",
        "chunkIndex": 66,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-67",
      "content": "- Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, and Smaranda Muresan. Multitask instruction-based prompting for fallacy recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 8172-8187, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main.560 . 9\n- Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= Byg1v1HKDB . 1\n- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 67,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-68",
      "content": "her Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //aclanthology.org/D15-1075 . 9\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 68,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-69",
      "content": "s, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf . 6\n- David Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn. Res. , 3:507-554, 2002. URL http://jmlr.org/papers/v3/chickering02b.html . 3\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171-4186, Minneapolis, Minnesota, June 2019.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 69,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-70",
      "content": "erence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423 . 1, 6\n- Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in Genetics , 10:524, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019. 00524. URL https://www.frontiersin.org/article/10.3389/fgene.2019. 00524 . 2, 3\n- Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer . John Wiley and Sons, 2016. 2, 3\n- Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 70,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-71",
      "content": "primer . John Wiley and Sons, 2016. 2, 3\n- Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pp. 394-398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052 . 1\n- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced Bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:// openreview.net/forum?id=XPZIaotutsD . 6\n- Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Schölkopf.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 71,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-72",
      "content": "ual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:// openreview.net/forum?id=XPZIaotutsD . 6\n- Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Léon Bottou (eds.), Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008 , pp. 689-696. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/hash/ f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html . 3\n- Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 72,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-73",
      "content": "0bcedc94a4b6-Abstract.html . 3\n- Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 80188025. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/ view/6311 . 9\n- Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. Logical fallacy detection. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 7180Ã¢â,¬âCœ-7198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://arxiv.org/abs/2202.13758 .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 73,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-74",
      "content": "ion for Computational Linguistics: EMNLP 2022 , pp. 7180Ã¢â,¬âCœ-7198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://arxiv.org/abs/2202.13758 . 9, 10\n- Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050 , 2023. 1, 9\n- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703 .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 74,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-75",
      "content": "iation for Computational Linguistics , pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703 . 6\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692 . 6\n\n- Bill MacCartney and Christopher D. Manning. Modeling semantic containment and exclusion in natural language inference. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008) , pp. 521-528, Manchester, UK, August 2008. Coling 2008 Organizing Committee. URL https://aclanthology.org/C08-1066 . 9\n- Brendan D. McKay and Adolfo Piperno. Practical graph isomorphism, II. J. Symb. Comput. , 60: 94-112, 2014. doi: 10.1016/j.jsc.2013.09.003. URL https://doi.org/10.1016/j.jsc. 2013.09.003 .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 75,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-76",
      "content": "8-1066 . 9\n- Brendan D. McKay and Adolfo Piperno. Practical graph isomorphism, II. J. Symb. Comput. , 60: 94-112, 2014. doi: 10.1016/j.jsc.2013.09.003. URL https://doi.org/10.1016/j.jsc. 2013.09.003 . 5\n- John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 119-126, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-demos.16. URL https://aclanthology.org/2020.emnlp-demos.16 . 9\n- OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774 . 1, 6\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 76,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-77",
      "content": "I. GPT-4 technical report. CoRR , abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774 . 1, 6\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR , abs/2203.02155, 2022. doi: 10.48550/arXiv.2203.02155. URL https://doi.org/10. 48550/arXiv.2203.02155 . 1, 6\n- Judea Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference . Morgan Kaufmann, 1988. 3\n- Judea Pearl. Causality: Models, reasoning and inference (2nd ed.) . Cambridge University Press, 2009. 1\n- Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: Foundations and learning algorithms . The MIT Press, 2017.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 77,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-78",
      "content": "erence (2nd ed.) . Cambridge University Press, 2009. 1\n- Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: Foundations and learning algorithms . The MIT Press, 2017. URL https://mitpress.mit.edu/ books/elements-causal-inference . 1\n- Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi. Counterfactual story reasoning and generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 5043-5053, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1509. URL https: //aclanthology.org/D19-1509 . 1, 9\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog , 1(8), 2019. 1, 6\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 78,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-79",
      "content": "hild, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog , 1(8), 2019. 1, 6\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. CoRR , abs/1910.01108, 2019. URL http: //arxiv.org/abs/1910.01108 . 6\n- Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. ATOMIC: an atlas of machine commonsense for if-then reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pp. 3027-3035. AAAI Press, 2019a. doi: 10.1609/aaai.v33i01.33013027. URL https://doi.org/10.1609/aaai.v33i01. 33013027 .",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 79,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-80",
      "content": "Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pp. 3027-3035. AAAI Press, 2019a. doi: 10.1609/aaai.v33i01.33013027. URL https://doi.org/10.1609/aaai.v33i01. 33013027 . 1, 9\n- Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In EMNLP 2019 , 2019b. 1, 9\n\n- Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti J. Kerminen. A linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res. , 7:2003-2030, 2006. URL http: //jmlr.org/papers/v7/shimizu06a.html . 3\n- Sam Shleifer and Alexander M. Rush. Pre-trained summarization distillation. CoRR , abs/2010.13002, 2020. URL https://arxiv.org/abs/2010.13002 . 6\n- Peter Spirtes and Kun Zhang. Causal discovery and inference: Concepts and recent methodological advances. In Applied informatics , volume 3, pp. 1-28. SpringerOpen, 2016. 2, 3\n- Peter Spirtes, Clark Glymour, and Richard Scheines.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 80,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-81",
      "content": "Causal discovery and inference: Concepts and recent methodological advances. In Applied informatics , volume 3, pp. 1-28. SpringerOpen, 2016. 2, 3\n- Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. 1993. 2, 3\n- Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search, Second Edition . Adaptive computation and machine learning. MIT Press, 2000. ISBN 978-0-262-19440-2. 1, 2, 3, 10\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023. 7\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 81,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-82",
      "content": "imothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR , abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi. org/10.48550/arXiv.2302.13971 . 7\n- Ruibo Tu, Chao Ma, and Cheng Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. arXiv preprint arXiv:2301.13819 , 2023. 1\n- Moritz Willig, Matej Zeˇ cevi´ c, Devendra Singh Dhami, and Kristian Kersting. Probing for correlations of causal facts: Large language models and causality, 2023. URL https://openreview. net/forum?id=UPwzqPOs4. 9\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 82,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-83",
      "content": "c, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6 . 6, 7, 14\n- Yuxi Xie, Guanzhen Li, and Min-Yen Kan. Echo: Event causality inference via human-centric reasoning. arXiv preprint arXiv:2305.14740 , 2023. 1\n- Matej Zeˇ cevi´ c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. arXiv preprint arXiv:2308.13067 , 2023. 1\n- Kun Zhang and Aapo Hyvärinen.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 83,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-84",
      "content": "lig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. arXiv preprint arXiv:2308.13067 , 2023. 1\n- Kun Zhang and Aapo Hyvärinen. Causality discovery with additive disturbances: An informationtheoretical perspective. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20 , pp. 570-585. Springer, 2009. 3\n- Li Zhang, Qing Lyu, and Chris Callison-Burch. Reasoning about goals, steps, and temporal ordering with WikiHow. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4630-4639, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.374. URL https://aclanthology.org/ 2020.emnlp-main.374 . 9\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 84,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-85",
      "content": "i: 10.18653/v1/2020.emnlp-main.374. URL https://aclanthology.org/ 2020.emnlp-main.374 . 9\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR , abs/2205.01068, 2022. doi: 10.48550/ arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068 . 1",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "REFERENCES",
        "chunkIndex": 85,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-86",
      "content": "When finetuning on our data, for GPT-based models, we use the default settings of the OpenAI finetuning API; and for BERT-based models, we use the transformers library (Wolf et al., 2020) and train the models on a server with an NVIDIA Tesla A100 GPU with 40G of memory. To fit for the GPU memory, we set the batch size to be 8. We use the validation set to tune the learning rate, which takes value in {2e-6, 5e-6, 1e-5, 2e-5, 5e-5}; dropout rate, which takes value in {0, 0.1, 0.2, 0.3}; and weight decay, which takes value in {1e-4, 1e-5}. We train the models until convergence, which is usually around ten epochs.\n\nPrompts When querying the autoregressive LLMs, we formulate the prompt as follows:\n\nQuestion: [premise]\n\nCan we deduct the following: [hypothesis] ? Just answer \"Yes\" or \"No.\"\n\nAnswer:",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "A IMPLEMENTATION DETAILS",
        "chunkIndex": 86,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-87",
      "content": "To generate the natural stories based on our symbolic expressions, we utilize the state-of-the-art LLM, GPT-4, which is very good at story generation. We design detailed instructions in the prompt, and generate around 200 stories in our case study. We show two examples stories in Table 7, and the report the overall statistics in Table 8.\n\n|               | Example 1 (Label=Negative)",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 87,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-88",
      "content": "| Example 2 (Label=Positive)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|---------------|--",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 88,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-89",
      "content": "|\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 89,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-90",
      "content": "------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Symbolic Form | Premise: Suppose there is a closed system of 2 variables, A and B. All the statistical relations among these 2 variables are as fol- lows: A correlates with B.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 90,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-91",
      "content": "---------------------|\n| Symbolic Form | Premise: Suppose there is a closed system of 2 variables, A and B. All the statistical relations among these 2 variables are as fol- lows: A correlates with B. Hypothesis: A directly affects B. Relation between the promise and hypothe- sis: The premise does not a necessary con- dition for the hypothesis.                                                                                                                                                                                                                                                                                                                                                                                                                                | Premise: Suppose there is a closed system of 3 variables, A, B and C. All the statisti- cal relations among these 3 variables are as follows: A correlates with C. B correlates with C. However, A is independent of B.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 91,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-92",
      "content": "e there is a closed system of 3 variables, A, B and C. All the statisti- cal relations among these 3 variables are as follows: A correlates with C. B correlates with C. However, A is independent of B. Hypothesis: A directly affects C. Relation between the promise and hypoth- esis: The premise is a necessary condition for the hypothesis. So if the premise is true, the hypothesis must be true.                                                                                                                                                                                                                                                                                                |\n| Natural Story | Premise: Suppose there is a closed system of 2 variables, ice cream sales and swim- ming pool attendance. All the statistical relations among these 2 variables are as fol- lows: ice cream sales correlate with swim- ming pool attendance.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 92,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-93",
      "content": "ystem of 2 variables, ice cream sales and swim- ming pool attendance. All the statistical relations among these 2 variables are as fol- lows: ice cream sales correlate with swim- ming pool attendance. Hypothesis: Ice cream sales directly affect swimming pool attendance. Relation between the premise and hypoth- esis: The premise does not provide a nec- essary condition for the hypothesis. The correlation between ice cream sales and swimming pool attendance could be due to a third variable, such as hot weather, which increases both ice cream sales and swim- ming pool attendance. Therefore, it is not necessarily true that ice cream sales directly affect swimming pool attendance. | Premise: Let's consider three factors: eat- ing junk food (A), obesity (C), and watch- ing television (B). There is a correlation between eating junk food and obesity, and between watching television and obesity. However, eating junk food and watching television are independent from each other.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 93,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-94",
      "content": "vision (B). There is a correlation between eating junk food and obesity, and between watching television and obesity. However, eating junk food and watching television are independent from each other. Hypothesis: Eating junk food directly af- fects obesity. Relation between the premise and hypoth- esis: The premise provides the necessary conditions for the hypothesis. It establishes the independent variables A (eating junk food) and B (watching television) and their correlations with obesity. Given that these are true, it supports the hypothesis that eat- ing junk food directly affects obesity. |\n\nTable 7: Examples of natural stories generated based on the symbolic form in our CORR2CAUSE dataset, showing the broad application value of our dataset as the starting point for various verbalizations of the correlation-to-causation inference task.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 94,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-95",
      "content": "ased on the symbolic form in our CORR2CAUSE dataset, showing the broad application value of our dataset as the starting point for various verbalizations of the correlation-to-causation inference task.\n\nTable 8: Statistics of our generated natural stories. We report the number of samples in the test and development sets; number of tokens per premise (# Tokens/Premise), hypothesis (# Tokens/Hypothesis), and explanation (# Tokens/Explanation); and percentage of the positive labels (% Positive Labels).\n\n| Test Set Size        |    102 |\n|----------------------|--------|\n| Dev Set Size         | 102    |\n| # Tokens/Premise     |  64.88 |\n| # Tokens/Hypothesis  |  13.54 |\n| # Tokens/Explanation |  64.66 |\n| %Positive Labels     |   1.67 |\n\nFor more information, the exact prompt we use is ' Here is a causal inference rule: [symbolic form] Please provide a real-world example instantiating this phenomenon.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 95,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-96",
      "content": "%Positive Labels     |   1.67 |\n\nFor more information, the exact prompt we use is ' Here is a causal inference rule: [symbolic form] Please provide a real-world example instantiating this phenomenon. Format it also as \"Premise:\", \"Hypothesis:\", and \"Relation between the promise and hypothesis:\". '",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "B GENERATING NATURAL STORIES",
        "chunkIndex": 96,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-97",
      "content": "We use the verbalization templates in Table 9 to compose the hypotheses for all six causal relations.\n\nTable 9: Templates and their paraphrases for each causal relation in the hypothesis. We use {Var i} and {Var j} as placeholders for the two variables.\n\n| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "C TEMPLATES AND PARAPHRASES",
        "chunkIndex": 97,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-98",
      "content": "|\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |\n| Paraphrases       |                                                                                    |\n| Is-Parent         | {Var i} directly affects {Var j} .                                                 |\n| Is-Ancestor       | {Var i} influences {Var j} through some mediator(s).                               |\n| Is-Child          | {Var j} directly affects {Var i} .                                                 |\n| Is-Descendant     | {Var j} influences {Var i} through some mediator(s).",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "C TEMPLATES AND PARAPHRASES",
        "chunkIndex": 98,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-99",
      "content": "|\n| Is-Child          | {Var j} directly affects {Var i} .                                                 |\n| Is-Descendant     | {Var j} influences {Var i} through some mediator(s).                               |\n| Has-Collider      | {Var i} and {Var j} together cause some other variable(s).                         |\n| Has-Confounder    | Some variable(s) cause(s) both {Var i} and {Var j} .                               |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "C TEMPLATES AND PARAPHRASES",
        "chunkIndex": 99,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-100",
      "content": "Table 10: De-duplication methods for the six causal relation types and their verbalizations.\n\n| Two Equivalent Forms                                                      | Duplication Property                               | De-Duplication Method              |\n|---------------------------------------------------------------------------|----------------------------------------------------|------------------------------------|\n| ß Is-Parent( i , j ) Is-Child( j , i )                                    | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Is-Ancestor( i , j ) (Original) Is-Descendent( j , i ) (Original)       | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Is-Ancestor( i , j ) (Paraphrased) Is-Descendent( j , i ) (Paraphrased) | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Has-Collider( i , j ) Has-Collider( j , i )",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "D CHANGE LOG FOR THE DATASET VERSION UPDATE",
        "chunkIndex": 100,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-101",
      "content": "sed) Is-Descendent( j , i ) (Paraphrased) | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Has-Collider( i , j ) Has-Collider( j , i )                             | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Has-Confounder( i , j ) Has-Confounder( j , i )                         | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n\nDe-Duplication Strategy As mentioned in Section 3.7 in the main paper, our original dataset (v1.0) has duplication due to symmetric relations and verbalizations. We introduce in Table 10 several reasons for why duplicated hypotheses exist in our original data. One typical reason is symmetric relations such as Is-Parent(A, B) and Is-Child(B, A), and, similarly, the paraphrased version of",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "D CHANGE LOG FOR THE DATASET VERSION UPDATE",
        "chunkIndex": 101,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-102",
      "content": "veral reasons for why duplicated hypotheses exist in our original data. One typical reason is symmetric relations such as Is-Parent(A, B) and Is-Child(B, A), and, similarly, the paraphrased version of\n\nIs-Ancestor(A, B) and Is-Descendent(B, A). Another typical reason is the semantic equivalence in the verbalization templates, which applies to the Has-Collider and Has-Confounder relations. For example, the verbalized texts of Has-Collider(A, B) and Collider(B, A) are 'There exists at least one collider (i.e., common effect) of {A and B, B and A},' respectively, which are semantically-equivalent paraphrases of each other, so we randomly keep one out of the two.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "D CHANGE LOG FOR THE DATASET VERSION UPDATE",
        "chunkIndex": 102,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-103",
      "content": "Since the reason for duplication in the first place is due to symmetry in the causal relation, or verbalization, the resulting new data, CORR2CAUSE v2.0, is exactly a half of the original data. As we reported previously in Table 3 of Section 3.7, the total number of samples cuts down to half, while the label distribution and all other properties are the same. To compose each split, we apply the same de-duplication method for the test, train, and development sets. We notice that some duplicates are across the splits, so we prioritize keeping the test and training sets untouched (to minimally affect the experimental results), and then reduce the development set by removing the cross-split duplicates, namely:\n\n- test\\_2.0 = deduplicate(test\\_1.0)\n- train\\_2.0 = deduplicate(train\\_1.0)\n- dev\\_2.0 = deduplicate(dev\\_1.0) \\ {test\\_2.0, train\\_2.0}",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "Resulting Dataset Statistics after De-Duplication",
        "chunkIndex": 103,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-104",
      "content": "evelopment set by removing the cross-split duplicates, namely:\n\n- test\\_2.0 = deduplicate(test\\_1.0)\n- train\\_2.0 = deduplicate(train\\_1.0)\n- dev\\_2.0 = deduplicate(dev\\_1.0) \\ {test\\_2.0, train\\_2.0}\n\nWe expect minimal or almost no change to the experimental results. In case of the slight possibility that this change in the development set might affect the model selection in the training process, future work can feel free to re-train the models and update the exact performance number.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "Resulting Dataset Statistics after De-Duplication",
        "chunkIndex": 104,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-105",
      "content": "The inspirations of our two robustness tests (paraphrasing and variable refactorization) come from our data analysis. We check for spurious correlations in the data by reporting in Table 11 the point-wise mutual information (PMI) between the label and any n-gram with no more than four tokens. In addition, we also report the difference of the PMI with the two labels in the | Diff | column of Table 11, and report the top 10 n-grams.\n\nThe design spirit for our robustness test is that if the models' correct judgment relies on exploiting these spurious correlations, then such reliance will be broken in our perturbations.\n\nTable 11: PMI between the labels and n-grams. The labels include non-entailment (Non-Ent.) and entailment (Ent.). And the n-grams include all with no more than four words. The | Diff | column shows the absolute value of the difference between the PMIs with two labels.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "E SPURIOUS CORRELATION ANALYSIS",
        "chunkIndex": 105,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-106",
      "content": "-entailment (Non-Ent.) and entailment (Ent.). And the n-grams include all with no more than four words. The | Diff | column shows the absolute value of the difference between the PMIs with two labels. We show the top 10 n-grams with the largest differences of their PMIs with the two classes in the | Diff | column.\n\n| N-Gram             |   PMI w/ Non-Ent. Label |   PMI w/ Ent. Label |   | Diff | |\n|--------------------|-------------------------|---------------------|------------|\n| a cause            |                 1.69221 |           -1.02561  |    2.71782 |\n| a cause for        |                 1.66364 |           -0.98379  |    2.64743 |\n| A causes           |                 1.64068 |           -0.95161  |    2.59229 |\n| A causes something |                 1.62182 |           -0.926075 |    2.5479  |\n| a direct           |                 1.60605 |           -0.905316 |    2.51137 |\n| a direct one       |                 1.59267 |           -0.888107 |    2.48078 |\n| for D",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "E SPURIOUS CORRELATION ANALYSIS",
        "chunkIndex": 106,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-107",
      "content": "26075 |    2.5479  |\n| a direct           |                 1.60605 |           -0.905316 |    2.51137 |\n| a direct one       |                 1.59267 |           -0.888107 |    2.48078 |\n| for D              |                 1.58483 |           -0.87818  |    2.46301 |\n| for D but          |                 1.5839  |           -0.877014 |    2.46091 |\n| for E              |                 1.58298 |           -0.875864 |    2.45884 |\n| for E but          |                 1.58207 |           -0.874728 |    2.4568  |\n\nWe can see that some spurious correlations are rooted in the framing of the hypothesis, such as 'a cause (for)', and 'a direct (one)' (which we use the paraphrasing task to break), and others are connected to the variable names, such as 'for D (but)' and 'for E (but)' (which we use the variable refactorization to break).",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "E SPURIOUS CORRELATION ANALYSIS",
        "chunkIndex": 107,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-108",
      "content": "In addition to the fine-grained analysis by causal relation type in Table 6a for fine-tuned models, we also report such error analysis for non-finetuned models in Table 12.\n\nTable 12: Fine-grained evaluation results for some selected non-fine-tuned models.\n\n| Selected Models   | Relation Type   |    F1 |   Precision |   Recall |   Accuracy |\n|-------------------|-----------------|-------|-------------|----------|------------|\n| GPT-3.5           | All             | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5           | Is-Parent       |  8.82 |      100    |     4.62 |      83.47 |\n| GPT-3.5           | Is-Ancestor     |  0    |        0    |     0    |      90.67 |\n| GPT-3.5           | Is-Child        |  9.84 |      100    |     5.17 |      85.33 |\n| GPT-3.5           | Is-Descendant   | 14.29 |       11.9  |    17.86 |      84    |\n| GPT-3.5           | Has-Collider    | 34.24 |       25.51 |    52.07 |      35.12 |\n| GPT-3.5           | Has-Confounder  | 15.33 |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "F FINE-GRAINED ERROR ANALYSIS",
        "chunkIndex": 108,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-109",
      "content": "escendant   | 14.29 |       11.9  |    17.86 |      84    |\n| GPT-3.5           | Has-Collider    | 34.24 |       25.51 |    52.07 |      35.12 |\n| GPT-3.5           | Has-Confounder  | 15.33 |        8.86 |    56.76 |      37.8  |\n| GPT-4             | All             | 29.08 |       20.92 |    47.66 |      64.6  |\n| GPT-4             | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| GPT-4             | Is-Ancestor     | 30.77 |       31.25 |    30.3  |      88    |\n| GPT-4             | Is-Child        |  0    |        0    |     0    |      84.53 |\n| GPT-4             | Is-Descendant   | 26.98 |       17.35 |    60.71 |      75.47 |\n| GPT-4             | Has-Collider    | 44.1  |       30.18 |    81.82 |      32.71 |\n| GPT-4             | Has-Confounder  | 20.67 |       11.53 |   100    |      23.86 |\n| RoBERTa MNLI      | All             | 22.79 |       34.73 |    16.96 |      82.5  |\n| RoBERTa MNLI      | Is-Parent       |  0    |        0    |     0    |      82",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "F FINE-GRAINED ERROR ANALYSIS",
        "chunkIndex": 109,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-110",
      "content": "11.53 |   100    |      23.86 |\n| RoBERTa MNLI      | All             | 22.79 |       34.73 |    16.96 |      82.5  |\n| RoBERTa MNLI      | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| RoBERTa MNLI      | Is-Ancestor     |  0    |        0    |     0    |      91.2  |\n| RoBERTa MNLI      | Is-Child        |  0    |        0    |     0    |      84.53 |\n| RoBERTa MNLI      | Is-Descendant   |  0    |        0    |     0    |      92.53 |\n| RoBERTa MNLI      | Has-Collider    | 43.45 |       39.73 |    47.93 |      59.52 |\n| RoBERTa MNLI      | Has-Confounder  |  0    |        0    |     0    |      84.45 |\n\nThese results are particularly revealing, showing how off-the-shelf models perform in recognizing specific relations. Specifically, GPT-3.5 cannot recognize ancestor relations, whereas GPT-4 fails at all direct causation recognition with parents and children. And RoBERTa MNLI only did collider relation relatively correctly.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "F FINE-GRAINED ERROR ANALYSIS",
        "chunkIndex": 110,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-111",
      "content": "cally, GPT-3.5 cannot recognize ancestor relations, whereas GPT-4 fails at all direct causation recognition with parents and children. And RoBERTa MNLI only did collider relation relatively correctly. Note that, when the F1 score is zero, the accuracy number is a result of always predicting the negative class of that relation.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "F FINE-GRAINED ERROR ANALYSIS",
        "chunkIndex": 111,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-112",
      "content": "Since our experiments in Section 4.2 are based on plain, zero-shot prompts, we explore whether better prompting strategies could improve the performance. We enhance the query prompt by incorporating several strategies: (1) Utilizing a system prompt that specifies the model's expertise ('You are a highly intelligent question-answering bot with profound knowledge of causal inference.'); (2) Including a pair of few-shot examples, one positive and one negative; (3) Implementing chain-of-thought prompting with 'Let's think step by step.' to encourage the language model to generate step-by-step reasoning. In Table 13, we present the evaluation results on the relatively affordable model, GPT-3.5, where the optimized prompt leads to a 4-point improvement in F1 over the original performance. However, we can see that despite the deployment of all three strategies, the model continues to struggle with this challenging task.",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "G LLM PERFORMANCE OPTIMIZATION",
        "chunkIndex": 112,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.05836v3-chunk-113",
      "content": "leads to a 4-point improvement in F1 over the original performance. However, we can see that despite the deployment of all three strategies, the model continues to struggle with this challenging task.\n\nTable 13: Performance of GPT-3.5 with different queries. We quote the original performance from Table 4.\n\n|                                 |    F1 |   Precision |   Recall |   Accuracy |\n|---------------------------------|-------|-------------|----------|------------|\n| GPT-3.5 (plain query; original) | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5 (enhanced query)        | 25.44 |       17.29 |    48.11 |      52.01 |",
      "metadata": {
        "source": "arxiv:2306.05836v3",
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
          "Zhijing Jin",
          "Jiarui Liu",
          "Zhiheng Lyu",
          "Spencer Poff",
          "Mrinmaya Sachan",
          "Rada Mihalcea",
          "Mona Diab",
          "Bernhard Schölkopf"
        ],
        "section": "G LLM PERFORMANCE OPTIMIZATION",
        "chunkIndex": 113,
        "totalChunks": 114
      }
    }
  ],
  "fullText": "## CAN LARGE LANGUAGE MODELS INFER CAUSATION FROM CORRELATION?\n\nZhijing Jin 1,2, ∗ , ‡ Jiarui Liu 3, ∗ Zhiheng Lyu 4 Spencer Poff 5 Mrinmaya Sachan 2 Rada Mihalcea 6 Mona Diab 3, ‡ , † Bernhard Schölkopf 1, † 1 Max Planck Institute for Intelligent Systems, Tübingen, Germany 2 ETH Zürich 3 LTI, CMU 4 University of Hong Kong 5 Meta AI 6 University of Michigan jinzhi@ethz.ch jiarui@cmu.edu zhihenglyu.cs@gmail.com\n\n## ABSTRACT\n\nCausal inference is one of the hallmarks of human intelligence. While the field of Causal NLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task CORR2CAUSE, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize - they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. CORR2CAUSE is a challenging task for LLMs, and can be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. 1\n\n## 1 INTRODUCTION\n\nCausal inference, i.e., the ability to establish the correct causal relationships between variables or events, is fundamental to human intelligence. There are two distinct ways this causal inference capability can be acquired: one through empirical knowledge, e.g., we know from common sense that touching a hot stove will get us burned; the other through pure causal reasoning , as causality can be formally argued and reasoned about using known procedures and rules from causal inference (Spirtes et al., 2000; Pearl, 2009; Peters et al., 2017). One example is that we have the a priori knowledge that the correlation between A and B does not necessarily imply causality. This is a formal rule that holds true regardless of the realizations of the variables A and B.\n\nWith the rise of large language models (LLMs) (Radford et al., 2019; Devlin et al., 2019; Ouyang et al., 2022; Zhang et al., 2022; OpenAI, 2023, inter alia ), a crucial research question is whether they can do causal reasoning well. Recent studies have pointed out that LLMs are 'causal parrots,' which recite the causal knowledge in the training data (Zeˇ cevi´ c et al., 2023). Moreover, the vast majority of studies frame causal reasoning as a skill to navigate around empirical knowledge (Gordon et al., 2012; Sap et al., 2019a;b; Qin et al., 2019; Bhagavatula et al., 2020), and also treat LLMs as a knowledge base when evaluating its causal skills (Kıcıman et al., 2023; Tu et al., 2023; Xie et al., 2023). However, all the above lines of research frame causality as empirical knowledge, thus relying heavily on the quality and the coverage of the training data, overlooking the great potential of the formal causal reasoning skills to process correlational information to causal conclusions.\n\n∗ Equal contribution. † Equal supervision. ‡ Work originated as a Meta AI internship project involving Zhijing, Mona, and Spencer.\n\n1 Our data is at https://huggingface.co/datasets/causalnlp/corr2cause . Our code is at https://github.com/causalNLP/corr2cause .\n\n<!-- image -->\n\n*Assumption that we explicitly mention in the samples: We suppose a close system of the given variables and correlations.\n\nFigure 1: Illustration of the motivation behind our task and dataset.\n\nDrawing inspirations from technical studies on causal discovery (Spirtes et al., 2000; Spirtes &amp; Zhang, 2016; Glymour et al., 2019), we formulate a novel task for NLP, correlation-to-causation inference (CORR2CAUSE), which is an important skill for LLMs. Imagine the scenario in Figure 1, where the training corpus does not tediously cover every causal relation, but more pervasively talk about correlations, such as which events tend to co-occur. Learning a good CORR2CAUSE skill can enable LLMs to draw causal relations behind the mere correlational information on the surface. For example, several decades ago, there might be an observation that female university students tend to perform better, but behind the correlational statistics is the causal graph that female students have to achieve extra good performance to get into universities as the first place.\n\nTo this end, we collect the CORR2CAUSE dataset, the first dataset to test the pure causal reasoning abilities of LLMs. All the questions in this dataset are centered around testing when it is valid or invalid to infer causation from correlation. To systematically compose this dataset, we ground our generalization process in the formal framework of causal discovery (Spirtes et al., 1993; 2000; Glymour et al., 2016; Spirtes &amp; Zhang, 2016), which provides rules about how to deduce causal relations among variables given their statistical correlation in the observational data. We generate more than 200K data points, and label a correlation-causation statement pair as valid if and only if there is a bijective mapping between the statistical correlation and the underlying causality.\n\nBased on our CORR2CAUSE dataset with 200K samples, we investigate two main research questions: (1) How well do existing LLMs perform on this task? (2) Can existing LLMs be re-trained or re-purposed on this task and obtain robust causal inference skills? Through extensive experiments, we show empirically that none of the 17 existing LLMs we investigate perform well on this pure causal inference task. We also show that although LLMs can demonstrate better performance after being finetuned on the data, the causal inference skills attained by them are not robust. In summary, our contributions are as follows:\n\n1. We propose the novel task of CORR2CAUSE, to probe an aspect of LLM's reasoning ability, pure causal inference ;\n2. We compose a dataset of over 200K samples, using insights from causal discovery;\n3. We evaluate the performance of 17 LLMs on our dataset, finding that all of them perform poorly, close to the random baseline;\n4. We further explored whether LLMs can learn the skill through finetuning, and find that LLMs fail to robustly acquire this skill in out-of-distribution settings. Finally, we suggest future work to explore more ways to enhance the pure causal inference skill in LLMs.\n\n## 2 PRELIMINARIES: CAUSAL INFERENCE\n\n## 2.1 DIRECTED GRAPHICAL CAUSAL MODELS (DGCMS)\n\nA directed graphical causal model (DGCM) is a commonly used representation to express the causal relations among a set of variables. Given a set of N variables X = { X 1 , . . . , X N } , we can encode the causal relations among them using a directed graph G := ( X , E ) , where E is the set of directed edges. Each edge e i,j ∈ E represents a causal link X i → X j , meaning that X i is a direct cause of X j . In the context of this work, we take the common assumption of directed acyclic graphs (DAGs), which most causal discovery methods use (Glymour et al., 2019), as graphs with cycles can make the causal discovery process arbitrarily hard.\n\nFollowing the graph-theoretic terminology, we use an analogy of the ancestry tree to denote the relations between two variables. For example, we call X i as a parent of X j if there is a directed edge X i → X j in the graph, and, thus, X j is a child of X i . Similarly, we denote X i as an ancestor of X j if there exists a directed path from X i to X j , and, thus, X j is a descendent of X i . Note that a parent is a special case of an ancestor where the directed path has a length of 1.\n\nFor convenience, we also introduce the notions for some special three-variable relations. Given two variables X i and X j , we call a third variable X k a confounder (i.e., common cause ) if X k is a parent of both X i and X j ; a collider (i.e., common effect ) if X k is a child of both X i and X j ; and a mediator if X k is both a child of X i , and a parent of X j .\n\n## 2.2 D-SEPARATION AND MARKOV PROPERTY\n\nD-Separation D-separation (Pearl, 1988) is a fundamental concept in graphical models used to determine whether two sets of nodes X and Y in a DAG G are conditionally independent given a third set of nodes Z , where the three sets are disjoint. We say that X and Y are d-separated by Z if all paths between any node in X and any node in Y are blocked by the conditioning set Z . A path between X and Y is blocked by Z if there exists a node A ∈ Z which satisfies one of the following conditions: A is the parent node in a fork structure on the path (i.e., ·← A →· ); A is the mediator node in a chain structure on the path (i.e., ·→ A →· ); or in any collider structure on the path (i.e., ·→ A ←· ), Z does not contain A or its descendants.\n\nMarkov Property The Markov property in a DAG G states that each node X i is conditionally independent of its non-descendants given its parents, namely X i ⊥ ⊥ NonDe ( X i ) | Pa ( X i ) , where NonDe ( X i ) denotes the non-descendants of X i excluding itself, and Pa ( X i ) denotes the parents of X i . Using the Markov property, we can factorize the joint distribution of all the nodes in the graph into P ( X 1 , . . . , X N ) = ∏ N i =1 P ( X i | PA ( X i )) . To infer the causal graph from probability distributions, a common assumption is faithfulness, namely the validity to infer all the d-separation sets in the graph from the independence relations in the probability distribution. In our work, we also take this broadly taken assumption which holds for most real-world scenarios.\n\nMarkov Equivalence of Graphs We denote two DAGs as Markov equivalent if they induce the same joint distribution P ( X ) . The set of DAGs that are Markov equivalent to each other is called a Markov equivalence class (MEC). Causal graphs in the same MEC can be easily identified since they have the same skeleton (i.e., undirected edges) and V-structures (i.e., structures in the form of A → B ← C where A and C are not connected).\n\nObviously, there is a one-to-many mapping (i.e., surjection) between the causal graph and statistical distribution. Namely, each causal graph sufficiently determines a statistical distribution, but from a statistical distribution, we cannot necessarily induce a unique causal graph. This is why we say 'correlation does not necessarily mean causation'.\n\n## 2.3 CAUSAL DISCOVERY\n\nCausal discovery aims to learn the causal relations by analyzing statistical properties in the observational data (Spirtes et al., 1993; 2000; Glymour et al., 2016; Spirtes &amp; Zhang, 2016; Glymour et al., 2019). It can be achieved through constraint-based methods (Spirtes et al., 2000), score-based methods (Chickering, 2002), or other methods taking advantage of the functional causal models (Shimizu et al., 2006; Hoyer et al., 2008; Zhang &amp; Hyvärinen, 2009).\n\nTo fit for the spirit of this paper to infer from correlation (expressed in natural language) to causation, we base our dataset design on the widely-used Peter-Clark (PC) algorithm (Spirtes et al., 2000). The PC algorithm is based on the principles of conditional independence and the causal Markov assumption, which allows it to efficiently identify causal relationships among variables in a given dataset. The algorithm first starts with a fully connected undirected graph among all the variables. Then it removes the edge between two variables if there is an unconditional or conditional independence relationship between them. Afterwards, it orients the directed edges whenever there is a V-structure. And finally, it iteratively checks the direction of the other edges until the entire causal graph is consistent with all the statistical correlations.\n\nFigure 2: Pipeline of the data construction process.\n\n<!-- image -->\n\n## 3 DATASET CONSTRUCTION\n\nWe introduce the construction of our dataset in this section. We start with our task formulation for CORR2CAUSE, and then briefly give an overview of the data generation process, followed by detailed descriptions of each step. We conclude the section with the overall statistics of the dataset.\n\n## 3.1 TASK FORMULATION\n\nGiven a set of N variables X = { X 1 , . . . , X N } , we have a statement s about all the correlations among the variables, and a hypothesis h describing the causal relation r between the pair of variables X i and X j . The task is to learn a function f : ( s , h ) ↦→ v which maps the correlation statement s and the causal relation hypothesis h to their validity v ∈ { 0 , 1 } , which takes the value 0 if this inference is invalid, and the value 1 if this inference is valid.\n\n## 3.2 OVERVIEW OF THE DATA GENERATION PROCESS\n\nWe base the construction our dataset on several concepts of causal inference, including the DGCM, d-separation, and MECs, as introduced in Section 2.\n\nAs in the overview of our data generation process in Figure 2, we first choose the number N of variables (Step 1) and generate all the unique DGCMs with N nodes (Step 2), which we will introduce in the Section 3.3. Then we collect all the d-separation sets from these graphs to identify MECs (Step 3) in Section 3.4. Then, in Step 4, we create the formal form of data in Section 3.5. For each correspondence of the MEC to causal graphs, we compose the correlation statement based on the statistical relations in the MEC, and hypothesize a causal relation between two variables, and produce the validity v = 1 if the hypothesis is a shared property of all causal graphs in the MEC, and v = 0 if the hypothesis is not necessarily true for all the MEC graphs. Finally, we introduce the verbalization process in Section 3.6.\n\n## 3.3 CONSTRUCTING THE GRAPHS WITH ISOMORPHISM CHECKS\n\nThe first step of the data generation is to compose the causal graphs, as in Step 1 and 2 of Figure 2. For a set of N variables X = { X 1 , . . . , X N } , there are N ( N -1) possible directed edges, since each node can link to any node other than itself. To remove cycles in the graph, we make the nodes in topological order, which only allows edges X i → X j , where i &lt; j . We achieve this by limiting the adjacency matrix of the graph to only having non-zero values above the diagonal, resulting in N ( N -1) / 2 possible directed edges for the DAGs.\n\nAt the first glance, for N nodes, there should be 2 N ( N -1) / 2 possible DAGs (i.e., the power set of all edges). However, there could be isomorphic graphs in this set. To avoid this, we perform a graph\n\nTable 1: Statistics about the source causal graphs in our dataset. Given the number of nodes, we report the number of unique DAGs, average number of edges per DAG, number of MECs, and average number of DAGs per MEC.\n\n| # Nodes   | # Unique DAGs     |   # Edges/DAG | # MECs   |   # DAGs/MEC |\n|-----------|-------------------|---------------|----------|--------------|\n| 2         | 2 out of 2        |          0.5  | 2        |         1    |\n| 3         | 6 out of 2 3      |          1.67 | 5        |         1.2  |\n| 4         | 31 out of 2 6     |          3.48 | 20       |         1.55 |\n| 5         | 302 out of 2 10   |          5.89 | 142      |         2.13 |\n| 6         | 5,984 out of 2 15 |          8.77 | 2,207    |         2.71 |\n| Total     | 6,325             |          8.6  | 2,376    |         2.66 |\n\nisomorphism check (McKay &amp; Piperno, 2014), and reduce the set so that only unique DAGs are retained, and we show their statistics in Table 1. Although we can handle large graphs, we mostly focus on smaller graphs that can still lead to a reasonably sized dataset, so we empirically set N = 6 , but future work can use our open-sourced codes to extend to more nodes.\n\n## 3.4 PROGRAMMATICALLY GENERATING THE D-SEPARATION SETS\n\nBased on the set of unique DAGs, we then programmatically generate the d-separation sets by graph theoretical conditions, as in Step 3 of Figure 2. To realize this step, we code an efficient graph-theoretic algorithm to check for all the chain, fork, and collider structures to automatically identify the set of nodes that d-separate each pair of nodes. Using the d-separation sets and the faithfulness assumption, we form the statistical correlations as follows. For each pair of nodes, they are conditionally independent given the variables in the d-separation set. If the d-separation set is empty, then the two nodes are unconditionally independent. If no d-separation set can be found for the two nodes, then they are directly correlated.\n\nMoreover, using the d-separation sets, we are able to cluster causal graphs to MECs. We achieve it by tracing the mapping between the causal graphs and the set of statistical correlations, and backtracking the graphs with the same d-separation sets to group them in the same MEC. We show in Table 1 that each MEC contains on average 2.66 DAGs.\n\n## 3.5 COMPOSING THE HYPOTHESES AND LABEL\n\nAfter generating the set of correlations based on the d-separation sets, we now generate the causal hypotheses. For the causal relation r , we focus on six common causal relations between two nodes introduced in Section 2.1: Is-Parent, Is-Child, Is-Ancestor (excluding the parents), Is-Descendant (excluding the children), Has-Confounder (i.e., there exists a confounder, or common cause, of the two nodes), and Has-Collider (i.e., there exists a collider, or common effect, of the two nodes). In this way, the set of hypotheses contains all six meaningful causal relations between every pair of variables, resulting in a total size of 6 · N ( N -1) / 2 = 3 N ( N -1) hypotheses for a graph with N variables.\n\nTo generate the ground-truth validity label, we start from the correlation sets in Step 3, then look up all the causal graphs in the same MEC corresponding to the given set of correlations, and check the necessity of the hypothesized causal relation. If the causal relationship proposed in the hypothesis is valid for all causal graphs within the MEC, then we generate the validity v = 1 ; otherwise, we generate v = 0 . A special case of valid samples is that when the size of the MEC is 1, then there is a bijective mapping between the causal graph and the d-separation sets, so any hypothesis stating the causal properties of that unique causal graph is valid.\n\n## 3.6 VERBALIZING INTO LANGUAGE\n\nFinally, as in the last step of Figure 2, we convert all the information above to text data for our CORR2CAUSE task. For the correlation statement, we verbalize the set of correlations in Step 3 into a natural language statement s . When two variables cannot be d-separated, i.e., A ̸⊥ ⊥ B , then we describe them as ' A correlates with B ' since they are directly correlated and cannot be independent by any condition. And if two variables have a valid d-separation set C , then we describe them as ' A is independent of B given C .' In the special case when the d-separation set is empty, we directly say ' A is independent of B .' In addition, we disambiguate the setting by starting the correlation statement with the setup of a closed system of the given variables, and no hidden variables: 'Suppose there is a closed system of N variables, A, B, . . . All the statistical relations among these N variables are as follows:'. Finally, to verbalize the hypothesis, we feed the causal relation triplet ( X i , r , X j )\n\n| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |\n\nTable 2: Templates for each causal relation in the hypothesis. We use {Var i} and {Var j} as placeholders for the two variables.\n\ninto their hypothesis templates in Table 2. For example, we turn the triplet ( A, Is-Parent , B ) into ' A directly causes B ', as in the example of Figure 2.\n\n## 3.7 STATISTICS OF THE RESULTING DATA\n\nWe show the statistics of our CORR2CAUSE dataset in Table 3. Overall, our dataset contains 207,972 samples, where 18.57% of the samples have the positive label (i.e., with validity=1). The average length of the premise is 424.11 tokens, and hypothesis 10.83 tokens. We split the data into 205,734 training samples, 1,076 development and 1,162 test samples. 2 Since the main purpose of the dataset is to benchmark the performance of LLMs, we prioritize the test and development sets to have a comprehensive coverage over all sizes of graphs. Specifically, we iterate through the subset of our data for each N , and split it entirely for only the test and development sets if the data is less than 1K, which is the case for N = 2 and 3 . For the other subsets that are larger, we randomly sample up to 1K or 10% of the data, whichever is smaller, to the test and development sets. We set the cap to be 1K in order to form a reasonable computation budget, since many LLMs are expensive to query in the inference mode. Aside from the test and valid sets, all the rest of the data goes into the training set.\n\n|                     | Overall   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   |\n|---------------------|-----------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\n|                     |           | N = 2                                 | N = 3                                 | N = 4                                 | N = 5                                 | N = 6                                 |\n| # Samples           | 207,972   | 12                                    | 90                                    | 720                                   | 8,520                                 | 198,630                               |\n| # Test              | 1,162     | 6                                     | 48                                    | 72                                    | 514                                   | 522                                   |\n| # Dev               | 1,076     | 6                                     | 42                                    | 72                                    | 482                                   | 474                                   |\n| # Train             | 205,734   | 0                                     | 0                                     | 576                                   | 7,524                                 | 197,634                               |\n| # Tokens/Premise    | 424.11    | 31.5                                  | 52.0                                  | 104.0                                 | 212.61                                | 434.54                                |\n| # Tokens/Hypothesis | 10.83     | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 |\n| %Positive Labels    | 18.57     | 0.00                                  | 3.33                                  | 7.50                                  | 13.01                                 | 18.85                                 |\n| Vocab Size          | 65        | 49                                    | 53                                    | 55                                    | 57                                    | 61                                    |\n\nTable 3: Statistics of our CORR2CAUSE dataset, and by subsets. We report the total number of samples (# Samples); splits of the test (# Test), developement (# Dev) and training sets (# Train); number of tokens per premise (# Tokens/Premise) and hypothesis (# Tokens/Hypothesis); percentage of the positive labels (% Positive Labels), and vocabulary size by the number of unique tokens (Vocab Size). Note that the number of unique graphs and MECs are in Table 1.\n\n## 4 EXPERIMENTS\n\n## 4.1 EXPERIMENTAL SETUP\n\nWe set up a diverse list of LLMs for the experiments on our CORR2CAUSE dataset. To test existing LLMs , we first include six commonly used BERT-based NLI models in the transformers library (Wolf et al., 2020): BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), DeBERTa (He et al., 2021), DistilBERT (Sanh et al., 2019), and DistilBART (Shleifer &amp; Rush, 2020). Apart from these BERT-based NLI models, we also evaluate the general-purpose autoregressive LLMs based on GPT (Radford et al., 2019): GPT-3 Ada, Babbage, Curie, Davinci (Brown et al., 2020); its instruction-tuned versions (Ouyang et al., 2022), text-davinci-001, text-davinci-002, and text-davinci-003; and GPT-3.5 (i.e., ChatGPT), and the latest GPT-4 (OpenAI, 2023) by April 2023,\n\n2\n\nNote for our dataset v2.0: We notice that our original data (v1.0) has duplication due to symmetric relations and verbalizations of the hypothesis. E.g., Is-Parent(A, B) has the exact hypothesis verbalization as Is-Child(B, A). Hence, for our data v2.0, we perform a careful de-duplication, and update the data statistics in Table 3. See more version comparison details in Appendix D. Note that, due to the symmetry, the current version is a random sample half of the size of the original version, so the modeling results in the experiment section roughly hold.\n\nTable 4: Overall performance. We report F1 (main metric), precision, recall and accuracy. For the main metric, F1 score, we use the bold font to highlight the overall best performance, and underline to highlight the best performance within each category of models.\n\n|                                   | F1    | Precision   | Recall   | Accuracy   |\n|-----------------------------------|-------|-------------|----------|------------|\n| Random Baselines                  |       |             |          |            |\n| Always Majority                   | 0.0   | 0.0         | 0.0      | 84.77      |\n| Random (Proportional)             | 13.5  | 12.53       | 14.62    | 71.46      |\n| Random (Uniform)                  | 20.38 | 15.11       | 31.29    | 62.78      |\n| BERT-Based Models                 |       |             |          |            |\n| BERT MNLI                         | 2.82  | 7.23        | 1.75     | 81.61      |\n| RoBERTa MNLI                      | 22.79 | 34.73       | 16.96    | 82.50      |\n| DeBERTa MNLI                      | 14.52 | 14.71       | 14.33    | 74.31      |\n| DistilBERT MNLI                   | 20.70 | 24.12       | 18.13    | 78.85      |\n| DistilBART MNLI                   | 26.74 | 15.92       | 83.63    | 30.23      |\n| BART MNLI                         | 33.38 | 31.59       | 35.38    | 78.50      |\n| LLaMa-Based Models                |       |             |          |            |\n| LLaMa-7B                          | 26.81 | 15.50       | 99.42    | 17.36      |\n| Alpaca-7B                         | 27.37 | 15.93       | 97.37    | 21.33      |\n| GPT-Based Models                  |       |             |          |            |\n| GPT-3 Ada                         | 0.00  | 0.00        | 0.00     | 84.77      |\n| GPT-3 Babbage                     | 27.45 | 15.96       | 97.95    | 21.15      |\n| GPT-3 Curie                       | 26.43 | 15.23       | 100.00   | 15.23      |\n| GPT-3 Davinci                     | 27.82 | 16.57       | 86.55    | 31.61      |\n| GPT-3 Instruct (text-davinci-001) | 17.99 | 11.84       | 37.43    | 48.04      |\n| GPT-3 Instruct (text-davinci-002) | 21.87 | 13.46       | 58.19    | 36.69      |\n| GPT-3 Instruct (text-davinci-003) | 15.72 | 13.4        | 19.01    | 68.97      |\n| GPT-3.5                           | 21.69 | 17.79       | 27.78    | 69.46      |\n| GPT-4                             | 29.08 | 20.92       | 47.66    | 64.60      |\n\nusing the OpenAI API ( https://openai.com/api/ ) with temperature 0. We also evaluate the recent, more efficient models, LLaMa (Touvron et al., 2023) and Alpaca (Taori et al., 2023).\n\nWhen inspecting the behavior of finetuned models , we adopt a large set of models, including GPTbased models (GPT-3 Ada, Babbage, Curie, and Davinci) using the OpenAI finetuning API for classification at https://platform.openai.com/docs/guides/fine-tuning , open-sourced decoder-only models (GPT2, GPT2-Large, GPT2-XL, LLaMA-7B, and LLaMA2-7B), BERT-based models from scratch (BERT-Base, BERT-Large, RoBERTa-Base, and RoBERTa-Large), and BERTBased NLI models (BERT-Base MNLI, BERT-Large MNLI, RoBERTa-Base MNLI, and RoBERTaLarge MNLI) using the transformers library (Wolf et al., 2020). See training details in Appendix A.\n\nFor the random baselines , we provide 'always majority' to predict the majority class 100% of the time, 'random (uniform)' to uniformly sample a label (i.e., 50% for each), and 'random (proportional)' to sample a label from a Bernouli distribution proportional to the development set label distribution.\n\n## 4.2 THE CORR2CAUSE SKILL IN EXISTING LLMS\n\nWe show the performance of seventeen LLMs in Table 4. We can see that pure causal inference is a very challenging task across all existing LLMs. Among all the LLMs, the best performance is 33.38% F1 by BART MNLI, which is even higher than the latest GPT-based model, GPT-4. Notably, many models are worse than random guess, which means that they totally fail at this pure causal inference task. The observation still holds for few-shot chain-of-thought prompts tested in Appendix G.\n\n## 4.3 FINETUNED PERFORMANCE\n\nNext, we address the question: Can we re-purpose LLMs to learn this task? The experimental results in Table 5a of 17 models finetuned on our CORR2CAUSE seem very strong at first sight. Most models see a substantial increase, among which the finetuned BERT-based NLI models demonstrate the strongest performance. The best-performing one, RoBERTa-Large MNLI, achieves 94.74% F1 score on this task, as well as very high precision, recall and accuracy scores.\n\n|                                             | F1                                          | Precison                                    | Recall                                      | Accuracy                                    | F1 (Paraph.)                    | F1 (Var. Ref.)                  |\n|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------|---------------------------------|\n| Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API |                                 |                                 |\n| GPT-3 Ada                                   | 79.85                                       | 70.47                                       | 92.11                                       | 92.92                                       | 61.73                           | 41.57                           |\n| GPT-3 Babbage                               | 78.19                                       | 69.98                                       | 88.60                                       | 92.48                                       | 62.34                           | 43.28                           |\n| GPT-3 Curie                                 | 81.23                                       | 75.00                                       | 88.60                                       | 93.77                                       | 64.93                           | 45.32                           |\n| GPT-3 Davinci                               | 85.52                                       | 80.26                                       | 91.52                                       | 95.28                                       | 65.01                           | 46.96                           |\n| Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  |                                 |                                 |\n| GPT2                                        | 89.18                                       | 88.03                                       | 90.35                                       | 96.66                                       | 56.76                           | 31.70                           |\n| GPT2-Large                                  | 94.29                                       | 92.18                                       | 96.49                                       | 98.22                                       | 55.95                           | 31.99                           |\n| GPT2-XL                                     | 94.30                                       | 91.94                                       | 96.78                                       | 98.22                                       | 60.32                           | 43.95                           |\n| LLaMA-7B                                    | 91.98                                       | 88.62                                       | 95.61                                       | 97.46                                       | 56.41                           | 53.92                           |\n| LLaMA2-7B                                   | 92.92                                       | 90.11                                       | 95.91                                       | 97.77                                       | 52.24                           | 49.47                           |\n| Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models     | Finetuned BERT-Based Models     |\n| BERT-Base                                   | 69.29                                       | 54.42                                       | 95.32                                       | 87.13                                       | 61.13                           | 35.20                           |\n| BERT-Large                                  | 85.26                                       | 77.51                                       | 94.74                                       | 95.01                                       | 63.64                           | 38.54                           |\n| RoBERTa-Base                                | 87.60                                       | 78.47                                       | 99.12                                       | 95.73                                       | 65.58                           | 53.12                           |\n| RoBERTa-Large                               | 89.10                                       | 82.54                                       | 96.78                                       | 96.39                                       | 65.05                           | 60.20                           |\n| Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models | Finetuned BERT-Based NLI Models |\n| BERT-Base MNLI                              | 89.88                                       | 85.49                                       | 94.74                                       | 86.51                                       | 65.56                           | 31.50                           |\n| BERT-Large MNLI                             | 90.19                                       | 84.44                                       | 96.78                                       | 96.79                                       | 67.24                           | 52.04                           |\n| RoBERTa-Base MNLI                           | 94.27                                       | 90.35                                       | 98.54                                       | 98.17                                       | 57.42                           | 62.83                           |\n| RoBERTa-Large MNLI                          | 94.74                                       | 92.24                                       | 97.37                                       | 98.35                                       | 55.45                           | 67.87                           |\n\n(a) Performance of finetuned models on the original test set.\n\n(b) F1 scores of finetuned models on the perturbed test sets by paraphrasing (Paraph.) and variable refactorization (Var. Ref.).\n\nTable 5: Performance of finetuned models on the original test set and perturbed test sets.\n\n| Relation Type   |    F1 |   Precision |   Recall |   Accuracy |    F1 |   Precision |   Recall |   Accuracy |\n|-----------------|-------|-------------|----------|------------|-------|-------------|----------|------------|\n| Is-Parent       | 96.18 |       95.45 |    96.92 |      98.67 | 74.8  |       79.31 |    70.77 |      91.73 |\n| Is-Ancestor     | 93.94 |       93.94 |    93.94 |      98.93 | 45.45 |       90.91 |    30.3  |      93.6  |\n| Is-Child        | 95.73 |       94.92 |    96.56 |      98.67 | 73.39 |       78.43 |    68.97 |      92.27 |\n| Is-Descendant   | 96.55 |       93.33 |   100    |      99.47 | 29.41 |       83.33 |    17.86 |      93.6  |\n| Has-Collider    | 92.19 |       87.41 |    97.52 |      94.64 | 70.7  |       75    |    66.9  |      82.04 |\n| Has-Confounder  | 98.67 |       97.37 |   100    |      99.73 | 70.42 |       73.53 |    67.57 |      94.37 |\n\n(a) Fine-grained performance of RoBERTa-Large by causal relation type on the original test set.\n\n(b) Its fine-grained performance by relation type after variable refactorization.\n\nTable 6: Fine-grained analysis of the best-performing model, RoBERTa-Large MNLI.\n\n## 4.4 FINE-GRAINED PERFORMANCE BY CAUSAL RELATION\n\nIn addition to the overall results mentioned above, we conduct a fine-grained analysis to check the performance of the strongest finetuned model, RoBERTa-Large MNLI, by our six causal relation types. As in Table 6a, the model is very good at judging relations such as Is-Parent, Is-Descendant and Has-Confounder, all with more than 96% F1 scores, whereas it is several points weaker on the Has-Collider relations. This could be due to that the collider relation is the most special type, requiring identification of the V-structure based on both the unconditional independence based on the two variables only and correlations whenever conditioned on a common descendant. We also conduct error analysis for non-finetuned models in Appendix F.\n\n## 4.5 ROBUSTNESS ANALYSIS\n\nLooking at the very high performance of the finetuned models, we raise the next question: Did the models really robustly learn the causal inference skills?\n\nTwo Robustness Tests We design two simple robustness tests: (1) paraphrasing, and (2) variable refactorization. For (1) paraphrasing, we simply paraphrase the hypothesis by changing the text template for each causal relation to some semantically-equivalent alternatives in Appendix C. For (2) variable refactorization, we reverse the alphabet of the variable names, namely flipping A, B, C, to Z, Y, X and so on. The inspiration behind the two robustness tests comes from the spurious correlation analysis described in Appendix E.\n\nSpecifically, we adopt the common setup of text adversarial attack (Morris et al., 2020; Jin et al., 2020) to preserve the training set and keep the same saved models, but run the inference on the perturbed test set. In this way, we separate the possibilities of the models only overfitting on the training data vs. mastering the reasoning skills.\n\nResults after Perturbation We can see from Table 5b that all the models drop drastically, by up to 39.29 on the paraphrased test set, and up to 62.30 after variable refactorization. The best-performing model, RoBERTa-Large MNLI, is especially sensitive towards paraphrasing, demonstrating the most drop among all models; however, it is the most robust against the variable refactorization, maintaining a high F1 score of 67.87. We conduct fine-grained analysis for RoBERTa-Large MNLI under perturbation in Table 6b. We can see the the main source of the performance drop of the model comes from the two classes, Is-Ancestor (decreasing to 45.45%) and Is-Descendant (decreasing to 29.41%), while the other classes stay relatively robust, keeping their F1 scores over 70%.\n\nFrom this analysis, we make the following suggestions to future studies testing this CORR2CAUSE skill of LLMs. First, it is safe to use it as a test set to benchmark existing LLMs' performance, since the data we generate is out-of-distribution from the training data of the current LLMs. Then, when testing finetuned models, it is very important to accompany adversarial attack together with the i.i.d. test set. We open-source our perturbed test sets for future work to test the generalizability skill.\n\n## 4.6 EXTENSION TO NATURAL STORIES\n\nWe envision our CORR2CAUSE dataset to be a foundation for future extensions to various settings, such as instantiating the variables with actual phenomena and situating the story in a more natural setting. For example, the correlation does not imply causation rule can be instantiated with the ice cream sales and swimming pool attendance as the two variables, and argue that ice cream sales does not necessarily affect swimming pool attendance, because their correlation could be due to a third variable, such as hot weather. We provide a case study for how to instantiate the symbolic expressions in our dataset to more natural stories, and find that LLMs such as GPT-4 can generate realistic, daily life stories that has foreseeably broad applications. See more details in Appendix B.\n\n## 5 RELATED WORK\n\nExisting Causal Reasoning Tasks A large body of existing research of causal reasoning in NLP focuses on leveraging empirical knowledge to do tasks such as inferring the cause and effect of why an agent perform certain tasks (Sap et al., 2019a), the motivation and emotional reaction in a social context (Sap et al., 2019b), how people achieve a given goal with a set of concrete steps (Zhang et al., 2020), the development of a story given a different beginning (Qin et al., 2019), and how in general LLMs serve as a knowledge base of cause and effect (Willig et al., 2023; Kıcıman et al., 2023). In contrast, our CORR2CAUSE task focuses on the pure causal inference skill of models, which is a knowledge-dependent reasoning skill based on formally correct rules from causal inference.\n\nExisting Logical and Inference Tasks Another related area of literature is logical and inference tasks, of which a well-established one is natural language inference (NLI), to identify the semantic relationship between a pair of sentences (MacCartney &amp; Manning, 2008; Bowman et al., 2015). NLI datasets mainly focus on the set and paraphrase relations. For example, 'a group of boys are playing football' can entail 'some guys are playing football,' where 'boys' are a sub-concept of 'guys,' and 'a group of' and 'some' are paraphrases. Recently, there have been increasing efforts to extend the inference task to various logical inference skills such as deductive logic and propaganda techniques (Jin et al., 2022; Alhindi et al., 2022). Our CORR2CAUSE dataset is the first dataset testing the correlation-to-causation inference skill, which is unique of its type.\n\n## 6 CONCLUSION\n\nIn this work, we introduced a novel task, CORR2CAUSE, to infer causation from correlation, and collected a large-scale dataset of over 200K samples. We evaluated an extensive list of LLMs on this new task, and showed that off-the-shelf LLMs perform poorly on this task. We also show that it is possible to re-purpose LLMs on this task by finetuning, but future work needs to be aware of the out-of-distribution generalization problem. To avoid the Goodhart's law, we recommend using this dataset to benchmark the pure causal inference skills for LLMs that have not seen this dataset. Given the limited reasoning abilities of current LLMs, and the difficulty of separating actual reasoning from training-corpus-derived knowledge, it is imperative that our community focus on work aiming to accurately disentangle and measure both abilities. We believe the present work is a first such step.\n\n## LIMITATIONS AND FUTURE WORK\n\nWe identify several limitations of this work and open future directions: First, in the context of this work, we limit the causal graphs to two to six nodes, but future work can feel free to explore larger graphs. Another aspect is that we do not assume hidden confounders in this inference problem, so we welcome future work to generate an even more challenging dataset to infer the existence of hidden confounders, analogous to the causal discovery algorithm of fast causal inference (FCI) (Spirtes et al., 2000). And also in general, explorations of other causal discovery algorithms are welcomed too. Finally, a lot of motivation behind proposing this task is inspired by the problem of invalid reasoning patterns in our daily reasoning (Jin et al., 2022), which could fertilize the ground for more pervasive spread of fake news. We believe false causal inference is a prevalent type of fallacious beliefs, and welcome future work to connect the idea of this benchmark to more real-world false beliefs based on confusing correlation with causation.\n\n## ACKNOWLEDGMENT\n\nWe thank Riley Goodside for valuable suggestions to improve our prompts to LLMs. We thank Luigi Gresele and Amir Hossein Karimi for their suggestions to help us improve the formulation of our causal discovery questions.\n\nThis material is based in part upon work supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645; by a National Science Foundation award (#2306372); by a Swiss National Science Foundation award (#201009) and a Responsible AI grant by the Haslerstiftung. Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy. We also thank OpenAI for granting Zhijing quota to their API of GPT series through the Researcher Access Program.\n\n## REFERENCES\n\n- Tariq Alhindi, Tuhin Chakrabarty, Elena Musi, and Smaranda Muresan. Multitask instruction-based prompting for fallacy recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 8172-8187, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main.560 . 9\n- Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/forum?id= Byg1v1HKDB . 1\n- Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https: //aclanthology.org/D15-1075 . 9\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf . 6\n- David Maxwell Chickering. Optimal structure identification with greedy search. J. Mach. Learn. Res. , 3:507-554, 2002. URL http://jmlr.org/papers/v3/chickering02b.html . 3\n\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423 . 1, 6\n- Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in Genetics , 10:524, 2019. ISSN 1664-8021. doi: 10.3389/fgene.2019. 00524. URL https://www.frontiersin.org/article/10.3389/fgene.2019. 00524 . 2, 3\n- Madelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer . John Wiley and Sons, 2016. 2, 3\n- Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pp. 394-398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052 . 1\n- Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced Bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:// openreview.net/forum?id=XPZIaotutsD . 6\n- Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Léon Bottou (eds.), Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008 , pp. 689-696. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper/2008/hash/ f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html . 3\n- Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 80188025. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/ view/6311 . 9\n- Zhijing Jin, Abhinav Lalwani, Tejas Vaidhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schölkopf. Logical fallacy detection. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pp. 7180Ã¢â,¬âCœ-7198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://arxiv.org/abs/2202.13758 . 9, 10\n- Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050 , 2023. 1, 9\n- Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.acl-main.703 . 6\n- Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692 . 6\n\n- Bill MacCartney and Christopher D. Manning. Modeling semantic containment and exclusion in natural language inference. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008) , pp. 521-528, Manchester, UK, August 2008. Coling 2008 Organizing Committee. URL https://aclanthology.org/C08-1066 . 9\n- Brendan D. McKay and Adolfo Piperno. Practical graph isomorphism, II. J. Symb. Comput. , 60: 94-112, 2014. doi: 10.1016/j.jsc.2013.09.003. URL https://doi.org/10.1016/j.jsc. 2013.09.003 . 5\n- John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 119-126, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.emnlp-demos.16. URL https://aclanthology.org/2020.emnlp-demos.16 . 9\n- OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774 . 1, 6\n- Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR , abs/2203.02155, 2022. doi: 10.48550/arXiv.2203.02155. URL https://doi.org/10. 48550/arXiv.2203.02155 . 1, 6\n- Judea Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference . Morgan Kaufmann, 1988. 3\n- Judea Pearl. Causality: Models, reasoning and inference (2nd ed.) . Cambridge University Press, 2009. 1\n- Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: Foundations and learning algorithms . The MIT Press, 2017. URL https://mitpress.mit.edu/ books/elements-causal-inference . 1\n- Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi. Counterfactual story reasoning and generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 5043-5053, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1509. URL https: //aclanthology.org/D19-1509 . 1, 9\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog , 1(8), 2019. 1, 6\n- Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. CoRR , abs/1910.01108, 2019. URL http: //arxiv.org/abs/1910.01108 . 6\n- Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. ATOMIC: an atlas of machine commonsense for if-then reasoning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pp. 3027-3035. AAAI Press, 2019a. doi: 10.1609/aaai.v33i01.33013027. URL https://doi.org/10.1609/aaai.v33i01. 33013027 . 1, 9\n- Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In EMNLP 2019 , 2019b. 1, 9\n\n- Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti J. Kerminen. A linear non-gaussian acyclic model for causal discovery. J. Mach. Learn. Res. , 7:2003-2030, 2006. URL http: //jmlr.org/papers/v7/shimizu06a.html . 3\n- Sam Shleifer and Alexander M. Rush. Pre-trained summarization distillation. CoRR , abs/2010.13002, 2020. URL https://arxiv.org/abs/2010.13002 . 6\n- Peter Spirtes and Kun Zhang. Causal discovery and inference: Concepts and recent methodological advances. In Applied informatics , volume 3, pp. 1-28. SpringerOpen, 2016. 2, 3\n- Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, prediction, and search. 1993. 2, 3\n- Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search, Second Edition . Adaptive computation and machine learning. MIT Press, 2000. ISBN 978-0-262-19440-2. 1, 2, 3, 10\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023. 7\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR , abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi. org/10.48550/arXiv.2302.13971 . 7\n- Ruibo Tu, Chao Ma, and Cheng Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. arXiv preprint arXiv:2301.13819 , 2023. 1\n- Moritz Willig, Matej Zeˇ cevi´ c, Devendra Singh Dhami, and Kristian Kersting. Probing for correlations of causal facts: Large language models and causality, 2023. URL https://openreview. net/forum?id=UPwzqPOs4. 9\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp. 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6 . 6, 7, 14\n- Yuxi Xie, Guanzhen Li, and Min-Yen Kan. Echo: Event causality inference via human-centric reasoning. arXiv preprint arXiv:2305.14740 , 2023. 1\n- Matej Zeˇ cevi´ c, Moritz Willig, Devendra Singh Dhami, and Kristian Kersting. Causal parrots: Large language models may talk causality but are not causal. arXiv preprint arXiv:2308.13067 , 2023. 1\n- Kun Zhang and Aapo Hyvärinen. Causality discovery with additive disturbances: An informationtheoretical perspective. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20 , pp. 570-585. Springer, 2009. 3\n- Li Zhang, Qing Lyu, and Chris Callison-Burch. Reasoning about goals, steps, and temporal ordering with WikiHow. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4630-4639, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.374. URL https://aclanthology.org/ 2020.emnlp-main.374 . 9\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR , abs/2205.01068, 2022. doi: 10.48550/ arXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068 . 1\n\n## A IMPLEMENTATION DETAILS\n\nWhen finetuning on our data, for GPT-based models, we use the default settings of the OpenAI finetuning API; and for BERT-based models, we use the transformers library (Wolf et al., 2020) and train the models on a server with an NVIDIA Tesla A100 GPU with 40G of memory. To fit for the GPU memory, we set the batch size to be 8. We use the validation set to tune the learning rate, which takes value in {2e-6, 5e-6, 1e-5, 2e-5, 5e-5}; dropout rate, which takes value in {0, 0.1, 0.2, 0.3}; and weight decay, which takes value in {1e-4, 1e-5}. We train the models until convergence, which is usually around ten epochs.\n\nPrompts When querying the autoregressive LLMs, we formulate the prompt as follows:\n\nQuestion: [premise]\n\nCan we deduct the following: [hypothesis] ? Just answer \"Yes\" or \"No.\"\n\nAnswer:\n\n## B GENERATING NATURAL STORIES\n\nTo generate the natural stories based on our symbolic expressions, we utilize the state-of-the-art LLM, GPT-4, which is very good at story generation. We design detailed instructions in the prompt, and generate around 200 stories in our case study. We show two examples stories in Table 7, and the report the overall statistics in Table 8.\n\n|               | Example 1 (Label=Negative)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Example 2 (Label=Positive)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Symbolic Form | Premise: Suppose there is a closed system of 2 variables, A and B. All the statistical relations among these 2 variables are as fol- lows: A correlates with B. Hypothesis: A directly affects B. Relation between the promise and hypothe- sis: The premise does not a necessary con- dition for the hypothesis.                                                                                                                                                                                                                                                                                                                                                                                                                                | Premise: Suppose there is a closed system of 3 variables, A, B and C. All the statisti- cal relations among these 3 variables are as follows: A correlates with C. B correlates with C. However, A is independent of B. Hypothesis: A directly affects C. Relation between the promise and hypoth- esis: The premise is a necessary condition for the hypothesis. So if the premise is true, the hypothesis must be true.                                                                                                                                                                                                                                                                                                |\n| Natural Story | Premise: Suppose there is a closed system of 2 variables, ice cream sales and swim- ming pool attendance. All the statistical relations among these 2 variables are as fol- lows: ice cream sales correlate with swim- ming pool attendance. Hypothesis: Ice cream sales directly affect swimming pool attendance. Relation between the premise and hypoth- esis: The premise does not provide a nec- essary condition for the hypothesis. The correlation between ice cream sales and swimming pool attendance could be due to a third variable, such as hot weather, which increases both ice cream sales and swim- ming pool attendance. Therefore, it is not necessarily true that ice cream sales directly affect swimming pool attendance. | Premise: Let's consider three factors: eat- ing junk food (A), obesity (C), and watch- ing television (B). There is a correlation between eating junk food and obesity, and between watching television and obesity. However, eating junk food and watching television are independent from each other. Hypothesis: Eating junk food directly af- fects obesity. Relation between the premise and hypoth- esis: The premise provides the necessary conditions for the hypothesis. It establishes the independent variables A (eating junk food) and B (watching television) and their correlations with obesity. Given that these are true, it supports the hypothesis that eat- ing junk food directly affects obesity. |\n\nTable 7: Examples of natural stories generated based on the symbolic form in our CORR2CAUSE dataset, showing the broad application value of our dataset as the starting point for various verbalizations of the correlation-to-causation inference task.\n\nTable 8: Statistics of our generated natural stories. We report the number of samples in the test and development sets; number of tokens per premise (# Tokens/Premise), hypothesis (# Tokens/Hypothesis), and explanation (# Tokens/Explanation); and percentage of the positive labels (% Positive Labels).\n\n| Test Set Size        |    102 |\n|----------------------|--------|\n| Dev Set Size         | 102    |\n| # Tokens/Premise     |  64.88 |\n| # Tokens/Hypothesis  |  13.54 |\n| # Tokens/Explanation |  64.66 |\n| %Positive Labels     |   1.67 |\n\nFor more information, the exact prompt we use is ' Here is a causal inference rule: [symbolic form] Please provide a real-world example instantiating this phenomenon. Format it also as \"Premise:\", \"Hypothesis:\", and \"Relation between the promise and hypothesis:\". '\n\n## C TEMPLATES AND PARAPHRASES\n\nWe use the verbalization templates in Table 9 to compose the hypotheses for all six causal relations.\n\nTable 9: Templates and their paraphrases for each causal relation in the hypothesis. We use {Var i} and {Var j} as placeholders for the two variables.\n\n| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |\n| Paraphrases       |                                                                                    |\n| Is-Parent         | {Var i} directly affects {Var j} .                                                 |\n| Is-Ancestor       | {Var i} influences {Var j} through some mediator(s).                               |\n| Is-Child          | {Var j} directly affects {Var i} .                                                 |\n| Is-Descendant     | {Var j} influences {Var i} through some mediator(s).                               |\n| Has-Collider      | {Var i} and {Var j} together cause some other variable(s).                         |\n| Has-Confounder    | Some variable(s) cause(s) both {Var i} and {Var j} .                               |\n\n## D CHANGE LOG FOR THE DATASET VERSION UPDATE\n\nTable 10: De-duplication methods for the six causal relation types and their verbalizations.\n\n| Two Equivalent Forms                                                      | Duplication Property                               | De-Duplication Method              |\n|---------------------------------------------------------------------------|----------------------------------------------------|------------------------------------|\n| ß Is-Parent( i , j ) Is-Child( j , i )                                    | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Is-Ancestor( i , j ) (Original) Is-Descendent( j , i ) (Original)       | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Is-Ancestor( i , j ) (Paraphrased) Is-Descendent( j , i ) (Paraphrased) | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Has-Collider( i , j ) Has-Collider( j , i )                             | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Has-Confounder( i , j ) Has-Confounder( j , i )                         | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n\nDe-Duplication Strategy As mentioned in Section 3.7 in the main paper, our original dataset (v1.0) has duplication due to symmetric relations and verbalizations. We introduce in Table 10 several reasons for why duplicated hypotheses exist in our original data. One typical reason is symmetric relations such as Is-Parent(A, B) and Is-Child(B, A), and, similarly, the paraphrased version of\n\nIs-Ancestor(A, B) and Is-Descendent(B, A). Another typical reason is the semantic equivalence in the verbalization templates, which applies to the Has-Collider and Has-Confounder relations. For example, the verbalized texts of Has-Collider(A, B) and Collider(B, A) are 'There exists at least one collider (i.e., common effect) of {A and B, B and A},' respectively, which are semantically-equivalent paraphrases of each other, so we randomly keep one out of the two.\n\n## Resulting Dataset Statistics after De-Duplication\n\nSince the reason for duplication in the first place is due to symmetry in the causal relation, or verbalization, the resulting new data, CORR2CAUSE v2.0, is exactly a half of the original data. As we reported previously in Table 3 of Section 3.7, the total number of samples cuts down to half, while the label distribution and all other properties are the same. To compose each split, we apply the same de-duplication method for the test, train, and development sets. We notice that some duplicates are across the splits, so we prioritize keeping the test and training sets untouched (to minimally affect the experimental results), and then reduce the development set by removing the cross-split duplicates, namely:\n\n- test\\_2.0 = deduplicate(test\\_1.0)\n- train\\_2.0 = deduplicate(train\\_1.0)\n- dev\\_2.0 = deduplicate(dev\\_1.0) \\ {test\\_2.0, train\\_2.0}\n\nWe expect minimal or almost no change to the experimental results. In case of the slight possibility that this change in the development set might affect the model selection in the training process, future work can feel free to re-train the models and update the exact performance number.\n\n## E SPURIOUS CORRELATION ANALYSIS\n\nThe inspirations of our two robustness tests (paraphrasing and variable refactorization) come from our data analysis. We check for spurious correlations in the data by reporting in Table 11 the point-wise mutual information (PMI) between the label and any n-gram with no more than four tokens. In addition, we also report the difference of the PMI with the two labels in the | Diff | column of Table 11, and report the top 10 n-grams.\n\nThe design spirit for our robustness test is that if the models' correct judgment relies on exploiting these spurious correlations, then such reliance will be broken in our perturbations.\n\nTable 11: PMI between the labels and n-grams. The labels include non-entailment (Non-Ent.) and entailment (Ent.). And the n-grams include all with no more than four words. The | Diff | column shows the absolute value of the difference between the PMIs with two labels. We show the top 10 n-grams with the largest differences of their PMIs with the two classes in the | Diff | column.\n\n| N-Gram             |   PMI w/ Non-Ent. Label |   PMI w/ Ent. Label |   | Diff | |\n|--------------------|-------------------------|---------------------|------------|\n| a cause            |                 1.69221 |           -1.02561  |    2.71782 |\n| a cause for        |                 1.66364 |           -0.98379  |    2.64743 |\n| A causes           |                 1.64068 |           -0.95161  |    2.59229 |\n| A causes something |                 1.62182 |           -0.926075 |    2.5479  |\n| a direct           |                 1.60605 |           -0.905316 |    2.51137 |\n| a direct one       |                 1.59267 |           -0.888107 |    2.48078 |\n| for D              |                 1.58483 |           -0.87818  |    2.46301 |\n| for D but          |                 1.5839  |           -0.877014 |    2.46091 |\n| for E              |                 1.58298 |           -0.875864 |    2.45884 |\n| for E but          |                 1.58207 |           -0.874728 |    2.4568  |\n\nWe can see that some spurious correlations are rooted in the framing of the hypothesis, such as 'a cause (for)', and 'a direct (one)' (which we use the paraphrasing task to break), and others are connected to the variable names, such as 'for D (but)' and 'for E (but)' (which we use the variable refactorization to break).\n\n## F FINE-GRAINED ERROR ANALYSIS\n\nIn addition to the fine-grained analysis by causal relation type in Table 6a for fine-tuned models, we also report such error analysis for non-finetuned models in Table 12.\n\nTable 12: Fine-grained evaluation results for some selected non-fine-tuned models.\n\n| Selected Models   | Relation Type   |    F1 |   Precision |   Recall |   Accuracy |\n|-------------------|-----------------|-------|-------------|----------|------------|\n| GPT-3.5           | All             | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5           | Is-Parent       |  8.82 |      100    |     4.62 |      83.47 |\n| GPT-3.5           | Is-Ancestor     |  0    |        0    |     0    |      90.67 |\n| GPT-3.5           | Is-Child        |  9.84 |      100    |     5.17 |      85.33 |\n| GPT-3.5           | Is-Descendant   | 14.29 |       11.9  |    17.86 |      84    |\n| GPT-3.5           | Has-Collider    | 34.24 |       25.51 |    52.07 |      35.12 |\n| GPT-3.5           | Has-Confounder  | 15.33 |        8.86 |    56.76 |      37.8  |\n| GPT-4             | All             | 29.08 |       20.92 |    47.66 |      64.6  |\n| GPT-4             | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| GPT-4             | Is-Ancestor     | 30.77 |       31.25 |    30.3  |      88    |\n| GPT-4             | Is-Child        |  0    |        0    |     0    |      84.53 |\n| GPT-4             | Is-Descendant   | 26.98 |       17.35 |    60.71 |      75.47 |\n| GPT-4             | Has-Collider    | 44.1  |       30.18 |    81.82 |      32.71 |\n| GPT-4             | Has-Confounder  | 20.67 |       11.53 |   100    |      23.86 |\n| RoBERTa MNLI      | All             | 22.79 |       34.73 |    16.96 |      82.5  |\n| RoBERTa MNLI      | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| RoBERTa MNLI      | Is-Ancestor     |  0    |        0    |     0    |      91.2  |\n| RoBERTa MNLI      | Is-Child        |  0    |        0    |     0    |      84.53 |\n| RoBERTa MNLI      | Is-Descendant   |  0    |        0    |     0    |      92.53 |\n| RoBERTa MNLI      | Has-Collider    | 43.45 |       39.73 |    47.93 |      59.52 |\n| RoBERTa MNLI      | Has-Confounder  |  0    |        0    |     0    |      84.45 |\n\nThese results are particularly revealing, showing how off-the-shelf models perform in recognizing specific relations. Specifically, GPT-3.5 cannot recognize ancestor relations, whereas GPT-4 fails at all direct causation recognition with parents and children. And RoBERTa MNLI only did collider relation relatively correctly. Note that, when the F1 score is zero, the accuracy number is a result of always predicting the negative class of that relation.\n\n## G LLM PERFORMANCE OPTIMIZATION\n\nSince our experiments in Section 4.2 are based on plain, zero-shot prompts, we explore whether better prompting strategies could improve the performance. We enhance the query prompt by incorporating several strategies: (1) Utilizing a system prompt that specifies the model's expertise ('You are a highly intelligent question-answering bot with profound knowledge of causal inference.'); (2) Including a pair of few-shot examples, one positive and one negative; (3) Implementing chain-of-thought prompting with 'Let's think step by step.' to encourage the language model to generate step-by-step reasoning. In Table 13, we present the evaluation results on the relatively affordable model, GPT-3.5, where the optimized prompt leads to a 4-point improvement in F1 over the original performance. However, we can see that despite the deployment of all three strategies, the model continues to struggle with this challenging task.\n\nTable 13: Performance of GPT-3.5 with different queries. We quote the original performance from Table 4.\n\n|                                 |    F1 |   Precision |   Recall |   Accuracy |\n|---------------------------------|-------|-------------|----------|------------|\n| GPT-3.5 (plain query; original) | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5 (enhanced query)        | 25.44 |       17.29 |    48.11 |      52.01 |",
  "tables": [
    {
      "index": 0,
      "markdown": "| # Nodes   | # Unique DAGs     |   # Edges/DAG | # MECs   |   # DAGs/MEC |\n|-----------|-------------------|---------------|----------|--------------|\n| 2         | 2 out of 2        |          0.5  | 2        |         1    |\n| 3         | 6 out of 2 3      |          1.67 | 5        |         1.2  |\n| 4         | 31 out of 2 6     |          3.48 | 20       |         1.55 |\n| 5         | 302 out of 2 10   |          5.89 | 142      |         2.13 |\n| 6         | 5,984 out of 2 15 |          8.77 | 2,207    |         2.71 |\n| Total     | 6,325             |          8.6  | 2,376    |         2.66 |"
    },
    {
      "index": 1,
      "markdown": "| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |"
    },
    {
      "index": 2,
      "markdown": "|                     | Overall   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   | Statistics by the Number of Nodes N   |\n|---------------------|-----------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\n|                     |           | N = 2                                 | N = 3                                 | N = 4                                 | N = 5                                 | N = 6                                 |\n| # Samples           | 207,972   | 12                                    | 90                                    | 720                                   | 8,520                                 | 198,630                               |\n| # Test              | 1,162     | 6                                     | 48                                    | 72                                    | 514                                   | 522                                   |\n| # Dev               | 1,076     | 6                                     | 42                                    | 72                                    | 482                                   | 474                                   |\n| # Train             | 205,734   | 0                                     | 0                                     | 576                                   | 7,524                                 | 197,634                               |\n| # Tokens/Premise    | 424.11    | 31.5                                  | 52.0                                  | 104.0                                 | 212.61                                | 434.54                                |\n| # Tokens/Hypothesis | 10.83     | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 | 10.83                                 |\n| %Positive Labels    | 18.57     | 0.00                                  | 3.33                                  | 7.50                                  | 13.01                                 | 18.85                                 |\n| Vocab Size          | 65        | 49                                    | 53                                    | 55                                    | 57                                    | 61                                    |"
    },
    {
      "index": 3,
      "markdown": "|                                   | F1    | Precision   | Recall   | Accuracy   |\n|-----------------------------------|-------|-------------|----------|------------|\n| Random Baselines                  |       |             |          |            |\n| Always Majority                   | 0.0   | 0.0         | 0.0      | 84.77      |\n| Random (Proportional)             | 13.5  | 12.53       | 14.62    | 71.46      |\n| Random (Uniform)                  | 20.38 | 15.11       | 31.29    | 62.78      |\n| BERT-Based Models                 |       |             |          |            |\n| BERT MNLI                         | 2.82  | 7.23        | 1.75     | 81.61      |\n| RoBERTa MNLI                      | 22.79 | 34.73       | 16.96    | 82.50      |\n| DeBERTa MNLI                      | 14.52 | 14.71       | 14.33    | 74.31      |\n| DistilBERT MNLI                   | 20.70 | 24.12       | 18.13    | 78.85      |\n| DistilBART MNLI                   | 26.74 | 15.92       | 83.63    | 30.23      |\n| BART MNLI                         | 33.38 | 31.59       | 35.38    | 78.50      |\n| LLaMa-Based Models                |       |             |          |            |\n| LLaMa-7B                          | 26.81 | 15.50       | 99.42    | 17.36      |\n| Alpaca-7B                         | 27.37 | 15.93       | 97.37    | 21.33      |\n| GPT-Based Models                  |       |             |          |            |\n| GPT-3 Ada                         | 0.00  | 0.00        | 0.00     | 84.77      |\n| GPT-3 Babbage                     | 27.45 | 15.96       | 97.95    | 21.15      |\n| GPT-3 Curie                       | 26.43 | 15.23       | 100.00   | 15.23      |\n| GPT-3 Davinci                     | 27.82 | 16.57       | 86.55    | 31.61      |\n| GPT-3 Instruct (text-davinci-001) | 17.99 | 11.84       | 37.43    | 48.04      |\n| GPT-3 Instruct (text-davinci-002) | 21.87 | 13.46       | 58.19    | 36.69      |\n| GPT-3 Instruct (text-davinci-003) | 15.72 | 13.4        | 19.01    | 68.97      |\n| GPT-3.5                           | 21.69 | 17.79       | 27.78    | 69.46      |\n| GPT-4                             | 29.08 | 20.92       | 47.66    | 64.60      |"
    },
    {
      "index": 4,
      "markdown": "|                                             | F1                                          | Precison                                    | Recall                                      | Accuracy                                    | F1 (Paraph.)                    | F1 (Var. Ref.)                  |\n|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------------------|---------------------------------|---------------------------------|\n| Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API | Finetuned GPT-Based Models Using OpenAI API |                                 |                                 |\n| GPT-3 Ada                                   | 79.85                                       | 70.47                                       | 92.11                                       | 92.92                                       | 61.73                           | 41.57                           |\n| GPT-3 Babbage                               | 78.19                                       | 69.98                                       | 88.60                                       | 92.48                                       | 62.34                           | 43.28                           |\n| GPT-3 Curie                                 | 81.23                                       | 75.00                                       | 88.60                                       | 93.77                                       | 64.93                           | 45.32                           |\n| GPT-3 Davinci                               | 85.52                                       | 80.26                                       | 91.52                                       | 95.28                                       | 65.01                           | 46.96                           |\n| Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  | Finetuned Open-Sourced Decoder-Only Models  |                                 |                                 |\n| GPT2                                        | 89.18                                       | 88.03                                       | 90.35                                       | 96.66                                       | 56.76                           | 31.70                           |\n| GPT2-Large                                  | 94.29                                       | 92.18                                       | 96.49                                       | 98.22                                       | 55.95                           | 31.99                           |\n| GPT2-XL                                     | 94.30                                       | 91.94                                       | 96.78                                       | 98.22                                       | 60.32                           | 43.95                           |\n| LLaMA-7B                                    | 91.98                                       | 88.62                                       | 95.61                                       | 97.46                                       | 56.41                           | 53.92                           |\n| LLaMA2-7B                                   | 92.92                                       | 90.11                                       | 95.91                                       | 97.77                                       | 52.24                           | 49.47                           |\n| Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models                 | Finetuned BERT-Based Models     | Finetuned BERT-Based Models     |\n| BERT-Base                                   | 69.29                                       | 54.42                                       | 95.32                                       | 87.13                                       | 61.13                           | 35.20                           |\n| BERT-Large                                  | 85.26                                       | 77.51                                       | 94.74                                       | 95.01                                       | 63.64                           | 38.54                           |\n| RoBERTa-Base                                | 87.60                                       | 78.47                                       | 99.12                                       | 95.73                                       | 65.58                           | 53.12                           |\n| RoBERTa-Large                               | 89.10                                       | 82.54                                       | 96.78                                       | 96.39                                       | 65.05                           | 60.20                           |\n| Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models             | Finetuned BERT-Based NLI Models | Finetuned BERT-Based NLI Models |\n| BERT-Base MNLI                              | 89.88                                       | 85.49                                       | 94.74                                       | 86.51                                       | 65.56                           | 31.50                           |\n| BERT-Large MNLI                             | 90.19                                       | 84.44                                       | 96.78                                       | 96.79                                       | 67.24                           | 52.04                           |\n| RoBERTa-Base MNLI                           | 94.27                                       | 90.35                                       | 98.54                                       | 98.17                                       | 57.42                           | 62.83                           |\n| RoBERTa-Large MNLI                          | 94.74                                       | 92.24                                       | 97.37                                       | 98.35                                       | 55.45                           | 67.87                           |"
    },
    {
      "index": 5,
      "markdown": "| Relation Type   |    F1 |   Precision |   Recall |   Accuracy |    F1 |   Precision |   Recall |   Accuracy |\n|-----------------|-------|-------------|----------|------------|-------|-------------|----------|------------|\n| Is-Parent       | 96.18 |       95.45 |    96.92 |      98.67 | 74.8  |       79.31 |    70.77 |      91.73 |\n| Is-Ancestor     | 93.94 |       93.94 |    93.94 |      98.93 | 45.45 |       90.91 |    30.3  |      93.6  |\n| Is-Child        | 95.73 |       94.92 |    96.56 |      98.67 | 73.39 |       78.43 |    68.97 |      92.27 |\n| Is-Descendant   | 96.55 |       93.33 |   100    |      99.47 | 29.41 |       83.33 |    17.86 |      93.6  |\n| Has-Collider    | 92.19 |       87.41 |    97.52 |      94.64 | 70.7  |       75    |    66.9  |      82.04 |\n| Has-Confounder  | 98.67 |       97.37 |   100    |      99.73 | 70.42 |       73.53 |    67.57 |      94.37 |"
    },
    {
      "index": 6,
      "markdown": "|               | Example 1 (Label=Negative)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Example 2 (Label=Positive)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Symbolic Form | Premise: Suppose there is a closed system of 2 variables, A and B. All the statistical relations among these 2 variables are as fol- lows: A correlates with B. Hypothesis: A directly affects B. Relation between the promise and hypothe- sis: The premise does not a necessary con- dition for the hypothesis.                                                                                                                                                                                                                                                                                                                                                                                                                                | Premise: Suppose there is a closed system of 3 variables, A, B and C. All the statisti- cal relations among these 3 variables are as follows: A correlates with C. B correlates with C. However, A is independent of B. Hypothesis: A directly affects C. Relation between the promise and hypoth- esis: The premise is a necessary condition for the hypothesis. So if the premise is true, the hypothesis must be true.                                                                                                                                                                                                                                                                                                |\n| Natural Story | Premise: Suppose there is a closed system of 2 variables, ice cream sales and swim- ming pool attendance. All the statistical relations among these 2 variables are as fol- lows: ice cream sales correlate with swim- ming pool attendance. Hypothesis: Ice cream sales directly affect swimming pool attendance. Relation between the premise and hypoth- esis: The premise does not provide a nec- essary condition for the hypothesis. The correlation between ice cream sales and swimming pool attendance could be due to a third variable, such as hot weather, which increases both ice cream sales and swim- ming pool attendance. Therefore, it is not necessarily true that ice cream sales directly affect swimming pool attendance. | Premise: Let's consider three factors: eat- ing junk food (A), obesity (C), and watch- ing television (B). There is a correlation between eating junk food and obesity, and between watching television and obesity. However, eating junk food and watching television are independent from each other. Hypothesis: Eating junk food directly af- fects obesity. Relation between the premise and hypoth- esis: The premise provides the necessary conditions for the hypothesis. It establishes the independent variables A (eating junk food) and B (watching television) and their correlations with obesity. Given that these are true, it supports the hypothesis that eat- ing junk food directly affects obesity. |"
    },
    {
      "index": 7,
      "markdown": "| Test Set Size        |    102 |\n|----------------------|--------|\n| Dev Set Size         | 102    |\n| # Tokens/Premise     |  64.88 |\n| # Tokens/Hypothesis  |  13.54 |\n| # Tokens/Explanation |  64.66 |\n| %Positive Labels     |   1.67 |"
    },
    {
      "index": 8,
      "markdown": "| Causal Relation   | Hypothesis Template                                                                |\n|-------------------|------------------------------------------------------------------------------------|\n| Is-Parent         | {Var i} directly causes {Var j} .                                                  |\n| Is-Ancestor       | {Var i} causes something else which causes {Var j} .                               |\n| Is-Child          | {Var j} directly causes {Var i} .                                                  |\n| Is-Descendant     | {Var j} is a cause for {Var i} , but not a direct one.                             |\n| Has-Collider      | There exists at least one collider (i.e., common effect) of {Var i} and {Var j} .  |\n| Has-Confounder    | There exists at least one confounder (i.e., common cause) of {Var i} and {Var j} . |\n| Paraphrases       |                                                                                    |\n| Is-Parent         | {Var i} directly affects {Var j} .                                                 |\n| Is-Ancestor       | {Var i} influences {Var j} through some mediator(s).                               |\n| Is-Child          | {Var j} directly affects {Var i} .                                                 |\n| Is-Descendant     | {Var j} influences {Var i} through some mediator(s).                               |\n| Has-Collider      | {Var i} and {Var j} together cause some other variable(s).                         |\n| Has-Confounder    | Some variable(s) cause(s) both {Var i} and {Var j} .                               |"
    },
    {
      "index": 9,
      "markdown": "| Two Equivalent Forms                                                      | Duplication Property                               | De-Duplication Method              |\n|---------------------------------------------------------------------------|----------------------------------------------------|------------------------------------|\n| ß Is-Parent( i , j ) Is-Child( j , i )                                    | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Is-Ancestor( i , j ) (Original) Is-Descendent( j , i ) (Original)       | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Is-Ancestor( i , j ) (Paraphrased) Is-Descendent( j , i ) (Paraphrased) | Two exact same strings                             | Keep only one, by forcing i < j    |\n| ß Has-Collider( i , j ) Has-Collider( j , i )                             | Two different strings, but semantically equivalent | Randomly sample one out of the two |\n| ß Has-Confounder( i , j ) Has-Confounder( j , i )                         | Two different strings, but semantically equivalent | Randomly sample one out of the two |"
    },
    {
      "index": 10,
      "markdown": "| N-Gram             |   PMI w/ Non-Ent. Label |   PMI w/ Ent. Label |   | Diff | |\n|--------------------|-------------------------|---------------------|------------|\n| a cause            |                 1.69221 |           -1.02561  |    2.71782 |\n| a cause for        |                 1.66364 |           -0.98379  |    2.64743 |\n| A causes           |                 1.64068 |           -0.95161  |    2.59229 |\n| A causes something |                 1.62182 |           -0.926075 |    2.5479  |\n| a direct           |                 1.60605 |           -0.905316 |    2.51137 |\n| a direct one       |                 1.59267 |           -0.888107 |    2.48078 |\n| for D              |                 1.58483 |           -0.87818  |    2.46301 |\n| for D but          |                 1.5839  |           -0.877014 |    2.46091 |\n| for E              |                 1.58298 |           -0.875864 |    2.45884 |\n| for E but          |                 1.58207 |           -0.874728 |    2.4568  |"
    },
    {
      "index": 11,
      "markdown": "| Selected Models   | Relation Type   |    F1 |   Precision |   Recall |   Accuracy |\n|-------------------|-----------------|-------|-------------|----------|------------|\n| GPT-3.5           | All             | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5           | Is-Parent       |  8.82 |      100    |     4.62 |      83.47 |\n| GPT-3.5           | Is-Ancestor     |  0    |        0    |     0    |      90.67 |\n| GPT-3.5           | Is-Child        |  9.84 |      100    |     5.17 |      85.33 |\n| GPT-3.5           | Is-Descendant   | 14.29 |       11.9  |    17.86 |      84    |\n| GPT-3.5           | Has-Collider    | 34.24 |       25.51 |    52.07 |      35.12 |\n| GPT-3.5           | Has-Confounder  | 15.33 |        8.86 |    56.76 |      37.8  |\n| GPT-4             | All             | 29.08 |       20.92 |    47.66 |      64.6  |\n| GPT-4             | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| GPT-4             | Is-Ancestor     | 30.77 |       31.25 |    30.3  |      88    |\n| GPT-4             | Is-Child        |  0    |        0    |     0    |      84.53 |\n| GPT-4             | Is-Descendant   | 26.98 |       17.35 |    60.71 |      75.47 |\n| GPT-4             | Has-Collider    | 44.1  |       30.18 |    81.82 |      32.71 |\n| GPT-4             | Has-Confounder  | 20.67 |       11.53 |   100    |      23.86 |\n| RoBERTa MNLI      | All             | 22.79 |       34.73 |    16.96 |      82.5  |\n| RoBERTa MNLI      | Is-Parent       |  0    |        0    |     0    |      82.67 |\n| RoBERTa MNLI      | Is-Ancestor     |  0    |        0    |     0    |      91.2  |\n| RoBERTa MNLI      | Is-Child        |  0    |        0    |     0    |      84.53 |\n| RoBERTa MNLI      | Is-Descendant   |  0    |        0    |     0    |      92.53 |\n| RoBERTa MNLI      | Has-Collider    | 43.45 |       39.73 |    47.93 |      59.52 |\n| RoBERTa MNLI      | Has-Confounder  |  0    |        0    |     0    |      84.45 |"
    },
    {
      "index": 12,
      "markdown": "|                                 |    F1 |   Precision |   Recall |   Accuracy |\n|---------------------------------|-------|-------------|----------|------------|\n| GPT-3.5 (plain query; original) | 21.69 |       17.79 |    27.78 |      69.46 |\n| GPT-3.5 (enhanced query)        | 25.44 |       17.29 |    48.11 |      52.01 |"
    }
  ],
  "stats": {
    "pages": 17,
    "chunksCreated": 114,
    "totalCharacters": 81989,
    "totalWords": 11212,
    "numTables": 13,
    "processingTimeMs": 40249
  }
}