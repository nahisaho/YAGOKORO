{
  "paper": {
    "id": "2311.01282v4",
    "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
    "abstract": "As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference. We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource considering input dynamics. Due to the versatility of optimizations in FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37x compared to state-of-the-art LLM inference engines on mainstream LLMs.",
    "authors": [
      "Ke Hong",
      "Guohao Dai",
      "Jiaming Xu",
      "Qiuli Mao",
      "Xiuhong Li",
      "Jun Liu",
      "Kangdi Chen",
      "Yuhan Dong",
      "Yu Wang"
    ],
    "published": "2023-11-02T14:57:03.000Z",
    "updated": "2024-01-05T12:41:13.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2311.01282v4",
    "absUrl": "https://arxiv.org/abs/2311.01282v4"
  },
  "chunks": [
    {
      "id": "2311.01282v4-chunk-0",
      "content": "Ke Hong †\n\nTsinghua University &amp;Infinigence-AI\n\nQiuli Mao Tsinghua University &amp;Infinigence-AI\n\nKangdi Chen Infinigence-AI\n\nGuohao Dai † GLYPH&lt;12&gt;\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nXiuhong Li Peking University\n\nYuhan Dong\n\nTsinghua University\n\nJiaming Xu †\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nJun Liu\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nYu Wang GLYPH&lt;12&gt; Tsinghua University\n\nGLYPH&lt;12&gt; daiguohao@sjtu.edu.cn, daiguohao@infini-ai.com, yu-wang@tsinghua.edu.cn",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "FLASHDECODING++: FASTER LARGE LANGUAGE MODEL INFERENCE ON GPUS",
        "chunkIndex": 0,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-1",
      "content": "As the Large Language Model (LLM) becomes increasingly important in various domains, the performance of LLM inference is crucial to massive LLM applications. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ∼ 20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt; 50% performance loss after padding zeros in previous designs ( e.g., cuBLAS, CUTLASS, etc.). (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-2",
      "content": "rformance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.\n\nWe present FlashDecoding++ , a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. Based on this, the fine-grained pipelining is proposed. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 2,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-3",
      "content": "ashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource ( e.g., Tensor Core or CUDA core) considering input dynamics.Due to the versatility of optimizations in FlashDecoding++ , FlashDecoding++ can achieve up to 4.86 × and 3.93 × speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37 × compared to state-of-the-art LLM inference engines on mainstream LLMs.\n\n† These authors contributed equally to this work.\n\n‡ Prof. Guohao Dai is the Chief Scientist at Infinigence-AI, Ke Hong, Jiaming Xu, Qiuli Mao, and Jun Liu are interns at Infinigence-AI.\n\nGLYPH&lt;12&gt; Prof. Guohao Dai and Prof. Yu Wang are the corresponding authors of this paper.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 3,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-4",
      "content": "Qiuli Mao, and Jun Liu are interns at Infinigence-AI.\n\nGLYPH&lt;12&gt; Prof. Guohao Dai and Prof. Yu Wang are the corresponding authors of this paper.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 4,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-5",
      "content": "As the Large Language Model (LLM) achieved unprecedented success in various domains [2, 3, 4, 5], the LLM inference workload is skyrocketing. For example, OpenAI reports that GPT-4 inference with 8K context length costs $0.03 per 1K input tokens and $0.06 per 1K output tokens [6]. Currently, OpenAI has 180.5 million users and receives over 10 million queries per day [7]. Consequently, the cost to operate OpenAI's model like ChatGPT is approximately $7 million per day for the necessary computing hardware [8]. Thus, optimizations on LLM inference performance will have a huge impact considering massive LLM inference scenarios. Many recent works have proposed techniques to accelerate LLM inference tasks, including DeepSpeed [9], FlexGen [10], vLLM [11], OpenPPL [12], FlashDecoding [13], TensorRT-LLM [14], and etc [15, 16, 17, 12].",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-6",
      "content": "ecent works have proposed techniques to accelerate LLM inference tasks, including DeepSpeed [9], FlexGen [10], vLLM [11], OpenPPL [12], FlashDecoding [13], TensorRT-LLM [14], and etc [15, 16, 17, 12].\n\nThe LLM inference task generates tokens ( e.g., words) from the input sequence autoregressively, and can be organized into two typical phases: the prefill phase and the decode phase. The prefill phase generates the first token by processing the input prompt, and previous research ( e.g., FlashAttention [18, 19]) optimizes latency for this phase. The decode phase generates the following tokens sequentially, and many works [9, 10, 11, 15, 13, 14, 20] focus on improving the throughput of generating tokens ( i.e., reducing latency of each token). The prefill phase dominates total time for scenarios of long-sequence input or generating short outputs [21, 22], while the decode phase constitutes a significant portion of the time when processing long output sequences [23].",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-7",
      "content": "s total time for scenarios of long-sequence input or generating short outputs [21, 22], while the decode phase constitutes a significant portion of the time when processing long output sequences [23].\n\nFigure 2 shows the main dataflow of the LLM inference with one transformer layer for both the prefill phase and the decode phase. A transformer layer can be divided into linear GEMM (General Matrix Multiplication) operations ( e.g., K, Q, V, O weight projection and the feedforward) and the attention/softmax computation. For the attention computation, a softmax operation is adopted for a row in the attention matrix. To improve the parallelism, previous designs [18, 13] divide the attention matrices into smaller tiles and rows are also split to compute partial softmax results. A synchronized softmax operation is adopted to update previous partial softmax results when a new partial softmax result is calculated.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-8",
      "content": "iles and rows are also split to compute partial softmax results. A synchronized softmax operation is adopted to update previous partial softmax results when a new partial softmax result is calculated. Such a synchronized partial softmax update accounts for 18.8% for the attention computation of Llama2-7B inference according to our profiling on NVIDIA Tesla A100 GPU with 1024 input length, resulting in the first challenge for accelerating LLM inference. Secondly, the computation resources is under-utilized for the flat GEMM operation during the decode phase. Because the decode phase sequentially generates tokens, the linear GEMM operation tends to be flat-shape (even turning into the GEMV (General Matrix-Vector Multiplication) operation when the batch size is 1). For the small batch size ( e.g., 8), previous designs [24, 25] pad the matrix with zeros to perform GEMMs of larger sizes ( e.g., 64), leading to over 50% computation under-utilization.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-9",
      "content": "ch size is 1). For the small batch size ( e.g., 8), previous designs [24, 25] pad the matrix with zeros to perform GEMMs of larger sizes ( e.g., 64), leading to over 50% computation under-utilization. Thirdly, the performance of LLM inference suffers from the static dataflow considering input dynamics and hardware configuration. For example, the small batch size makes the decode phase of LLM inference memory-bounded and the large batch size makes it compute-bounded. A single and static dataflow may lead to 50.25% performance loss for GEMMs of different shapes in LLM inference.\n\nFigure 1: Overview of comparison between FlashDecoding++ and state-of-the-art designs. The results in the figure are reported with Llama2-7B model [1]. The left is with batch size=1 and input length=1K, and TensorRT-LLM and Hugging Face are the SOTA baseline for NVIDIA/AMD according to our experimental results. The right shows the comprehensive comparison of both first token latency and each token latency.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-10",
      "content": "nsorRT-LLM and Hugging Face are the SOTA baseline for NVIDIA/AMD according to our experimental results. The right shows the comprehensive comparison of both first token latency and each token latency.\n\n<!-- image -->\n\nFigure 2: Overview of Large Language Model inference dataflow. We show the dataflow comparison between the prefill phase and the decode phase. The prefill phase mainly involves the GEMM operation, while the decode phase mainly involves the GEMV/Flat GEMM operation.\n\n<!-- image -->\n\nTo tackle these challenges and enable a faster Large Language Model (LLM) inference, we present FlashDecoding++ in this paper. FlashDecoding++ creatively proposes the following contributions:\n\n- Asynchronized softmax with unified max value. FlashDecoding++ leverages a unified max value for different partial softmax computations. Each partial softmax result can be processed individually without synchronized update.\n- Flat GEMM optimization with double buffering.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-11",
      "content": "s a unified max value for different partial softmax computations. Each partial softmax result can be processed individually without synchronized update.\n- Flat GEMM optimization with double buffering. FlashDecoding++ only pads the matrix size to 8 rather than 64 in previous designs for flat-shaped GEMM to improve computation utilization. We point out that flat GEMMs with different shapes face varied bottlenecks, and further improve the kernel performance with techniques like double buffering.\n- Heuristic dataflow with hardware resource adaption. FlashDecoding++ takes both input dynamics and hardware configurations into consideration and dynamically applies kernel optimization for the LLM inference dataflow.\n\nBecause of the versatility of optimizations, the effectiveness of FlashDecoding++ can be proved on both NVIDIA and AMDGPUs. FlashDecoding++ achieves up to 4.86 × and 3.93 × speedup on both NVIDIA and AMD GPUs compared with Hugging Face implementations, respectively.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 11,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-12",
      "content": "FlashDecoding++ can be proved on both NVIDIA and AMDGPUs. FlashDecoding++ achieves up to 4.86 × and 3.93 × speedup on both NVIDIA and AMD GPUs compared with Hugging Face implementations, respectively. Our extensive results show that FlashDecoding++ achieves an average of 1.37 × speedup compared with FlashDecoding [13], a state-of-the-art LLM inference engine on various LLMs ( e.g., Llama2, ChatGLM2, etc.).\n\nThe rest of this paper is organized as follows. Section 2 introduces preliminaries of LLMs and related works on LLM inference acceleration. Our three techniques, the asynchronized softmax with unified max value, the flat GEMMoptimization with double buffering, and the heuristic dataflow with hardware resource adaption are detailed in Section 3, 4, and 5, respectively. Section 6 presents the evaluation results. Related works on LLM inference are introduced in Section 7, and Section 8 concludes the paper.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 12,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-13",
      "content": "ce adaption are detailed in Section 3, 4, and 5, respectively. Section 6 presents the evaluation results. Related works on LLM inference are introduced in Section 7, and Section 8 concludes the paper.\n\nFigure 3: FlashDecoding++ proposes three solutions for corresponding challenges in Large Language Model inference. (a) FlashDecoding++ proposes the asynchronized softmax with unified max value technique, avoiding synchronized update to previous partial attention results. (b) FlashDecoding++ optimizes flat GEMM by improving computation utilization. (c) FlashDecoding++ heuristically optimizes dataflow.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 13,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-14",
      "content": "The task of LLM inference is to generate tokens from the input sequence, which can be used to complete a sentence or answer a question. An overview of the LLM inference dataflow is shown in Figure 2. As we can see, the LLM inference dataflow can be organized into two typical phases with similar operations: one prefill phase and several decode phases. The prefill phase 'understands\" the input sequence ( i.e., 'What is the largest ocean?'). Each token (we set one word as a token in Figure 2 is encoded as an embedding vector, and the input sequence is organized into a matrix. The main output of the prefill phase is a new token, which is predicted to be the next token after the input sequence ( i.e., 'Pacific\" in this figure). The decode phase 'generates\" the output sequence ( i.e., 'Pacific', 'Ocean\", etc.) The output token of the prefill phase is taken as the input of the decode phase.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.1 LLMInference Dataflow Overview",
        "chunkIndex": 14,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-15",
      "content": "e ( i.e., 'Pacific\" in this figure). The decode phase 'generates\" the output sequence ( i.e., 'Pacific', 'Ocean\", etc.) The output token of the prefill phase is taken as the input of the decode phase. The decode phase is executed autogressively, and each output token is used as the input token for the next The decode ( e.g., 'Ocean\" is further used as the input).",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.1 LLMInference Dataflow Overview",
        "chunkIndex": 15,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-16",
      "content": "The main operations in LLM inference are depicted as operation ① to ⑥ in Figure 2, including the linear projection ( ① and ⑤ ), the attention ( ② , ③ , and ④ ), and the feedforward network ( ⑥ ). For simplicity, operations like position embedding [26], non-linear activation [27, 28, 29], mask [26], and others are not shown in the figure. Operations in the prefill phase and the decode phase are different in the shape of data. Because only one token (batch size = 1) or few tokens (batch size &gt; 1) are processed at one time, input matrices in the decode phase are flat-shape matrices or even vectors.\n\nLinear Projection. The linear projection performs as the fully connected layer, multiplying the input with weight matrices ( i.e., W K , W Q , W V , W O , called K,Q,V projection and O projection). For the prefill phase, the K,Q,V projection generates matrices K,Q,V .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.2 Operations in LLM Inference",
        "chunkIndex": 16,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-17",
      "content": "cted layer, multiplying the input with weight matrices ( i.e., W K , W Q , W V , W O , called K,Q,V projection and O projection). For the prefill phase, the K,Q,V projection generates matrices K,Q,V . For the decode phase, the K,Q,V projection generates three corresponding vectors and concatenated with K and V ( i.e., KVcache, yellow and light blue in Figure 2 in the prefill phase.\n\n<!-- formula-not-decoded -->\n\nFigure 4: Comparison of different softmax computation schemes. (a) Softmax computation for the whole vector. (b) Computing partial softmax for each partial vector, and a synchronized update operation is required for all partial softmax results. (c) Computing partial softmax using a unified max value, and each partial vector is processed individually without synchronized update.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.2 Operations in LLM Inference",
        "chunkIndex": 17,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-18",
      "content": "ion is required for all partial softmax results. (c) Computing partial softmax using a unified max value, and each partial vector is processed individually without synchronized update.\n\n<!-- image -->\n\nAttention. The attention operation is mainly divided into three operations ( ② to ④ Q × K , softmax , Attention × V ), as shown in Eq. (1). For P = Q × K T , the softmax operation is performed for each row of the result matrix of P . The detailed softmax computation is shown in Figure 4(a). The maximum value m ( x ) is first calculated. The exponent of each element divided by e m ( x ) , f ( x ) , is then processed. These exponents are normalized to the summation of all exponents ( i.e., l ( x ) ) to get the softmax result.\n\nFeedforward Network. The feedforward network primarily comprises two fully connected layers. The first one ( ⑥ FFN 1 ) expands the feature dimensions to enhance the representational capacity.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.2 Operations in LLM Inference",
        "chunkIndex": 18,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-19",
      "content": "result.\n\nFeedforward Network. The feedforward network primarily comprises two fully connected layers. The first one ( ⑥ FFN 1 ) expands the feature dimensions to enhance the representational capacity. The second one ( ⑥ FFN 2 ) restores the feature dimensions and serves as the output layer.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.2 Operations in LLM Inference",
        "chunkIndex": 19,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-20",
      "content": "The softmax operation shown in Figure 4(a) requires all global data to be calculated and stored before it can proceed. This results in high memory consumption and low parallelism. Latter works propose the partial softmax technique to reduce memory consumption [18, 19] or improve parallelism [13]. Figure 4(b) shows the diagram of the partial softmax operation. The main idea is to divide the vector x into partial vectors ( i.e, x ′ and x ′′ ). The partial softmax results of x ′ and x ′′ are calculated separately according to Figure 4(a), and then synchronously updated by each other. The detailed computation of this synchronized update is shown in Equation (2). With the implementation of partial softmax, we can achieve efficient parallelism of computation while reducing memory cost for attention computation.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.3 Attention Optimization",
        "chunkIndex": 20,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-21",
      "content": "own in Equation (2). With the implementation of partial softmax, we can achieve efficient parallelism of computation while reducing memory cost for attention computation.\n\n<!-- formula-not-decoded -->\n\nHowever, since the partial softmax needs to be updated according to other partial softmax results, it unavoidably introduces data synchronization operations. According to our profiling result, such a synchronized update operation leads to 18.8% overheads in the attention computation for Llama2-7B inference on NVIDIA Tesla A100 GPU with 1024 input length.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "2.3 Attention Optimization",
        "chunkIndex": 21,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-22",
      "content": "Motivation. The partial softmax operation requires synchronization among different partial vectors, leading to ∼ 20% overheads of the attention operation. As is shown in Figure 3(a), the synchronization is required after the maximum value of the partial vector is calculated. The maximum value is used to update previous partial softmax ( i.e., recompute previous attention) results. Thus, to reduce synchronization overheads, the key problem to be solved is how to compute each partial softmax result without requiring results from other partial softmax computation.\n\nFigure 5: The statistical distribution of x i (elements in the input vectors of softmax) in typical LLMs with different inputs.\n\n<!-- image -->\n\nChallenge. The reason that synchronization is required lies in that the maximum value of each partial vector is different.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 22,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-23",
      "content": "t vectors of softmax) in typical LLMs with different inputs.\n\n<!-- image -->\n\nChallenge. The reason that synchronization is required lies in that the maximum value of each partial vector is different. The maximum value is used to avoid overflow of the exponent operation ( f ( x ) in Figure 4(a)), and exponents are summed ( l ( x ) in Figure 4(a)) as the denominator of the softmax operation. Such a non-linear operation on each partial maximum value makes the synchronization among each partial softmax computation unavoidable.\n\nAnalysis and Insights. According to the formula of softmax computation, the maximum value is used as the scaling factor for both the numerator and the denominator ( i.e., f ( x ) and l ( x ) in Figure 4(a)). Our key insight is, the scaling factor can be an arbitrary number rather than using the maximum value mathematically, shown in Equation (3). When we set ϕ = 0 , it becomes the original softmax computation [30].\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 23,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-24",
      "content": "an be an arbitrary number rather than using the maximum value mathematically, shown in Equation (3). When we set ϕ = 0 , it becomes the original softmax computation [30].\n\n<!-- formula-not-decoded -->\n\nHowever, the scaling factor cannot be an arbitrary number considering the overflowing of the exponent computation. For the case where x i ≫ ϕ , e x i -ϕ overflows and cannot be represented using a fix-width floating point number ( e.g., float32 for exponent results in current LLM engines). For another case where x i ≪ ϕ , e x i -ϕ → 0 , leading to precision loss. Thus, a proper scaling factor ϕ should be carefully selected to avoid the two cases above. Figure 5 shows the statistical distribution of x i (elements in the input vectors of softmax) in typical LLMs with different inputs [31]. Our key insight is, &gt; 99 . 99% x i are within a certain range . Specifically, for Llama2-7B, we have -16 . 8 &lt; x i &lt; 6 . 5 for &gt; 99 . 99% x i .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 24,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-25",
      "content": "in typical LLMs with different inputs [31]. Our key insight is, &gt; 99 . 99% x i are within a certain range . Specifically, for Llama2-7B, we have -16 . 8 &lt; x i &lt; 6 . 5 for &gt; 99 . 99% x i . Because e b -a and e a -b can be represented by a float32 format, we can set ϕ = a in Equation (3). For OPT-6.7B, we do not apply the technique in this section because of the large range in Figure 5.\n\nApproach: Asynchronization. Based on the insights above, each partial softmax computation shares a unified maximum value, ϕ . After the softmax operation, an inner product operation is executed between the softmax result and a column of V ( i.e., v ). Assume that the input vector x can be divided into p partial vectors, x = [ x (1) , ..., x ( p ) ] ( v = [ v (1) , ..., v ( p ) ] correspondingly), we have:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 25,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-26",
      "content": "( i.e., v ). Assume that the input vector x can be divided into p partial vectors, x = [ x (1) , ..., x ( p ) ] ( v = [ v (1) , ..., v ( p ) ] correspondingly), we have:\n\n<!-- formula-not-decoded -->\n\nThe inner accumulation in both the numerator and the denominator only take the partial vectors x ( j ) and v ( j ) as input, thus they can be processed asynchronously and individually. The outer accumulation is only processed after all partial vectors are processed. As we can see in Figure 4(c), each f ( x ( j ) ) is calculated individually, and softmax ( x ) is calculated after all x ( j ) is calculated.\n\n(b) Calculate softmax(y) × v T\n\n<!-- image -->\n\nFigure 6: Example of asynchronized partial softmax computation. (a) Each partial softmax result is process individually without the synchronized update. (b) The recomputation process for all parital softmax computation is required when overflow happens.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 26,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-27",
      "content": "mputation. (a) Each partial softmax result is process individually without the synchronized update. (b) The recomputation process for all parital softmax computation is required when overflow happens.\n\nApproach: Recomputation. Without loss of generality, we assume a &lt; x i -ϕ &lt; b for each x i to ensure precision and avoid overflow. Then, the partial softmax operation is processed individually. However, when x i -ϕ ≤ a or x i -ϕ ≥ b , the asynchronized partial softmax computation is terminated for the vector x where x i belongs to. The softmax is then recomputed using the synchronized partial softmax scheme (used in FlashAttention [18, 19] and FlashDecoding [13]) shown in Figure 4(b). Such a recomputation scheme avoids overflow while introducing negligible overheads based on the statistical data shown in Figure 5.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 27,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-28",
      "content": "lashAttention [18, 19] and FlashDecoding [13]) shown in Figure 4(b). Such a recomputation scheme avoids overflow while introducing negligible overheads based on the statistical data shown in Figure 5.\n\nExample. Figure 6 shows an example of the asynchronized softmax scheme. We set a = -3 , b = 3 , ϕ = 6 . Two vectors x and y are calculated from Q × K T in Equation (1), and are divided into 2 partial vectors. We omit the process from Q × K T to these partial vectors. For each x i , we have a &lt; x i -ϕ &lt; b , we process e x 1 -ϕ · v 1 + e x 2 -ϕ · v 2 and e x 1 -ϕ + e x 2 -ϕ for the first partial vector of x using two asynchronized threads. Then, each thread moves to the next partial vector for the corresponding computation ( i.e., e x 3 -ϕ · v 3 + e x 4 -ϕ · v 4 and e x 3 -ϕ + e x 4 -ϕ ). Two threads are synchronized when all partial vectors are processed, and perform the division operation in Equation (4). For y , the first partial vector is processed similarly.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 28,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-29",
      "content": "e x 3 -ϕ + e x 4 -ϕ ). Two threads are synchronized when all partial vectors are processed, and perform the division operation in Equation (4). For y , the first partial vector is processed similarly. However, we find that y 3 -ϕ &gt; b , then two threads are terminated and the first thread recomputes all partial vectors according to the synchronized partial softmax scheme in Figure 4(b).",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "3 Asynchronized Softmax with Unified Maximum Value",
        "chunkIndex": 29,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-30",
      "content": "Motivation. The process of the decode phase is mainly composed of GEMV (batch size=1) or flat GEMM (batch size &gt; 1) operation. Without loss of generality, GEMV/GEMM operations can be represented using M,N,K , where the sizes of two multiplied matrices are M × K and K × N . Previous LLM inference engines utilize Tensor Core to accelerate these operations using libraries like cuBLAS [24] and CUTLASS [25]. Although modern Tensor Core architectures [32] process GEMM with M = 8 , these libraries usually tile the M -dimension to 64 to hide memory latency. However, for GEMV or flat GEMM operations in the decode phase, we usually have M ≪ 64 and the M -dimension is padded to 64 with zeros. The padding leads to under-utilized computation, and the key problem is to process GEMV or flat GEMM operations with smaller tiles ( i.e., padding to 8 corresponding to modern Tensor Core architectures) in the M -dimension .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 30,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-31",
      "content": "er-utilized computation, and the key problem is to process GEMV or flat GEMM operations with smaller tiles ( i.e., padding to 8 corresponding to modern Tensor Core architectures) in the M -dimension .\n\nChallenge. Processing GEMV or flat GEMM operations is non-trivial when the M -dimension is padded to 8. The tiling technique in modern libraries like cuBLAS [24] and CUTLASS [25] can only be applied to the N -dimension and\n\nFigure 7: Normalized flat GEMM performance under different N -dimension sizes and N -dimension tiling sizes. We set M = 8 and execute GEMM on the NVIDIA Tesla A100 GPU.\n\n<!-- image -->\n\nthe K -dimension. Tiles on the K -dimension are processed sequentially in a GPU block to avoid atomic operations during reduction. Tiling on the N -dimension affects both parallelism and computation/memory ratio, which are both important for GEMV and flat GEMM acceleration.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 31,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-32",
      "content": "a GPU block to avoid atomic operations during reduction. Tiling on the N -dimension affects both parallelism and computation/memory ratio, which are both important for GEMV and flat GEMM acceleration.\n\nAnalysis and Insights. Assume that tiling sizes of the N -dimension and the K -dimension are B N and B K , respectively. The computation of each GEMM tile is 2 × M × B N × B K with total B = N × K B N × B K GEMMtiles. The total memory access is ( M × B K + B N × B K ) × B + M × N . Thus, the computation/memory ratio is:\n\n<!-- formula-not-decoded -->\n\nOn the other hand, the parallelism is N B N . Thus, the computation/memory ratio shows a positive correlation with B N while the parallelism shows a negative correlation with B N , exposing a contradiction on improving the performance of GEMV or flat GEMM. We depict the normalized performance of the flat GEMM in Figure 7 with different N and B N . Our key insight is, for the smaller N , the flat GEMM is parallelism-bounded .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 32,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-33",
      "content": "rmance of GEMV or flat GEMM. We depict the normalized performance of the flat GEMM in Figure 7 with different N and B N . Our key insight is, for the smaller N , the flat GEMM is parallelism-bounded . There are 108 Streaming Multiprocessors (SMs) in the NVIDIA Tesla A100. N B N tends to be a constant ( e.g., 128 or 256), which is related to the hardware parallelism (number of SMs). Another key insight is, for the larger N , the flat GEMM becomes memory-bounded . The performance of these cases can be improved by hiding memory access latency.\n\nApproach: Double Buffering. In order to hide memory access latency, we introduce the double buffering technique. for the flat GEMM operation. We allocate two separate buffers in the shared memory. The tile in one buffer performs the GEMM operation, while another buffer loads a new tile for the next GEMM operation. Thus, the computation and the memory access are overlapped. We apply such a technique when N is large in our practice.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 33,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-34",
      "content": "EMM operation, while another buffer loads a new tile for the next GEMM operation. Thus, the computation and the memory access are overlapped. We apply such a technique when N is large in our practice.\n\nExample. Figure 8 shows the example of our flat GEMM optimization with double buffering. For M &lt; 8 , the M -dimension is first padded to 8 considering modern Tensor Core architectures. Workloads in the K -dimension are processed within one GPU block ( e.g., A 1 , A 2 , A 3 , ... ), while workloads in the N -dimension are processed in parallel using different GPU blocks ( e.g., C 1 , C 2 , ... ). We take GPU Block 1 as an example, the first tile for each matrix in the K -dimension ( i.e., A 1 and B 1 ) is loaded to the left buffer in the shared memory. Then, the GEMM operation is performed between A 1 and B 1 . Consequently, A 2 and B 2 are loaded to the right buffer in the shared memory. The following tiles are processed similarly according to the double buffering scheme.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 34,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-35",
      "content": "is performed between A 1 and B 1 . Consequently, A 2 and B 2 are loaded to the right buffer in the shared memory. The following tiles are processed similarly according to the double buffering scheme.\n\nFigure 8: Double buffering for flat GEMM when N -dimension is large. The M -dimension is padded to 8 and not tiled.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "4 Flat GEMM Optimization with Double Buffering",
        "chunkIndex": 35,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-36",
      "content": "Motivation. Although FlashDecoding++ optimizes the flat GEMM operation in Section 4, it does not cover all operations (even only for GEMMs) in the LLM inference. As mentioned in Figure 2, the shapes of GEMMs in different operations and two phases vary. Thus, the GEMM workload in the LLM inference can be GEMV (batch size=1 for the decode phase), flat GEMM (small batch size for the decode phase and short sequence length for the prefill phase) and conventional GEMM (large batch size or long sequence length for the prefill phase). In order to leverage the powerful computational ability of Tensor Core, current frameworks like FasterTransformer [33] and DeepSpeed [9] tend to utilize the highly optimized GEMM implementation from cuBLAS [24] to deal with different workloads. However, the Tensor Core implementation fails with the GEMV workload. The GEMV workload can be optimized by utilizing CUDA Core in previous designs like FastGEMV [34].",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 36,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-37",
      "content": "o deal with different workloads. However, the Tensor Core implementation fails with the GEMV workload. The GEMV workload can be optimized by utilizing CUDA Core in previous designs like FastGEMV [34]. For a Llama2-7B linear layer in the decode phase, the Tensor Core implementation from cuBLAS only achieves 82.15% of the performance of CUDA Core implementation using FastGEMV on an NVIDIA A100 GPU. On the other hand, using CUDA Core to do the projection on a batchsize=4 decoding input only achieves 49.75% performance compared with the Tensor Core implementation. Thus, in order to approach the optimal computation performance, a heuristic dataflow is supposed to be exploited in for different workloads.\n\nChallenge. Although a heuristic dataflow potentially exists in the implementation of different linear workloads, it is challenging to build the mapping from a certain workload to an optimal implementation.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 37,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-38",
      "content": "enge. Although a heuristic dataflow potentially exists in the implementation of different linear workloads, it is challenging to build the mapping from a certain workload to an optimal implementation. In the scenario of LLM inference, there are various factors that influence the implementation performance of linear workloads: (a) Input dynamics. The variety of the batch size and the input sequence length brings dynamic workloads. (b) Model diversity. The linear workload varies with different model structures and sizes. (c) GPU capacities. The relative performance between implementations changes with GPU characteristics, such as memory bandwidth, cache size, and computational ability. (d) Engineering effects. The engineering effort also highly impacts the kernel performance. All these influential factors build a large search space, making it non-trivial to generate an effective mapping between the linear workload and the corresponding optimal implementation.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 38,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-39",
      "content": "performance. All these influential factors build a large search space, making it non-trivial to generate an effective mapping between the linear workload and the corresponding optimal implementation.\n\nAnalysis and Insights. Although all influential factors form a large search space, the homogeneity of different layers in LLM significantly reduces the search space for operator optimization. Figure 2 shows four linear GEMV/GEMM operations in the prefill phase and the decode phase, i.e., K,Q,V projection, O projection, and two feedforward operations. Each GEMV/GEMM operation can be can be abstracted as a multiplication between an ( M × K )-shaped matrix and a ( K × N )-shaped matrix. Our key insight is, there are only four [ K,N ] shapes for a certain LLM. Moreover, M is only related to the input sequence length and the batch size for the prefill phase, and the batch size for the decode phase. Figure 9(a) shows limited shapes of GEMV/GEMM operations in the LLM inference.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 39,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-40",
      "content": "y related to the input sequence length and the batch size for the prefill phase, and the batch size for the decode phase. Figure 9(a) shows limited shapes of GEMV/GEMM operations in the LLM inference.\n\nApproach: Decision flow for inflection points. Because only four [ K,N ] shapes exist for a certain LLM, we use three types of implementations for GEMV/GEMM operations when M varies: FastGEMV for the GEMV and flat GEMM operations (ImplA), our flat GEMM optimization in Section 4 (ImplB), and the CUTLASS [25] libraries optimized for the conventional GEMM (ImplC). Thus, it is important to decide whether applying ImplA or ImplB for a small M , and ImplB or ImplC for a large M . Figure 9(b) shows the decision flow. FlashDecoding++ profiles the performance of ImplA and ImplB for a certain M , and increases M to find an inflection point M 1 where the performance of ImplB is",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 40,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-41",
      "content": "M . Figure 9(b) shows the decision flow. FlashDecoding++ profiles the performance of ImplA and ImplB for a certain M , and increases M to find an inflection point M 1 where the performance of ImplB is\n\nFigure 9: Heuristic dataflow with hardware resource adaption in FlashDecoding++ . (a) Only four [ N,K ] shapes exist for a certain LLM. (b) The decision flow. We traverse all [ N,K ] selections and profile the performance of three representative implementations. M is increased to find two inflection points for runtime heuristic dataflow. (c) FlashDecoding++ heuristically utilizes Tensor Core/CUDA Core with the corresponding GEMV/GEMM implementation by referring to a lookup table.\n\n<!-- image -->\n\nbetter than ImplA. Another inflection point M 2 is found similarly where the performance of ImplC is better than ImplB. Note that each [ N,K ] gets its individual M 1 and M 2 .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 41,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-42",
      "content": "table.\n\n<!-- image -->\n\nbetter than ImplA. Another inflection point M 2 is found similarly where the performance of ImplC is better than ImplB. Note that each [ N,K ] gets its individual M 1 and M 2 .\n\nApproach: Heuristic dataflow. For the runtime LLM inference, FlashDecoding++ adopts ImplA using CUDA Core when M &lt; M 1 , and ImplB/ImplC using Tensor Core when M 1 ≤ M &lt; M 2 / M 2 ≤ M . Note that the decision flow are executed offline, it does not affect the performance of runtime LLM inference.\n\nExample. Figure 9(c) shows an example of applying the heuristic dataflow for the Llama2-7B model. Four [ N,K ] shapes are [12288, 4096] for K,Q,V projection, [4096, 4096] for O projection, [11008, 4096] and [4096, 11008] for FFN. For each [ N,K ] , the inflection points are found based on the decision flow in Figure 9(c). Then, a lookup table is formed, and each GEMV/GEMM operation is executed according to corresponding implementations during runtime.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 42,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-43",
      "content": "flection points are found based on the decision flow in Figure 9(c). Then, a lookup table is formed, and each GEMV/GEMM operation is executed according to corresponding implementations during runtime. In this example, FastGEMV is adopted for the K,Q,V projection when batch size=1 ( M = 1 ) for the decode phase, and our flat GEMM optimization is applied when batch size=1/input sequence length=8 for FFN 1 ( M = 8 ).",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "5 Heuristic Dataflow with Hardware Resource Adaption",
        "chunkIndex": 43,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-44",
      "content": "We evaluate the performance of FlashDecoding++ on different GPUs with various Large Language Models. We compare the performance with several state-of-the-art LLM inference engines.\n\nTable 1: Hardware Platforms\n\n|     | NVIDIA                           | NVIDIA                        | AMD                  | AMD                          |\n|-----|----------------------------------|-------------------------------|----------------------|------------------------------|\n| GPU | Tesla A100 80 GB CUDA 12.2       | RTX3090 24 GB CUDA 11.6       | MI210 64GB ROCm 5.7  | RX7900XTX 24GB ROCm 5.6      |\n| CPU | Intel Xeon Silver 8358P 2.60 GHz | Intel Xeon Gold 6226R 2.90GHz | AMDEPYC 7K62 2.60GHz | Intel Core i9-10940X 3.30GHz |\n\nTable 2: Model Configuration\n\n| Model       |   Dimension |   Heads |   Layers | Context Length   |\n|-------------|-------------|---------|----------|------------------|\n| Llama2-7B   |        4096 |      32 |       32 | 4k               |\n| Llama2-13B  |        5120 |",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.1 Experiments Setup",
        "chunkIndex": 44,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-45",
      "content": "Layers | Context Length   |\n|-------------|-------------|---------|----------|------------------|\n| Llama2-7B   |        4096 |      32 |       32 | 4k               |\n| Llama2-13B  |        5120 |      40 |       40 | 4k               |\n| OPT-6.7B    |        4096 |      32 |       32 | 2k               |\n| ChatGLM2-6B |        4096 |      32 |       32 | 32k              |",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.1 Experiments Setup",
        "chunkIndex": 45,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-46",
      "content": "We evaluate the performance of FlashDecoding++ and other LLM engines on both NVIDIA and AMD platforms to make a comprehensive comparison. We choose two different GPUs for each platform: Tesla A100 and RTX3090 for NVIDIA, MI210 and RX7900XTX for AMD. We show the detailed configuration in Table 6.1.1.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.1.1 Hardware Platforms",
        "chunkIndex": 46,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-47",
      "content": "We implement our FlashDecoding++ using the Pytorch-based front-end with the C++ and CUDA backend for NVIDIA GPUs while ROCm for AMD GPUs. We compare the inference performance in both prefill phase and decode phase with the following LLM engine baselines: Hugging Face (HF) [35], vLLM [11], DeepSpeed [9], TensorRT-LLM [14], OpenPPL [12], and FlashAttention2/FlashDecoding [19, 13]. These baselines are introduced in Section 7.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.1.2 LLMEngine Baselines",
        "chunkIndex": 47,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-48",
      "content": "We evaluate the performance of FlashDecoding++ with other LLM inference engines on three typical Large Language Models: Llama2, OPT, and ChatGLM2. Table 6.1.2 shows the detailed configuration of these models. Note that there may be several models in one LLM ( e.g., Llama2-7B, Llama2-13B) with different configurations ( e.g., number of heads and layers).\n\n- Llama2 [1] is a mainstream open-source LLM set released by Meta in 2023. It is a collection of pretrained and fine-tuned generative text models ranging in scale from 7B to 70B parameters.\n- OPT [36], is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters released by Meta AI.\n- ChatGLM2 [37] is an open-source LLM supporting bilingual (Chinese-English) chat.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.1.3 Models",
        "chunkIndex": 48,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-49",
      "content": "We compare FlashDecoding++ with state-of-the-art LLM inference engines in Figure 10 and Figure 11 on NVIDIA GPUs, Figure 12 and Figure 13 for AMD GPUs. For the decode phase, FlashDecoding++ achieves up to 4.86 × speedup compared with Hugging Face implementations on three LLMs and two GPUs. The average speedup over vLLM, DeepSpeed, TensorRT-LLM, OpenPPL, and FlashDecoding is 1.24 × , 1.44 × , 1.13 × , 1.24 × , and 1.21 × ( 1.37 × on Tesla A100 compared with FlashDecoding), respectively. For the prefill phase, FlashDecoding++ achieves up to 1.40 × speedup compared with Hugging Face implementations. The average speedup over DeepSpeed, TensorRT-LLM, OpenPPL, FlashAttention2 and FlashDecoding is 1.05 × , 1.06 × , 1.08 × , 1.09 × , and 1.08 × , respectively. We also show the decode results on two AMD GPUs. Currently, only the original Hugging Face implementation can be executed on AMD GPUs as the baseline.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.2 Comparison with State-of-the-art",
        "chunkIndex": 49,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-50",
      "content": "× , 1.08 × , 1.09 × , and 1.08 × , respectively. We also show the decode results on two AMD GPUs. Currently, only the original Hugging Face implementation can be executed on AMD GPUs as the baseline. FlashDecoding++ achieves up to 2.27 × and 3.93 × compared with the baseline on RX7900XTX and MI210, respectively.\n\nFigure 10: Speedup of the decode phase on NVIDIA GPUs. Blank bars represent the model cannot be executed ( e.g., OpenPPL does not support OPT-6.7B/ChatGLM2-6B, TensorRT-LLM fails to compile the model with &gt; 8 K input length, and etc.)\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "6.2 Comparison with State-of-the-art",
        "chunkIndex": 50,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-51",
      "content": "Large language model inference acceleration has gained significant attention in recent research, with several notable approaches and techniques emerging in the field. DeepSpeed [9] is a comprehensive engine that optimizes both the training and inference phases for LLMs. It achieves robust inference performance through kernel fusion and efficient GPU memory management, with a particular focus on optimizing memory usage for KVcache. vLLM [11] improves\n\nFigure 11: Speedup of the prefill phase on NVIDIA GPUs.\n\n<!-- image -->\n\nGPU memory utilization by efficient memory management techniques and the PageAttention method, leading to increased maximum batch sizes and elevating the upper limit of inference performance. FlashAttention [18, 19] optimizes the self-attention computation process during the prefill phase through improved parallelism and workload distribution.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "7 Related Works",
        "chunkIndex": 51,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-52",
      "content": "ting the upper limit of inference performance. FlashAttention [18, 19] optimizes the self-attention computation process during the prefill phase through improved parallelism and workload distribution. FlashDecoding [13] is an extension of FlashAttention and enhances the parallelism through spliting K and V , supporting efficient self-attention computation for long sequence during the decode phase. FasterTransformer [33] and OpenPPL [12] implement large model inference engines using C++ to reduce overhead resulting from kernels scheduling, compared to Python implementations. They also employ memory management techniques and kernel fusion to achieve efficient LLM inference. TensorRT-LLM [14] is built upon the TensorRT [38] and the FasterTransformer [33] engine ( C++ ) and incorporates cutting-edge open-source technologies such as FlashAttention [18, 19]. Additionally, it enhances its ease of use by providing the Python API .\n\nFigure 12: Speedup of the decode phase on AMD RX7900XTX.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "7 Related Works",
        "chunkIndex": 52,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-53",
      "content": "cutting-edge open-source technologies such as FlashAttention [18, 19]. Additionally, it enhances its ease of use by providing the Python API .\n\nFigure 12: Speedup of the decode phase on AMD RX7900XTX.\n\n<!-- image -->\n\nFigure 13: Speedup of the decode phase on AMD MI210.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "7 Related Works",
        "chunkIndex": 53,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-54",
      "content": "100 3 HF Ours Ours (token/s) We propose FlashDecoding++ , a fast Large Language Model inference engine in this paper. FlashDecoding++ accelerates mainstream LLMs with multiple hardware backend support. FlashDecoding++ proposes three novel designs: the asynchronized softmax with unified max value, the flat GEMM optimization with double buffering, and the heuristic dataflow with hardware resource adaption, achieving up to 4.86 × and 3.93 × speedup on NVIDIA and AMD GPUs compared with Hugging Face implementations. FlashDecoding++ also achieves an average of 1.37 × speedup compared with state-of-the-art LLM inference engines, FlashDecoding, on various LLMs.\n\n2\n\n1\n\n0\n\n50\n\n0\n\nSpeedup\n\n128\n\n512\n\n1k\n\n(a) Llama2-7B\n\n2k\n\n128\n\n512\n\n1k\n\n(b) Llama2-13B",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "8 Conclusion",
        "chunkIndex": 54,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-55",
      "content": "- [1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n- [2] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine , 29(8):1930-1940, 2023.\n- [3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-56",
      "content": "efan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, M",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-57",
      "content": "ao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\n- [4] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-58",
      "content": "a R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine. Communications Medicine , 3(1):141, 2023.\n- [5] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Receive, reason, and react: Drive as you say with large language models in autonomous vehicles. arXiv preprint arXiv:2310.08034 , 2023.\n- [6] OpenAI. Openai pricing. [Online], 2023. https://openai.com/pricing .\n- [7] Nerdynav. Up-to-date chatgpt statistics &amp; user numbers [oct 2023]. [Online], 2023. https://nerdynav.com/ chatgpt-statistics .\n- [8] AFZAL AHMAD DYLAN PATEL. The inference cost of search disruption - large language model cost analysis. [Online], 2023.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-59",
      "content": "bers [oct 2023]. [Online], 2023. https://nerdynav.com/ chatgpt-statistics .\n- [8] AFZAL AHMAD DYLAN PATEL. The inference cost of search disruption - large language model cost analysis. [Online], 2023. https://www.semianalysis.com/p/the-inference-cost-of-search-disruption .\n- [9] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis , pages 1-15. IEEE, 2022.\n- [10] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-60",
      "content": "ng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. 2023.\n- [11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles , pages 611-626, 2023.\n- [12] Sensetime. Openppl: A high-performance deep learning inference platform. [Online], 2023. https://openppl. ai/home .\n- [13] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. [Online], 2023. https://crfm.stanford.edu/2023/10/12/flashdecoding.html .\n- [14] Neal Vaidya, Fred Oh, and Nick Comly. Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. [Online], 2023.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-61",
      "content": "tanford.edu/2023/10/12/flashdecoding.html .\n- [14] Neal Vaidya, Fred Oh, and Nick Comly. Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. [Online], 2023. https://github.com/NVIDIA/TensorRT-LLM .\n- [15] Sensetime. A light and fast inference service for llm. [Online], 2023. https://github.com/ModelTC/ lightllm .\n\n- [16] Text generation inference: Fast inference optimize for llms. [Online], 2023. https://github.com/ huggingface/text-generation-inference/ .\n- [17] Mlc llm: Machine learning compilation for large language models. [Online], 2023. https://github.com/ mlc-ai/mlc-llm .\n- [18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-62",
      "content": "attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 , 2023.\n- [20] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan, and Frost Ming. OpenLLM: Operating LLMs in production, June 2023.\n- [21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.\n- [22] Z Dong, T Tang, L Li, and WX Zhao. A survey on long text modeling with transformers. arxiv 2023. arXiv preprint arXiv:2302.14502 .\n- [23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.\n- [24] NVIDIA. cublas: Basic linear algebra on nvidia gpus.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-63",
      "content": "ian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.\n- [24] NVIDIA. cublas: Basic linear algebra on nvidia gpus. [Online], 2017. https://developer.nvidia.com/ cublas .\n- [25] NVIDIA. Cutlass: Cuda templates for linear algebra subroutines. [Online], 2017. https://github.com/ NVIDIA/cutlass .\n- [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\n- [27] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) , pages 807-814, 2010.\n- [28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016.\n- [29] Prajit Ramachandran, Barret Zoph, and Quoc V Le.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-64",
      "content": "L-10) , pages 807-814, 2010.\n- [28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016.\n- [29] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941 , 2017.\n- [30] John Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Advances in neural information processing systems , 2, 1989.\n- [31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n- [32] NVIDIA. Nvidia tensor core. [Online], 2023. https://www.nvidia.com/en-us/data-center/ tensor-cores/ .\n- [33] NVIDIA. Fastertransformer: About transformer related optimization, including bert, gpt. [Online], 2017. https://github.com/NVIDIA/FasterTransformer .\n- [34] Siping Wang. Fastgemv: High-speed gemv kernels. [Online], 2023.",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-65",
      "content": "ormer: About transformer related optimization, including bert, gpt. [Online], 2017. https://github.com/NVIDIA/FasterTransformer .\n- [34] Siping Wang. Fastgemv: High-speed gemv kernels. [Online], 2023. https://github.com/wangsiping97/ FastGEMV .\n- [35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, October 2020. Association for Computational Linguistics.\n- [36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott,",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 67
      }
    },
    {
      "id": "2311.01282v4-chunk-66",
      "content": "r Computational Linguistics.\n- [36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n- [37] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 320-335, 2022.\n- [38] NVIDIA. Nvidia tensorrt: An sdk for high-performance deep learning inference. [Online]. https://developer. nvidia.com/tensorrt .",
      "metadata": {
        "source": "arxiv:2311.01282v4",
        "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
        "authors": [
          "Ke Hong",
          "Guohao Dai",
          "Jiaming Xu",
          "Qiuli Mao",
          "Xiuhong Li",
          "Jun Liu",
          "Kangdi Chen",
          "Yuhan Dong",
          "Yu Wang"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 67
      }
    }
  ],
  "fullText": "## FLASHDECODING++: FASTER LARGE LANGUAGE MODEL INFERENCE ON GPUS\n\nKe Hong †\n\nTsinghua University &amp;Infinigence-AI\n\nQiuli Mao Tsinghua University &amp;Infinigence-AI\n\nKangdi Chen Infinigence-AI\n\nGuohao Dai † GLYPH&lt;12&gt;\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nXiuhong Li Peking University\n\nYuhan Dong\n\nTsinghua University\n\nJiaming Xu †\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nJun Liu\n\nShanghai Jiao Tong University &amp;Infinigence-AI\n\nYu Wang GLYPH&lt;12&gt; Tsinghua University\n\nGLYPH&lt;12&gt; daiguohao@sjtu.edu.cn, daiguohao@infini-ai.com, yu-wang@tsinghua.edu.cn\n\n## ABSTRACT\n\nAs the Large Language Model (LLM) becomes increasingly important in various domains, the performance of LLM inference is crucial to massive LLM applications. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ∼ 20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and &gt; 50% performance loss after padding zeros in previous designs ( e.g., cuBLAS, CUTLASS, etc.). (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.\n\nWe present FlashDecoding++ , a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1) Asynchronized softmax with unified max value. FlashDecoding++ introduces a unified max value technique for different partial softmax computations to avoid synchronization. Based on this, the fine-grained pipelining is proposed. (2) Flat GEMM optimization with double buffering. FlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource adaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource ( e.g., Tensor Core or CUDA core) considering input dynamics.Due to the versatility of optimizations in FlashDecoding++ , FlashDecoding++ can achieve up to 4.86 × and 3.93 × speedup on both NVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also achieves an average speedup of 1.37 × compared to state-of-the-art LLM inference engines on mainstream LLMs.\n\n† These authors contributed equally to this work.\n\n‡ Prof. Guohao Dai is the Chief Scientist at Infinigence-AI, Ke Hong, Jiaming Xu, Qiuli Mao, and Jun Liu are interns at Infinigence-AI.\n\nGLYPH&lt;12&gt; Prof. Guohao Dai and Prof. Yu Wang are the corresponding authors of this paper.\n\n## 1 Introduction\n\nAs the Large Language Model (LLM) achieved unprecedented success in various domains [2, 3, 4, 5], the LLM inference workload is skyrocketing. For example, OpenAI reports that GPT-4 inference with 8K context length costs $0.03 per 1K input tokens and $0.06 per 1K output tokens [6]. Currently, OpenAI has 180.5 million users and receives over 10 million queries per day [7]. Consequently, the cost to operate OpenAI's model like ChatGPT is approximately $7 million per day for the necessary computing hardware [8]. Thus, optimizations on LLM inference performance will have a huge impact considering massive LLM inference scenarios. Many recent works have proposed techniques to accelerate LLM inference tasks, including DeepSpeed [9], FlexGen [10], vLLM [11], OpenPPL [12], FlashDecoding [13], TensorRT-LLM [14], and etc [15, 16, 17, 12].\n\nThe LLM inference task generates tokens ( e.g., words) from the input sequence autoregressively, and can be organized into two typical phases: the prefill phase and the decode phase. The prefill phase generates the first token by processing the input prompt, and previous research ( e.g., FlashAttention [18, 19]) optimizes latency for this phase. The decode phase generates the following tokens sequentially, and many works [9, 10, 11, 15, 13, 14, 20] focus on improving the throughput of generating tokens ( i.e., reducing latency of each token). The prefill phase dominates total time for scenarios of long-sequence input or generating short outputs [21, 22], while the decode phase constitutes a significant portion of the time when processing long output sequences [23].\n\nFigure 2 shows the main dataflow of the LLM inference with one transformer layer for both the prefill phase and the decode phase. A transformer layer can be divided into linear GEMM (General Matrix Multiplication) operations ( e.g., K, Q, V, O weight projection and the feedforward) and the attention/softmax computation. For the attention computation, a softmax operation is adopted for a row in the attention matrix. To improve the parallelism, previous designs [18, 13] divide the attention matrices into smaller tiles and rows are also split to compute partial softmax results. A synchronized softmax operation is adopted to update previous partial softmax results when a new partial softmax result is calculated. Such a synchronized partial softmax update accounts for 18.8% for the attention computation of Llama2-7B inference according to our profiling on NVIDIA Tesla A100 GPU with 1024 input length, resulting in the first challenge for accelerating LLM inference. Secondly, the computation resources is under-utilized for the flat GEMM operation during the decode phase. Because the decode phase sequentially generates tokens, the linear GEMM operation tends to be flat-shape (even turning into the GEMV (General Matrix-Vector Multiplication) operation when the batch size is 1). For the small batch size ( e.g., 8), previous designs [24, 25] pad the matrix with zeros to perform GEMMs of larger sizes ( e.g., 64), leading to over 50% computation under-utilization. Thirdly, the performance of LLM inference suffers from the static dataflow considering input dynamics and hardware configuration. For example, the small batch size makes the decode phase of LLM inference memory-bounded and the large batch size makes it compute-bounded. A single and static dataflow may lead to 50.25% performance loss for GEMMs of different shapes in LLM inference.\n\nFigure 1: Overview of comparison between FlashDecoding++ and state-of-the-art designs. The results in the figure are reported with Llama2-7B model [1]. The left is with batch size=1 and input length=1K, and TensorRT-LLM and Hugging Face are the SOTA baseline for NVIDIA/AMD according to our experimental results. The right shows the comprehensive comparison of both first token latency and each token latency.\n\n<!-- image -->\n\nFigure 2: Overview of Large Language Model inference dataflow. We show the dataflow comparison between the prefill phase and the decode phase. The prefill phase mainly involves the GEMM operation, while the decode phase mainly involves the GEMV/Flat GEMM operation.\n\n<!-- image -->\n\nTo tackle these challenges and enable a faster Large Language Model (LLM) inference, we present FlashDecoding++ in this paper. FlashDecoding++ creatively proposes the following contributions:\n\n- Asynchronized softmax with unified max value. FlashDecoding++ leverages a unified max value for different partial softmax computations. Each partial softmax result can be processed individually without synchronized update.\n- Flat GEMM optimization with double buffering. FlashDecoding++ only pads the matrix size to 8 rather than 64 in previous designs for flat-shaped GEMM to improve computation utilization. We point out that flat GEMMs with different shapes face varied bottlenecks, and further improve the kernel performance with techniques like double buffering.\n- Heuristic dataflow with hardware resource adaption. FlashDecoding++ takes both input dynamics and hardware configurations into consideration and dynamically applies kernel optimization for the LLM inference dataflow.\n\nBecause of the versatility of optimizations, the effectiveness of FlashDecoding++ can be proved on both NVIDIA and AMDGPUs. FlashDecoding++ achieves up to 4.86 × and 3.93 × speedup on both NVIDIA and AMD GPUs compared with Hugging Face implementations, respectively. Our extensive results show that FlashDecoding++ achieves an average of 1.37 × speedup compared with FlashDecoding [13], a state-of-the-art LLM inference engine on various LLMs ( e.g., Llama2, ChatGLM2, etc.).\n\nThe rest of this paper is organized as follows. Section 2 introduces preliminaries of LLMs and related works on LLM inference acceleration. Our three techniques, the asynchronized softmax with unified max value, the flat GEMMoptimization with double buffering, and the heuristic dataflow with hardware resource adaption are detailed in Section 3, 4, and 5, respectively. Section 6 presents the evaluation results. Related works on LLM inference are introduced in Section 7, and Section 8 concludes the paper.\n\nFigure 3: FlashDecoding++ proposes three solutions for corresponding challenges in Large Language Model inference. (a) FlashDecoding++ proposes the asynchronized softmax with unified max value technique, avoiding synchronized update to previous partial attention results. (b) FlashDecoding++ optimizes flat GEMM by improving computation utilization. (c) FlashDecoding++ heuristically optimizes dataflow.\n\n<!-- image -->\n\n## 2 Background\n\n## 2.1 LLMInference Dataflow Overview\n\nThe task of LLM inference is to generate tokens from the input sequence, which can be used to complete a sentence or answer a question. An overview of the LLM inference dataflow is shown in Figure 2. As we can see, the LLM inference dataflow can be organized into two typical phases with similar operations: one prefill phase and several decode phases. The prefill phase 'understands\" the input sequence ( i.e., 'What is the largest ocean?'). Each token (we set one word as a token in Figure 2 is encoded as an embedding vector, and the input sequence is organized into a matrix. The main output of the prefill phase is a new token, which is predicted to be the next token after the input sequence ( i.e., 'Pacific\" in this figure). The decode phase 'generates\" the output sequence ( i.e., 'Pacific', 'Ocean\", etc.) The output token of the prefill phase is taken as the input of the decode phase. The decode phase is executed autogressively, and each output token is used as the input token for the next The decode ( e.g., 'Ocean\" is further used as the input).\n\n## 2.2 Operations in LLM Inference\n\nThe main operations in LLM inference are depicted as operation ① to ⑥ in Figure 2, including the linear projection ( ① and ⑤ ), the attention ( ② , ③ , and ④ ), and the feedforward network ( ⑥ ). For simplicity, operations like position embedding [26], non-linear activation [27, 28, 29], mask [26], and others are not shown in the figure. Operations in the prefill phase and the decode phase are different in the shape of data. Because only one token (batch size = 1) or few tokens (batch size &gt; 1) are processed at one time, input matrices in the decode phase are flat-shape matrices or even vectors.\n\nLinear Projection. The linear projection performs as the fully connected layer, multiplying the input with weight matrices ( i.e., W K , W Q , W V , W O , called K,Q,V projection and O projection). For the prefill phase, the K,Q,V projection generates matrices K,Q,V . For the decode phase, the K,Q,V projection generates three corresponding vectors and concatenated with K and V ( i.e., KVcache, yellow and light blue in Figure 2 in the prefill phase.\n\n<!-- formula-not-decoded -->\n\nFigure 4: Comparison of different softmax computation schemes. (a) Softmax computation for the whole vector. (b) Computing partial softmax for each partial vector, and a synchronized update operation is required for all partial softmax results. (c) Computing partial softmax using a unified max value, and each partial vector is processed individually without synchronized update.\n\n<!-- image -->\n\nAttention. The attention operation is mainly divided into three operations ( ② to ④ Q × K , softmax , Attention × V ), as shown in Eq. (1). For P = Q × K T , the softmax operation is performed for each row of the result matrix of P . The detailed softmax computation is shown in Figure 4(a). The maximum value m ( x ) is first calculated. The exponent of each element divided by e m ( x ) , f ( x ) , is then processed. These exponents are normalized to the summation of all exponents ( i.e., l ( x ) ) to get the softmax result.\n\nFeedforward Network. The feedforward network primarily comprises two fully connected layers. The first one ( ⑥ FFN 1 ) expands the feature dimensions to enhance the representational capacity. The second one ( ⑥ FFN 2 ) restores the feature dimensions and serves as the output layer.\n\n## 2.3 Attention Optimization\n\nThe softmax operation shown in Figure 4(a) requires all global data to be calculated and stored before it can proceed. This results in high memory consumption and low parallelism. Latter works propose the partial softmax technique to reduce memory consumption [18, 19] or improve parallelism [13]. Figure 4(b) shows the diagram of the partial softmax operation. The main idea is to divide the vector x into partial vectors ( i.e, x ′ and x ′′ ). The partial softmax results of x ′ and x ′′ are calculated separately according to Figure 4(a), and then synchronously updated by each other. The detailed computation of this synchronized update is shown in Equation (2). With the implementation of partial softmax, we can achieve efficient parallelism of computation while reducing memory cost for attention computation.\n\n<!-- formula-not-decoded -->\n\nHowever, since the partial softmax needs to be updated according to other partial softmax results, it unavoidably introduces data synchronization operations. According to our profiling result, such a synchronized update operation leads to 18.8% overheads in the attention computation for Llama2-7B inference on NVIDIA Tesla A100 GPU with 1024 input length.\n\n## 3 Asynchronized Softmax with Unified Maximum Value\n\nMotivation. The partial softmax operation requires synchronization among different partial vectors, leading to ∼ 20% overheads of the attention operation. As is shown in Figure 3(a), the synchronization is required after the maximum value of the partial vector is calculated. The maximum value is used to update previous partial softmax ( i.e., recompute previous attention) results. Thus, to reduce synchronization overheads, the key problem to be solved is how to compute each partial softmax result without requiring results from other partial softmax computation.\n\nFigure 5: The statistical distribution of x i (elements in the input vectors of softmax) in typical LLMs with different inputs.\n\n<!-- image -->\n\nChallenge. The reason that synchronization is required lies in that the maximum value of each partial vector is different. The maximum value is used to avoid overflow of the exponent operation ( f ( x ) in Figure 4(a)), and exponents are summed ( l ( x ) in Figure 4(a)) as the denominator of the softmax operation. Such a non-linear operation on each partial maximum value makes the synchronization among each partial softmax computation unavoidable.\n\nAnalysis and Insights. According to the formula of softmax computation, the maximum value is used as the scaling factor for both the numerator and the denominator ( i.e., f ( x ) and l ( x ) in Figure 4(a)). Our key insight is, the scaling factor can be an arbitrary number rather than using the maximum value mathematically, shown in Equation (3). When we set ϕ = 0 , it becomes the original softmax computation [30].\n\n<!-- formula-not-decoded -->\n\nHowever, the scaling factor cannot be an arbitrary number considering the overflowing of the exponent computation. For the case where x i ≫ ϕ , e x i -ϕ overflows and cannot be represented using a fix-width floating point number ( e.g., float32 for exponent results in current LLM engines). For another case where x i ≪ ϕ , e x i -ϕ → 0 , leading to precision loss. Thus, a proper scaling factor ϕ should be carefully selected to avoid the two cases above. Figure 5 shows the statistical distribution of x i (elements in the input vectors of softmax) in typical LLMs with different inputs [31]. Our key insight is, &gt; 99 . 99% x i are within a certain range . Specifically, for Llama2-7B, we have -16 . 8 &lt; x i &lt; 6 . 5 for &gt; 99 . 99% x i . Because e b -a and e a -b can be represented by a float32 format, we can set ϕ = a in Equation (3). For OPT-6.7B, we do not apply the technique in this section because of the large range in Figure 5.\n\nApproach: Asynchronization. Based on the insights above, each partial softmax computation shares a unified maximum value, ϕ . After the softmax operation, an inner product operation is executed between the softmax result and a column of V ( i.e., v ). Assume that the input vector x can be divided into p partial vectors, x = [ x (1) , ..., x ( p ) ] ( v = [ v (1) , ..., v ( p ) ] correspondingly), we have:\n\n<!-- formula-not-decoded -->\n\nThe inner accumulation in both the numerator and the denominator only take the partial vectors x ( j ) and v ( j ) as input, thus they can be processed asynchronously and individually. The outer accumulation is only processed after all partial vectors are processed. As we can see in Figure 4(c), each f ( x ( j ) ) is calculated individually, and softmax ( x ) is calculated after all x ( j ) is calculated.\n\n(b) Calculate softmax(y) × v T\n\n<!-- image -->\n\nFigure 6: Example of asynchronized partial softmax computation. (a) Each partial softmax result is process individually without the synchronized update. (b) The recomputation process for all parital softmax computation is required when overflow happens.\n\nApproach: Recomputation. Without loss of generality, we assume a &lt; x i -ϕ &lt; b for each x i to ensure precision and avoid overflow. Then, the partial softmax operation is processed individually. However, when x i -ϕ ≤ a or x i -ϕ ≥ b , the asynchronized partial softmax computation is terminated for the vector x where x i belongs to. The softmax is then recomputed using the synchronized partial softmax scheme (used in FlashAttention [18, 19] and FlashDecoding [13]) shown in Figure 4(b). Such a recomputation scheme avoids overflow while introducing negligible overheads based on the statistical data shown in Figure 5.\n\nExample. Figure 6 shows an example of the asynchronized softmax scheme. We set a = -3 , b = 3 , ϕ = 6 . Two vectors x and y are calculated from Q × K T in Equation (1), and are divided into 2 partial vectors. We omit the process from Q × K T to these partial vectors. For each x i , we have a &lt; x i -ϕ &lt; b , we process e x 1 -ϕ · v 1 + e x 2 -ϕ · v 2 and e x 1 -ϕ + e x 2 -ϕ for the first partial vector of x using two asynchronized threads. Then, each thread moves to the next partial vector for the corresponding computation ( i.e., e x 3 -ϕ · v 3 + e x 4 -ϕ · v 4 and e x 3 -ϕ + e x 4 -ϕ ). Two threads are synchronized when all partial vectors are processed, and perform the division operation in Equation (4). For y , the first partial vector is processed similarly. However, we find that y 3 -ϕ &gt; b , then two threads are terminated and the first thread recomputes all partial vectors according to the synchronized partial softmax scheme in Figure 4(b).\n\n## 4 Flat GEMM Optimization with Double Buffering\n\nMotivation. The process of the decode phase is mainly composed of GEMV (batch size=1) or flat GEMM (batch size &gt; 1) operation. Without loss of generality, GEMV/GEMM operations can be represented using M,N,K , where the sizes of two multiplied matrices are M × K and K × N . Previous LLM inference engines utilize Tensor Core to accelerate these operations using libraries like cuBLAS [24] and CUTLASS [25]. Although modern Tensor Core architectures [32] process GEMM with M = 8 , these libraries usually tile the M -dimension to 64 to hide memory latency. However, for GEMV or flat GEMM operations in the decode phase, we usually have M ≪ 64 and the M -dimension is padded to 64 with zeros. The padding leads to under-utilized computation, and the key problem is to process GEMV or flat GEMM operations with smaller tiles ( i.e., padding to 8 corresponding to modern Tensor Core architectures) in the M -dimension .\n\nChallenge. Processing GEMV or flat GEMM operations is non-trivial when the M -dimension is padded to 8. The tiling technique in modern libraries like cuBLAS [24] and CUTLASS [25] can only be applied to the N -dimension and\n\nFigure 7: Normalized flat GEMM performance under different N -dimension sizes and N -dimension tiling sizes. We set M = 8 and execute GEMM on the NVIDIA Tesla A100 GPU.\n\n<!-- image -->\n\nthe K -dimension. Tiles on the K -dimension are processed sequentially in a GPU block to avoid atomic operations during reduction. Tiling on the N -dimension affects both parallelism and computation/memory ratio, which are both important for GEMV and flat GEMM acceleration.\n\nAnalysis and Insights. Assume that tiling sizes of the N -dimension and the K -dimension are B N and B K , respectively. The computation of each GEMM tile is 2 × M × B N × B K with total B = N × K B N × B K GEMMtiles. The total memory access is ( M × B K + B N × B K ) × B + M × N . Thus, the computation/memory ratio is:\n\n<!-- formula-not-decoded -->\n\nOn the other hand, the parallelism is N B N . Thus, the computation/memory ratio shows a positive correlation with B N while the parallelism shows a negative correlation with B N , exposing a contradiction on improving the performance of GEMV or flat GEMM. We depict the normalized performance of the flat GEMM in Figure 7 with different N and B N . Our key insight is, for the smaller N , the flat GEMM is parallelism-bounded . There are 108 Streaming Multiprocessors (SMs) in the NVIDIA Tesla A100. N B N tends to be a constant ( e.g., 128 or 256), which is related to the hardware parallelism (number of SMs). Another key insight is, for the larger N , the flat GEMM becomes memory-bounded . The performance of these cases can be improved by hiding memory access latency.\n\nApproach: Double Buffering. In order to hide memory access latency, we introduce the double buffering technique. for the flat GEMM operation. We allocate two separate buffers in the shared memory. The tile in one buffer performs the GEMM operation, while another buffer loads a new tile for the next GEMM operation. Thus, the computation and the memory access are overlapped. We apply such a technique when N is large in our practice.\n\nExample. Figure 8 shows the example of our flat GEMM optimization with double buffering. For M &lt; 8 , the M -dimension is first padded to 8 considering modern Tensor Core architectures. Workloads in the K -dimension are processed within one GPU block ( e.g., A 1 , A 2 , A 3 , ... ), while workloads in the N -dimension are processed in parallel using different GPU blocks ( e.g., C 1 , C 2 , ... ). We take GPU Block 1 as an example, the first tile for each matrix in the K -dimension ( i.e., A 1 and B 1 ) is loaded to the left buffer in the shared memory. Then, the GEMM operation is performed between A 1 and B 1 . Consequently, A 2 and B 2 are loaded to the right buffer in the shared memory. The following tiles are processed similarly according to the double buffering scheme.\n\nFigure 8: Double buffering for flat GEMM when N -dimension is large. The M -dimension is padded to 8 and not tiled.\n\n<!-- image -->\n\n## 5 Heuristic Dataflow with Hardware Resource Adaption\n\nMotivation. Although FlashDecoding++ optimizes the flat GEMM operation in Section 4, it does not cover all operations (even only for GEMMs) in the LLM inference. As mentioned in Figure 2, the shapes of GEMMs in different operations and two phases vary. Thus, the GEMM workload in the LLM inference can be GEMV (batch size=1 for the decode phase), flat GEMM (small batch size for the decode phase and short sequence length for the prefill phase) and conventional GEMM (large batch size or long sequence length for the prefill phase). In order to leverage the powerful computational ability of Tensor Core, current frameworks like FasterTransformer [33] and DeepSpeed [9] tend to utilize the highly optimized GEMM implementation from cuBLAS [24] to deal with different workloads. However, the Tensor Core implementation fails with the GEMV workload. The GEMV workload can be optimized by utilizing CUDA Core in previous designs like FastGEMV [34]. For a Llama2-7B linear layer in the decode phase, the Tensor Core implementation from cuBLAS only achieves 82.15% of the performance of CUDA Core implementation using FastGEMV on an NVIDIA A100 GPU. On the other hand, using CUDA Core to do the projection on a batchsize=4 decoding input only achieves 49.75% performance compared with the Tensor Core implementation. Thus, in order to approach the optimal computation performance, a heuristic dataflow is supposed to be exploited in for different workloads.\n\nChallenge. Although a heuristic dataflow potentially exists in the implementation of different linear workloads, it is challenging to build the mapping from a certain workload to an optimal implementation. In the scenario of LLM inference, there are various factors that influence the implementation performance of linear workloads: (a) Input dynamics. The variety of the batch size and the input sequence length brings dynamic workloads. (b) Model diversity. The linear workload varies with different model structures and sizes. (c) GPU capacities. The relative performance between implementations changes with GPU characteristics, such as memory bandwidth, cache size, and computational ability. (d) Engineering effects. The engineering effort also highly impacts the kernel performance. All these influential factors build a large search space, making it non-trivial to generate an effective mapping between the linear workload and the corresponding optimal implementation.\n\nAnalysis and Insights. Although all influential factors form a large search space, the homogeneity of different layers in LLM significantly reduces the search space for operator optimization. Figure 2 shows four linear GEMV/GEMM operations in the prefill phase and the decode phase, i.e., K,Q,V projection, O projection, and two feedforward operations. Each GEMV/GEMM operation can be can be abstracted as a multiplication between an ( M × K )-shaped matrix and a ( K × N )-shaped matrix. Our key insight is, there are only four [ K,N ] shapes for a certain LLM. Moreover, M is only related to the input sequence length and the batch size for the prefill phase, and the batch size for the decode phase. Figure 9(a) shows limited shapes of GEMV/GEMM operations in the LLM inference.\n\nApproach: Decision flow for inflection points. Because only four [ K,N ] shapes exist for a certain LLM, we use three types of implementations for GEMV/GEMM operations when M varies: FastGEMV for the GEMV and flat GEMM operations (ImplA), our flat GEMM optimization in Section 4 (ImplB), and the CUTLASS [25] libraries optimized for the conventional GEMM (ImplC). Thus, it is important to decide whether applying ImplA or ImplB for a small M , and ImplB or ImplC for a large M . Figure 9(b) shows the decision flow. FlashDecoding++ profiles the performance of ImplA and ImplB for a certain M , and increases M to find an inflection point M 1 where the performance of ImplB is\n\nFigure 9: Heuristic dataflow with hardware resource adaption in FlashDecoding++ . (a) Only four [ N,K ] shapes exist for a certain LLM. (b) The decision flow. We traverse all [ N,K ] selections and profile the performance of three representative implementations. M is increased to find two inflection points for runtime heuristic dataflow. (c) FlashDecoding++ heuristically utilizes Tensor Core/CUDA Core with the corresponding GEMV/GEMM implementation by referring to a lookup table.\n\n<!-- image -->\n\nbetter than ImplA. Another inflection point M 2 is found similarly where the performance of ImplC is better than ImplB. Note that each [ N,K ] gets its individual M 1 and M 2 .\n\nApproach: Heuristic dataflow. For the runtime LLM inference, FlashDecoding++ adopts ImplA using CUDA Core when M &lt; M 1 , and ImplB/ImplC using Tensor Core when M 1 ≤ M &lt; M 2 / M 2 ≤ M . Note that the decision flow are executed offline, it does not affect the performance of runtime LLM inference.\n\nExample. Figure 9(c) shows an example of applying the heuristic dataflow for the Llama2-7B model. Four [ N,K ] shapes are [12288, 4096] for K,Q,V projection, [4096, 4096] for O projection, [11008, 4096] and [4096, 11008] for FFN. For each [ N,K ] , the inflection points are found based on the decision flow in Figure 9(c). Then, a lookup table is formed, and each GEMV/GEMM operation is executed according to corresponding implementations during runtime. In this example, FastGEMV is adopted for the K,Q,V projection when batch size=1 ( M = 1 ) for the decode phase, and our flat GEMM optimization is applied when batch size=1/input sequence length=8 for FFN 1 ( M = 8 ).\n\n## 6 Evaluation\n\n## 6.1 Experiments Setup\n\nWe evaluate the performance of FlashDecoding++ on different GPUs with various Large Language Models. We compare the performance with several state-of-the-art LLM inference engines.\n\nTable 1: Hardware Platforms\n\n|     | NVIDIA                           | NVIDIA                        | AMD                  | AMD                          |\n|-----|----------------------------------|-------------------------------|----------------------|------------------------------|\n| GPU | Tesla A100 80 GB CUDA 12.2       | RTX3090 24 GB CUDA 11.6       | MI210 64GB ROCm 5.7  | RX7900XTX 24GB ROCm 5.6      |\n| CPU | Intel Xeon Silver 8358P 2.60 GHz | Intel Xeon Gold 6226R 2.90GHz | AMDEPYC 7K62 2.60GHz | Intel Core i9-10940X 3.30GHz |\n\nTable 2: Model Configuration\n\n| Model       |   Dimension |   Heads |   Layers | Context Length   |\n|-------------|-------------|---------|----------|------------------|\n| Llama2-7B   |        4096 |      32 |       32 | 4k               |\n| Llama2-13B  |        5120 |      40 |       40 | 4k               |\n| OPT-6.7B    |        4096 |      32 |       32 | 2k               |\n| ChatGLM2-6B |        4096 |      32 |       32 | 32k              |\n\n## 6.1.1 Hardware Platforms\n\nWe evaluate the performance of FlashDecoding++ and other LLM engines on both NVIDIA and AMD platforms to make a comprehensive comparison. We choose two different GPUs for each platform: Tesla A100 and RTX3090 for NVIDIA, MI210 and RX7900XTX for AMD. We show the detailed configuration in Table 6.1.1.\n\n## 6.1.2 LLMEngine Baselines\n\nWe implement our FlashDecoding++ using the Pytorch-based front-end with the C++ and CUDA backend for NVIDIA GPUs while ROCm for AMD GPUs. We compare the inference performance in both prefill phase and decode phase with the following LLM engine baselines: Hugging Face (HF) [35], vLLM [11], DeepSpeed [9], TensorRT-LLM [14], OpenPPL [12], and FlashAttention2/FlashDecoding [19, 13]. These baselines are introduced in Section 7.\n\n## 6.1.3 Models\n\nWe evaluate the performance of FlashDecoding++ with other LLM inference engines on three typical Large Language Models: Llama2, OPT, and ChatGLM2. Table 6.1.2 shows the detailed configuration of these models. Note that there may be several models in one LLM ( e.g., Llama2-7B, Llama2-13B) with different configurations ( e.g., number of heads and layers).\n\n- Llama2 [1] is a mainstream open-source LLM set released by Meta in 2023. It is a collection of pretrained and fine-tuned generative text models ranging in scale from 7B to 70B parameters.\n- OPT [36], is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters released by Meta AI.\n- ChatGLM2 [37] is an open-source LLM supporting bilingual (Chinese-English) chat.\n\n## 6.2 Comparison with State-of-the-art\n\nWe compare FlashDecoding++ with state-of-the-art LLM inference engines in Figure 10 and Figure 11 on NVIDIA GPUs, Figure 12 and Figure 13 for AMD GPUs. For the decode phase, FlashDecoding++ achieves up to 4.86 × speedup compared with Hugging Face implementations on three LLMs and two GPUs. The average speedup over vLLM, DeepSpeed, TensorRT-LLM, OpenPPL, and FlashDecoding is 1.24 × , 1.44 × , 1.13 × , 1.24 × , and 1.21 × ( 1.37 × on Tesla A100 compared with FlashDecoding), respectively. For the prefill phase, FlashDecoding++ achieves up to 1.40 × speedup compared with Hugging Face implementations. The average speedup over DeepSpeed, TensorRT-LLM, OpenPPL, FlashAttention2 and FlashDecoding is 1.05 × , 1.06 × , 1.08 × , 1.09 × , and 1.08 × , respectively. We also show the decode results on two AMD GPUs. Currently, only the original Hugging Face implementation can be executed on AMD GPUs as the baseline. FlashDecoding++ achieves up to 2.27 × and 3.93 × compared with the baseline on RX7900XTX and MI210, respectively.\n\nFigure 10: Speedup of the decode phase on NVIDIA GPUs. Blank bars represent the model cannot be executed ( e.g., OpenPPL does not support OPT-6.7B/ChatGLM2-6B, TensorRT-LLM fails to compile the model with &gt; 8 K input length, and etc.)\n\n<!-- image -->\n\n## 7 Related Works\n\nLarge language model inference acceleration has gained significant attention in recent research, with several notable approaches and techniques emerging in the field. DeepSpeed [9] is a comprehensive engine that optimizes both the training and inference phases for LLMs. It achieves robust inference performance through kernel fusion and efficient GPU memory management, with a particular focus on optimizing memory usage for KVcache. vLLM [11] improves\n\nFigure 11: Speedup of the prefill phase on NVIDIA GPUs.\n\n<!-- image -->\n\nGPU memory utilization by efficient memory management techniques and the PageAttention method, leading to increased maximum batch sizes and elevating the upper limit of inference performance. FlashAttention [18, 19] optimizes the self-attention computation process during the prefill phase through improved parallelism and workload distribution. FlashDecoding [13] is an extension of FlashAttention and enhances the parallelism through spliting K and V , supporting efficient self-attention computation for long sequence during the decode phase. FasterTransformer [33] and OpenPPL [12] implement large model inference engines using C++ to reduce overhead resulting from kernels scheduling, compared to Python implementations. They also employ memory management techniques and kernel fusion to achieve efficient LLM inference. TensorRT-LLM [14] is built upon the TensorRT [38] and the FasterTransformer [33] engine ( C++ ) and incorporates cutting-edge open-source technologies such as FlashAttention [18, 19]. Additionally, it enhances its ease of use by providing the Python API .\n\nFigure 12: Speedup of the decode phase on AMD RX7900XTX.\n\n<!-- image -->\n\nFigure 13: Speedup of the decode phase on AMD MI210.\n\n<!-- image -->\n\n## 8 Conclusion\n\n100 3 HF Ours Ours (token/s) We propose FlashDecoding++ , a fast Large Language Model inference engine in this paper. FlashDecoding++ accelerates mainstream LLMs with multiple hardware backend support. FlashDecoding++ proposes three novel designs: the asynchronized softmax with unified max value, the flat GEMM optimization with double buffering, and the heuristic dataflow with hardware resource adaption, achieving up to 4.86 × and 3.93 × speedup on NVIDIA and AMD GPUs compared with Hugging Face implementations. FlashDecoding++ also achieves an average of 1.37 × speedup compared with state-of-the-art LLM inference engines, FlashDecoding, on various LLMs.\n\n2\n\n1\n\n0\n\n50\n\n0\n\nSpeedup\n\n128\n\n512\n\n1k\n\n(a) Llama2-7B\n\n2k\n\n128\n\n512\n\n1k\n\n(b) Llama2-13B\n\n## References\n\n- [1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.\n- [2] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine , 29(8):1930-1940, 2023.\n- [3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\n- [4] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine. Communications Medicine , 3(1):141, 2023.\n- [5] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Receive, reason, and react: Drive as you say with large language models in autonomous vehicles. arXiv preprint arXiv:2310.08034 , 2023.\n- [6] OpenAI. Openai pricing. [Online], 2023. https://openai.com/pricing .\n- [7] Nerdynav. Up-to-date chatgpt statistics &amp; user numbers [oct 2023]. [Online], 2023. https://nerdynav.com/ chatgpt-statistics .\n- [8] AFZAL AHMAD DYLAN PATEL. The inference cost of search disruption - large language model cost analysis. [Online], 2023. https://www.semianalysis.com/p/the-inference-cost-of-search-disruption .\n- [9] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis , pages 1-15. IEEE, 2022.\n- [10] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. 2023.\n- [11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles , pages 611-626, 2023.\n- [12] Sensetime. Openppl: A high-performance deep learning inference platform. [Online], 2023. https://openppl. ai/home .\n- [13] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. [Online], 2023. https://crfm.stanford.edu/2023/10/12/flashdecoding.html .\n- [14] Neal Vaidya, Fred Oh, and Nick Comly. Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. [Online], 2023. https://github.com/NVIDIA/TensorRT-LLM .\n- [15] Sensetime. A light and fast inference service for llm. [Online], 2023. https://github.com/ModelTC/ lightllm .\n\n- [16] Text generation inference: Fast inference optimize for llms. [Online], 2023. https://github.com/ huggingface/text-generation-inference/ .\n- [17] Mlc llm: Machine learning compilation for large language models. [Online], 2023. https://github.com/ mlc-ai/mlc-llm .\n- [18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems , 35:16344-16359, 2022.\n- [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 , 2023.\n- [20] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan, and Frost Ming. OpenLLM: Operating LLMs in production, June 2023.\n- [21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860 , 2019.\n- [22] Z Dong, T Tang, L Li, and WX Zhao. A survey on long text modeling with transformers. arxiv 2023. arXiv preprint arXiv:2302.14502 .\n- [23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023.\n- [24] NVIDIA. cublas: Basic linear algebra on nvidia gpus. [Online], 2017. https://developer.nvidia.com/ cublas .\n- [25] NVIDIA. Cutlass: Cuda templates for linear algebra subroutines. [Online], 2017. https://github.com/ NVIDIA/cutlass .\n- [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\n- [27] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10) , pages 807-814, 2010.\n- [28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016.\n- [29] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941 , 2017.\n- [30] John Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Advances in neural information processing systems , 2, 1989.\n- [31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n- [32] NVIDIA. Nvidia tensor core. [Online], 2023. https://www.nvidia.com/en-us/data-center/ tensor-cores/ .\n- [33] NVIDIA. Fastertransformer: About transformer related optimization, including bert, gpt. [Online], 2017. https://github.com/NVIDIA/FasterTransformer .\n- [34] Siping Wang. Fastgemv: High-speed gemv kernels. [Online], 2023. https://github.com/wangsiping97/ FastGEMV .\n- [35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, October 2020. Association for Computational Linguistics.\n- [36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.\n- [37] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 320-335, 2022.\n- [38] NVIDIA. Nvidia tensorrt: An sdk for high-performance deep learning inference. [Online]. https://developer. nvidia.com/tensorrt .",
  "tables": [
    {
      "index": 0,
      "markdown": "|     | NVIDIA                           | NVIDIA                        | AMD                  | AMD                          |\n|-----|----------------------------------|-------------------------------|----------------------|------------------------------|\n| GPU | Tesla A100 80 GB CUDA 12.2       | RTX3090 24 GB CUDA 11.6       | MI210 64GB ROCm 5.7  | RX7900XTX 24GB ROCm 5.6      |\n| CPU | Intel Xeon Silver 8358P 2.60 GHz | Intel Xeon Gold 6226R 2.90GHz | AMDEPYC 7K62 2.60GHz | Intel Core i9-10940X 3.30GHz |"
    },
    {
      "index": 1,
      "markdown": "| Model       |   Dimension |   Heads |   Layers | Context Length   |\n|-------------|-------------|---------|----------|------------------|\n| Llama2-7B   |        4096 |      32 |       32 | 4k               |\n| Llama2-13B  |        5120 |      40 |       40 | 4k               |\n| OPT-6.7B    |        4096 |      32 |       32 | 2k               |\n| ChatGLM2-6B |        4096 |      32 |       32 | 32k              |"
    }
  ],
  "stats": {
    "pages": 16,
    "chunksCreated": 67,
    "totalCharacters": 45206,
    "totalWords": 7090,
    "numTables": 2,
    "processingTimeMs": 14270
  }
}