{
  "paper": {
    "id": "2308.00245v3",
    "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
    "abstract": "Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appearing to have no missing bugs. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.",
    "authors": [
      "Haonan Li",
      "Yu Hao",
      "Yizhuo Zhai",
      "Zhiyun Qian"
    ],
    "published": "2023-08-01T02:57:43.000Z",
    "updated": "2023-11-15T21:02:47.000Z",
    "primaryCategory": "cs.SE",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2308.00245v3",
    "absUrl": "https://arxiv.org/abs/2308.00245v3"
  },
  "chunks": [
    {
      "id": "2308.00245v3-chunk-0",
      "content": "Haonan Li\n\nRiverside, California, USA\n\nhli333@ucr.edu UC Riverside\n\nYizhuo Zhai yzhai003@ucr.edu UC Riverside Riverside, California, USA\n\nYu Hao\n\nRiverside, California, USA\n\nyhao016@ucr.edu UC Riverside\n\nZhiyun Qian\n\nRiverside, California, USA\n\nzhiyunq@cs.ucr.edu UC Riverside and (2) a precise symbolic execution with limited scalability. The solution illuminates the need for alternative strategies to navigate the complex trade-offs between precision and scalability effectively. Despite this strategic combination of analysis techniques, nearly 40% of the potential bugs reported from the static analysis phase experience a timeout or memory exhaustion during the static symbolic execution phase, preventing any conclusive results on such cases. This limitation hinders the overall effectiveness of the tool, leading to the potential of two distinct outcomes: missed bugs if these potential bug reports are ignored (what UBITect performs), or false positives if they are sent to developers for in",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-1",
      "content": "of the tool, leading to the potential of two distinct outcomes: missed bugs if these potential bug reports are ignored (what UBITect performs), or false positives if they are sent to developers for inspection.\n\nIn response, we propose LLift, a fully automated framework that bridges static analysis with LLMs in analyzing UBI bugs. Our solution packages several novel components. First, LLift performs post-constraint guided path analysis , which helps verify the path feasibility of the 'use' of an initialized variable, a difficult task for static analysis and symbolic execution. Second, to efficiently interact with LLMs, we employ task decomposition to break down the analysis into more than a single step. Third, we employ progressive prompting by providing information incrementally only when necessary, instead of providing an enormous scope of code at once. Finally, we propose self-validation by requesting LLMs to",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "chunkIndex": 1,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-2",
      "content": "ploy progressive prompting by providing information incrementally only when necessary, instead of providing an enormous scope of code at once. Finally, we propose self-validation by requesting LLMs to\n\nIn this paper, we investigate the possibility of leveraging Large Language Model s (LLMs) as an alternative to handle such 'difficult cases'. This is because recent LLMs have exhibited strong potential in understanding, generating, and even debugging code [4, 8, 13]. Nevertheless, navigating the intricacies of utilizing LLMs for bug discovery proves to be a complex feat. The technical report on GPT-4 underscores this challenge, admitting that when it comes to discovering new vulnerabilities, it may not be the best solution standalone [21]: '... is less effective than existing tools for complex and high-level activities like novel vulnerability identification'. In the same vein, prior research demonstrates the competence of LLMs mostly in simpler tasks or programs [1, 25, 26].",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "chunkIndex": 2,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-3",
      "content": "ls for complex and high-level activities like novel vulnerability identification'. In the same vein, prior research demonstrates the competence of LLMs mostly in simpler tasks or programs [1, 25, 26]. This is because LLMs are far from perfect. For instance, they suffer from hallucination [11] where instead of identifying the bugs in faulty code, LLMs may create non-existent facts in an attempt to rationalize the original intention behind the problematic code [17, 31]. Another issue is the stochasticity of LLMs which can result in inconsistent or outright incorrect results, thus throwing another wrench into the gears of bug discovery [41]. Finally, LLMs have limited context windows, meaning they can only scrutinize a relatively small codebase.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "chunkIndex": 3,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-4",
      "content": "Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Model s (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 4,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-5",
      "content": "ted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appears to have no missing bug. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 5,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-6",
      "content": "Static analysis is a popular technique in software engineering, particularly in the area of bug discovery, that can improve code quality, reliability, and security. However, the effectiveness of these techniques is influenced by the fundamental trade-off between precision and scalability, especially when dealing with extensive and complex programs [9, 24]. On the one hand, static analysis solutions with lower precision tend to generate numerous false positives. On the other hand, expensive static analysis or symbolic execution solutions with higher precision often struggle to complete the analysis. Consequently, achieving comprehensive and accurate static program analysis for sizable programs like the Linux kernel poses a significant challenge.\n\nUBITect [40], a powerful static analysis solution illustrates these inherent limitations thoroughly.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-7",
      "content": "static program analysis for sizable programs like the Linux kernel poses a significant challenge.\n\nUBITect [40], a powerful static analysis solution illustrates these inherent limitations thoroughly. Targeting Use-Before-Initialization (UBI) bugs in the Linux kernel, it packages a pipeline of (1) a scalable bottom-up summary-based static analysis with limited precision,\n\n```\n1 static int libcfs_ip_str2addr(...){ 2 unsigned int a, b, c, d; 3 if (sscanf(str, \"%u.%u.%u.%u%n\", &a, &b, &c, &d, &n) >= 4){ 4 // use of a, b, c, d 5 } 6 } 7 int sscanf( const char *buf, const char *fmt, ...){ 8 va_list args; 9 int i; 10 va_start(args, fmt); 11 i = vsscanf(buf, fmt, args); 12 va_end(args); 13 }\n```\n\nFigure 1: Code snippet of sscanf and its usecase Figure 1: Code snippet of sscanf and its usecase\n\nTable 1: UBITect's summary for sscanf . Both use and initialization for va\\_args are incorrect. âœ“ and âœ— stand for whether this parameter will be used/initialized after its call. '...",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-8",
      "content": "and its usecase\n\nTable 1: UBITect's summary for sscanf . Both use and initialization for va\\_args are incorrect. âœ“ and âœ— stand for whether this parameter will be used/initialized after its call. '... ' represents all other parameters of va\\_args . Table 1: UBITect's summary for sscanf . Both use and initialization for va\\_args are incorrect. âœ“ and âœ— stand for whether this parameter will be used/initialized after its call. '... ' represents all other parameters of va\\_args .\n\n|                       | buf buf   | fmt fmt   | ... ...   | *buf *buf   | *fmt *fmt   |\n|-----------------------|-----------|-----------|-----------|-------------|-------------|\n| Use Use               | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“         | âœ“ âœ“         |\n| Initialize Initialize | âœ— âœ—       | âœ— âœ—       | âœ— âœ—       | âœ— âœ—         | âœ— âœ—         |",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 8,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-9",
      "content": "-----|-------------|\n| Use Use               | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“         | âœ“ âœ“         |\n| Initialize Initialize | âœ— âœ—       | âœ— âœ—       | âœ— âœ—       | âœ— âœ—         | âœ— âœ—         |\n\nwe propose self-validation by requesting LLMs to review responses at various stages to obtain accurate and reliable responses. review responses at various stages to obtain accurate and reliable responses.\n\nWe summarize our contributions as follows: We summarize our contributions as follows:\n\nWe implement a prototype of LLift and test it in real-world scenarios. Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LLift successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community. With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%. We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 9,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-10",
      "content": "mmunity. With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%. We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%. We implement a prototype of LLift and test it in real-world scenarios. Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LLift successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community. With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%. We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%.\n\nâ€¢\n\n- New Opportunities. We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs. To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 10,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-11",
      "content": "he same time by harnessing the capabilities of LLMs. To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets. Â· New Opportunities. We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs. To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets.\n\nâ€¢\n\nâ€¢\n\n- Results. We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a high precision rate (50%) and recall rate (100%). Additionally, our examination led to the discovery of 13 previously unknown bugs. Â· Results. We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a reasonable precision rate (50%).",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 11,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-12",
      "content": "n led to the discovery of 13 previously unknown bugs. Â· Results. We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a reasonable precision rate (50%). Additionally, our examination led to the discovery of 13 previously unknown bugs.\n\nâ€¢\n\n- Open source. Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis. Â· Open source. Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis.\n\n2",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 12,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-13",
      "content": "UBITect is a state-of-the-art static analysis solution aiming at finding Use Before Initialization (UBI) bugs in the Linux kernel [41]. It employs a two-stage pipeline where the first stage employs a bottom-up summary-based static analysis of the Linux kernel. By design, this stage aims for scalability and sacrifices precision, producing a significant number of potential bugs ( i.e., âˆ¼ 140k), most of UBITect is a state-of-the-art static analysis solution aiming at finding Use Before Initialization (UBI) bugs in the Linux kernel [40]. It employs a two-stage pipeline where the first stage employs a bottom-up summary-based static analysis of the Linux kernel. By design, this stage aims for scalability and sacrifices precision, producing a significant number of potential bugs ( i.e., âˆ¼ 140k), most of",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "BACKGROUND &amp; MOTIVATION 2 BACKGROUND &amp; MOTIVATION",
        "chunkIndex": 13,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-14",
      "content": "- New Methodologies. We develop LLift, an innovative and fully automated agent that arms static analysis with LLMs. LLift employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses. Â· NewMethodologies. Wedevelop LLift, an innovative and fully automated framework that arms static analysis with LLMs. LLift employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses.\n\n2\n\nwhich are false alarms. The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs). It is complemented by a second stage of static symbolic execution that filters as many false alarms as possible by verifying their path feasibility. However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs. which are false alarms.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example",
        "chunkIndex": 14,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-15",
      "content": "ility. However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs. which are false alarms. The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs). It is complemented by a second stage of static symbolic execution that filters as many false alarms as possible by verifying their path feasibility. However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs.\n\nd\n\nFigure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion. In other words, UBITect failed to rule out this case as a false alarm.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example",
        "chunkIndex": 15,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-16",
      "content": "tial UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion. In other words, UBITect failed to rule out this case as a false alarm. As Table 1 presents, the static analysis stage generates a summary of sscanf() as ' may not initialize parameters a , b , c , and ' but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively. There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions : For soundness, UBITect assumed the va\\_start() is a normal function. However, since it cannot find its definition, it has to conservatively assume that the arguments passed to it will be used inside. Unfortunately, in reality, va\\_start is a compiler built-in function that simply 'prepares' the arguments without any uses.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example",
        "chunkIndex": 16,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-17",
      "content": "o conservatively assume that the arguments passed to it will be used inside. Unfortunately, in reality, va\\_start is a compiler built-in function that simply 'prepares' the arguments without any uses. 2) insensitivity of postconditions : It fails to recognize the check of its return value, i.e., if(sscanf(...)&gt;=4) , which ensures its arguments a to d must be initialized before use. Figure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion. In other words, UBITect failed to rule out this case as a false alarm. As Table 1 presents, the static analysis stage generates a summary of sscanf() as ' may not initialize parameters a , b , c , and d ' but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example",
        "chunkIndex": 17,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-18",
      "content": "ot initialize parameters a , b , c , and d ' but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively. There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions : For soundness, UBITect assumed the va\\_start() is a normal function. However, since it cannot find its definition, it has to conservatively assume that the arguments passed to it will be used inside. Unfortunately, in reality, va\\_start is a compiler built-in function that simply 'prepares' the arguments without any uses. 2) insensitivity of path constraints : It fails to recognize the path constraint, i.e., if(sscanf(...)&gt;=4) , which ensures its arguments a to d must be initialized before use.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example",
        "chunkIndex": 18,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-19",
      "content": "Inherent Knowledge Boundaries. Developers need to model specific functions or language features. Otherwise, they influence the correctness of the results. For compiler built-in functions, e.g., va\\_start() , their definitions are simply not available. Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel. These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions. However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time. This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired. Inherent Knowledge Boundaries. Developers need to model specific functions or language features. Otherwise, they influence the correctness of the results.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 19,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-20",
      "content": "less precise and comprehensive than desired. Inherent Knowledge Boundaries. Developers need to model specific functions or language features. Otherwise, they influence the correctness of the results. For compiler built-in functions, e.g., va\\_start() , their definitions are simply not available. Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel. These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions. However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time. This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 20,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-21",
      "content": "hey can be highly dependent on the analysis target and evolve over time. This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired.\n\nIn light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows: In light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows:\n\nExhaustive Path Exploration. Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4 . Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase. While this approach is theoretically comprehensive, it often leads to a combinatorial explosion.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 21,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-22",
      "content": "der a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase. While this approach is theoretically comprehensive, it often leads to a combinatorial explosion. The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specific analysis task at hand. In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf . This causes UBITect's symbolic execution to time out exactly due to this problem. Exhaustive Path Exploration. Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4 . Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 22,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-23",
      "content": "rtunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase. While this approach is theoretically comprehensive, it often leads to a combinatorial explosion. The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specific analysis task at hand. In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf . This causes UBITect's symbolic execution to time out exactly due to this problem.\n\nUBITect\n\n<!-- image -->\n\nFigure 2: The overview of LLift. Start with the discarded cases by UBITect and determine whether these potential bugs are true or false. Figure 2: The overview of LLift. Start with the discarded cases by UBITect and determine whether these potential bugs are true or false.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 23,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-24",
      "content": "t and determine whether these potential bugs are true or false. Figure 2: The overview of LLift. Start with the discarded cases by UBITect and determine whether these potential bugs are true or false.\n\n```\n1 int caller_function(){ 2 int X; // declare of suspicious variable /u1D44B 3 ... 4 init(&X); // initializer of /u1D44B 5 ... 6 use(X); // use of /u1D44B 7 }\n```\n\n,\n\nFigure 3: A typical type of potential UBI bug. For each suspicious variable /u1D44B we expect it to 1) have an initializer function that probably initializes /u1D44B and 2) use /u1D44B . Figure 3: A typical type of potential UBI bug. For each suspicious variable ğ‘‹ , we expect it to 1) have an initializer function that probably initializes ğ‘‹ and 2) use ğ‘‹ .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis",
        "chunkIndex": 24,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-25",
      "content": "Domain-specific Programming Constructs Recognition. This proficiency is showcased in three key areas: 1) Function Recognition : LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf() , kzalloc() , kstrtoul() and 'list for each' , simplifying the analysis and making the analysis more scalable. 2) Function pointers and callbacks : LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling. We will show an interesting Domain-specific Programming Constructs Recognition. This proficiency is showcased in three key areas: 1) Function Recognition : LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf() , kzalloc() , kstrtoul() , and 'list for each' , simplifying the analysis and making the analysis more scalable.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.3 Capability of LLMs 2.3 Capability of LLMs",
        "chunkIndex": 25,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-26",
      "content": "ify frequently used interfaces in the Linux kernel from its semantics, such as sscanf() , kzalloc() , kstrtoul() , and 'list for each' , simplifying the analysis and making the analysis more scalable. 2) Function pointers and callbacks : LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling. We will show an interesting case in Â§6.6.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "2.3 Capability of LLMs 2.3 Capability of LLMs",
        "chunkIndex": 26,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-27",
      "content": "Fortunately, LLMs [21] offers a promising alternative to summarizing code behaviors [22] in a flexible way and bypassing the aforementioned challenges. This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs. Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization . Fortunately, LLMs [21] offers a promising alternative to summarizing code behaviors [22] in a flexible way and bypassing the aforementioned challenges. This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs. Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization .\n\n,",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3 PROBLEM FORMULATION",
        "chunkIndex": 27,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-28",
      "content": "anguage and programs. Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization .\n\n,\n\ncase in Â§6.7. Smart Code Summarization. LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently difficult task in program analysis. This is likely because it has been trained on various functions with loops and their semantics. In contrast, traditional static analysis follows explicitly defined rules without a limited ability to generalize. Smart Code Summarization. LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently difficult task in program analysis. This is likely because it has been trained on various functions with loops and their semantics.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3 PROBLEM FORMULATION",
        "chunkIndex": 28,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-29",
      "content": "that they can summarize loop invariants [26], which is an inherently difficult task in program analysis. This is likely because it has been trained on various functions with loops and their semantics. In contrast, traditional static analysis follows explicitly defined rules without a limited ability to generalize.\n\n3\n\nBased on our observation, we propose LLift, an automated agent that interfaces with UBITect and LLMs (e.g., ChatGPT) to help identify and reason about UBI bugs in the Linux kernel. 3.1.1 Use-Before-Initialization. A Use Before Initialization (UBI) bug refers to the erroneous scenario where a variable ğ‘£ is accessed or involved in any operation prior to its correct initialization. Let:",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3 PROBLEM FORMULATION",
        "chunkIndex": 29,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-30",
      "content": "Scope.\n\n- Our goal of developing LLift is not to replace UBITect and Â· ğ‘‘ ( ğ‘£ ) represent the declaration of ğ‘£ .\n- the difficult cases for UBITect, i.e., 40% of cases that are considered Â· ğ‘– ( ğ‘£ ) denote the initialization operation of ğ‘£ .\n- static analysis in general. Instead, we aim to use LLift to analyze Â· ğ‘¢ ( ğ‘£ ) signify a use operation involving ğ‘£ .\n\ninconclusive. In other words, we aim to use LLift primarily as a complementary solution to UBITect. This relationship is depicted if there exists ğ‘‘ ( ğ‘£ ) and ğ‘¢ ( ğ‘£ ) , then ğ‘£ is used before initialization if:\n\nin Figure 2.\n\nAssumptions. As a first exploratory study of assisting static analysis with LLMs, we restrict our scope to commonly observed patterns where &lt; indicates a temporal sequence in the program execution.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 30,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-31",
      "content": "atory study of assisting static analysis with LLMs, we restrict our scope to commonly observed patterns where &lt; indicates a temporal sequence in the program execution.\n\n<!-- formula-not-decoded -->\n\nthat exceed the time and memory limits set for UBITect's symbolic execution stage. Specifically, depicted in Figure 3, we observe that 3.1.2 Postcondition. Postconditions encapsulate the expected state or behavior of a system upon the conclusion of a routine [18].\n\nit is often the case that the variable is declared in one function and\n\n3\n\nUBITect reports potential usebefore-initialization bugs Specifically, they detail the guarantees a routine offers based on its observable outcomes.\n\nSummarize the initializer with\n\nIdentify the initializer: sscanf Extract the post-constraint: sscanf(...)&gt;=4 1 2 3 For a routine ğ‘… , consider its set of outcomes as O . These outcomes are defined as updates to its parameters (and return value) for a path of ğ‘… .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 31,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-32",
      "content": "xtract the post-constraint: sscanf(...)&gt;=4 1 2 3 For a routine ğ‘… , consider its set of outcomes as O . These outcomes are defined as updates to its parameters (and return value) for a path of ğ‘… . Particularly, O does not include initialization for variables for convenience. In the study of UBI bug, for a routine ğ‘… that can yield a set of outcomes O , the postcondition P can be defined as:\n\nFigure 4: Example run of LLift. For each potential bug, we â‘  identify its initializer, â‘¡ extract the post-constraints of the initializer, and â‘¢ analyze the behavior of the initializer with the post-constraints via LLM. Here, S( ğ‘… ) signifies all possible execution paths through the routine ğ‘… , O describes all updates of ğ‘… on its variables, and must\\_init is a set of variables that must be initialized.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 32,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-33",
      "content": "ifies all possible execution paths through the routine ğ‘… , O describes all updates of ğ‘… on its variables, and must\\_init is a set of variables that must be initialized.\n\n<!-- formula-not-decoded -->\n\nit is passed to a callee function to be initialized (typically conditionally) before its use. We assume there can be multiple layers of Motivating Example. Consider the sscanf() function in our motivating example. Based on these return values, the postconditions assure the initialization of certain variables:\n\n```\nare other less common cases, i.e., the variable is initialized directly in the function where the variable is declared. We choose not to account for them, as most such cases are easier cases and more likely solved successfully by UBITect already. Conceptual Workflow. Figure 4 demonstrates the workflow of LLift.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 33,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-34",
      "content": "clared. We choose not to account for them, as most such cases are easier cases and more likely solved successfully by UBITect already. Conceptual Workflow. Figure 4 demonstrates the workflow of LLift. LLift takes bug reports from UBITect as inputs, which contain the suspicious variable that may be used before initialization, P( ğ‘ğ‘ğ‘¡â„ 1 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0 , must_init â†¦â†’âˆ…} P( ğ‘ğ‘ğ‘¡â„ 2 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 1 , must_init â†¦â†’{ ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 3 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 2 , must_init â†¦â†’{ ğ‘, ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 4 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 3 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 5 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 4 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘, ğ‘‘ }} P( ğ‘ğ‘ğ‘¡â„ 6 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 5 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘› }}\n```\n\nindirection where the initialization actually occurs. Note that there and the function that it sits in. We follow the following three key",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 34,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-35",
      "content": "6 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 5 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘› }}\n```\n\nindirection where the initialization actually occurs. Note that there and the function that it sits in. We follow the following three key\n\nmotivating example, sscanf() is the initializer. â‘¡ Extract the path constraints from the initializer return to the use, i.e., the constraints that ensure the use is reachable. In our example, the constraint is sscanf(...)&gt;=4 . â‘¢ Summarize the behavior of the initializer, capturing the variFor UBI detection, not every associated postcondition is relevant; instead, only the outcomes making the ğ‘¢ ( ğ‘£ ) reachable are critical . The constraints of the use are post-constraints C ğ‘ğ‘œğ‘ ğ‘¡ [ ? ]. The qualified postcondition , P ğ‘ğ‘¢ğ‘ğ‘™ , is a subset of P refined by C ğ‘ğ‘œğ‘ ğ‘¡ :\n\nsubtasks when interacting with LLMs: â‘  Identify potential initializers of the suspicious variable.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 35,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-36",
      "content": "ğ‘ğ‘œğ‘ ğ‘¡ [ ? ]. The qualified postcondition , P ğ‘ğ‘¢ğ‘ğ‘™ , is a subset of P refined by C ğ‘ğ‘œğ‘ ğ‘¡ :\n\nsubtasks when interacting with LLMs: â‘  Identify potential initializers of the suspicious variable. In our Here, the ğ‘ğ‘ğ‘¡â„ 1 -ğ‘ğ‘ğ‘¡â„ 6 represent different possible paths in the sscanf() and each path corresponds with a different postcondition.\n\nable initialization status,\n\n<!-- formula-not-decoded -->\n\nmust\\_init or\n\nmay\\_init\n\n, given potential bug. Note that this simple policy may still lead to false alarms (which we evaluate in Â§6). For example, our workflow by design does not take into account preconditions, In subsequent discussions, unless otherwise specified, the term 'postcondition' shall denote 'qualified postcondition' .\n\nthe path constraints learned in . In our example, a correct response should be must\\_init: a,b,c,d .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 36,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-37",
      "content": "ons, unless otherwise specified, the term 'postcondition' shall denote 'qualified postcondition' .\n\nthe path constraints learned in . In our example, a correct response should be must\\_init: a,b,c,d . If the final output is must\\_init , we can safely filter out such cases as non-bugs; if the output is may\\_init , we will consider it a For the sscanf() case, if the post-constraint is C ğ‘ğ‘œğ‘ ğ‘¡ = ret â‰¥ 4, the qualified postcondition would be P( ğ‘ğ‘ğ‘¡â„ 5 ) âˆ§ P( ğ‘ğ‘ğ‘¡â„ 6 ) , which ensures that variables a, b, c, and d must be initialized; therefore, all variables used subsequently are initialized, and no UBI happens.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "PROBLEM FORMULATION 3.1 Definitions and Scope",
        "chunkIndex": 37,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-38",
      "content": "will follow a number of principles described in this section. 4.1 Design Challenges It is non-trivial to prompt LLMs effectively [28, 42]. We meet the Let ğ‘… be the routine or function under analysis and S( ğ‘… ) be its path set. Let ğ‘ğ‘ğ‘¡â„ âˆˆ S( ğ‘… ) refer to a specific path in ğ‘… . Besides, Each path ğ‘ğ‘ğ‘¡â„ has an associated path constraint ğ‘ that dictates its feasibility. These two optimizations can be formed with:\n\n4 DESIGN We have illustrated the conceptual workflow previously in Figure 4 that works together logically to solve our formulated problem. However, to achieve better results, we have to overcome a number of challenges in interacting with LLMs. This means that will need to map the conceptual workflow into an instantiated workflow which When analyzing a routine or function in a path-sensitive manner, the number of paths to explore can grow rapidly.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.2 Post-Constraint Guided Path Analysis",
        "chunkIndex": 38,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-39",
      "content": "eans that will need to map the conceptual workflow into an instantiated workflow which When analyzing a routine or function in a path-sensitive manner, the number of paths to explore can grow rapidly. Fortunately, if we have information about what the function is expected to achieve (given by C ğ‘ğ‘œğ‘ ğ‘¡ ), we can prune paths that inherently don't meet those expectations. We categorize two scenarios, direct application and outcome conflicts , in applying this optimization.\n\nfollowing challenges and propose solutions correspondingly in designing LLift. Â· C1. Limited Understanding of Postconditions. Despite LLMs Direct Application. For direct application, the post-constraint C ğ‘ğ‘œğ‘ ğ‘¡ can be directly applied as a path constraint. A path can be discarded if:\n\n<!-- formula-not-decoded -->\n\n(\n\ne.g.,\n\nGPT-4) are able to comprehend the definition of postcondi- to utilize this knowledge in actual program analysis-such as",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.2 Post-Constraint Guided Path Analysis",
        "chunkIndex": 39,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-40",
      "content": "constraint. A path can be discarded if:\n\n<!-- formula-not-decoded -->\n\n(\n\ne.g.,\n\nGPT-4) are able to comprehend the definition of postcondi- to utilize this knowledge in actual program analysis-such as\n\nThis implies that if a ğ‘ğ‘ğ‘¡â„ inherently contradicts the post-constraint, it can be removed from consideration.\n\nOutcome Conflicts. Let O( ğ‘ ) denote the set of all outcomes or effects produced by path ğ‘ . A path can be pruned if any of its outcomes conflict with the post-constraint:\n\n<!-- formula-not-decoded -->\n\nThis stipulates that if an outcome from ğ‘ğ‘ğ‘¡â„ inherently contradicts the post-constraint, that path can be disregarded in the analysis.\n\nCorrectness. The validity of these optimization methods can be proved by contradiction. Consider an instance where one of these paths is executed. If this path conflicts with the C post, it would render ğ‘¢ ( ğ‘£ ) unreachable. Thus, it becomes evident that such paths can be pruned without sacrificing the correctness of the analysis.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.2 Post-Constraint Guided Path Analysis",
        "chunkIndex": 40,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-41",
      "content": "is executed. If this path conflicts with the C post, it would render ğ‘¢ ( ğ‘£ ) unreachable. Thus, it becomes evident that such paths can be pruned without sacrificing the correctness of the analysis.\n\nWe provide a concrete example of how we perform these optimizations in Â§4.3.3.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.2 Post-Constraint Guided Path Analysis",
        "chunkIndex": 41,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-42",
      "content": "Given a bug report containing a suspicious variable ğ‘£ and its residing function ğ¹ , the workflow Î¦ is as follows:\n\n- (1) Î¦ 1 ( ğ¹, ğ‘£ ) â†’ { ğ‘– ( ğ‘£ )} : Identify potential initializers for ğ‘£ from the bug report.\n- (3) Î¦ 3 ( ğ¹, { ğ‘– ( ğ‘£ ) , C ğ‘ğ‘œğ‘ ğ‘¡ }) â†’ InitStatus ( ğ‘£ ) : Summarize the initialization status for variable ğ‘£ after all possible initializers completion (merge multiple initializers).\n- (2) Î¦ 2 ( ğ¹, ğ‘– ( ğ‘£ )) â†’ C ğ‘ğ‘œğ‘ ğ‘¡ : Extract the C ğ‘ğ‘œğ‘ ğ‘¡ from the bug report for each ğ‘– ( ğ‘£ ) .\n\nDecision Policy. The decision policy Î” is defined as:\n\nÎ” ( InitStatus ( ğ‘£ ) = must\\_init ) : non-bug Î” ( InitStatus ( ğ‘£ ) â‰  must\\_init ) : potential bug\n\nConceptually, LLift will not miss more bugs. The post-constraint guided path optimizations and decision policies are safe.\n\nIn this policy, we adopt a conservative approach by treating all variables not explicitly marked as must\\_init as potential vulnerabilities.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.3 Conceptual Workflow",
        "chunkIndex": 42,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-43",
      "content": "guided path optimizations and decision policies are safe.\n\nIn this policy, we adopt a conservative approach by treating all variables not explicitly marked as must\\_init as potential vulnerabilities. And it is worth noting that this policy may introduce some false positives. For example, it might over-approximate preconditions.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.3 Conceptual Workflow",
        "chunkIndex": 43,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-44",
      "content": "We define two key concepts in interacting with LLMs: turn and conversation .\n\n- Conversation: Leveraging the capabilities of LLMs often necessitates a series of interactions, especially for complex problem-solving. A conversation is an ordered sequence of turns. A conversation comprising ğ‘› turns can be expressed as [( ğ‘ 1 , ğ‘Ÿ 1 ) , ( ğ‘ 2 , ğ‘Ÿ 2 ) , . . . , ( ğ‘ ğ‘› , ğ‘Ÿ ğ‘› )] .\n- Turn: A turn encapsulates a singular interaction with the LLM. Formally, it's defined as a tuple, ( ğ‘, ğ‘Ÿ ) , where ğ‘ represents the problem or question, and ğ‘Ÿ denotes the LLM's response.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "3.4 Turns and Conversations in LLMs",
        "chunkIndex": 44,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-45",
      "content": "In Section Â§3.3, we introduced a conceptual workflow. Elaborating on that foundation, Figure 4 showcases a compelling illustration of our methodological approach. Yet, translating this workflow into\n\nFigure 4: Example run of LLift. For each potential bug, LLift â‘  ( Î¦ 1 ) identifies its initializer, â‘¡ ( Î¦ 2 ) extracts the post-constraints of the initializer, and â‘¢ ( Î¦ 3 ) analyzes the behavior of the initializer with the post-constraints via LLM.\n\n<!-- image -->\n\npractice presents its challenges. Even with the advanced knowledge and analytical capabilities of cutting-edge LLMs, achieving optimal results remains a challenge. Throughout the development of LLift, we identified several obstacles and subsequently introduced four distinct design components to effectively address these challenges.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4 DESIGN",
        "chunkIndex": 45,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-46",
      "content": "It is non-trivial to prompt LLMs effectively [28, 41]. We meet the following challenges and propose solutions correspondingly in designing LLift.\n\n- C2. Token Limitations. It is known that LLMs have token limitations. For example, GPT-3.5 supports 16k tokens and GPT-4 supports 32k tokens [20]. This means that we do not want to copy a large number of function bodies in our prompts to LLMs.\n- C1. Limited Understanding of Post-constraint. Despite LLMs ( e.g., GPT-4) are able to comprehend the definition of post-constraint and apply them in simple scenarios, we found their capacity to utilize this knowledge in actual program analysis-such as summarizing function behavior in line with specific post-constraint -to be limited. This critical limitation often results in unpredictable and inconsistent outcomes.\n- C3. Unreliable and Inconsistent Response. LLMs are known to result in unreliable and inconsistent responses due to hallucination and stochasticity [41].",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.1 Design Challenges",
        "chunkIndex": 46,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-47",
      "content": "s in unpredictable and inconsistent outcomes.\n- C3. Unreliable and Inconsistent Response. LLMs are known to result in unreliable and inconsistent responses due to hallucination and stochasticity [41]. Stochasticity refers to the inherent unpredictability in the model's outputs [32]; and the hallucination refers to LLMs generating nonsensical or unfaithful responses [11, 42]. By design, the stochasticity can be mitigated with lower temperature , a hyperparameter controlling the degree of randomness in outputs [27]; however, reducing temperature may impair the model's exploring ability [37] and therefore may miss corner cases that result in vulnerabilities.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.1 Design Challenges",
        "chunkIndex": 47,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-48",
      "content": "We will discuss our design strategies to address the above challenges in the rest of the section. Before that, we provide a high-level overview of our solution.\n\n- To tackle challenge C1 (Post-constraint), we propose to encode (D#1) Post-Constraint Guided Path Analysis by teaching LLMs with examples, or few-shot in-context learning , of postconstraints. This approach enables LLMs to learn from a small number of demonstrative examples, assimilate the underlying patterns, and apply this understanding to process post-constraint guidance in our analysis.\n\nFigure 5: The workflow of LLift. Given a potential bug, we let LLM first identify the initializer and then extract its post-constraints (Convo.1), then leverage them to summarize the behavior of the initializer (Convo.2). A conversation consists of prompts (boxes) and responses (edges).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.2 Design Overview",
        "chunkIndex": 48,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-49",
      "content": "hen extract its post-constraints (Convo.1), then leverage them to summarize the behavior of the initializer (Convo.2). A conversation consists of prompts (boxes) and responses (edges).\n\n<!-- image -->\n\n- To tackle challenge C2 (Token Limitation), We employ two strategies: (D#2) Progressive Prompt . Instead of copying a large number of function bodies ( i.e., subroutines), we only provide function details on demand, i.e., when LLMs are not able to conduct a result immediately. (D#3) Task Decomposition. We break down the problem into sub-problems that can be solved in independent conversations, i.e., a sequence of prompt and response pairs .\n\nWe elaborate the design of (D#1 - #4) Post Constraint Guided Path Analysis , Progressive Prompts , Task Decomposition , and Self-Validation detailed in the rest of this section. The effectiveness and efficiency of these design strategies are rigorously evaluated in Â§6.4, revealing a substantial enhancement in bug detection within the Linux kernel.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.2 Design Overview",
        "chunkIndex": 49,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-50",
      "content": "in the rest of this section. The effectiveness and efficiency of these design strategies are rigorously evaluated in Â§6.4, revealing a substantial enhancement in bug detection within the Linux kernel.\n\n- To tackle challenge C3 (Unreliable Response), we employ the following strategies: (D#4) Self-Validation. We ask LLMs to review and correct their previous responses. This helps improve the consistency and accuracy based on our observation. Besides, (D#2) Progressive Prompt and (D#3) Task Decomposition also help to deal with this challenge. Additionally, we implement majority voting by running each case multiple times and use majority voting to combat stochasticity.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.2 Design Overview",
        "chunkIndex": 50,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-51",
      "content": "The Linux kernel frequently employs return value checks as illustrated in Table 2. Through our detailed examination of non-bug instances, we found that a path-sensitivity analysis can effectively eliminate over 70% of these negative cases. However, path-sensitive static analysis usually suffers from path explosion, especially in large-scale codebases like the Linux kernel.\n\nFortunately, we can prompt the LLM to collect C ğ‘ğ‘œğ‘ ğ‘¡ and summarize the function with respective to the C ğ‘ğ‘œğ‘ ğ‘¡ . It is worth noting\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\n```\nCheck Before Use Failure Check Type A: Type B: if (sscanf(...) >= 4) { use(a, b, c, d); } err = func(&a); if (err) { return / break / goto ; } use(a) Type A': Type B': switch (ret=func(&a)){ case some_irrelevant_case: do_something(...); break ; case critical_case: use(a); } while (func(&a)){ do_something(...); } use(a);\n```",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 51,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-52",
      "content": "break / goto ; } use(a) Type A': Type B': switch (ret=func(&a)){ case some_irrelevant_case: do_something(...); break ; case critical_case: use(a); } while (func(&a)){ do_something(...); } use(a);\n```\n\nTable 2: Two types of post-constraints and their variants. Table 2: Two types of post-constraints and their variants.\n\nerror conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is /u1D452 /u1D45F /u1D45F â†¦â†’ 0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output /u1D45F /u1D452 /u1D461 â†¦â†’ 0, which indicates its first successful execution to break the endless loop. that current LLMs ( e.g., GPT-4) are not natively sensitive to the sensitivity; without any additional instructions, LLMs usually overlook the post-constraints. Therefore, we teach the LLM to be sensitive to post-constraints rules through few-shots in-context learning. We describe the design details as follows:",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 52,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-53",
      "content": "ons, LLMs usually overlook the post-constraints. Therefore, we teach the LLM to be sensitive to post-constraints rules through few-shots in-context learning. We describe the design details as follows:\n\n- { \"ret\" : \"success\", \"response\" : { \"must\\_init\" : [\"a\", \"b\", \"c\", \"d\"], \"may\\_init\" : [{ \"name\" :\"n\", \"condition\" : \"ret &gt; 4\"}] } } Â· Check Before Use. Type A is our motivating example; by looking at its check, the post-constraint should be ğ‘Ÿğ‘’ğ‘¡ â‰¥ 4. Type A' describes a similar case with switch-cases , with expected output ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ crticial\\_case .\n- 4.3.2 Function Behavior Summarization. Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 . For example, we provide the following: { \"initializer\" : \"ret = sscanf(str,'%u.%u.%u.%u%n',&amp;a,&amp;b,&amp;c,&amp;d,&amp;n)\", \"suspicious\" : [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\" : \"ret &gt;= 4\" } The LLM may respond with 4.3.1 Post-Constraints Extraction.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 53,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-54",
      "content": "t = sscanf(str,'%u.%u.%u.%u%n',&amp;a,&amp;b,&amp;c,&amp;d,&amp;n)\", \"suspicious\" : [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\" : \"ret &gt;= 4\" } The LLM may respond with 4.3.1 Post-Constraints Extraction. To extract the qualified postcondition , we first determine the post-constraints that lead to the use of suspicious variables. We incorporate few-shot in-context learning to teach LLMs how to extract such constraints from the caller context. Table 2 demonstrates how we teach LLM with in-context learning. We focus primarily on two types of code patterns:\n- The response succinctly encapsulates the postcondition, where variables a,b,c,d are classified as must\\_init , and n is categorized as may\\_init . This is due to the initialization of n only occurring when /u1D45F /u1D452 /u1D461 &gt; 4, and not when /u1D45F /u1D452 /u1D461 â†¦â†’ 4. Note that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 54,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-55",
      "content": "ring when /u1D45F /u1D452 /u1D461 &gt; 4, and not when /u1D45F /u1D452 /u1D461 â†¦â†’ 4. Note that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution. Consider the sscanf() example, even if the analysis is aware that the qualified postcondition should be limited to those where 4, it would still Â· Failure Check. This pattern captures the opposite of the first pattern. They commonly occur in the Linux kernel where the error conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is ğ‘’ğ‘Ÿğ‘Ÿ â†¦â†’ 0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0, which indicates its first successful execution to break the endless loop.\n- /u1D45F /u1D452 /u1D461 â‰¥ need to enumerate the paths inside of sscanf() , which involves loops and can easily lead to timeouts as explained in Â§2.1. 4.3.2 Function Behavior Summarization.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 55,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-56",
      "content": "ess loop.\n- /u1D45F /u1D452 /u1D461 â‰¥ need to enumerate the paths inside of sscanf() , which involves loops and can easily lead to timeouts as explained in Â§2.1. 4.3.2 Function Behavior Summarization. Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 . For example, we provide the following:\n\n```\n4.4 Design #2: Progressive Prompt The Linux kernel has an extremely large codebase. Summarizing an initializer using LLMs without providing any supplementary function definitions can result in incomplete or erroneous responses. { \"initializer\": \"ret = sscanf(str,'%u.%u.%u.%u%n',&a,&b,&c,&d,&n)\", \"suspicious\": [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\": \"ret >= 4\" }\n```\n\nOn the other hand, flooding the LLM with every relevant function The LLM may respond with",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 56,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-57",
      "content": "(str,'%u.%u.%u.%u%n',&a,&b,&c,&d,&n)\", \"suspicious\": [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\": \"ret >= 4\" }\n```\n\nOn the other hand, flooding the LLM with every relevant function The LLM may respond with\n\n```\ndefinition upfront risks exceeding their context window limitations. To address this dilemma, we choose to progressively provide function definitions as needed. Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt , fosters a dynamic interaction with the LLM rather than expecting a response in one shot. Throughout this iterative exchange, we consistently prompt the LLM: { \"ret\": \"success\", \"response\": { \"must_init\": [\"a\", \"b\", \"c\", \"d\"], \"may_init\": [{\"name\":\"n\", \"condition\": \"ret > 4\"}] } }\n```\n\n'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 57,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-58",
      "content": "[\"a\", \"b\", \"c\", \"d\"], \"may_init\": [{\"name\":\"n\", \"condition\": \"ret > 4\"}] } }\n```\n\n'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' . Should the LLM need more information, LLift will promptly extract the relevant The response succinctly encapsulates the function behavior, where variables a,b,c,d are classified as must\\_init , and n is categorized as may\\_init . This is due to the initialization of n only occurring when ğ‘Ÿğ‘’ğ‘¡ &gt; 4, and not when ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 4.\n\n5\n\ndetails o automati\n\nSpecifi response.\n\na specific\n\n[{\n\n\"t\n\nSubse ing code\n\neach req code. Th\n\nsupport t function\n\nThe it details. I\n\nquests ad informat\n\nthe LLM\n\nprompt t data and\n\nanalysis\n\n4.5\n\nD\n\nWe syste primaril\n\nvital ele\n\nMultista versatio\n\nploy a tw and resp\n\ntasks 1 a extractin\n\non sum and effec\n\nidentified three sub\n\ndecompo\n\nThinkin serve tha\n\noutput, s prompte\n\nprompts mentally\n\nThis emp\n\nConsequ natural l\n\nprocesses their res\n\n4.6\n\nD",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 58,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-59",
      "content": "oy a tw and resp\n\ntasks 1 a extractin\n\non sum and effec\n\nidentified three sub\n\ndecompo\n\nThinkin serve tha\n\noutput, s prompte\n\nprompts mentally\n\nThis emp\n\nConsequ natural l\n\nprocesses their res\n\n4.6\n\nD\n\nAt times, construct\n\niors, part dition\n\nmu may\n\nto be\n\n/u1D45F\n\n/u1D452 /u1D461\n\nâ†¦â†’\n\n0.\n\n```\n1 int func(int* a){ 2 if(some_condi) 3 return -1; 4 *a = ... // init 5 return 0; 6 }\n```\n\nFigure 6: A sample case of initializer func , *a is may\\_init or must\\_init under different post-constraints.\n\n```\nmust_init = âˆ… if: C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ or âˆ€ ğ‘ğ‘  âˆˆ {Â¬ some_condi } : ğ‘ğ‘  âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ âˆ§ âˆ€ ğ‘œ âˆˆ { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0 } : ğ‘œ âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ must_init = { ğ‘ } if: (Â¬ some_condi ) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ or ğ‘Ÿğ‘’ğ‘¡ 0 ğ‘ğ‘œğ‘ ğ‘¡\n```\n\n```\n( â†¦â†’ ) âˆ§ C\n```\n\nNote that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 59,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-60",
      "content": "ğ‘ } if: (Â¬ some_condi ) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ or ğ‘Ÿğ‘’ğ‘¡ 0 ğ‘ğ‘œğ‘ ğ‘¡\n```\n\n```\n( â†¦â†’ ) âˆ§ C\n```\n\nNote that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution. Consider the sscanf() example, even if the analysis is aware that the qualified postcondition should be limited to those where ğ‘Ÿğ‘’ğ‘¡ â‰¥ 4, it would still need to enumerate the paths inside of sscanf() , which involves loops and can easily lead to timeouts as explained in Â§2.1.\n\n- 4.3.3 Apply Path Analysis. Following Â§3.2, Figure 6 presents a concert example of post-constraint guided path analysis. This case shows a simple initializer ğ‘– ( ğ‘ ) of the variable ğ‘ . Given an early return, the initialization in line 4 may not be executed. As such, the qualified postconditions become contingent on the post-constraints C ğ‘ğ‘œğ‘ ğ‘¡ .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 60,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-61",
      "content": "zer ğ‘– ( ğ‘ ) of the variable ğ‘ . Given an early return, the initialization in line 4 may not be executed. As such, the qualified postconditions become contingent on the post-constraints C ğ‘ğ‘œğ‘ ğ‘¡ . There are:\n- If the use of variable ğ‘ is conditional with constraints, i.e., C ğ‘ğ‘œğ‘ ğ‘¡ â‰  âŠ¤ , two cases emerge:\n- If the use of variable a is unconditional, i.e., C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ . In this case, the variable ğ‘ is labeled as may\\_init given that the initialization may not be reached. In general, if all path constraints and outcomes of must\\_init are disjoint from C ğ‘ğ‘œğ‘ ğ‘¡ , no path can be pruned out. We could also conclude ğ‘ as may\\_init .\n- (1) C ğ‘ğ‘œğ‘ ğ‘¡ clashes with the constraints of the path (e.g., some\\_condi ), or\n- (2) C ğ‘ğ‘œğ‘ ğ‘¡ conflicts with the path outcome (e.g., return -1 ). In these instances, C ğ‘ğ‘œğ‘ ğ‘¡ could be some\\_condi or func(...)==0 and we can designate *a as must\\_init .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 61,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-62",
      "content": "In these instances, C ğ‘ğ‘œğ‘ ğ‘¡ could be some\\_condi or func(...)==0 and we can designate *a as must\\_init .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.3 Design #1: Post-Constraint Guided Path Analysis",
        "chunkIndex": 62,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-63",
      "content": "The Linux kernel has an extremely large codebase. Summarizing an initializer using LLMs without providing any supplementary function definitions can result in incomplete or erroneous responses. On the other hand, flooding the LLM with every relevant function definition upfront risks exceeding their context window limitations.\n\nSpecifically, We teach the LLM to ask for more information with a specific format:\n\nTo address this dilemma, we choose to progressively provide function definitions as needed. Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt , fosters a dynamic interaction with the LLM rather than expecting a response in one shot. Throughout this iterative exchange, we consistently prompt the LLM: 'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.4 Design #2: Progressive Prompt",
        "chunkIndex": 63,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-64",
      "content": "e in one shot. Throughout this iterative exchange, we consistently prompt the LLM: 'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' . Should the LLM need more information, LLift will promptly extract the relevant details on demand from the source code and provide it to the LLM automatically , enabling it to reassess and generate a more accurate response.\n\n```\n[{\"type\":\"function_def\", \"name\":\"some_func\" }]\n```\n\nSubsequently, LLift scans this format in the LLM's response. For each requested function definition, LLift supplies its corresponding code along with comments extracted from the Linux source code. Though GPT-4 may seek other types of information beyond function definitions ( e.g., struct definitions), we currently limit our support to requests pertaining to function definitions.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.4 Design #2: Progressive Prompt",
        "chunkIndex": 64,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-65",
      "content": "ux source code. Though GPT-4 may seek other types of information beyond function definitions ( e.g., struct definitions), we currently limit our support to requests pertaining to function definitions.\n\nThe iterative process continues until either the LLM no longer requests additional information, or LLift cannot supply the requested details. In certain situations where LLift is unable to provide more information ( e.g., the definition of an indirect call), LLift will still prompt the LLM to proceed with the analysis. In these instances, the LLM is encouraged to infer the behavior based on the available data and its inherent knowledge, thereby facilitating continued analysis even when not all information is directly accessible.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.4 Design #2: Progressive Prompt",
        "chunkIndex": 65,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-66",
      "content": "We systematically apply the principle of task decomposition, a vital element of our design process. This concept is incorporated primarily in two distinct ways.\n\nMultistage Problem Solving. As illustrated in Figure 5, we employ a two-conversation approach to complete the task. Each conversation, essentially consists of multiple iterations of prompts and responses. The first conversation (Convo.1) is dedicated to extracting the initializer and its associated post-constraints (subtasks 1 and 2), while the second conversation (Convo.2) focuses on summarizing the function (subtask 3) based on the previously identified post-constraints. This division allows a more manageable and effective way of achieving the task, compared to combining all three subtasks into a single conversation. The efficacy of this task decomposition approach is further evaluated in Â§6.5.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.5 Design #3: Task Decomposition",
        "chunkIndex": 66,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-67",
      "content": "manageable and effective way of achieving the task, compared to combining all three subtasks into a single conversation. The efficacy of this task decomposition approach is further evaluated in Â§6.5.\n\nThinking in English. Our workflow necessitates a structured output, such as a JSON format, for automation. However, we observe that LLMs often produce suboptimal results when directly prompted to output in this format. As LLMs build responses incrementally, word-by-word, based on preceding outputs [32], direct prompts to output JSON may interrupt their thought progression. This emphasizes the importance of initially soliciting responses in natural language to ensure comprehensive and effective reasoning. Consequently, we instruct the LLM to first articulate their thought processes in English, followed by a subsequent prompt to transform their response into a JSON summary.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.5 Design #3: Task Decomposition",
        "chunkIndex": 67,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-68",
      "content": "At times, LLMs can display unpredictable or inconsistent behaviors, particularly in complex scenarios involving detailed logical constructs. Consider a case where an initializer carries the postcondition must\\_init if ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0. LLMs may still mistakenly assume it to be may\\_init , despite the explicit presence of the post-constraint ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0.\n\nIn addition to task decomposition, we also introduce the concept of self-validation to enhance reliability. Before the LLM reaches its\n\nConversely, an LLM might erroneously interpret a non-existent post-constraint and incorrectly infer a may\\_init case as must\\_init . This phenomenon is known as hallucination . Essentially, the hallucination can lead to both false positives and false negatives in bug detection, thereby affecting accuracy and reliability.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.6 Design #4: Self-Validation",
        "chunkIndex": 68,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-69",
      "content": "st\\_init . This phenomenon is known as hallucination . Essentially, the hallucination can lead to both false positives and false negatives in bug detection, thereby affecting accuracy and reliability.\n\nfinal conclusion, this method reinforces specific rules, allowing the LLM to reassess their previous responses for adherence and make necessary corrections. We observed that this practice yields better results. We evaluate the effect of self-validation in Â§6.4.\n\nAs seen in Figure 5, we employ self-validation in both conversations. By prompting a list of correct properties that we expect, LLMs can verify and correct their results by themselves automatically.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.6 Design #4: Self-Validation",
        "chunkIndex": 69,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-70",
      "content": "In order to further optimize the efficacy of our model, we have incorporated several additional strategies into our prompt design:\n\n- Source Code Analysis. Rather than analyzing abstract representations, we opt to focus our attention directly on the functions within the source code. This approach not only economizes on token use compared to LLVM IR, but also allows the model to leverage the semantic richness of variable names and other programming constructs to conduct a more nuanced analysis.\n- Chain-of-Thought. Leveraging the Chain-of-Thought (CoT) approach, we encourage the LLMs to engage in stepwise reasoning, using the phrase 'think step by step' . This not only helps generate longer, comprehensive responses, but it also provides intermediate results at each juncture of the thought process. Previous studies suggest the CoT approach considerably enhances the LLMs' reasoning capabilities [3]. We incorporate the CoT strategy into every prompt.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.7 Additional Prompting Strategies",
        "chunkIndex": 70,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-71",
      "content": "esults at each juncture of the thought process. Previous studies suggest the CoT approach considerably enhances the LLMs' reasoning capabilities [3]. We incorporate the CoT strategy into every prompt.\n\nThere are still some interesting details in designing an effective prompt but due to space constraints and without changing the overall strategy, we will not list them all. Readers intrigued can delve into the intricacies of our open-sourced prompt 1 design and experimental implementations to gain a deeper understanding.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "4.7 Additional Prompting Strategies",
        "chunkIndex": 71,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-72",
      "content": "We implement the prototype of LLift based on OpenAI's API [19] ( i.e., gpt-4-0613). We describe some implementation details in the following aspects:\n\nInteraction with LLMs. LLift's interaction with LLMs is managed by a simple agent developed in Python, containing roughly 1,000 lines of code. In addition, it uses seven prompts, which altogether constitute about 2,000 tokens in two conversations. All interactions are fully automated via APIs of OpenAI. Besides sending prompts and waiting for responses, our agent also 1) interacts with LLMs according to the progressive prompt design, 2) locates function definitions within the Linux source code, and 3) processes responses from LLMs, then receives and stores to a database.\n\nHyper-Parameters. There are several hyper-parameters in calling the APIs provided by OpenAI. We choose max\\_token and temperature to 1,024 and 1.0, respectively.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "5 IMPLEMENTATION",
        "chunkIndex": 72,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-73",
      "content": "en receives and stores to a database.\n\nHyper-Parameters. There are several hyper-parameters in calling the APIs provided by OpenAI. We choose max\\_token and temperature to 1,024 and 1.0, respectively. max\\_token controls the output length; since LLMs always predict the next words by the previous output, the longer output can benefit and allow its reasoning. However, too many tokens will exhaust the context window quickly, so we pick 1024 as a reasonable balance.\n\nThe temperature controls the randomness and also the ability to reason. Intuitively, we want the analysis to be as non-random as possible and reduce the temperature (it can take a value between 0\n\n1 https://sites.google.com/view/llift-open/prompt\n\nand 2 for GPT models); however, an overly low temperature can result in repetitive or overly simplistic responses.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "5 IMPLEMENTATION",
        "chunkIndex": 73,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-74",
      "content": "re (it can take a value between 0\n\n1 https://sites.google.com/view/llift-open/prompt\n\nand 2 for GPT models); however, an overly low temperature can result in repetitive or overly simplistic responses. We set it to 1.0 (also the default of gpt-4-0613), which allows for higher-quality responses, and use strategies such as self-validation and majority voting to improve the consistency of responses.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "5 IMPLEMENTATION",
        "chunkIndex": 74,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-75",
      "content": "Our evaluation aims to address the following research questions.\n\n- RQ2 (Recall): Is there a possibility for LLift to miss real bugs?\n- RQ1 (Precision): How accurately is LLift able to identify bugs?\n- RQ3 (Comparison): How does the performance of individual components within LLift compare to that of the final design?\n- RQ4 (Model Versatility): How does LLift perform when applied to LLMs other than GPT-4?\n\nWe evaluate RQ1 to RQ3 in GPT-4, under API from OpenAI with version gpt4-0613. For RQ4, we also test GPT-3.5 with version gpt-3.5-turbo-0613 and Claude 2 additionally for comparison.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6 EVALUATION",
        "chunkIndex": 75,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-76",
      "content": "Our experiment data, sourced from UBITect, includes all potential bugs labeled by its static analysis stage but experienced timeout or memory exhaustion during its symbolic execution stage. Overall, UBITect's static analysis stage produced 140,000 potential bugs, with symbolic execution able to process only 60%, leaving 53,000 cases unattended, which means that these cases are generally difficult for static analysis or symbolic execution to decide We craft the following dataset from 53,000 cases to evaluate LLift:\n\n- (2) Bug-50. This dataset comprises the 52 confirmed UBI bugs previously identified by UBITect. It is used as ground truth for assessing recall by verifying if any true bugs were overlooked.\n- (1) Random-1000. We randomly chose 1,000 from the 53,000 cases for testing. However, there are 182 cases where there are no initializers, which are automatically recognized and filtered (see Â§3).",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.1 Dataset",
        "chunkIndex": 76,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-77",
      "content": "d.\n- (1) Random-1000. We randomly chose 1,000 from the 53,000 cases for testing. However, there are 182 cases where there are no initializers, which are automatically recognized and filtered (see Â§3). The remaining 818 cases are used in evaluating precision, i.e., the ratio of true positives to false positives.\n- (3) Cmp-40. This dataset comprises 27 negative and 13 positive cases selected from the Random-1000. We utilize this dataset to illustrate which of our design strategies contributed most to the outcome of our solution.\n\nTurns and Conversations. Due to the progressive prompt, each case may require different turns (pairs of a prompt and a response). In Random-1000, the average number of turns is 2.78, with a max of 8 and a variance of 1.20.\n\nCost. On average, it costs 7,000 tokens in GPT-4 to analyze each potential bug.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.1 Dataset",
        "chunkIndex": 77,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-78",
      "content": "LLift reports 26 positives among the Random-1000 dataset, where half of them are true bugs based on our manual inspection. This represents a precision of 50%. In keeping with UBITect and we focus on the analysis of Linux v4.14, 12 of the bugs still exist in the latest Linux kernel. We are in the process of reporting the 12 bugs to the Linux community. So far, we have submitted patches for 4 bugs and received confirmation that they are true bugs.\n\nTable 3: True bugs identified by LLift from Random-1000, analyzing in Linux v4.14\n\n| Initializer                                                                                                                                                                                             | Caller",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 78,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-79",
      "content": "| File Path                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Variable                                                                                                  | Line                                                   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 79,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-80",
      "content": "|\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 80,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-81",
      "content": "---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|--------------------------------------------------------|\n| read_reg regmap_read ep0_read_setup regmap_read bcm3510_do_hab_cmd readCapabilityRid e1e_rphy pci_read_config_dword lan78xx_read_reg t1_tpi_read pci_read_config_dword ata_timing_compute pt_completion | get_signal_parameters isc_update_profile ep0_handle_setup mdio_sc_cfg_reg_write bcm3510_check_firmware_version airo_get_range __e1000_resume adm8211_probe lan78xx_write_raw_otp my3126_phy_reset quirk_intel_purley_xeon_ras_cap opti82c46x_set_piomode pt_req_sense | drivers/media/dvb-frontends/stv0910.c drivers/media/platform/atmel/atmel-isc.c drivers/usb/mtu3/mtu3_gadget_ep0.c drivers/net/ethernet/hisilicon/hns_mdio.c drivers/media/dvb-frontends/bcm3510.c drivers/net/wireless/cis",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 81,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-82",
      "content": "frontends/stv0910.c drivers/media/platform/atmel/atmel-isc.c drivers/usb/mtu3/mtu3_gadget_ep0.c drivers/net/ethernet/hisilicon/hns_mdio.c drivers/media/dvb-frontends/bcm3510.c drivers/net/wireless/cisco/airo.c drivers/net/ethernet/intel/e1000e/netdev.c drivers/net/wireless/admtek/adm8211.c drivers/net/usb/lan78xx.c drivers/net/ethernet/chelsio/cxgb/my3126.c arch/x86/kernel/quirks.c drivers/ata/pata_legacy.c drivers/block/paride/pt.c | tmp sr setup.bRequestType reg_value ver.demod_version cap_rid.softCap phy_data reg buf val capid0 &tp buf | 504 664 637 169 666 6936 6580 1814 873 193 562 564 368 |\n\nImprecise and Failed Cases. Despite the effectiveness of LLift, there are instances where it does not yield precise results, resulting in 13 false positives by mistakenly classifying must\\_init cases as may\\_init . Upon a careful examination of these cases, we attribute the imprecision to a variety of factors, which we discuss in detail in Â§6.7.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 82,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-83",
      "content": "positives by mistakenly classifying must\\_init cases as may\\_init . Upon a careful examination of these cases, we attribute the imprecision to a variety of factors, which we discuss in detail in Â§6.7. Briefly, we give a breakdown of them here: Incomplete constraint extraction (4 cases), Information gaps in UBITect (5 cases), Variable reuse (1 case), Indirect call (1 case), and Additional constraints (1 case). Additionally, there is one false positive caused by inconsistent output (i.e., two false positives in three runs). Four cases exceed the maximum context length while exploring deeper functions in the progressive prompt.\n\nTakeaway 1. LLift Can effectively summarize initializer behavior and discover new bugs with high precision (50%).",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.2 RQ1: Precision",
        "chunkIndex": 83,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-84",
      "content": "Conceptually, the core optimization (post-constraint guided path analysis) of LLift is sound, and we also prompt a series of rules to let LLMs tend to respond ' may\\_init when uncertain. We expect LLift would not reject true bugs or with a high recall.\n\nFurther, we test LLift on the Bug-50 dataset to see whether it will miss any bugs discovered by UBITect . LLift has demonstrated full effectiveness in identifying all real bugs from Bug-50. This result, while encouraging, does not imply that LLift is flawless. Detailed data analysis reveals that: 1) There remain some inconsistencies in 3 âˆ¼ 5 cases occasionally, though they are mitigated by majority voting; and 2) all the bugs found by UBITect have trivial postconstraints (C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ ) and postcondition of may\\_init ( P ğ‘ğ‘¢ğ‘ğ‘™ : must\\_init â†¦â†’ âˆ… ). Hence, LLift could identify them easily. It is noteworthy that these cases are already those cases detectable by UBITect.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.3 RQ2: Recall Estimate",
        "chunkIndex": 84,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-85",
      "content": "\udc5dğ‘œğ‘ ğ‘¡ = âŠ¤ ) and postcondition of may\\_init ( P ğ‘ğ‘¢ğ‘ğ‘™ : must\\_init â†¦â†’ âˆ… ). Hence, LLift could identify them easily. It is noteworthy that these cases are already those cases detectable by UBITect. Such cases tend to be simpler in nature and can be verified by symbolic execution in UBITect.\n\nWe sample 300 negative cases from Random-1000 in an effort to see whether we will miss any true bugs. We confirm that all are true negatives. Despite the limited data sampled, this result indicates that integrating GPT-4 into our implementation does not introduce apparent unsoundness.\n\nTable 4: Performance evaluation of bug detection tool with progressive addition of design components: Post-Constraint Guided Path Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV), and Task Decomposition (TD). (C) indicates the number of C onsistent cases.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.3 RQ2: Recall Estimate",
        "chunkIndex": 85,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-86",
      "content": "ve addition of design components: Post-Constraint Guided Path Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV), and Task Decomposition (TD). (C) indicates the number of C onsistent cases.\n\n| Combination   | TN(C)   | TP(C)   | Precision   | Recall   | Accuracy   | F1 Score   |\n|---------------|---------|---------|-------------|----------|------------|------------|\n| Simple Prompt | 12(9)   | 2(1)    | 0.12        | 0.15     | 0.35       | 0.13       |\n| PCA           | 13(9)   | 5(1)    | 0.26        | 0.38     | 0.45       | 0.31       |\n| PCA+PP        | 5(3)    | 6(1)    | 0.21        | 0.46     | 0.28       | 0.29       |\n| PCA+PP+SV     | 5(2)    | 11(8)   | 0.33        | 0.85     | 0.40       | 0.48       |\n| PCA+PP+TD     | 22(14)  | 6(4)    | 0.55        | 0.46     | 0.70       | 0.50       |\n| PCA+PP+SV+TD  | 25(17)  | 13(12)  | 0.87        | 1.00     | 0.95       | 0.93       |\n| Oracle        | 27(27)  | 13(13)  | -           | -        | -          | -",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.3 RQ2: Recall Estimate",
        "chunkIndex": 86,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-87",
      "content": ".70       | 0.50       |\n| PCA+PP+SV+TD  | 25(17)  | 13(12)  | 0.87        | 1.00     | 0.95       | 0.93       |\n| Oracle        | 27(27)  | 13(13)  | -           | -        | -          | -          |\n\nTakeaway 2. LLift has proven effective in identifying UBI bugs, consistently detecting all known instances.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.3 RQ2: Recall Estimate",
        "chunkIndex": 87,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-88",
      "content": "In our effort to delineate the contributions of distinct design strategies to the final results, we undertook an evaluative exercise against the Cmp-40 dataset, employing varying configurations of our solution, each entailing a unique combination of our proposed strategies. As illustrated in Table 4, the strategies under consideration encompass Post-constraint Analysis ( PCA ), Progressive Prompt ( PP ), Self-Validation ( SV ), and Task Decomposition ( TD ). The findings underscore an overall trend of enhanced performance with the integration of additional design strategies.\n\nIncorporating PCA offers a notable enhancement, enabling the LLM to uncover a wider array of vulnerabilities. As shown in Table 4, there is a substantial improvement in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 88,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-89",
      "content": "nt in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization.\n\nIn this study, the Baseline corresponds to a straightforward prompt, \"check this code to determine if there are any UBI bugs\" , a strategy that has been found to be rather insufficient for discovering new vulnerabilities, as corroborated by past studies [17, 21, 31], reflecting a modest recall rate of 0.15 and a precision of 0.12.\n\nThe influence of Progressive Prompt ( PP ) on the results is quite intriguing. While its impact appears to lower precision initially, the introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance. Without\n\nTable 5: Comparison of different LLMs on real bugs, from a subset of Bug-50 Table 5: Comparison of different LLMs on real bugs, from a subset of Bug-50",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 89,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-90",
      "content": "als a substantial boost in performance. Without\n\nTable 5: Comparison of different LLMs on real bugs, from a subset of Bug-50 Table 5: Comparison of different LLMs on real bugs, from a subset of Bug-50\n\n| Caller Caller                                                                                                                                                                                                                                                                                                                             | GPT 4 3.5 GPT 4 3.5                                                     | Claude2 Claude2                     | Bard Bard                           |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 90,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-91",
      "content": "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------|-------------------------------------|\n| hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf | âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ | âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— | âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ |",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 91,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-92",
      "content": "ge mt9m114_detect ec_read_u8 compress_sliced_buf | âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ | âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— | âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ |\n\nPP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis. Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data. By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the post-constraints, subsequently leading to errors. in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization. The influence of Progressive Prompt ( PP ) on the results is quite intriguing. While its impact appears to lower precision initially, the",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 92,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-93",
      "content": "on this strategy still leaves a lot of room for optimization. The influence of Progressive Prompt ( PP ) on the results is quite intriguing. While its impact appears to lower precision initially, the\n\nFinally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4. However, this constraint was not breached in any case when TD was incorporated. ( TD ) and Self-Validation ( SV ) also play a crucial role in enhancing consistency . In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs. A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results,",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 93,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-94",
      "content": "rison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results,\n\nBeyond influencing precision and recall, Task Decomposition ( TD ) and Self-Validation ( SV ) also play a crucial role in enhancing consistency . In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs. A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results, and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring reliable and consistent outcomes. introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 94,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-95",
      "content": "y, underscoring their importance in ensuring reliable and consistent outcomes. introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance. Without PP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis. Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data. By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the postcondition, subsequently leading to errors. Beyond influencing precision and recall, Task Decomposition\n\nTakeaway 3. All of LLift's design strategies contributed to the positive results. and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 95,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-96",
      "content": "results. and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.4 RQ3: Contributions of Design Strategies",
        "chunkIndex": 96,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-97",
      "content": "reliable and consistent outcomes.\n\nTable 5 provides a comprehensive view of the performance of our solution, LLift, when implemented across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 [2], and Bard [12]. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively. Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches. PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4. However, this constraint was not breached in any case when TD was incorporated. Takeaway. All of LLift's design strategies contributed to the positive results.\n\nIt is imperative to note that not all design strategies in our toolbox are universally applicable across all language models.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 97,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-98",
      "content": "ay. All of LLift's design strategies contributed to the positive results.\n\nIt is imperative to note that not all design strategies in our toolbox are universally applicable across all language models. Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies. Bard's interaction patterns suggest a preference for immediate response generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach. Similarly, when task 6.6 RQ4: Alternative Models Table 5 provides a comprehensive view of the performance of our solution, LLift, when implemented across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 [2], and Bard [12]. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 98,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-99",
      "content": "nted across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 [2], and Bard [12]. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively. Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches.\n\nHaonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian\n\n```\n1 static int sgl_map_user_pages(...){ 2 ... 3 if ((pages = kmalloc(..., GFP_KERNEL)) == NULL) 4 return -ENOMEM; 5 res = get_user_pages_unlocked(..., pages, ...); 6 /* Errors and no page mapped should return here */ 7 if (res < nr_pages) 8 goto out_unmap; 9 ... 10 out_unmap: 11 if (res > 0) { 12 for (j=0; j < res; j++) 13 put_page( pages[j] ); 14 res = 0; 15 } 16 kfree(pages); 17 }\n```\n\nFigure 6: Case Study I (Loop and Index). Derived from drivers/scsi/st.c Figure 7: Case Study I (Loop and Index). Derived from drivers/scsi/st.c",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 99,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-100",
      "content": "pages[j] ); 14 res = 0; 15 } 16 kfree(pages); 17 }\n```\n\nFigure 6: Case Study I (Loop and Index). Derived from drivers/scsi/st.c Figure 7: Case Study I (Loop and Index). Derived from drivers/scsi/st.c\n\nIt is imperative to note that not all design strategies in our toolbox are universally applicable across all language models. Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies. Bard's interaction patterns suggest a preference for immediate response decomposition is implemented, these models often misinterpret or inaccurately collect post-constraints, subsequently compromising the results. To harness their maximum potential, we only apply the PCA design specifically ( i.e., without other design strategies) for GPT-3.5 and Bard.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 100,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-101",
      "content": "ct post-constraints, subsequently compromising the results. To harness their maximum potential, we only apply the PCA design specifically ( i.e., without other design strategies) for GPT-3.5 and Bard.\n\ntask decomposition is implemented, these models often misinterpret generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach. Similarly, when Contrasting the GPT series, Bard and Claude 2 demonstrate less familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may\\_init possibility of initializers.\n\nor inaccurately collect post-constraints, subsequently compromising the results. To harness their maximum potential, we apply a Takeaway 4. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results.\n\ni.e., postcondition-aware design specifically (",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 101,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-102",
      "content": "s their maximum potential, we apply a Takeaway 4. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results.\n\ni.e., postcondition-aware design specifically (\n\nwithout other design familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may\\_init possibility of initializers. Takeaway. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results. In this case study, we pick three interesting cases demonstrating the effectiveness of LLift in analyzing function behaviors and detecting uninitialized variables. All these cases are undecided for the previous static analyzer, UBITect. We put the complete conversations on an anonymous online page for reference 2 .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the",
        "chunkIndex": 102,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-103",
      "content": "6.7 Case Study In this case study, we pick three interesting cases demonstrating the effectiveness of LLift in analyzing function behaviors and detecting uninitialized variables. All these cases are undecided for the previous static analyzer, UBITect. We put the complete conversations on an anonymous online page for reference 2 . Loop and Index. Figure 6 presents an intriguing case involving the variable pages[j] , which is reported by UBITect as used in Line 17 potentially without being initialized. Unfortunately, Loop and Index. Figure 7 presents an intriguing case involving the variable pages[j] , which is reported by UBITect as used in Line 17 potentially without being initialized. Unfortunately, this case is a false positive which is hard to prune due to loops. Specifically, the initializer function get\\_user\\_pages\\_unlocked() , which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 103,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-104",
      "content": "o loops. Specifically, the initializer function get\\_user\\_pages\\_unlocked() , which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If get\\_user\\_pages\\_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances.\n\n```\nget_user_pages_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances. To summarize the behavior, i.e., must_init facts under conditions where the use is reachable, we must first extract the postthat lead to the use of . Through interacting with { \"initializer\": \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", \"suspicious\": [\"pages[j]\"], \"postconstraint\": \"res < nr_pages && res > 0 && j < res\", }\n```",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 104,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-105",
      "content": "with { \"initializer\": \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", \"suspicious\": [\"pages[j]\"], \"postconstraint\": \"res < nr_pages && res > 0 && j < res\", }\n```\n\nthis case is a false positive which is hard to prune due to loops. Specifically, the initializer function get\\_user\\_pages\\_unlocked() , which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If To summarize the behavior, i.e., must\\_init facts under conditions where the use is reachable, we must first extract the postconstraints that lead to the use of pages . Through interacting with ChatGPT, LLift successfully extracts it:\n\nconstraints pages ChatGPT, LLift successfully extracts it: { After feeding the post-constraints to LLM, LLift then successfully obtains the result:\n\nâ†©\n\n```\n\"initializer\" : \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", â†’ {\n```\n\n2",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 105,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-106",
      "content": "eeding the post-constraints to LLM, LLift then successfully obtains the result:\n\nâ†©\n\n```\n\"initializer\" : \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", â†’ {\n```\n\n2\n\nhttps://sites.google.com/view/llift-open/case-studies 2 https://sites.google.com/view/llift-open/case-studies\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\n```\n1 static int hv_pci_enter_d0( struct hv_device *hdev){ 2 ... 3 init_completion(&comp_pkt.host_event); 4 pkt->completion_func = hv_pci_generic_compl; 5 pkt->compl_ctxt = &comp_pkt; 6 ... 7 8 wait_for_completion(&comp_pkt.host_event); 9 10 if ( comp_pkt.completion_status < 0) 11 ... 12 } 13 14 static void hv_pci_generic_compl( void *context, ...){ 15 struct hv_pci_compl *comp_pkt = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 106,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-107",
      "content": "_compl( void *context, ...){ 15 struct hv_pci_compl *comp_pkt = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_status = -1; 21 22 complete(&comp_pkt->host_event); 23 } 1 static int hv_pci_enter_d0( struct hv_device *hdev){ 2 ... 3 init_completion(&comp_pkt.host_event); 4 pkt->completion_func = hv_pci_generic_compl; 5 pkt->compl_ctxt = &comp_pkt; 6 ... 7 8 wait_for_completion(&comp_pkt.host_event); 9 10 if ( comp_pkt.completion_status < 0) 11 ... 12 } 13 14 static void hv_pci_generic_compl( void *context, ...){ 15 struct hv_pci_compl *comp_pkt = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_status = -1; 21 22 complete(&comp_pkt->host_event); 23 }\n```",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 107,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-108",
      "content": "t = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_status = -1; 21 22 complete(&comp_pkt->host_event); 23 }\n```\n\nFigure 7: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c Figure 8: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c Figure 7: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c\n\n\"suspicious\"\n\n\"suspicious\"\n\n: [\"pages[j]\"],\n\n: [\"pages[j]\"],\n\n: \"res &lt; nr\\_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res\",\n\n\"postconstraint\"\n\n\"postconstraint\"\n\n: \"res &lt; nr\\_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res\",\n\n\"must\\_init\"\n\n```\n} After feeding the post-constraints to LLM, LLift then successfully obtains the result: { \"ret\" : \"success\", \"response\" : { \"ret\": \"success\", \"response\": { \"must_init\": [\"pages[j]\"], \"may_init\": [], } } } After feeding the post-constraints to",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 108,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-109",
      "content": ", LLift then successfully obtains the result: { \"ret\" : \"success\", \"response\" : { \"ret\": \"success\", \"response\": { \"must_init\": [\"pages[j]\"], \"may_init\": [], } } } After feeding the post-constraints to LLM, LLift then successfully obtains the result: { \"ret\" : \"success\", \"response\" : {\n```\n\n\"may\\_init\" : [], } } As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 109,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-110",
      "content": "GPT4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. \"may\\_init\" : [], } } As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must\n\n\"must\\_init\"\n\n: [\"pages[j]\"],\n\n: [\"pages[j]\"], be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. Concurrency and Callback. Consider the case illustrated in Figure 7. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 110,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-111",
      "content": "or potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity, Concurrency and Callback. Consider the case illustrated in Figure 8. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 111,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-112",
      "content": "or potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. Concurrency and Callback.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 112,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-113",
      "content": "ializer and outputting a precise result. be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. Concurrency and Callback. Consider the case illustrated in Figure 7. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity,",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 113,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-114",
      "content": "hread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity,\n\nthe function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, the notion that 'the LLM simply learns everything from the internet and acts Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, some argue that 'the LLM simply learns everything from the internet and acts merely as a search engine' [6]. This viewpoint is challenged by the case illustrated in Figure 9.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 114,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-115",
      "content": "like sscanf() . However, some argue that 'the LLM simply learns everything from the internet and acts merely as a search engine' [6]. This viewpoint is challenged by the case illustrated in Figure 9. the function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, the notion that 'the LLM simply learns everything from the internet and acts\n\nGPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 115,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-116",
      "content": "th noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify the function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 116,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-117",
      "content": "ge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify\n\n```\n1 int p9_check_zc_errors(...){ 2 ... 3 err = p9pdu_readf(req->rc, c->proto_version, \"d\", &ecode); 4 err = -ecode ; 5 ... 6 } 7 8 int p9pdu_readf( struct p9_fcall *pdu, int proto_version, const char *fmt, ...) â†© â†’ 9 ... 10 ret = p9pdu_vreadf(pdu, proto_version, fmt, ap); 11 ... 12 return ret; 13 } 14 15 int p9pdu_vreadf( struct p9_fcall *pdu, int proto_version, const char *fmt, va_list ap){ â†© â†’ 16 switch (*fmt) { 17 case 'd':{ 18 int32_t *val = va_arg(ap, int32_t *); 19 if (pdu_read(...)) { 20 errcode = -EFAULT; 21 break ; 22 } 23 val = ...; // initialization 24 } 25 return errcode; 26 } 1 int p9_check_zc_errors(...){ 2 ... 3 err = p9pdu_readf(req->rc, c->proto_version, \"d\", &ecode); 4 err = -ecode ; 5 ...",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 117,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-118",
      "content": "T; 21 break ; 22 } 23 val = ...; // initialization 24 } 25 return errcode; 26 } 1 int p9_check_zc_errors(...){ 2 ... 3 err = p9pdu_readf(req->rc, c->proto_version, \"d\", &ecode); 4 err = -ecode ; 5 ... 6 } 7 8 int p9pdu_readf( struct p9_fcall *pdu, int proto_version, const char *fmt, ...) â†© â†’ 9 ... 10 ret = p9pdu_vreadf(pdu, proto_version, fmt, ap); 11 ... 12 return ret; 13 } 14 15 int p9pdu_vreadf( struct p9_fcall *pdu, int proto_version, const char *fmt, va_list ap){ â†© â†’ 16 switch (*fmt) { 17 case 'd':{ 18 int32_t *val = va_arg(ap, int32_t *); 19 if (pdu_read(...)) { 20 errcode = -EFAULT; 21 break ; 22 } 23 val = ...; // initialization 24 } 25 return errcode; 26 }\n```\n\nFigure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 9: Case Study III (Unfamiliar Function), derived from net/9p",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 118,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-119",
      "content": "ure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 9: Case Study III (Unfamiliar Function), derived from net/9p\n\nmerely as a blurry search engine' [6], is misguided according to our experiments. This viewpoint is robustly contradicted by the case illustrated in Figure 8. The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide merely as a blurry search engine' [6], is misguided according to our experiments. This viewpoint is robustly contradicted by the case illustrated in Figure 8. The case presents an intriguing real-world bug.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 119,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-120",
      "content": "as a blurry search engine' [6], is misguided according to our experiments. This viewpoint is robustly contradicted by the case illustrated in Figure 8. The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 120,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-121",
      "content": "nf() , where GPT-4 can provide a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() .\n\n```\necode be initialized when p9pdu_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follow: { \"initializer\" : \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\" : [\"ecode\"], \"postconstraint\" : null , \"response\" : { \"must_init\" : [], \"may_init\" : [{ \"name\" : \"ecode\", \"condition\" : \"p9pdu_readf returns 0\" }] } } ecode be initialized when p9pdu_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follow: { \"initializer\" : \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\" : [\"ecode\"], \"postconstraint\" : null , \"response\" : { \"must_init\" : [], \"may_init\" : [{ \"name\" : \"ecode\", \"condition\" : \"p9pdu_readf returns 0\" }] } } { \"initializer\": \"err = p9pdu_readf(req-",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 121,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-122",
      "content": "picious\" : [\"ecode\"], \"postconstraint\" : null , \"response\" : { \"must_init\" : [], \"may_init\" : [{ \"name\" : \"ecode\", \"condition\" : \"p9pdu_readf returns 0\" }] } } { \"initializer\": \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\": [\"ecode\"], \"postconstraint\": null, \"response\": { \"must_init\": [], \"may_init\": [{ \"name\": \"ecode\", \"condition\": \"p9pdu_readf returns 0\" }] } }\n```\n\na precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() . Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that could a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() .",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 122,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-123",
      "content": "but also pinpoints that could a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() . Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that could Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that ecode could be initialized when p9pdu\\_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follows:",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study",
        "chunkIndex": 123,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-124",
      "content": "Challenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints). Challenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints). Challenges in Constraint Extraction.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 124,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-125",
      "content": "ore examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints). Challenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints).\n\nDespite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis. Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 125,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-126",
      "content": "can be solved with better prompts or better integration with static analysis. Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis. Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis.\n\nInformation Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific Information Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific Information Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 126,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-127",
      "content": "ect does not provide explicit field names within a structure when a specific Information Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific\n\nfield is in use. This information gap can result in LLift lacking precision in its analysis. Additionally, UBITect only reports the variable utilized, not necessarily the same variable passed to an initializer. For example, consider an uninitialized variable a passed to an initializer, which is then assigned to variable b for usage. In such a scenario, LLift may fail to identify the initializer due to this incomplete information correctly. These challenges, primarily due to the interface design in UBITect, can be addressed with focused engineering efforts to enrich the output information from UBITect.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 127,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-128",
      "content": "s incomplete information correctly. These challenges, primarily due to the interface design in UBITect, can be addressed with focused engineering efforts to enrich the output information from UBITect.\n\nVariable Reuse. Varaible reuse is an interesting problem of LLM. In general, LLM usually confuses different variables in different scopes ( e.g., different function calls). For example, if the suspicious variable is ret and passed as a argument to its initializer (say, func(&amp;ret) ) and there is another stack variable defined in func also called ret , LLM will confuse them. Explicitly prompting and teaching LLM to note the difference does not appear to work. One solution is to leverage a simple static analysis to normalize the source code to ensure each variable has a unique name.\n\nIndirect Call. As mentioned Â§4.4, LLift follows a simple but imprecise strategy to handle indirect calls.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 128,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-129",
      "content": "imple static analysis to normalize the source code to ensure each variable has a unique name.\n\nIndirect Call. As mentioned Â§4.4, LLift follows a simple but imprecise strategy to handle indirect calls. Theoretically, existing static analysis tools, such as MLTA [16], can give possible targets for indirect calls. However, each indirect call may have multiple possible targets and dramatically increase the token usage. We leave the exploration of such an exhaustive strategy for future work. LLift may benefit from a more precise indirect call resolution.\n\nAdditional Constraints. There are many variables whose values are determined outside of the function we analyze, e.g., preconditions capturing constraints from the outer caller. Since our analysis is fundamentally under-constrained, this can lead LLift to incorrectly determine a must\\_init case to be may\\_init . Mitigating this imprecision relies on further analysis to provide more information.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 129,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-130",
      "content": "ead LLift to incorrectly determine a must\\_init case to be may\\_init . Mitigating this imprecision relies on further analysis to provide more information.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision",
        "chunkIndex": 130,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-131",
      "content": "Post-Constraint Analysis. Our approach prioritizes postconstraints over other constraints, such as preconditions. By focusing on the post-constraints, we enhance the precision and scalability significantly. Importantly, our utilization of large language models in program analysis suggests strong abilities in summarizing complex function behaviors involving loops, a classic hurdle in program analysis.\n\nBetter Integration with Static Analysis. Our work presents opportunities for greater integration and synergy with static analysis methods. Currently, our proposed solution operates largely independently of the static analysis methods, taking only inputs from static analysis initially. Looking into the future, we can consider integrating static analysis and LLMs in a holistic workflow. For example, this could involve selectively utilizing LLM as an assistant to overcome certain hurdles encountered by static analysis, e.g., difficulty in scaling up the analysis or summarizing loop invariant",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "7 DISCUSSION AND FUTURE WORK",
        "chunkIndex": 131,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-132",
      "content": "mple, this could involve selectively utilizing LLM as an assistant to overcome certain hurdles encountered by static analysis, e.g., difficulty in scaling up the analysis or summarizing loop invariants. In turn, further static analysis based on these findings can provide insights to refine the queries to the LLM. This iterative process could enable a more thorough and accurate analysis of complex cases. We believe such a more integrated approach is a very promising future direction.\n\nDeploying on Open-sourced LLMs. The reproducibility of LLift could be potentially challenged, considering its dependency on GPT-4, a closed-source API subject to frequent updates. At the time of writing, Meta introduced Llama 2, an open-source language model with capabilities rivaling GPT-3.5. Our initial assessments suggest that Llama 2 can understand our instructions and appears well-suited to support LLift.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "7 DISCUSSION AND FUTURE WORK",
        "chunkIndex": 132,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-133",
      "content": "oduced Llama 2, an open-source language model with capabilities rivaling GPT-3.5. Our initial assessments suggest that Llama 2 can understand our instructions and appears well-suited to support LLift. The open-source nature of Llama 2 provides us with opportunities to deploy and refine the model further. We plan to leverage these prospects in future studies.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "7 DISCUSSION AND FUTURE WORK",
        "chunkIndex": 133,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-134",
      "content": "Techniques of Utilizing LLMs. Wang et al. [33] propose an embodied lifelong learning agent based on LLMs. Pallagani et al. [23] explores the capabilities of LLMs for automated planning. Weng [35] summarizes recent work in building an autonomous agent based on LLMs and proposes two important components for planning: Task Decomposition and Self-reflection , which are similar to the design of LLift. Beyond dividing tasks into small pieces, task decomposition techniques also include some universal strategies such as Chain-of-thought [34] and Tree-of-thought [38]. The general strategy of self-reflection has been used in several flavors: ReAct [39], Reflexion [29] and Chain of Hindsight [15]. Despite the similarity in name, self-reflection is fundamentally different from self-validation in LLift where the former focuses on using external sources to provide feedback to their models. Huang et al.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "8 RELATED WORK",
        "chunkIndex": 134,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-135",
      "content": "te the similarity in name, self-reflection is fundamentally different from self-validation in LLift where the former focuses on using external sources to provide feedback to their models. Huang et al. [10] let an LLM self-improve its reasoning without supervised data by asking the LLM to lay out different possible results.\n\nLLMs for Program Analysis. Ma et al. [17] and Sun et al. [30] explore the capabilities of LLMs when performing various program analysis tasks such as control flow graph construction, call graph analysis, and code summarization. They conclude that while LLMs can comprehend basic code syntax, they are somewhat limited in performing more sophisticated analyses such as pointer analysis and code behavior summarization. In contrast to their findings, our research with LLift has yielded encouraging results. We conjecture that this might be due to several reasons: (1) benchmark selection, i.e., Linux kernel vs. others. (2) Prompt designs. (3) GPT-3.5 vs.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "8 RELATED WORK",
        "chunkIndex": 135,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-136",
      "content": "esearch with LLift has yielded encouraging results. We conjecture that this might be due to several reasons: (1) benchmark selection, i.e., Linux kernel vs. others. (2) Prompt designs. (3) GPT-3.5 vs. GPT4.0 - prior work only evaluated the results using only GPT-3.5. Pei et al. [26] use LLMs to reason about loop invariants with decent performance. In contrast, LLift leverages LLMs for a variety of tasks (including program behavior summarization) and integrates them successfully into a static analysis pipeline.\n\nLLMs for Software Engineering . Xia et al. [36] propose an automated conversation-driven program repair tool using ChatGPT, achieving nearly 50% success rate. Pearce et al. [25] examine zeroshot vulnerability repair using LLMs and found promise in synthetic and hand-crafted scenarios but faced challenges in real-world examples. Chen et al. [5] teach LLMs to debug its own predicted program to increase its correctness, but only performs on relatively simple programs.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "8 RELATED WORK",
        "chunkIndex": 136,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-137",
      "content": "afted scenarios but faced challenges in real-world examples. Chen et al. [5] teach LLMs to debug its own predicted program to increase its correctness, but only performs on relatively simple programs. Lemieux et al. [14] leverages LLM to generate tests for uncovered functions when the search-based approach got coverage stalled. Feng and Chen [7] use LLM to replay Android bug automatedly. Recently, LangChain proposed LangSimith [13], a LLM-powered platform for debugging, testing, and evaluating. These diverse applications underline the vast potential of LLMs in\n\nsoftware engineering. LLift complements these efforts by demonstrating the efficacy of LLMs in bug finding in the real world.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "8 RELATED WORK",
        "chunkIndex": 137,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-138",
      "content": "This work presents a novel approach that utilizes LLMs to aid static analysis using a completely automated agent. By carefully considering the scope and designing the interactions with LLMs, our solution has yielded promising results. We believe our effort only scratched the surface of the vast design space, and hope our work will inspire future research in this exciting direction.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "9 CONCLUSION",
        "chunkIndex": 138,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-139",
      "content": "- [2] Anthropic (2023). 2023. Claude 2. https://www.anthropic.com/index/claude-2\n- [1] Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl T. Barr. 2023. Improving Few-Shot Prompts with Relevant Static Analysis Products. http://arxiv.org/abs/2304.06815 arXiv:2304.06815 [cs].\n- [3] Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023. When do you need Chain-of-Thought Prompting for ChatGPT? http://arxiv.org/abs/2304.032 62 arXiv:2304.03262 [cs].\n- [5] Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. http://arxiv.org/abs/2304.05128\n- [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\n- [6] Ted Chiang. 2023. ChatGPT Is a Blurry JPEG of the Web. The New Yorker (Feb. 2023).",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 139,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-140",
      "content": "rockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\n- [6] Ted Chiang. 2023. ChatGPT Is a Blurry JPEG of the Web. The New Yorker (Feb. 2023). https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-ablurry-jpeg-of-the-web Section: annals of artificial intelligence.\n- [8] Github. 2023. GitHub Copilot documentation. https://ghdocs-prod.azurewebsit es.net/\\_next/data/mHA\\_XfBBaMPyfcP0Q05C5/en/free-pro-team@latest/copi lot.json?versionId=free-pro-team%40latest&amp;productId=copilot\n- [7] Sidong Feng and Chunyang Chen. 2023. Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models. https://doi.org/10.48550/arX iv.2306.01987 arXiv:2306.01987 [cs].\n- [9] Anjana Gosain and Ganga Sharma. 2015. Static Analysis: A Survey of Techniques and Tools.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 140,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-141",
      "content": "Bug Replay with Large Language Models. https://doi.org/10.48550/arX iv.2306.01987 arXiv:2306.01987 [cs].\n- [9] Anjana Gosain and Ganga Sharma. 2015. Static Analysis: A Survey of Techniques and Tools. In Intelligent Computing and Applications (Advances in Intelligent Systems and Computing) , Durbadal Mandal, Rajib Kar, Swagatam Das, and Bijaya Ketan Panigrahi (Eds.). Springer India, New Delhi, 581-591.\n- [11] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. Comput. Surveys 55, 12 (Dec. 2023), 1-38. https://doi.org/10.1145/3571730\n- [10] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve. http: //arxiv.org/abs/2210.11610 arXiv:2210.11610 [cs].\n- [12] Jack Krawczyk and Amarnag Subramanya. 2023. Bard's latest update: more features, languages and countries.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 141,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-142",
      "content": "nguage Models Can Self-Improve. http: //arxiv.org/abs/2210.11610 arXiv:2210.11610 [cs].\n- [12] Jack Krawczyk and Amarnag Subramanya. 2023. Bard's latest update: more features, languages and countries. https://blog.google/products/bard/googlebard-new-features-update-july-2023/\n- [14] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, and Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained Large Language Models. (2023).\n- [13] LangChain (2023). 2023. Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications. https: //blog.langchain.dev/announcing-langsmith/\n- [15] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of Hindsight Aligns Language Models with Feedback. http://arxiv.org/abs/2302.02676 arXiv:2302.02676 [cs].\n- [17] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 142,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-143",
      "content": "ght Aligns Language Models with Feedback. http://arxiv.org/abs/2302.02676 arXiv:2302.02676 [cs].\n- [17] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023. The Scope of ChatGPT in Software Engineering: A Thorough Investigation. http://arxiv.org/abs/2305.12138 arXiv:2305.12138 [cs].\n- [16] Kangjie Lu and Hong Hu. 2019. Where Does It Go?: Refining Indirect-Call Targets with Multi-Layer Type Analysis. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security . ACM, London United Kingdom. https://doi.org/10.1145/3319535.3354244\n- [18] Bertrand Meyer. 1997. Object-Oriented Software Construction, 2nd Edition . Prentice-Hall.\n- [20] OpenAI (2023). 2023. Function calling and other API updates. https://openai.c om/blog/function-calling-and-other-api-updates\n- [19] OpenAI (2022). 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n- [21] OpenAI (2023). 2023. GPT-4 Technical Report.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 143,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-144",
      "content": "es. https://openai.c om/blog/function-calling-and-other-api-updates\n- [19] OpenAI (2022). 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n- [21] OpenAI (2023). 2023. GPT-4 Technical Report. http://arxiv.org/abs/2303.08774 arXiv:2303.08774 [cs].\n- [22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\n23. Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. http://arxiv.org/ abs/2203.02155 arXiv:2203.02155 [cs].\n- [24] Jihyeok Park, Hongki Lee, and Sukyoung Ryu. 2022. A Survey of Parametric Static Analysis. ACM Comput. Surv. 54, 7 (2022), 149:1-149:37.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 144,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-145",
      "content": ". http://arxiv.org/ abs/2203.02155 arXiv:2203.02155 [cs].\n- [24] Jihyeok Park, Hongki Lee, and Sukyoung Ryu. 2022. A Survey of Parametric Static Analysis. ACM Comput. Surv. 54, 7 (2022), 149:1-149:37. https://doi.org/ 10.1145/3464457\n- [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia. 2023. Understanding the Capabilities of Large Language Models for Automated Planning. http://arxiv.org/abs/2305.16151 arXiv:2305.16151 [cs].\n- [25] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining Zero-Shot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (S&amp;P) . IEEE Computer Society, Los Alamitos, CA, USA. https://doi.org/10.1109/SP46215.20 23.00001\n- [27] Luke Salamone. 2021.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 145,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-146",
      "content": "Large Language Models. In 2023 IEEE Symposium on Security and Privacy (S&amp;P) . IEEE Computer Society, Los Alamitos, CA, USA. https://doi.org/10.1109/SP46215.20 23.00001\n- [27] Luke Salamone. 2021. What is Temperature in NLP? https://lukesalamone.git hub.io/posts/what-is-temperature/ Section: posts.\n- [26] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can Large Language Models Reason about Program Invariants?. In Proceedings of the 40th International Conference on Machine Learning .\n- [28] Jessica Shieh. 2023. Best practices for prompt engineering with OpenAI API | OpenAI Help Center. https://help.openai.com/en/articles/6654000-bestpractices-for-prompt-engineering-with-openai-api\n- [30] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 146,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-147",
      "content": "neering-with-openai-api\n- [30] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023. Automatic Code Summarization via ChatGPT: How Far Are We? http://arxiv.org/abs/2305.12865 arXiv:2305.12865 [cs].\n- [29] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. http://arxiv.org/abs/2303.11366 arXiv:2303.11366 [cs].\n- [31] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and TegawendÃ© F. BissyandÃ©. 2023. Is ChatGPT the Ultimate Programming Assistant - How far is it? http://arxiv.org/abs/2304.11938 arXiv:2304.11938 [cs].\n- [33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 147,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-148",
      "content": "1938 [cs].\n- [33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. http://arxiv.org/abs/2305.16291 arXiv:2305.16291 [cs].\n- [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran Associates, Inc.\n- [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. http://arxiv.org/abs/2201.11903 arXiv:2201.11903 [cs].\n- [36] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. http://arxiv.org/abs/ 2304.00385\n- [35] Lilian Weng. 2023. LLM-powered Autonomous Agents.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 148,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-149",
      "content": "nd Lingming Zhang. 2023. Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. http://arxiv.org/abs/ 2304.00385\n- [35] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/posts/2023-06-23-agent\n- [37] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming . ACM, San Diego CA USA, 1-10. https://doi.org/10.1145/3520312.3534862\n- [39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. International Conference on Learning Representations (ICLR) (2023).\n- [38] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 149,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-150",
      "content": "nguage Models. International Conference on Learning Representations (ICLR) (2023).\n- [38] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. http://arxiv.org/abs/2305.10601 arXiv:2305.10601 [cs].\n- [40] Yizhuo Zhai, Yu Hao, Hang Zhang, Daimeng Wang, Chengyu Song, Zhiyun Qian, Mohsen Lesani, Srikanth V. Krishnamurthy, and Paul Yu. 2020. UBITect: A Precise and Scalable Method to Detect Use-before-Initialization Bugs in Linux Kernel. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020) .\n- [42] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why Does ChatGPT Fall Short in Providing Truthful Answers? http://arxiv.org/abs/2304.10513 arXiv:2304.10513 [cs].\n- [41] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 150,
        "totalChunks": 152
      }
    },
    {
      "id": "2308.00245v3-chunk-151",
      "content": "Does ChatGPT Fall Short in Providing Truthful Answers? http://arxiv.org/abs/2304.10513 arXiv:2304.10513 [cs].\n- [41] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. arXiv:2303.18223 [cs.CL]",
      "metadata": {
        "source": "arxiv:2308.00245v3",
        "title": "The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models",
        "authors": [
          "Haonan Li",
          "Yu Hao",
          "Yizhuo Zhai",
          "Zhiyun Qian"
        ],
        "section": "REFERENCES",
        "chunkIndex": 151,
        "totalChunks": 152
      }
    }
  ],
  "fullText": "## The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\nHaonan Li\n\nRiverside, California, USA\n\nhli333@ucr.edu UC Riverside\n\nYizhuo Zhai yzhai003@ucr.edu UC Riverside Riverside, California, USA\n\nYu Hao\n\nRiverside, California, USA\n\nyhao016@ucr.edu UC Riverside\n\nZhiyun Qian\n\nRiverside, California, USA\n\nzhiyunq@cs.ucr.edu UC Riverside and (2) a precise symbolic execution with limited scalability. The solution illuminates the need for alternative strategies to navigate the complex trade-offs between precision and scalability effectively. Despite this strategic combination of analysis techniques, nearly 40% of the potential bugs reported from the static analysis phase experience a timeout or memory exhaustion during the static symbolic execution phase, preventing any conclusive results on such cases. This limitation hinders the overall effectiveness of the tool, leading to the potential of two distinct outcomes: missed bugs if these potential bug reports are ignored (what UBITect performs), or false positives if they are sent to developers for inspection.\n\nIn response, we propose LLift, a fully automated framework that bridges static analysis with LLMs in analyzing UBI bugs. Our solution packages several novel components. First, LLift performs post-constraint guided path analysis , which helps verify the path feasibility of the 'use' of an initialized variable, a difficult task for static analysis and symbolic execution. Second, to efficiently interact with LLMs, we employ task decomposition to break down the analysis into more than a single step. Third, we employ progressive prompting by providing information incrementally only when necessary, instead of providing an enormous scope of code at once. Finally, we propose self-validation by requesting LLMs to\n\nIn this paper, we investigate the possibility of leveraging Large Language Model s (LLMs) as an alternative to handle such 'difficult cases'. This is because recent LLMs have exhibited strong potential in understanding, generating, and even debugging code [4, 8, 13]. Nevertheless, navigating the intricacies of utilizing LLMs for bug discovery proves to be a complex feat. The technical report on GPT-4 underscores this challenge, admitting that when it comes to discovering new vulnerabilities, it may not be the best solution standalone [21]: '... is less effective than existing tools for complex and high-level activities like novel vulnerability identification'. In the same vein, prior research demonstrates the competence of LLMs mostly in simpler tasks or programs [1, 25, 26]. This is because LLMs are far from perfect. For instance, they suffer from hallucination [11] where instead of identifying the bugs in faulty code, LLMs may create non-existent facts in an attempt to rationalize the original intention behind the problematic code [17, 31]. Another issue is the stochasticity of LLMs which can result in inconsistent or outright incorrect results, thus throwing another wrench into the gears of bug discovery [41]. Finally, LLMs have limited context windows, meaning they can only scrutinize a relatively small codebase.\n\n## ABSTRACT\n\nStatic analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Model s (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50%) and appears to have no missing bug. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.\n\n## 1 INTRODUCTION\n\nStatic analysis is a popular technique in software engineering, particularly in the area of bug discovery, that can improve code quality, reliability, and security. However, the effectiveness of these techniques is influenced by the fundamental trade-off between precision and scalability, especially when dealing with extensive and complex programs [9, 24]. On the one hand, static analysis solutions with lower precision tend to generate numerous false positives. On the other hand, expensive static analysis or symbolic execution solutions with higher precision often struggle to complete the analysis. Consequently, achieving comprehensive and accurate static program analysis for sizable programs like the Linux kernel poses a significant challenge.\n\nUBITect [40], a powerful static analysis solution illustrates these inherent limitations thoroughly. Targeting Use-Before-Initialization (UBI) bugs in the Linux kernel, it packages a pipeline of (1) a scalable bottom-up summary-based static analysis with limited precision,\n\n```\n1 static int libcfs_ip_str2addr(...){ 2 unsigned int a, b, c, d; 3 if (sscanf(str, \"%u.%u.%u.%u%n\", &a, &b, &c, &d, &n) >= 4){ 4 // use of a, b, c, d 5 } 6 } 7 int sscanf( const char *buf, const char *fmt, ...){ 8 va_list args; 9 int i; 10 va_start(args, fmt); 11 i = vsscanf(buf, fmt, args); 12 va_end(args); 13 }\n```\n\nFigure 1: Code snippet of sscanf and its usecase Figure 1: Code snippet of sscanf and its usecase\n\nTable 1: UBITect's summary for sscanf . Both use and initialization for va\\_args are incorrect. âœ“ and âœ— stand for whether this parameter will be used/initialized after its call. '... ' represents all other parameters of va\\_args . Table 1: UBITect's summary for sscanf . Both use and initialization for va\\_args are incorrect. âœ“ and âœ— stand for whether this parameter will be used/initialized after its call. '... ' represents all other parameters of va\\_args .\n\n|                       | buf buf   | fmt fmt   | ... ...   | *buf *buf   | *fmt *fmt   |\n|-----------------------|-----------|-----------|-----------|-------------|-------------|\n| Use Use               | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“         | âœ“ âœ“         |\n| Initialize Initialize | âœ— âœ—       | âœ— âœ—       | âœ— âœ—       | âœ— âœ—         | âœ— âœ—         |\n\nwe propose self-validation by requesting LLMs to review responses at various stages to obtain accurate and reliable responses. review responses at various stages to obtain accurate and reliable responses.\n\nWe summarize our contributions as follows: We summarize our contributions as follows:\n\nWe implement a prototype of LLift and test it in real-world scenarios. Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LLift successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community. With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%. We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%. We implement a prototype of LLift and test it in real-world scenarios. Focusing on the inconclusive cases of UBITect caused by time or memory limitation, LLift successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community. With 26 positive reports out of nearly 1,000 cases, LLift reaches a high precision of 50%. We also test LLift against all previously known bugs found by UBITect, and observe a recall of 100%.\n\nâ€¢\n\n- New Opportunities. We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs. To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets. Â· New Opportunities. We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs. To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets.\n\nâ€¢\n\nâ€¢\n\n- Results. We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a high precision rate (50%) and recall rate (100%). Additionally, our examination led to the discovery of 13 previously unknown bugs. Â· Results. We rigorously investigate LLift by conducting an indepth analysis of nearly 1000 cases, resulting in a reasonable precision rate (50%). Additionally, our examination led to the discovery of 13 previously unknown bugs.\n\nâ€¢\n\n- Open source. Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis. Â· Open source. Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis.\n\n2\n\n## BACKGROUND &amp; MOTIVATION 2 BACKGROUND &amp; MOTIVATION\n\nUBITect is a state-of-the-art static analysis solution aiming at finding Use Before Initialization (UBI) bugs in the Linux kernel [41]. It employs a two-stage pipeline where the first stage employs a bottom-up summary-based static analysis of the Linux kernel. By design, this stage aims for scalability and sacrifices precision, producing a significant number of potential bugs ( i.e., âˆ¼ 140k), most of UBITect is a state-of-the-art static analysis solution aiming at finding Use Before Initialization (UBI) bugs in the Linux kernel [40]. It employs a two-stage pipeline where the first stage employs a bottom-up summary-based static analysis of the Linux kernel. By design, this stage aims for scalability and sacrifices precision, producing a significant number of potential bugs ( i.e., âˆ¼ 140k), most of\n\n## 2.1 UBITect and Motivating Example 2.1 UBITect and Motivating Example\n\n- New Methodologies. We develop LLift, an innovative and fully automated agent that arms static analysis with LLMs. LLift employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses. Â· NewMethodologies. Wedevelop LLift, an innovative and fully automated framework that arms static analysis with LLMs. LLift employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses.\n\n2\n\nwhich are false alarms. The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs). It is complemented by a second stage of static symbolic execution that filters as many false alarms as possible by verifying their path feasibility. However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs. which are false alarms. The static analysis is imprecise partly due to its lack of path sensitivity (often needed to discover UBI bugs). It is complemented by a second stage of static symbolic execution that filters as many false alarms as possible by verifying their path feasibility. However, 40% of the reported bugs are discarded due to timeout (10 minutes) or memory limitations (2 GB) during the symbolic execution, potentially missing genuine bugs.\n\nd\n\nFigure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion. In other words, UBITect failed to rule out this case as a false alarm. As Table 1 presents, the static analysis stage generates a summary of sscanf() as ' may not initialize parameters a , b , c , and ' but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively. There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions : For soundness, UBITect assumed the va\\_start() is a normal function. However, since it cannot find its definition, it has to conservatively assume that the arguments passed to it will be used inside. Unfortunately, in reality, va\\_start is a compiler built-in function that simply 'prepares' the arguments without any uses. 2) insensitivity of postconditions : It fails to recognize the check of its return value, i.e., if(sscanf(...)&gt;=4) , which ensures its arguments a to d must be initialized before use. Figure 1 shows a case where UBITect's static analysis stage considers it a potential UBI bug (a false alarm) and the subsequent symbolic execution stage times out and fails to generate a definitive conclusion. In other words, UBITect failed to rule out this case as a false alarm. As Table 1 presents, the static analysis stage generates a summary of sscanf() as ' may not initialize parameters a , b , c , and d ' but does use them at Line 3. Consequently, the static analysis stage reports two locations of use-before-initialization at Line 3 and Line 4, respectively. There are two reasons for the static analysis stage to consider the case a potential bug: 1) inability to recognize special functions : For soundness, UBITect assumed the va\\_start() is a normal function. However, since it cannot find its definition, it has to conservatively assume that the arguments passed to it will be used inside. Unfortunately, in reality, va\\_start is a compiler built-in function that simply 'prepares' the arguments without any uses. 2) insensitivity of path constraints : It fails to recognize the path constraint, i.e., if(sscanf(...)&gt;=4) , which ensures its arguments a to d must be initialized before use.\n\n## 2.2 Practical Challenges of Static Analysis 2.2 Practical Challenges of Static Analysis\n\nInherent Knowledge Boundaries. Developers need to model specific functions or language features. Otherwise, they influence the correctness of the results. For compiler built-in functions, e.g., va\\_start() , their definitions are simply not available. Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel. These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions. However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time. This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired. Inherent Knowledge Boundaries. Developers need to model specific functions or language features. Otherwise, they influence the correctness of the results. For compiler built-in functions, e.g., va\\_start() , their definitions are simply not available. Beyond this example, there exists an array of other scenarios, which are particularly prevalent in the Linux kernel. These situations include assembly code, hardware behaviors, callback functions, concurrency, and compiler built-in functions. However, in practical terms, it is often time-consuming to discover and model all these cases, because they can be highly dependent on the analysis target and evolve over time. This limitation often compromises the effectiveness of static analysis, leaving it less precise and comprehensive than desired.\n\nIn light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows: In light of our motivating example of the sscanf() case, we can summarize the reasons for UBITect's failure as follows:\n\nExhaustive Path Exploration. Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4 . Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase. While this approach is theoretically comprehensive, it often leads to a combinatorial explosion. The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specific analysis task at hand. In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf . This causes UBITect's symbolic execution to time out exactly due to this problem. Exhaustive Path Exploration. Correctly handling cases like sscanf() requires it to consider the check: sscanf(...)&gt;=4 . Unfortunately, existing path-sensitive static analysis (and symbolic execution) techniques operate under a methodical but exhaustive paradigm, exploring all potential execution paths through the codebase. While this approach is theoretically comprehensive, it often leads to a combinatorial explosion. The vast array of execution paths necessitates the exploration of myriad functions, many of which ultimately prove irrelevant to the specific analysis task at hand. In the sscanf() case, its return value is computed inside an unbounded loop when iterating over an unknown string variable buf . This causes UBITect's symbolic execution to time out exactly due to this problem.\n\nUBITect\n\n<!-- image -->\n\nFigure 2: The overview of LLift. Start with the discarded cases by UBITect and determine whether these potential bugs are true or false. Figure 2: The overview of LLift. Start with the discarded cases by UBITect and determine whether these potential bugs are true or false.\n\n```\n1 int caller_function(){ 2 int X; // declare of suspicious variable /u1D44B 3 ... 4 init(&X); // initializer of /u1D44B 5 ... 6 use(X); // use of /u1D44B 7 }\n```\n\n,\n\nFigure 3: A typical type of potential UBI bug. For each suspicious variable /u1D44B we expect it to 1) have an initializer function that probably initializes /u1D44B and 2) use /u1D44B . Figure 3: A typical type of potential UBI bug. For each suspicious variable ğ‘‹ , we expect it to 1) have an initializer function that probably initializes ğ‘‹ and 2) use ğ‘‹ .\n\n## 2.3 Capability of LLMs 2.3 Capability of LLMs\n\nDomain-specific Programming Constructs Recognition. This proficiency is showcased in three key areas: 1) Function Recognition : LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf() , kzalloc() , kstrtoul() and 'list for each' , simplifying the analysis and making the analysis more scalable. 2) Function pointers and callbacks : LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling. We will show an interesting Domain-specific Programming Constructs Recognition. This proficiency is showcased in three key areas: 1) Function Recognition : LLMs can identify frequently used interfaces in the Linux kernel from its semantics, such as sscanf() , kzalloc() , kstrtoul() , and 'list for each' , simplifying the analysis and making the analysis more scalable. 2) Function pointers and callbacks : LLMs can accurately interpret complex uses of function pointers as callbacks, which often require manual modeling. We will show an interesting case in Â§6.6.\n\n## 3 PROBLEM FORMULATION\n\nFortunately, LLMs [21] offers a promising alternative to summarizing code behaviors [22] in a flexible way and bypassing the aforementioned challenges. This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs. Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization . Fortunately, LLMs [21] offers a promising alternative to summarizing code behaviors [22] in a flexible way and bypassing the aforementioned challenges. This is because LLMs are trained and aligned with extensive datasets that include both natural language and programs. Specifically, we observe that LLMs possess fundamental abilities that assist in addressing each challenge: 1) domainspecific code recognition and 2) smart code summarization .\n\n,\n\ncase in Â§6.7. Smart Code Summarization. LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently difficult task in program analysis. This is likely because it has been trained on various functions with loops and their semantics. In contrast, traditional static analysis follows explicitly defined rules without a limited ability to generalize. Smart Code Summarization. LLMs can work with complicated functions; for example, that they can summarize loop invariants [26], which is an inherently difficult task in program analysis. This is likely because it has been trained on various functions with loops and their semantics. In contrast, traditional static analysis follows explicitly defined rules without a limited ability to generalize.\n\n3\n\nBased on our observation, we propose LLift, an automated agent that interfaces with UBITect and LLMs (e.g., ChatGPT) to help identify and reason about UBI bugs in the Linux kernel. 3.1.1 Use-Before-Initialization. A Use Before Initialization (UBI) bug refers to the erroneous scenario where a variable ğ‘£ is accessed or involved in any operation prior to its correct initialization. Let:\n\n## PROBLEM FORMULATION 3.1 Definitions and Scope\n\nScope.\n\n- Our goal of developing LLift is not to replace UBITect and Â· ğ‘‘ ( ğ‘£ ) represent the declaration of ğ‘£ .\n- the difficult cases for UBITect, i.e., 40% of cases that are considered Â· ğ‘– ( ğ‘£ ) denote the initialization operation of ğ‘£ .\n- static analysis in general. Instead, we aim to use LLift to analyze Â· ğ‘¢ ( ğ‘£ ) signify a use operation involving ğ‘£ .\n\ninconclusive. In other words, we aim to use LLift primarily as a complementary solution to UBITect. This relationship is depicted if there exists ğ‘‘ ( ğ‘£ ) and ğ‘¢ ( ğ‘£ ) , then ğ‘£ is used before initialization if:\n\nin Figure 2.\n\nAssumptions. As a first exploratory study of assisting static analysis with LLMs, we restrict our scope to commonly observed patterns where &lt; indicates a temporal sequence in the program execution.\n\n<!-- formula-not-decoded -->\n\nthat exceed the time and memory limits set for UBITect's symbolic execution stage. Specifically, depicted in Figure 3, we observe that 3.1.2 Postcondition. Postconditions encapsulate the expected state or behavior of a system upon the conclusion of a routine [18].\n\nit is often the case that the variable is declared in one function and\n\n3\n\nUBITect reports potential usebefore-initialization bugs Specifically, they detail the guarantees a routine offers based on its observable outcomes.\n\nSummarize the initializer with\n\nIdentify the initializer: sscanf Extract the post-constraint: sscanf(...)&gt;=4 1 2 3 For a routine ğ‘… , consider its set of outcomes as O . These outcomes are defined as updates to its parameters (and return value) for a path of ğ‘… . Particularly, O does not include initialization for variables for convenience. In the study of UBI bug, for a routine ğ‘… that can yield a set of outcomes O , the postcondition P can be defined as:\n\nFigure 4: Example run of LLift. For each potential bug, we â‘  identify its initializer, â‘¡ extract the post-constraints of the initializer, and â‘¢ analyze the behavior of the initializer with the post-constraints via LLM. Here, S( ğ‘… ) signifies all possible execution paths through the routine ğ‘… , O describes all updates of ğ‘… on its variables, and must\\_init is a set of variables that must be initialized.\n\n<!-- formula-not-decoded -->\n\nit is passed to a callee function to be initialized (typically conditionally) before its use. We assume there can be multiple layers of Motivating Example. Consider the sscanf() function in our motivating example. Based on these return values, the postconditions assure the initialization of certain variables:\n\n```\nare other less common cases, i.e., the variable is initialized directly in the function where the variable is declared. We choose not to account for them, as most such cases are easier cases and more likely solved successfully by UBITect already. Conceptual Workflow. Figure 4 demonstrates the workflow of LLift. LLift takes bug reports from UBITect as inputs, which contain the suspicious variable that may be used before initialization, P( ğ‘ğ‘ğ‘¡â„ 1 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0 , must_init â†¦â†’âˆ…} P( ğ‘ğ‘ğ‘¡â„ 2 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 1 , must_init â†¦â†’{ ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 3 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 2 , must_init â†¦â†’{ ğ‘, ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 4 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 3 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘ }} P( ğ‘ğ‘ğ‘¡â„ 5 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 4 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘, ğ‘‘ }} P( ğ‘ğ‘ğ‘¡â„ 6 ) : { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 5 , must_init â†¦â†’{ ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘› }}\n```\n\nindirection where the initialization actually occurs. Note that there and the function that it sits in. We follow the following three key\n\nmotivating example, sscanf() is the initializer. â‘¡ Extract the path constraints from the initializer return to the use, i.e., the constraints that ensure the use is reachable. In our example, the constraint is sscanf(...)&gt;=4 . â‘¢ Summarize the behavior of the initializer, capturing the variFor UBI detection, not every associated postcondition is relevant; instead, only the outcomes making the ğ‘¢ ( ğ‘£ ) reachable are critical . The constraints of the use are post-constraints C ğ‘ğ‘œğ‘ ğ‘¡ [ ? ]. The qualified postcondition , P ğ‘ğ‘¢ğ‘ğ‘™ , is a subset of P refined by C ğ‘ğ‘œğ‘ ğ‘¡ :\n\nsubtasks when interacting with LLMs: â‘  Identify potential initializers of the suspicious variable. In our Here, the ğ‘ğ‘ğ‘¡â„ 1 -ğ‘ğ‘ğ‘¡â„ 6 represent different possible paths in the sscanf() and each path corresponds with a different postcondition.\n\nable initialization status,\n\n<!-- formula-not-decoded -->\n\nmust\\_init or\n\nmay\\_init\n\n, given potential bug. Note that this simple policy may still lead to false alarms (which we evaluate in Â§6). For example, our workflow by design does not take into account preconditions, In subsequent discussions, unless otherwise specified, the term 'postcondition' shall denote 'qualified postcondition' .\n\nthe path constraints learned in . In our example, a correct response should be must\\_init: a,b,c,d . If the final output is must\\_init , we can safely filter out such cases as non-bugs; if the output is may\\_init , we will consider it a For the sscanf() case, if the post-constraint is C ğ‘ğ‘œğ‘ ğ‘¡ = ret â‰¥ 4, the qualified postcondition would be P( ğ‘ğ‘ğ‘¡â„ 5 ) âˆ§ P( ğ‘ğ‘ğ‘¡â„ 6 ) , which ensures that variables a, b, c, and d must be initialized; therefore, all variables used subsequently are initialized, and no UBI happens.\n\n## 3.2 Post-Constraint Guided Path Analysis\n\nwill follow a number of principles described in this section. 4.1 Design Challenges It is non-trivial to prompt LLMs effectively [28, 42]. We meet the Let ğ‘… be the routine or function under analysis and S( ğ‘… ) be its path set. Let ğ‘ğ‘ğ‘¡â„ âˆˆ S( ğ‘… ) refer to a specific path in ğ‘… . Besides, Each path ğ‘ğ‘ğ‘¡â„ has an associated path constraint ğ‘ that dictates its feasibility. These two optimizations can be formed with:\n\n4 DESIGN We have illustrated the conceptual workflow previously in Figure 4 that works together logically to solve our formulated problem. However, to achieve better results, we have to overcome a number of challenges in interacting with LLMs. This means that will need to map the conceptual workflow into an instantiated workflow which When analyzing a routine or function in a path-sensitive manner, the number of paths to explore can grow rapidly. Fortunately, if we have information about what the function is expected to achieve (given by C ğ‘ğ‘œğ‘ ğ‘¡ ), we can prune paths that inherently don't meet those expectations. We categorize two scenarios, direct application and outcome conflicts , in applying this optimization.\n\nfollowing challenges and propose solutions correspondingly in designing LLift. Â· C1. Limited Understanding of Postconditions. Despite LLMs Direct Application. For direct application, the post-constraint C ğ‘ğ‘œğ‘ ğ‘¡ can be directly applied as a path constraint. A path can be discarded if:\n\n<!-- formula-not-decoded -->\n\n(\n\ne.g.,\n\nGPT-4) are able to comprehend the definition of postcondi- to utilize this knowledge in actual program analysis-such as\n\nThis implies that if a ğ‘ğ‘ğ‘¡â„ inherently contradicts the post-constraint, it can be removed from consideration.\n\nOutcome Conflicts. Let O( ğ‘ ) denote the set of all outcomes or effects produced by path ğ‘ . A path can be pruned if any of its outcomes conflict with the post-constraint:\n\n<!-- formula-not-decoded -->\n\nThis stipulates that if an outcome from ğ‘ğ‘ğ‘¡â„ inherently contradicts the post-constraint, that path can be disregarded in the analysis.\n\nCorrectness. The validity of these optimization methods can be proved by contradiction. Consider an instance where one of these paths is executed. If this path conflicts with the C post, it would render ğ‘¢ ( ğ‘£ ) unreachable. Thus, it becomes evident that such paths can be pruned without sacrificing the correctness of the analysis.\n\nWe provide a concrete example of how we perform these optimizations in Â§4.3.3.\n\n## 3.3 Conceptual Workflow\n\nGiven a bug report containing a suspicious variable ğ‘£ and its residing function ğ¹ , the workflow Î¦ is as follows:\n\n- (1) Î¦ 1 ( ğ¹, ğ‘£ ) â†’ { ğ‘– ( ğ‘£ )} : Identify potential initializers for ğ‘£ from the bug report.\n- (3) Î¦ 3 ( ğ¹, { ğ‘– ( ğ‘£ ) , C ğ‘ğ‘œğ‘ ğ‘¡ }) â†’ InitStatus ( ğ‘£ ) : Summarize the initialization status for variable ğ‘£ after all possible initializers completion (merge multiple initializers).\n- (2) Î¦ 2 ( ğ¹, ğ‘– ( ğ‘£ )) â†’ C ğ‘ğ‘œğ‘ ğ‘¡ : Extract the C ğ‘ğ‘œğ‘ ğ‘¡ from the bug report for each ğ‘– ( ğ‘£ ) .\n\nDecision Policy. The decision policy Î” is defined as:\n\nÎ” ( InitStatus ( ğ‘£ ) = must\\_init ) : non-bug Î” ( InitStatus ( ğ‘£ ) â‰  must\\_init ) : potential bug\n\nConceptually, LLift will not miss more bugs. The post-constraint guided path optimizations and decision policies are safe.\n\nIn this policy, we adopt a conservative approach by treating all variables not explicitly marked as must\\_init as potential vulnerabilities. And it is worth noting that this policy may introduce some false positives. For example, it might over-approximate preconditions.\n\n## 3.4 Turns and Conversations in LLMs\n\nWe define two key concepts in interacting with LLMs: turn and conversation .\n\n- Conversation: Leveraging the capabilities of LLMs often necessitates a series of interactions, especially for complex problem-solving. A conversation is an ordered sequence of turns. A conversation comprising ğ‘› turns can be expressed as [( ğ‘ 1 , ğ‘Ÿ 1 ) , ( ğ‘ 2 , ğ‘Ÿ 2 ) , . . . , ( ğ‘ ğ‘› , ğ‘Ÿ ğ‘› )] .\n- Turn: A turn encapsulates a singular interaction with the LLM. Formally, it's defined as a tuple, ( ğ‘, ğ‘Ÿ ) , where ğ‘ represents the problem or question, and ğ‘Ÿ denotes the LLM's response.\n\n## 4 DESIGN\n\nIn Section Â§3.3, we introduced a conceptual workflow. Elaborating on that foundation, Figure 4 showcases a compelling illustration of our methodological approach. Yet, translating this workflow into\n\nFigure 4: Example run of LLift. For each potential bug, LLift â‘  ( Î¦ 1 ) identifies its initializer, â‘¡ ( Î¦ 2 ) extracts the post-constraints of the initializer, and â‘¢ ( Î¦ 3 ) analyzes the behavior of the initializer with the post-constraints via LLM.\n\n<!-- image -->\n\npractice presents its challenges. Even with the advanced knowledge and analytical capabilities of cutting-edge LLMs, achieving optimal results remains a challenge. Throughout the development of LLift, we identified several obstacles and subsequently introduced four distinct design components to effectively address these challenges.\n\n## 4.1 Design Challenges\n\nIt is non-trivial to prompt LLMs effectively [28, 41]. We meet the following challenges and propose solutions correspondingly in designing LLift.\n\n- C2. Token Limitations. It is known that LLMs have token limitations. For example, GPT-3.5 supports 16k tokens and GPT-4 supports 32k tokens [20]. This means that we do not want to copy a large number of function bodies in our prompts to LLMs.\n- C1. Limited Understanding of Post-constraint. Despite LLMs ( e.g., GPT-4) are able to comprehend the definition of post-constraint and apply them in simple scenarios, we found their capacity to utilize this knowledge in actual program analysis-such as summarizing function behavior in line with specific post-constraint -to be limited. This critical limitation often results in unpredictable and inconsistent outcomes.\n- C3. Unreliable and Inconsistent Response. LLMs are known to result in unreliable and inconsistent responses due to hallucination and stochasticity [41]. Stochasticity refers to the inherent unpredictability in the model's outputs [32]; and the hallucination refers to LLMs generating nonsensical or unfaithful responses [11, 42]. By design, the stochasticity can be mitigated with lower temperature , a hyperparameter controlling the degree of randomness in outputs [27]; however, reducing temperature may impair the model's exploring ability [37] and therefore may miss corner cases that result in vulnerabilities.\n\n## 4.2 Design Overview\n\nWe will discuss our design strategies to address the above challenges in the rest of the section. Before that, we provide a high-level overview of our solution.\n\n- To tackle challenge C1 (Post-constraint), we propose to encode (D#1) Post-Constraint Guided Path Analysis by teaching LLMs with examples, or few-shot in-context learning , of postconstraints. This approach enables LLMs to learn from a small number of demonstrative examples, assimilate the underlying patterns, and apply this understanding to process post-constraint guidance in our analysis.\n\nFigure 5: The workflow of LLift. Given a potential bug, we let LLM first identify the initializer and then extract its post-constraints (Convo.1), then leverage them to summarize the behavior of the initializer (Convo.2). A conversation consists of prompts (boxes) and responses (edges).\n\n<!-- image -->\n\n- To tackle challenge C2 (Token Limitation), We employ two strategies: (D#2) Progressive Prompt . Instead of copying a large number of function bodies ( i.e., subroutines), we only provide function details on demand, i.e., when LLMs are not able to conduct a result immediately. (D#3) Task Decomposition. We break down the problem into sub-problems that can be solved in independent conversations, i.e., a sequence of prompt and response pairs .\n\nWe elaborate the design of (D#1 - #4) Post Constraint Guided Path Analysis , Progressive Prompts , Task Decomposition , and Self-Validation detailed in the rest of this section. The effectiveness and efficiency of these design strategies are rigorously evaluated in Â§6.4, revealing a substantial enhancement in bug detection within the Linux kernel.\n\n- To tackle challenge C3 (Unreliable Response), we employ the following strategies: (D#4) Self-Validation. We ask LLMs to review and correct their previous responses. This helps improve the consistency and accuracy based on our observation. Besides, (D#2) Progressive Prompt and (D#3) Task Decomposition also help to deal with this challenge. Additionally, we implement majority voting by running each case multiple times and use majority voting to combat stochasticity.\n\n## 4.3 Design #1: Post-Constraint Guided Path Analysis\n\nThe Linux kernel frequently employs return value checks as illustrated in Table 2. Through our detailed examination of non-bug instances, we found that a path-sensitivity analysis can effectively eliminate over 70% of these negative cases. However, path-sensitive static analysis usually suffers from path explosion, especially in large-scale codebases like the Linux kernel.\n\nFortunately, we can prompt the LLM to collect C ğ‘ğ‘œğ‘ ğ‘¡ and summarize the function with respective to the C ğ‘ğ‘œğ‘ ğ‘¡ . It is worth noting\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\n```\nCheck Before Use Failure Check Type A: Type B: if (sscanf(...) >= 4) { use(a, b, c, d); } err = func(&a); if (err) { return / break / goto ; } use(a) Type A': Type B': switch (ret=func(&a)){ case some_irrelevant_case: do_something(...); break ; case critical_case: use(a); } while (func(&a)){ do_something(...); } use(a);\n```\n\nTable 2: Two types of post-constraints and their variants. Table 2: Two types of post-constraints and their variants.\n\nerror conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is /u1D452 /u1D45F /u1D45F â†¦â†’ 0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output /u1D45F /u1D452 /u1D461 â†¦â†’ 0, which indicates its first successful execution to break the endless loop. that current LLMs ( e.g., GPT-4) are not natively sensitive to the sensitivity; without any additional instructions, LLMs usually overlook the post-constraints. Therefore, we teach the LLM to be sensitive to post-constraints rules through few-shots in-context learning. We describe the design details as follows:\n\n- { \"ret\" : \"success\", \"response\" : { \"must\\_init\" : [\"a\", \"b\", \"c\", \"d\"], \"may\\_init\" : [{ \"name\" :\"n\", \"condition\" : \"ret &gt; 4\"}] } } Â· Check Before Use. Type A is our motivating example; by looking at its check, the post-constraint should be ğ‘Ÿğ‘’ğ‘¡ â‰¥ 4. Type A' describes a similar case with switch-cases , with expected output ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ crticial\\_case .\n- 4.3.2 Function Behavior Summarization. Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 . For example, we provide the following: { \"initializer\" : \"ret = sscanf(str,'%u.%u.%u.%u%n',&amp;a,&amp;b,&amp;c,&amp;d,&amp;n)\", \"suspicious\" : [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\" : \"ret &gt;= 4\" } The LLM may respond with 4.3.1 Post-Constraints Extraction. To extract the qualified postcondition , we first determine the post-constraints that lead to the use of suspicious variables. We incorporate few-shot in-context learning to teach LLMs how to extract such constraints from the caller context. Table 2 demonstrates how we teach LLM with in-context learning. We focus primarily on two types of code patterns:\n- The response succinctly encapsulates the postcondition, where variables a,b,c,d are classified as must\\_init , and n is categorized as may\\_init . This is due to the initialization of n only occurring when /u1D45F /u1D452 /u1D461 &gt; 4, and not when /u1D45F /u1D452 /u1D461 â†¦â†’ 4. Note that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution. Consider the sscanf() example, even if the analysis is aware that the qualified postcondition should be limited to those where 4, it would still Â· Failure Check. This pattern captures the opposite of the first pattern. They commonly occur in the Linux kernel where the error conditions cause the use to become unreachable, as illustrated in Type B, the post-constraint is ğ‘’ğ‘Ÿğ‘Ÿ â†¦â†’ 0. Type B' depicts a variant where the initializer keeps retrying til success, and therefore with expected output ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0, which indicates its first successful execution to break the endless loop.\n- /u1D45F /u1D452 /u1D461 â‰¥ need to enumerate the paths inside of sscanf() , which involves loops and can easily lead to timeouts as explained in Â§2.1. 4.3.2 Function Behavior Summarization. Once we obtain the postcontraints in Convo.1, we feed them to the LLM to obtain the behavior summary in Convo.2 . For example, we provide the following:\n\n```\n4.4 Design #2: Progressive Prompt The Linux kernel has an extremely large codebase. Summarizing an initializer using LLMs without providing any supplementary function definitions can result in incomplete or erroneous responses. { \"initializer\": \"ret = sscanf(str,'%u.%u.%u.%u%n',&a,&b,&c,&d,&n)\", \"suspicious\": [\"a\", \"b\", \"c\", \"d\"], \"postconstraint\": \"ret >= 4\" }\n```\n\nOn the other hand, flooding the LLM with every relevant function The LLM may respond with\n\n```\ndefinition upfront risks exceeding their context window limitations. To address this dilemma, we choose to progressively provide function definitions as needed. Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt , fosters a dynamic interaction with the LLM rather than expecting a response in one shot. Throughout this iterative exchange, we consistently prompt the LLM: { \"ret\": \"success\", \"response\": { \"must_init\": [\"a\", \"b\", \"c\", \"d\"], \"may_init\": [{\"name\":\"n\", \"condition\": \"ret > 4\"}] } }\n```\n\n'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' . Should the LLM need more information, LLift will promptly extract the relevant The response succinctly encapsulates the function behavior, where variables a,b,c,d are classified as must\\_init , and n is categorized as may\\_init . This is due to the initialization of n only occurring when ğ‘Ÿğ‘’ğ‘¡ &gt; 4, and not when ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 4.\n\n5\n\ndetails o automati\n\nSpecifi response.\n\na specific\n\n[{\n\n\"t\n\nSubse ing code\n\neach req code. Th\n\nsupport t function\n\nThe it details. I\n\nquests ad informat\n\nthe LLM\n\nprompt t data and\n\nanalysis\n\n4.5\n\nD\n\nWe syste primaril\n\nvital ele\n\nMultista versatio\n\nploy a tw and resp\n\ntasks 1 a extractin\n\non sum and effec\n\nidentified three sub\n\ndecompo\n\nThinkin serve tha\n\noutput, s prompte\n\nprompts mentally\n\nThis emp\n\nConsequ natural l\n\nprocesses their res\n\n4.6\n\nD\n\nAt times, construct\n\niors, part dition\n\nmu may\n\nto be\n\n/u1D45F\n\n/u1D452 /u1D461\n\nâ†¦â†’\n\n0.\n\n```\n1 int func(int* a){ 2 if(some_condi) 3 return -1; 4 *a = ... // init 5 return 0; 6 }\n```\n\nFigure 6: A sample case of initializer func , *a is may\\_init or must\\_init under different post-constraints.\n\n```\nmust_init = âˆ… if: C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ or âˆ€ ğ‘ğ‘  âˆˆ {Â¬ some_condi } : ğ‘ğ‘  âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ âˆ§ âˆ€ ğ‘œ âˆˆ { ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0 } : ğ‘œ âŠ¥ C ğ‘ğ‘œğ‘ ğ‘¡ must_init = { ğ‘ } if: (Â¬ some_condi ) âˆ§ C ğ‘ğ‘œğ‘ ğ‘¡ or ğ‘Ÿğ‘’ğ‘¡ 0 ğ‘ğ‘œğ‘ ğ‘¡\n```\n\n```\n( â†¦â†’ ) âˆ§ C\n```\n\nNote that this seemingly simple interaction with LLMs can be challenging for static analysis or symbolic execution. Consider the sscanf() example, even if the analysis is aware that the qualified postcondition should be limited to those where ğ‘Ÿğ‘’ğ‘¡ â‰¥ 4, it would still need to enumerate the paths inside of sscanf() , which involves loops and can easily lead to timeouts as explained in Â§2.1.\n\n- 4.3.3 Apply Path Analysis. Following Â§3.2, Figure 6 presents a concert example of post-constraint guided path analysis. This case shows a simple initializer ğ‘– ( ğ‘ ) of the variable ğ‘ . Given an early return, the initialization in line 4 may not be executed. As such, the qualified postconditions become contingent on the post-constraints C ğ‘ğ‘œğ‘ ğ‘¡ . There are:\n- If the use of variable ğ‘ is conditional with constraints, i.e., C ğ‘ğ‘œğ‘ ğ‘¡ â‰  âŠ¤ , two cases emerge:\n- If the use of variable a is unconditional, i.e., C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ . In this case, the variable ğ‘ is labeled as may\\_init given that the initialization may not be reached. In general, if all path constraints and outcomes of must\\_init are disjoint from C ğ‘ğ‘œğ‘ ğ‘¡ , no path can be pruned out. We could also conclude ğ‘ as may\\_init .\n- (1) C ğ‘ğ‘œğ‘ ğ‘¡ clashes with the constraints of the path (e.g., some\\_condi ), or\n- (2) C ğ‘ğ‘œğ‘ ğ‘¡ conflicts with the path outcome (e.g., return -1 ). In these instances, C ğ‘ğ‘œğ‘ ğ‘¡ could be some\\_condi or func(...)==0 and we can designate *a as must\\_init .\n\n## 4.4 Design #2: Progressive Prompt\n\nThe Linux kernel has an extremely large codebase. Summarizing an initializer using LLMs without providing any supplementary function definitions can result in incomplete or erroneous responses. On the other hand, flooding the LLM with every relevant function definition upfront risks exceeding their context window limitations.\n\nSpecifically, We teach the LLM to ask for more information with a specific format:\n\nTo address this dilemma, we choose to progressively provide function definitions as needed. Illustrated in Figure 5, this approach, which we refer to as Progressive Prompt , fosters a dynamic interaction with the LLM rather than expecting a response in one shot. Throughout this iterative exchange, we consistently prompt the LLM: 'If you encounter uncertainty due to a lack of function definitions, please signal your need, and I'll supply them' . Should the LLM need more information, LLift will promptly extract the relevant details on demand from the source code and provide it to the LLM automatically , enabling it to reassess and generate a more accurate response.\n\n```\n[{\"type\":\"function_def\", \"name\":\"some_func\" }]\n```\n\nSubsequently, LLift scans this format in the LLM's response. For each requested function definition, LLift supplies its corresponding code along with comments extracted from the Linux source code. Though GPT-4 may seek other types of information beyond function definitions ( e.g., struct definitions), we currently limit our support to requests pertaining to function definitions.\n\nThe iterative process continues until either the LLM no longer requests additional information, or LLift cannot supply the requested details. In certain situations where LLift is unable to provide more information ( e.g., the definition of an indirect call), LLift will still prompt the LLM to proceed with the analysis. In these instances, the LLM is encouraged to infer the behavior based on the available data and its inherent knowledge, thereby facilitating continued analysis even when not all information is directly accessible.\n\n## 4.5 Design #3: Task Decomposition\n\nWe systematically apply the principle of task decomposition, a vital element of our design process. This concept is incorporated primarily in two distinct ways.\n\nMultistage Problem Solving. As illustrated in Figure 5, we employ a two-conversation approach to complete the task. Each conversation, essentially consists of multiple iterations of prompts and responses. The first conversation (Convo.1) is dedicated to extracting the initializer and its associated post-constraints (subtasks 1 and 2), while the second conversation (Convo.2) focuses on summarizing the function (subtask 3) based on the previously identified post-constraints. This division allows a more manageable and effective way of achieving the task, compared to combining all three subtasks into a single conversation. The efficacy of this task decomposition approach is further evaluated in Â§6.5.\n\nThinking in English. Our workflow necessitates a structured output, such as a JSON format, for automation. However, we observe that LLMs often produce suboptimal results when directly prompted to output in this format. As LLMs build responses incrementally, word-by-word, based on preceding outputs [32], direct prompts to output JSON may interrupt their thought progression. This emphasizes the importance of initially soliciting responses in natural language to ensure comprehensive and effective reasoning. Consequently, we instruct the LLM to first articulate their thought processes in English, followed by a subsequent prompt to transform their response into a JSON summary.\n\n## 4.6 Design #4: Self-Validation\n\nAt times, LLMs can display unpredictable or inconsistent behaviors, particularly in complex scenarios involving detailed logical constructs. Consider a case where an initializer carries the postcondition must\\_init if ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0. LLMs may still mistakenly assume it to be may\\_init , despite the explicit presence of the post-constraint ğ‘Ÿğ‘’ğ‘¡ â†¦â†’ 0.\n\nIn addition to task decomposition, we also introduce the concept of self-validation to enhance reliability. Before the LLM reaches its\n\nConversely, an LLM might erroneously interpret a non-existent post-constraint and incorrectly infer a may\\_init case as must\\_init . This phenomenon is known as hallucination . Essentially, the hallucination can lead to both false positives and false negatives in bug detection, thereby affecting accuracy and reliability.\n\nfinal conclusion, this method reinforces specific rules, allowing the LLM to reassess their previous responses for adherence and make necessary corrections. We observed that this practice yields better results. We evaluate the effect of self-validation in Â§6.4.\n\nAs seen in Figure 5, we employ self-validation in both conversations. By prompting a list of correct properties that we expect, LLMs can verify and correct their results by themselves automatically.\n\n## 4.7 Additional Prompting Strategies\n\nIn order to further optimize the efficacy of our model, we have incorporated several additional strategies into our prompt design:\n\n- Source Code Analysis. Rather than analyzing abstract representations, we opt to focus our attention directly on the functions within the source code. This approach not only economizes on token use compared to LLVM IR, but also allows the model to leverage the semantic richness of variable names and other programming constructs to conduct a more nuanced analysis.\n- Chain-of-Thought. Leveraging the Chain-of-Thought (CoT) approach, we encourage the LLMs to engage in stepwise reasoning, using the phrase 'think step by step' . This not only helps generate longer, comprehensive responses, but it also provides intermediate results at each juncture of the thought process. Previous studies suggest the CoT approach considerably enhances the LLMs' reasoning capabilities [3]. We incorporate the CoT strategy into every prompt.\n\nThere are still some interesting details in designing an effective prompt but due to space constraints and without changing the overall strategy, we will not list them all. Readers intrigued can delve into the intricacies of our open-sourced prompt 1 design and experimental implementations to gain a deeper understanding.\n\n## 5 IMPLEMENTATION\n\nWe implement the prototype of LLift based on OpenAI's API [19] ( i.e., gpt-4-0613). We describe some implementation details in the following aspects:\n\nInteraction with LLMs. LLift's interaction with LLMs is managed by a simple agent developed in Python, containing roughly 1,000 lines of code. In addition, it uses seven prompts, which altogether constitute about 2,000 tokens in two conversations. All interactions are fully automated via APIs of OpenAI. Besides sending prompts and waiting for responses, our agent also 1) interacts with LLMs according to the progressive prompt design, 2) locates function definitions within the Linux source code, and 3) processes responses from LLMs, then receives and stores to a database.\n\nHyper-Parameters. There are several hyper-parameters in calling the APIs provided by OpenAI. We choose max\\_token and temperature to 1,024 and 1.0, respectively. max\\_token controls the output length; since LLMs always predict the next words by the previous output, the longer output can benefit and allow its reasoning. However, too many tokens will exhaust the context window quickly, so we pick 1024 as a reasonable balance.\n\nThe temperature controls the randomness and also the ability to reason. Intuitively, we want the analysis to be as non-random as possible and reduce the temperature (it can take a value between 0\n\n1 https://sites.google.com/view/llift-open/prompt\n\nand 2 for GPT models); however, an overly low temperature can result in repetitive or overly simplistic responses. We set it to 1.0 (also the default of gpt-4-0613), which allows for higher-quality responses, and use strategies such as self-validation and majority voting to improve the consistency of responses.\n\n## 6 EVALUATION\n\nOur evaluation aims to address the following research questions.\n\n- RQ2 (Recall): Is there a possibility for LLift to miss real bugs?\n- RQ1 (Precision): How accurately is LLift able to identify bugs?\n- RQ3 (Comparison): How does the performance of individual components within LLift compare to that of the final design?\n- RQ4 (Model Versatility): How does LLift perform when applied to LLMs other than GPT-4?\n\nWe evaluate RQ1 to RQ3 in GPT-4, under API from OpenAI with version gpt4-0613. For RQ4, we also test GPT-3.5 with version gpt-3.5-turbo-0613 and Claude 2 additionally for comparison.\n\n## 6.1 Dataset\n\nOur experiment data, sourced from UBITect, includes all potential bugs labeled by its static analysis stage but experienced timeout or memory exhaustion during its symbolic execution stage. Overall, UBITect's static analysis stage produced 140,000 potential bugs, with symbolic execution able to process only 60%, leaving 53,000 cases unattended, which means that these cases are generally difficult for static analysis or symbolic execution to decide We craft the following dataset from 53,000 cases to evaluate LLift:\n\n- (2) Bug-50. This dataset comprises the 52 confirmed UBI bugs previously identified by UBITect. It is used as ground truth for assessing recall by verifying if any true bugs were overlooked.\n- (1) Random-1000. We randomly chose 1,000 from the 53,000 cases for testing. However, there are 182 cases where there are no initializers, which are automatically recognized and filtered (see Â§3). The remaining 818 cases are used in evaluating precision, i.e., the ratio of true positives to false positives.\n- (3) Cmp-40. This dataset comprises 27 negative and 13 positive cases selected from the Random-1000. We utilize this dataset to illustrate which of our design strategies contributed most to the outcome of our solution.\n\nTurns and Conversations. Due to the progressive prompt, each case may require different turns (pairs of a prompt and a response). In Random-1000, the average number of turns is 2.78, with a max of 8 and a variance of 1.20.\n\nCost. On average, it costs 7,000 tokens in GPT-4 to analyze each potential bug.\n\n## 6.2 RQ1: Precision\n\nLLift reports 26 positives among the Random-1000 dataset, where half of them are true bugs based on our manual inspection. This represents a precision of 50%. In keeping with UBITect and we focus on the analysis of Linux v4.14, 12 of the bugs still exist in the latest Linux kernel. We are in the process of reporting the 12 bugs to the Linux community. So far, we have submitted patches for 4 bugs and received confirmation that they are true bugs.\n\nTable 3: True bugs identified by LLift from Random-1000, analyzing in Linux v4.14\n\n| Initializer                                                                                                                                                                                             | Caller                                                                                                                                                                                                                                                                | File Path                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Variable                                                                                                  | Line                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|--------------------------------------------------------|\n| read_reg regmap_read ep0_read_setup regmap_read bcm3510_do_hab_cmd readCapabilityRid e1e_rphy pci_read_config_dword lan78xx_read_reg t1_tpi_read pci_read_config_dword ata_timing_compute pt_completion | get_signal_parameters isc_update_profile ep0_handle_setup mdio_sc_cfg_reg_write bcm3510_check_firmware_version airo_get_range __e1000_resume adm8211_probe lan78xx_write_raw_otp my3126_phy_reset quirk_intel_purley_xeon_ras_cap opti82c46x_set_piomode pt_req_sense | drivers/media/dvb-frontends/stv0910.c drivers/media/platform/atmel/atmel-isc.c drivers/usb/mtu3/mtu3_gadget_ep0.c drivers/net/ethernet/hisilicon/hns_mdio.c drivers/media/dvb-frontends/bcm3510.c drivers/net/wireless/cisco/airo.c drivers/net/ethernet/intel/e1000e/netdev.c drivers/net/wireless/admtek/adm8211.c drivers/net/usb/lan78xx.c drivers/net/ethernet/chelsio/cxgb/my3126.c arch/x86/kernel/quirks.c drivers/ata/pata_legacy.c drivers/block/paride/pt.c | tmp sr setup.bRequestType reg_value ver.demod_version cap_rid.softCap phy_data reg buf val capid0 &tp buf | 504 664 637 169 666 6936 6580 1814 873 193 562 564 368 |\n\nImprecise and Failed Cases. Despite the effectiveness of LLift, there are instances where it does not yield precise results, resulting in 13 false positives by mistakenly classifying must\\_init cases as may\\_init . Upon a careful examination of these cases, we attribute the imprecision to a variety of factors, which we discuss in detail in Â§6.7. Briefly, we give a breakdown of them here: Incomplete constraint extraction (4 cases), Information gaps in UBITect (5 cases), Variable reuse (1 case), Indirect call (1 case), and Additional constraints (1 case). Additionally, there is one false positive caused by inconsistent output (i.e., two false positives in three runs). Four cases exceed the maximum context length while exploring deeper functions in the progressive prompt.\n\nTakeaway 1. LLift Can effectively summarize initializer behavior and discover new bugs with high precision (50%).\n\n## 6.3 RQ2: Recall Estimate\n\nConceptually, the core optimization (post-constraint guided path analysis) of LLift is sound, and we also prompt a series of rules to let LLMs tend to respond ' may\\_init when uncertain. We expect LLift would not reject true bugs or with a high recall.\n\nFurther, we test LLift on the Bug-50 dataset to see whether it will miss any bugs discovered by UBITect . LLift has demonstrated full effectiveness in identifying all real bugs from Bug-50. This result, while encouraging, does not imply that LLift is flawless. Detailed data analysis reveals that: 1) There remain some inconsistencies in 3 âˆ¼ 5 cases occasionally, though they are mitigated by majority voting; and 2) all the bugs found by UBITect have trivial postconstraints (C ğ‘ğ‘œğ‘ ğ‘¡ = âŠ¤ ) and postcondition of may\\_init ( P ğ‘ğ‘¢ğ‘ğ‘™ : must\\_init â†¦â†’ âˆ… ). Hence, LLift could identify them easily. It is noteworthy that these cases are already those cases detectable by UBITect. Such cases tend to be simpler in nature and can be verified by symbolic execution in UBITect.\n\nWe sample 300 negative cases from Random-1000 in an effort to see whether we will miss any true bugs. We confirm that all are true negatives. Despite the limited data sampled, this result indicates that integrating GPT-4 into our implementation does not introduce apparent unsoundness.\n\nTable 4: Performance evaluation of bug detection tool with progressive addition of design components: Post-Constraint Guided Path Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV), and Task Decomposition (TD). (C) indicates the number of C onsistent cases.\n\n| Combination   | TN(C)   | TP(C)   | Precision   | Recall   | Accuracy   | F1 Score   |\n|---------------|---------|---------|-------------|----------|------------|------------|\n| Simple Prompt | 12(9)   | 2(1)    | 0.12        | 0.15     | 0.35       | 0.13       |\n| PCA           | 13(9)   | 5(1)    | 0.26        | 0.38     | 0.45       | 0.31       |\n| PCA+PP        | 5(3)    | 6(1)    | 0.21        | 0.46     | 0.28       | 0.29       |\n| PCA+PP+SV     | 5(2)    | 11(8)   | 0.33        | 0.85     | 0.40       | 0.48       |\n| PCA+PP+TD     | 22(14)  | 6(4)    | 0.55        | 0.46     | 0.70       | 0.50       |\n| PCA+PP+SV+TD  | 25(17)  | 13(12)  | 0.87        | 1.00     | 0.95       | 0.93       |\n| Oracle        | 27(27)  | 13(13)  | -           | -        | -          | -          |\n\nTakeaway 2. LLift has proven effective in identifying UBI bugs, consistently detecting all known instances.\n\n## 6.4 RQ3: Contributions of Design Strategies\n\nIn our effort to delineate the contributions of distinct design strategies to the final results, we undertook an evaluative exercise against the Cmp-40 dataset, employing varying configurations of our solution, each entailing a unique combination of our proposed strategies. As illustrated in Table 4, the strategies under consideration encompass Post-constraint Analysis ( PCA ), Progressive Prompt ( PP ), Self-Validation ( SV ), and Task Decomposition ( TD ). The findings underscore an overall trend of enhanced performance with the integration of additional design strategies.\n\nIncorporating PCA offers a notable enhancement, enabling the LLM to uncover a wider array of vulnerabilities. As shown in Table 4, there is a substantial improvement in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization.\n\nIn this study, the Baseline corresponds to a straightforward prompt, \"check this code to determine if there are any UBI bugs\" , a strategy that has been found to be rather insufficient for discovering new vulnerabilities, as corroborated by past studies [17, 21, 31], reflecting a modest recall rate of 0.15 and a precision of 0.12.\n\nThe influence of Progressive Prompt ( PP ) on the results is quite intriguing. While its impact appears to lower precision initially, the introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance. Without\n\nTable 5: Comparison of different LLMs on real bugs, from a subset of Bug-50 Table 5: Comparison of different LLMs on real bugs, from a subset of Bug-50\n\n| Caller Caller                                                                                                                                                                                                                                                                                                                             | GPT 4 3.5 GPT 4 3.5                                                     | Claude2 Claude2                     | Bard Bard                           |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------|-------------------------------------|\n| hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf | âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ | âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— | âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ |\n\nPP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis. Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data. By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the post-constraints, subsequently leading to errors. in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization. The influence of Progressive Prompt ( PP ) on the results is quite intriguing. While its impact appears to lower precision initially, the\n\nFinally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4. However, this constraint was not breached in any case when TD was incorporated. ( TD ) and Self-Validation ( SV ) also play a crucial role in enhancing consistency . In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs. A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results,\n\nBeyond influencing precision and recall, Task Decomposition ( TD ) and Self-Validation ( SV ) also play a crucial role in enhancing consistency . In this context, a result is deemed consistent if the LLM yields the same outcome across its initial two runs. A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results, and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring reliable and consistent outcomes. introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance. Without PP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis. Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data. By checking the detailed conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the postcondition, subsequently leading to errors. Beyond influencing precision and recall, Task Decomposition\n\nTakeaway 3. All of LLift's design strategies contributed to the positive results. and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring\n\n## 6.5 RQ4: Alternative Models Finally, TD also holds significance in terms of conserving tokens. In our evaluation phase, we identified two instances within the\n\nreliable and consistent outcomes.\n\nTable 5 provides a comprehensive view of the performance of our solution, LLift, when implemented across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 [2], and Bard [12]. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively. Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches. PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4. However, this constraint was not breached in any case when TD was incorporated. Takeaway. All of LLift's design strategies contributed to the positive results.\n\nIt is imperative to note that not all design strategies in our toolbox are universally applicable across all language models. Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies. Bard's interaction patterns suggest a preference for immediate response generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach. Similarly, when task 6.6 RQ4: Alternative Models Table 5 provides a comprehensive view of the performance of our solution, LLift, when implemented across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 [2], and Bard [12]. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89%, 67%, and 67%, respectively. Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches.\n\nHaonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian\n\n```\n1 static int sgl_map_user_pages(...){ 2 ... 3 if ((pages = kmalloc(..., GFP_KERNEL)) == NULL) 4 return -ENOMEM; 5 res = get_user_pages_unlocked(..., pages, ...); 6 /* Errors and no page mapped should return here */ 7 if (res < nr_pages) 8 goto out_unmap; 9 ... 10 out_unmap: 11 if (res > 0) { 12 for (j=0; j < res; j++) 13 put_page( pages[j] ); 14 res = 0; 15 } 16 kfree(pages); 17 }\n```\n\nFigure 6: Case Study I (Loop and Index). Derived from drivers/scsi/st.c Figure 7: Case Study I (Loop and Index). Derived from drivers/scsi/st.c\n\nIt is imperative to note that not all design strategies in our toolbox are universally applicable across all language models. Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies. Bard's interaction patterns suggest a preference for immediate response decomposition is implemented, these models often misinterpret or inaccurately collect post-constraints, subsequently compromising the results. To harness their maximum potential, we only apply the PCA design specifically ( i.e., without other design strategies) for GPT-3.5 and Bard.\n\ntask decomposition is implemented, these models often misinterpret generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach. Similarly, when Contrasting the GPT series, Bard and Claude 2 demonstrate less familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may\\_init possibility of initializers.\n\nor inaccurately collect post-constraints, subsequently compromising the results. To harness their maximum potential, we apply a Takeaway 4. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results.\n\ni.e., postcondition-aware design specifically (\n\nwithout other design familiarity with the Linux kernel and are more prone to failures due to their unawareness of the may\\_init possibility of initializers. Takeaway. GPT-4 remains at the pinnacle of performance for LLift, yet other LLMs can achieve promising results. In this case study, we pick three interesting cases demonstrating the effectiveness of LLift in analyzing function behaviors and detecting uninitialized variables. All these cases are undecided for the previous static analyzer, UBITect. We put the complete conversations on an anonymous online page for reference 2 .\n\n## strategies) for GPT-3.5 and Bard. Contrasting the GPT series, Bard and Claude 2 demonstrate less 6.6 Case Study\n\n6.7 Case Study In this case study, we pick three interesting cases demonstrating the effectiveness of LLift in analyzing function behaviors and detecting uninitialized variables. All these cases are undecided for the previous static analyzer, UBITect. We put the complete conversations on an anonymous online page for reference 2 . Loop and Index. Figure 6 presents an intriguing case involving the variable pages[j] , which is reported by UBITect as used in Line 17 potentially without being initialized. Unfortunately, Loop and Index. Figure 7 presents an intriguing case involving the variable pages[j] , which is reported by UBITect as used in Line 17 potentially without being initialized. Unfortunately, this case is a false positive which is hard to prune due to loops. Specifically, the initializer function get\\_user\\_pages\\_unlocked() , which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If get\\_user\\_pages\\_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances.\n\n```\nget_user_pages_unlocked() is successfully executed, pages[0] through pages[res-1] pointers will be initialized to point to struct page instances. To summarize the behavior, i.e., must_init facts under conditions where the use is reachable, we must first extract the postthat lead to the use of . Through interacting with { \"initializer\": \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", \"suspicious\": [\"pages[j]\"], \"postconstraint\": \"res < nr_pages && res > 0 && j < res\", }\n```\n\nthis case is a false positive which is hard to prune due to loops. Specifically, the initializer function get\\_user\\_pages\\_unlocked() , which is responsible for mapping user space pages into the kernel space, initializes the pages array allocated in Line 3. If To summarize the behavior, i.e., must\\_init facts under conditions where the use is reachable, we must first extract the postconstraints that lead to the use of pages . Through interacting with ChatGPT, LLift successfully extracts it:\n\nconstraints pages ChatGPT, LLift successfully extracts it: { After feeding the post-constraints to LLM, LLift then successfully obtains the result:\n\nâ†©\n\n```\n\"initializer\" : \"res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)\", â†’ {\n```\n\n2\n\nhttps://sites.google.com/view/llift-open/case-studies 2 https://sites.google.com/view/llift-open/case-studies\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\nThe Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models\n\n```\n1 static int hv_pci_enter_d0( struct hv_device *hdev){ 2 ... 3 init_completion(&comp_pkt.host_event); 4 pkt->completion_func = hv_pci_generic_compl; 5 pkt->compl_ctxt = &comp_pkt; 6 ... 7 8 wait_for_completion(&comp_pkt.host_event); 9 10 if ( comp_pkt.completion_status < 0) 11 ... 12 } 13 14 static void hv_pci_generic_compl( void *context, ...){ 15 struct hv_pci_compl *comp_pkt = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_status = -1; 21 22 complete(&comp_pkt->host_event); 23 } 1 static int hv_pci_enter_d0( struct hv_device *hdev){ 2 ... 3 init_completion(&comp_pkt.host_event); 4 pkt->completion_func = hv_pci_generic_compl; 5 pkt->compl_ctxt = &comp_pkt; 6 ... 7 8 wait_for_completion(&comp_pkt.host_event); 9 10 if ( comp_pkt.completion_status < 0) 11 ... 12 } 13 14 static void hv_pci_generic_compl( void *context, ...){ 15 struct hv_pci_compl *comp_pkt = context; 16 17 if (resp_packet_size >= offsetofend(...)) 18 comp_pkt->completion_status = resp->status; 19 else 20 comp_pkt->completion_status = -1; 21 22 complete(&comp_pkt->host_event); 23 }\n```\n\nFigure 7: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c Figure 8: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c Figure 7: Case Study II (Concurrency and Indirect Call). Derived from drivers/pci/host/pci-hyperv.c\n\n\"suspicious\"\n\n\"suspicious\"\n\n: [\"pages[j]\"],\n\n: [\"pages[j]\"],\n\n: \"res &lt; nr\\_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res\",\n\n\"postconstraint\"\n\n\"postconstraint\"\n\n: \"res &lt; nr\\_pages &amp;&amp; res &gt; 0 &amp;&amp; j &lt; res\",\n\n\"must\\_init\"\n\n```\n} After feeding the post-constraints to LLM, LLift then successfully obtains the result: { \"ret\" : \"success\", \"response\" : { \"ret\": \"success\", \"response\": { \"must_init\": [\"pages[j]\"], \"may_init\": [], } } } After feeding the post-constraints to LLM, LLift then successfully obtains the result: { \"ret\" : \"success\", \"response\" : {\n```\n\n\"may\\_init\" : [], } } As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. \"may\\_init\" : [], } } As we can see, GPT-4 exhibits impressive comprehension of this complex function. It perceives the variable pages[j] being used in a loop that iterates from 0 to res-1 . This insight leads GPT4 to correctly deduce that all elements in the pages array must\n\n\"must\\_init\"\n\n: [\"pages[j]\"],\n\n: [\"pages[j]\"], be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. Concurrency and Callback. Consider the case illustrated in Figure 7. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity, Concurrency and Callback. Consider the case illustrated in Figure 8. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. be initialized, i.e., they are must\\_init . This example underscores GPT-4's proficiency in handling loop and even index sensitivity. Concurrency and Callback. Consider the case illustrated in Figure 7. At first glance, UBITect flags Line 10 for potentially using the variable comp\\_pkt.completion\\_status before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug. However, the mystery unravels when we examine hv\\_pci\\_generic\\_compl() , the actual initializer function assigned to pkt in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here wait\\_for\\_completion() is a synchronization primitive that pauses the current thread and waits for the new thread ( i.e., hv\\_pci\\_generic\\_compl() ) to complete. Despite this complexity,\n\nthe function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, the notion that 'the LLM simply learns everything from the internet and acts Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, some argue that 'the LLM simply learns everything from the internet and acts merely as a search engine' [6]. This viewpoint is challenged by the case illustrated in Figure 9. the function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . Unfamiliar Function. As previously delineated in Â§2.3, LLMs possess the inherent ability to recognize the semantics ( e.g., postconditions) of common functions like sscanf() . However, the notion that 'the LLM simply learns everything from the internet and acts\n\nGPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify the function hv\\_pci\\_generic\\_compl() as the initializer of comp\\_pkt.completion\\_status . GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result. It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. LLift prompts LLMs with 'The 'initializer' must be the 'actual' function that initializes the variable. ' and then LLMs can automatically identify\n\n```\n1 int p9_check_zc_errors(...){ 2 ... 3 err = p9pdu_readf(req->rc, c->proto_version, \"d\", &ecode); 4 err = -ecode ; 5 ... 6 } 7 8 int p9pdu_readf( struct p9_fcall *pdu, int proto_version, const char *fmt, ...) â†© â†’ 9 ... 10 ret = p9pdu_vreadf(pdu, proto_version, fmt, ap); 11 ... 12 return ret; 13 } 14 15 int p9pdu_vreadf( struct p9_fcall *pdu, int proto_version, const char *fmt, va_list ap){ â†© â†’ 16 switch (*fmt) { 17 case 'd':{ 18 int32_t *val = va_arg(ap, int32_t *); 19 if (pdu_read(...)) { 20 errcode = -EFAULT; 21 break ; 22 } 23 val = ...; // initialization 24 } 25 return errcode; 26 } 1 int p9_check_zc_errors(...){ 2 ... 3 err = p9pdu_readf(req->rc, c->proto_version, \"d\", &ecode); 4 err = -ecode ; 5 ... 6 } 7 8 int p9pdu_readf( struct p9_fcall *pdu, int proto_version, const char *fmt, ...) â†© â†’ 9 ... 10 ret = p9pdu_vreadf(pdu, proto_version, fmt, ap); 11 ... 12 return ret; 13 } 14 15 int p9pdu_vreadf( struct p9_fcall *pdu, int proto_version, const char *fmt, va_list ap){ â†© â†’ 16 switch (*fmt) { 17 case 'd':{ 18 int32_t *val = va_arg(ap, int32_t *); 19 if (pdu_read(...)) { 20 errcode = -EFAULT; 21 break ; 22 } 23 val = ...; // initialization 24 } 25 return errcode; 26 }\n```\n\nFigure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 8: Case Study III (Unfamiliar Function), derived from net/9p Figure 9: Case Study III (Unfamiliar Function), derived from net/9p\n\nmerely as a blurry search engine' [6], is misguided according to our experiments. This viewpoint is robustly contradicted by the case illustrated in Figure 8. The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide merely as a blurry search engine' [6], is misguided according to our experiments. This viewpoint is robustly contradicted by the case illustrated in Figure 8. The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide The case presents an intriguing real-world bug. The function p9pdu\\_readf() mirrors sscanf() in structure, yet lacks a check of its return value, leaving the parameter ecode at risk of being uninitialized, i.e., if pdu\\_read() returns non-zero in line 19 (thus 'break' early). Notably, unlike sscanf() , where GPT-4 can provide a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() .\n\n```\necode be initialized when p9pdu_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follow: { \"initializer\" : \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\" : [\"ecode\"], \"postconstraint\" : null , \"response\" : { \"must_init\" : [], \"may_init\" : [{ \"name\" : \"ecode\", \"condition\" : \"p9pdu_readf returns 0\" }] } } ecode be initialized when p9pdu_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follow: { \"initializer\" : \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\" : [\"ecode\"], \"postconstraint\" : null , \"response\" : { \"must_init\" : [], \"may_init\" : [{ \"name\" : \"ecode\", \"condition\" : \"p9pdu_readf returns 0\" }] } } { \"initializer\": \"err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)\", \"suspicious\": [\"ecode\"], \"postconstraint\": null, \"response\": { \"must_init\": [], \"may_init\": [{ \"name\": \"ecode\", \"condition\": \"p9pdu_readf returns 0\" }] } }\n```\n\na precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() . Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that could a precise summary of the function without asking for its definition, it does request the function definition of p9pdu\\_readf() , as it is not as ubiquitous as sscanf() . Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that could Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that ecode could be initialized when p9pdu\\_readf() returns 0, demonstrating the efficacy of LLift for unfamiliar cases. The result is as follows:\n\n## 6.8 Reason for Imprecision 6.8 Reason for Imprecision 6.7 Reason for Imprecision\n\nChallenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints). Challenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints). Challenges in Constraint Extraction. Beyond the four primary code patterns we addressed in Â§4.3, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis ( e.g., path exploration in symbolic execution to collect the post-constraints).\n\nDespite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis. Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis. Despite LLift achieving a precision of 50% in real-world applications, the precision can still be improved in the future. Some can be solved with better prompts or better integration with static analysis.\n\nInformation Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific Information Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific Information Gaps in UBITect. For instance, UBITect does not provide explicit field names within a structure when a specific\n\nfield is in use. This information gap can result in LLift lacking precision in its analysis. Additionally, UBITect only reports the variable utilized, not necessarily the same variable passed to an initializer. For example, consider an uninitialized variable a passed to an initializer, which is then assigned to variable b for usage. In such a scenario, LLift may fail to identify the initializer due to this incomplete information correctly. These challenges, primarily due to the interface design in UBITect, can be addressed with focused engineering efforts to enrich the output information from UBITect.\n\nVariable Reuse. Varaible reuse is an interesting problem of LLM. In general, LLM usually confuses different variables in different scopes ( e.g., different function calls). For example, if the suspicious variable is ret and passed as a argument to its initializer (say, func(&amp;ret) ) and there is another stack variable defined in func also called ret , LLM will confuse them. Explicitly prompting and teaching LLM to note the difference does not appear to work. One solution is to leverage a simple static analysis to normalize the source code to ensure each variable has a unique name.\n\nIndirect Call. As mentioned Â§4.4, LLift follows a simple but imprecise strategy to handle indirect calls. Theoretically, existing static analysis tools, such as MLTA [16], can give possible targets for indirect calls. However, each indirect call may have multiple possible targets and dramatically increase the token usage. We leave the exploration of such an exhaustive strategy for future work. LLift may benefit from a more precise indirect call resolution.\n\nAdditional Constraints. There are many variables whose values are determined outside of the function we analyze, e.g., preconditions capturing constraints from the outer caller. Since our analysis is fundamentally under-constrained, this can lead LLift to incorrectly determine a must\\_init case to be may\\_init . Mitigating this imprecision relies on further analysis to provide more information.\n\n## 7 DISCUSSION AND FUTURE WORK\n\nPost-Constraint Analysis. Our approach prioritizes postconstraints over other constraints, such as preconditions. By focusing on the post-constraints, we enhance the precision and scalability significantly. Importantly, our utilization of large language models in program analysis suggests strong abilities in summarizing complex function behaviors involving loops, a classic hurdle in program analysis.\n\nBetter Integration with Static Analysis. Our work presents opportunities for greater integration and synergy with static analysis methods. Currently, our proposed solution operates largely independently of the static analysis methods, taking only inputs from static analysis initially. Looking into the future, we can consider integrating static analysis and LLMs in a holistic workflow. For example, this could involve selectively utilizing LLM as an assistant to overcome certain hurdles encountered by static analysis, e.g., difficulty in scaling up the analysis or summarizing loop invariants. In turn, further static analysis based on these findings can provide insights to refine the queries to the LLM. This iterative process could enable a more thorough and accurate analysis of complex cases. We believe such a more integrated approach is a very promising future direction.\n\nDeploying on Open-sourced LLMs. The reproducibility of LLift could be potentially challenged, considering its dependency on GPT-4, a closed-source API subject to frequent updates. At the time of writing, Meta introduced Llama 2, an open-source language model with capabilities rivaling GPT-3.5. Our initial assessments suggest that Llama 2 can understand our instructions and appears well-suited to support LLift. The open-source nature of Llama 2 provides us with opportunities to deploy and refine the model further. We plan to leverage these prospects in future studies.\n\n## 8 RELATED WORK\n\nTechniques of Utilizing LLMs. Wang et al. [33] propose an embodied lifelong learning agent based on LLMs. Pallagani et al. [23] explores the capabilities of LLMs for automated planning. Weng [35] summarizes recent work in building an autonomous agent based on LLMs and proposes two important components for planning: Task Decomposition and Self-reflection , which are similar to the design of LLift. Beyond dividing tasks into small pieces, task decomposition techniques also include some universal strategies such as Chain-of-thought [34] and Tree-of-thought [38]. The general strategy of self-reflection has been used in several flavors: ReAct [39], Reflexion [29] and Chain of Hindsight [15]. Despite the similarity in name, self-reflection is fundamentally different from self-validation in LLift where the former focuses on using external sources to provide feedback to their models. Huang et al. [10] let an LLM self-improve its reasoning without supervised data by asking the LLM to lay out different possible results.\n\nLLMs for Program Analysis. Ma et al. [17] and Sun et al. [30] explore the capabilities of LLMs when performing various program analysis tasks such as control flow graph construction, call graph analysis, and code summarization. They conclude that while LLMs can comprehend basic code syntax, they are somewhat limited in performing more sophisticated analyses such as pointer analysis and code behavior summarization. In contrast to their findings, our research with LLift has yielded encouraging results. We conjecture that this might be due to several reasons: (1) benchmark selection, i.e., Linux kernel vs. others. (2) Prompt designs. (3) GPT-3.5 vs. GPT4.0 - prior work only evaluated the results using only GPT-3.5. Pei et al. [26] use LLMs to reason about loop invariants with decent performance. In contrast, LLift leverages LLMs for a variety of tasks (including program behavior summarization) and integrates them successfully into a static analysis pipeline.\n\nLLMs for Software Engineering . Xia et al. [36] propose an automated conversation-driven program repair tool using ChatGPT, achieving nearly 50% success rate. Pearce et al. [25] examine zeroshot vulnerability repair using LLMs and found promise in synthetic and hand-crafted scenarios but faced challenges in real-world examples. Chen et al. [5] teach LLMs to debug its own predicted program to increase its correctness, but only performs on relatively simple programs. Lemieux et al. [14] leverages LLM to generate tests for uncovered functions when the search-based approach got coverage stalled. Feng and Chen [7] use LLM to replay Android bug automatedly. Recently, LangChain proposed LangSimith [13], a LLM-powered platform for debugging, testing, and evaluating. These diverse applications underline the vast potential of LLMs in\n\nsoftware engineering. LLift complements these efforts by demonstrating the efficacy of LLMs in bug finding in the real world.\n\n## 9 CONCLUSION\n\nThis work presents a novel approach that utilizes LLMs to aid static analysis using a completely automated agent. By carefully considering the scope and designing the interactions with LLMs, our solution has yielded promising results. We believe our effort only scratched the surface of the vast design space, and hope our work will inspire future research in this exciting direction.\n\n## REFERENCES\n\n- [2] Anthropic (2023). 2023. Claude 2. https://www.anthropic.com/index/claude-2\n- [1] Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl T. Barr. 2023. Improving Few-Shot Prompts with Relevant Static Analysis Products. http://arxiv.org/abs/2304.06815 arXiv:2304.06815 [cs].\n- [3] Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023. When do you need Chain-of-Thought Prompting for ChatGPT? http://arxiv.org/abs/2304.032 62 arXiv:2304.03262 [cs].\n- [5] Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. http://arxiv.org/abs/2304.05128\n- [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\n- [6] Ted Chiang. 2023. ChatGPT Is a Blurry JPEG of the Web. The New Yorker (Feb. 2023). https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-ablurry-jpeg-of-the-web Section: annals of artificial intelligence.\n- [8] Github. 2023. GitHub Copilot documentation. https://ghdocs-prod.azurewebsit es.net/\\_next/data/mHA\\_XfBBaMPyfcP0Q05C5/en/free-pro-team@latest/copi lot.json?versionId=free-pro-team%40latest&amp;productId=copilot\n- [7] Sidong Feng and Chunyang Chen. 2023. Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models. https://doi.org/10.48550/arX iv.2306.01987 arXiv:2306.01987 [cs].\n- [9] Anjana Gosain and Ganga Sharma. 2015. Static Analysis: A Survey of Techniques and Tools. In Intelligent Computing and Applications (Advances in Intelligent Systems and Computing) , Durbadal Mandal, Rajib Kar, Swagatam Das, and Bijaya Ketan Panigrahi (Eds.). Springer India, New Delhi, 581-591.\n- [11] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. Comput. Surveys 55, 12 (Dec. 2023), 1-38. https://doi.org/10.1145/3571730\n- [10] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large Language Models Can Self-Improve. http: //arxiv.org/abs/2210.11610 arXiv:2210.11610 [cs].\n- [12] Jack Krawczyk and Amarnag Subramanya. 2023. Bard's latest update: more features, languages and countries. https://blog.google/products/bard/googlebard-new-features-update-july-2023/\n- [14] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, and Siddhartha Sen. 2023. CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained Large Language Models. (2023).\n- [13] LangChain (2023). 2023. Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications. https: //blog.langchain.dev/announcing-langsmith/\n- [15] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of Hindsight Aligns Language Models with Feedback. http://arxiv.org/abs/2302.02676 arXiv:2302.02676 [cs].\n- [17] Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, and Yang Liu. 2023. The Scope of ChatGPT in Software Engineering: A Thorough Investigation. http://arxiv.org/abs/2305.12138 arXiv:2305.12138 [cs].\n- [16] Kangjie Lu and Hong Hu. 2019. Where Does It Go?: Refining Indirect-Call Targets with Multi-Layer Type Analysis. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security . ACM, London United Kingdom. https://doi.org/10.1145/3319535.3354244\n- [18] Bertrand Meyer. 1997. Object-Oriented Software Construction, 2nd Edition . Prentice-Hall.\n- [20] OpenAI (2023). 2023. Function calling and other API updates. https://openai.c om/blog/function-calling-and-other-api-updates\n- [19] OpenAI (2022). 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n- [21] OpenAI (2023). 2023. GPT-4 Technical Report. http://arxiv.org/abs/2303.08774 arXiv:2303.08774 [cs].\n- [22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\n23. Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. http://arxiv.org/ abs/2203.02155 arXiv:2203.02155 [cs].\n- [24] Jihyeok Park, Hongki Lee, and Sukyoung Ryu. 2022. A Survey of Parametric Static Analysis. ACM Comput. Surv. 54, 7 (2022), 149:1-149:37. https://doi.org/ 10.1145/3464457\n- [23] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi, Biplav Srivastava, Lior Horesh, Francesco Fabiano, and Andrea Loreggia. 2023. Understanding the Capabilities of Large Language Models for Automated Planning. http://arxiv.org/abs/2305.16151 arXiv:2305.16151 [cs].\n- [25] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2023. Examining Zero-Shot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (S&amp;P) . IEEE Computer Society, Los Alamitos, CA, USA. https://doi.org/10.1109/SP46215.20 23.00001\n- [27] Luke Salamone. 2021. What is Temperature in NLP? https://lukesalamone.git hub.io/posts/what-is-temperature/ Section: posts.\n- [26] Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can Large Language Models Reason about Program Invariants?. In Proceedings of the 40th International Conference on Machine Learning .\n- [28] Jessica Shieh. 2023. Best practices for prompt engineering with OpenAI API | OpenAI Help Center. https://help.openai.com/en/articles/6654000-bestpractices-for-prompt-engineering-with-openai-api\n- [30] Weisong Sun, Chunrong Fang, Yudu You, Yun Miao, Yi Liu, Yuekang Li, Gelei Deng, Shenghan Huang, Yuchen Chen, Quanjun Zhang, Hanwei Qian, Yang Liu, and Zhenyu Chen. 2023. Automatic Code Summarization via ChatGPT: How Far Are We? http://arxiv.org/abs/2305.12865 arXiv:2305.12865 [cs].\n- [29] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. http://arxiv.org/abs/2303.11366 arXiv:2303.11366 [cs].\n- [31] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and TegawendÃ© F. BissyandÃ©. 2023. Is ChatGPT the Ultimate Programming Assistant - How far is it? http://arxiv.org/abs/2304.11938 arXiv:2304.11938 [cs].\n- [33] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. http://arxiv.org/abs/2305.16291 arXiv:2305.16291 [cs].\n- [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems , Vol. 30. Curran Associates, Inc.\n- [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. http://arxiv.org/abs/2201.11903 arXiv:2201.11903 [cs].\n- [36] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. http://arxiv.org/abs/ 2304.00385\n- [35] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun 2023). https://lilianweng.github.io/posts/2023-06-23-agent\n- [37] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming . ACM, San Diego CA USA, 1-10. https://doi.org/10.1145/3520312.3534862\n- [39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. International Conference on Learning Representations (ICLR) (2023).\n- [38] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. http://arxiv.org/abs/2305.10601 arXiv:2305.10601 [cs].\n- [40] Yizhuo Zhai, Yu Hao, Hang Zhang, Daimeng Wang, Chengyu Song, Zhiyun Qian, Mohsen Lesani, Srikanth V. Krishnamurthy, and Paul Yu. 2020. UBITect: A Precise and Scalable Method to Detect Use-before-Initialization Bugs in Linux Kernel. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020) .\n- [42] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why Does ChatGPT Fall Short in Providing Truthful Answers? http://arxiv.org/abs/2304.10513 arXiv:2304.10513 [cs].\n- [41] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models. arXiv:2303.18223 [cs.CL]",
  "tables": [
    {
      "index": 0,
      "markdown": "|                       | buf buf   | fmt fmt   | ... ...   | *buf *buf   | *fmt *fmt   |\n|-----------------------|-----------|-----------|-----------|-------------|-------------|\n| Use Use               | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“       | âœ“ âœ“         | âœ“ âœ“         |\n| Initialize Initialize | âœ— âœ—       | âœ— âœ—       | âœ— âœ—       | âœ— âœ—         | âœ— âœ—         |"
    },
    {
      "index": 1,
      "markdown": "| Initializer                                                                                                                                                                                             | Caller                                                                                                                                                                                                                                                                | File Path                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Variable                                                                                                  | Line                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|--------------------------------------------------------|\n| read_reg regmap_read ep0_read_setup regmap_read bcm3510_do_hab_cmd readCapabilityRid e1e_rphy pci_read_config_dword lan78xx_read_reg t1_tpi_read pci_read_config_dword ata_timing_compute pt_completion | get_signal_parameters isc_update_profile ep0_handle_setup mdio_sc_cfg_reg_write bcm3510_check_firmware_version airo_get_range __e1000_resume adm8211_probe lan78xx_write_raw_otp my3126_phy_reset quirk_intel_purley_xeon_ras_cap opti82c46x_set_piomode pt_req_sense | drivers/media/dvb-frontends/stv0910.c drivers/media/platform/atmel/atmel-isc.c drivers/usb/mtu3/mtu3_gadget_ep0.c drivers/net/ethernet/hisilicon/hns_mdio.c drivers/media/dvb-frontends/bcm3510.c drivers/net/wireless/cisco/airo.c drivers/net/ethernet/intel/e1000e/netdev.c drivers/net/wireless/admtek/adm8211.c drivers/net/usb/lan78xx.c drivers/net/ethernet/chelsio/cxgb/my3126.c arch/x86/kernel/quirks.c drivers/ata/pata_legacy.c drivers/block/paride/pt.c | tmp sr setup.bRequestType reg_value ver.demod_version cap_rid.softCap phy_data reg buf val capid0 &tp buf | 504 664 637 169 666 6936 6580 1814 873 193 562 564 368 |"
    },
    {
      "index": 2,
      "markdown": "| Combination   | TN(C)   | TP(C)   | Precision   | Recall   | Accuracy   | F1 Score   |\n|---------------|---------|---------|-------------|----------|------------|------------|\n| Simple Prompt | 12(9)   | 2(1)    | 0.12        | 0.15     | 0.35       | 0.13       |\n| PCA           | 13(9)   | 5(1)    | 0.26        | 0.38     | 0.45       | 0.31       |\n| PCA+PP        | 5(3)    | 6(1)    | 0.21        | 0.46     | 0.28       | 0.29       |\n| PCA+PP+SV     | 5(2)    | 11(8)   | 0.33        | 0.85     | 0.40       | 0.48       |\n| PCA+PP+TD     | 22(14)  | 6(4)    | 0.55        | 0.46     | 0.70       | 0.50       |\n| PCA+PP+SV+TD  | 25(17)  | 13(12)  | 0.87        | 1.00     | 0.95       | 0.93       |\n| Oracle        | 27(27)  | 13(13)  | -           | -        | -          | -          |"
    },
    {
      "index": 3,
      "markdown": "| Caller Caller                                                                                                                                                                                                                                                                                                                             | GPT 4 3.5 GPT 4 3.5                                                     | Claude2 Claude2                     | Bard Bard                           |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------|-------------------------------------|\n| hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf hpet_msi_resume ctrl_cx2341x_getv4lflags axi_clkgen_recalc_rate max8907_regulator_probe ov5693_detect iommu_unmap_page mt9m114_detect ec_read_u8 compress_sliced_buf | âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ | âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ— âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— | âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ âœ— âœ— âœ“ âœ“ âœ“ âœ— âœ“ âœ“ âœ“ |"
    }
  ],
  "stats": {
    "pages": 12,
    "chunksCreated": 152,
    "totalCharacters": 106523,
    "totalWords": 15662,
    "numTables": 4,
    "processingTimeMs": 17104
  }
}