{
  "paper": {
    "id": "2403.19522v2",
    "title": "Model Stock: All we need is just a few fine-tuned models",
    "abstract": "This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. We demonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands. Our code and pre-trained models are available at https://github.com/naver-ai/model-stock.",
    "authors": [
      "Dong-Hwan Jang",
      "Sangdoo Yun",
      "Dongyoon Han"
    ],
    "published": "2024-03-28T15:57:20.000Z",
    "updated": "2025-08-01T16:30:12.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2403.19522v2",
    "absUrl": "https://arxiv.org/abs/2403.19522v2"
  },
  "chunks": [
    {
      "id": "2403.19522v2-chunk-0",
      "content": "Dong-Hwan Jang ⋆ Sangdoo Yun\n\nNAVER AI Lab\n\nAbstract. This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-ofdistribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two finetuned models, applicable during or after training. Our innovative layerwise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Model Stock: All we need is just a few fine-tuned models",
        "chunkIndex": 0,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-1",
      "content": "uned models, applicable during or after training. Our innovative layerwise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock , highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. Wedemonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands. Our code and pre-trained models are available at https://github.com/naver-ai/model-stock .",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Model Stock: All we need is just a few fine-tuned models",
        "chunkIndex": 1,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-2",
      "content": "Pre-train/fine-tune paradigm [18,22,37,40,41] has proven to be a strong framework for training models to reach state-of-the-art performance. This approach, especially pivotal in fine-tuning pre-trained models, involves models acquiring general knowledge during pre-training and task-specific knowledge during finetuning. How we perform a fine-tuning stage is crucial, affecting task performance and robustness against distribution shifts.\n\nRecent advancements, notably Model Soup [40], which merges weights from multiple fine-tuned models trained under different training setups, have shown impressive performance without increasing inference costs. This method is believed to be effective because these models often reside in the same loss basin, and their merging results in a lower and flat loss basin. However, Model Soup's requirement for multiple fine-tuned models (more than dozens) raises concerns about efficiency and practicality in general scenarios where the models need to",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 2,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-3",
      "content": "and flat loss basin. However, Model Soup's requirement for multiple fine-tuned models (more than dozens) raises concerns about efficiency and practicality in general scenarios where the models need to\n\n⋆ Work done during an internship at NAVER AI Lab.\n\nDongyoon Han\n\nFig. 1: Model Stock vs. Model Soup. Model Stock consistently enjoys improved accuracy on ImageNet (x-axis) and distribution shift benchmarks (y-axis) against individual fine-tuned models (gray circles). We plot WiSE-FT [41] curves to each fine-tuned model, highlighting Model Stock's better performance on distribution shifts compared to Model Soup [40]. Note that Model Stock has much smaller computational costs than Model Soups (24 × smaller), i.e ., Model Stock requires two fine-tuning procedures, whereas Model Soups are leveraging 48 various fine-tuned models in this experiment.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-4",
      "content": "ional costs than Model Soups (24 × smaller), i.e ., Model Stock requires two fine-tuning procedures, whereas Model Soups are leveraging 48 various fine-tuned models in this experiment.\n\n<!-- image -->\n\nbe prepared from scratch. Thus, our question is: Is there an efficient way to achieve an effective merged weight from very few fine-tuned models?\n\nWe initially explore the dynamics of fine-tuned weights under the simplest scenario: varying the random seeds while maintaining the other training setups. It reveals that the fine-tuned weights with different random seeds reside on a very thin shell layer-wise during and after training. We then delve into the impact of a model-soup-like weight averaging approach. Our findings show that the closer proximity of the averaged weights correlates with improved In-Distribution (ID) and Out-Of-Distribution (OOD) performance.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-5",
      "content": "model-soup-like weight averaging approach. Our findings show that the closer proximity of the averaged weights correlates with improved In-Distribution (ID) and Out-Of-Distribution (OOD) performance.\n\nBuilding upon the findings, we propose a novel approach of fine-tuning method coined Model Stock , analogous to chicken stock in cooking, distinguishing it from what Model Soup intended. Now, the answer to our question is indeed affirmative: Model Stock approximates the merged weight using just a few fine-tuned models, leveraging the weight space's geometric properties and a pre-trained model's anchoring effect. This strategy offers a more computationally efficient alternative to the labor-intensive averaging of fine-tuned models, streamlining the process while enhancing model performance. Fig. 1 illustrates our brief comparison of Model Stock vs. Model Soup [40].",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-6",
      "content": "ternative to the labor-intensive averaging of fine-tuned models, streamlining the process while enhancing model performance. Fig. 1 illustrates our brief comparison of Model Stock vs. Model Soup [40]. We reproduce the Model Soup experiments 1 based on the CLIP ViT-B/32 initialization by fine-tuning 48 models with various hyper-parameters, which is defined as zero-shot initialization setting. Fig. 1 shows that Model Stock outperforms Model Soups with much smaller computational costs.\n\n1 We follow the standard grid hyper-parameter sweep [40], fine-tuning for 10 epochs. Our results align with those presented in Fig. D.1 (right) of the original paper [40].\n\nOur comprehensive experiments demonstrate the effectiveness of Model Stock. We achieve performance comparable to, or even surpassing, that of the more resource-intensive methods such as Model Soup [40], using only a fraction of the models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-7",
      "content": "te the effectiveness of Model Stock. We achieve performance comparable to, or even surpassing, that of the more resource-intensive methods such as Model Soup [40], using only a fraction of the models. Specifically, our method achieves 87.8% ImageNet top-1 accuracy (ID) and averaged 74.9% in five distribution shift benchmarks (OOD) on ViTL/14 fairly compared with the prior arts using the CLIP pre-trained weight. We believe that our study not only underscores Model Stock's practicability but also opens new directions in the pre-train/fine-tune paradigm for superior performance across various tasks.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-8",
      "content": "Our study is driven by two fundamental findings related to the performance and robustness of fine-tuned models. The first one is that model weights are fine-tuned on different random seeds 2 lie on thin shell in weight space layerwise. The second posits that closer proximity to the center of this thin shell is beneficial for improving performance across the ImageNet and distribution shift benchmarks. The substantiation and implications of these observations are discussed in the subsequent sections.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2 Analyzing Fine-tuned Weights",
        "chunkIndex": 8,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-9",
      "content": "Angle and norm of weights. We begin by examining the intrinsic properties of the weights in fine-tuned models. We define the weight vector of the finetuned model at k -th layer as w ( k ) ∈ R n ( k ) where n ( k ) is the number of weight parameters at k -th layer, and the origin 0 as the pre-trained model weight w ( k ) 0 at k -th layer. Then the angle θ ( k ) between two weight w ( k ) 1 and w ( k ) 2 is defined by: θ ( k ) = arccos ( w ( k ) 1 · w ( k ) 2 ∥ w ( k ) 1 ∥∥ w ( k ) 2 ∥ ) , where the Euclidean l 2 -norm ∥ w ( k ) ∥ of a n ( k ) defined as ∥ w ( k ) ∥ = √ ∑ n ( k ) i =1 w ( k ) i 2 . Angle and norm will provide a geometric view of the weights at k -th layer between fine-tuned models.\n\nObservation 1: Angle and norm consistency among fine-tuned weights. We investigate the weight space of models fine-tuned on ImageNet from a pretrained model with various random seeds.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 9,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-10",
      "content": "e-tuned models.\n\nObservation 1: Angle and norm consistency among fine-tuned weights. We investigate the weight space of models fine-tuned on ImageNet from a pretrained model with various random seeds. Our first observation is that both angle θ ( k ) between two different models and norm ∥ w ( k ) ∥ of a weight exhibit consistent values with very low standard deviations, as shown in Fig. 2. This consistency can be mathematically represented as follows: For all i and j ∈ [1 , N ] when the number of fine-tuned weights, N , is sufficiently large, the following holds:\n\n<!-- formula-not-decoded -->\n\n2 Random seed influences training randomness, such as training data shuffling and data augmentation parameters.\n\n)\n\n(\n\nFig. 2: Layer-wise angle and norm of fine-tuned models. We measure the angle θ ( k ) (degree) and norm ∥ w ( k ) ∥ √ n ( k ) for 50 distinct weights, fine-tuned under different random seeds.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 10,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-11",
      "content": "s.\n\n)\n\n(\n\nFig. 2: Layer-wise angle and norm of fine-tuned models. We measure the angle θ ( k ) (degree) and norm ∥ w ( k ) ∥ √ n ( k ) for 50 distinct weights, fine-tuned under different random seeds. We separately visualize weight layers (red bars) and bias layers (blue bars), where bias layers have much smaller angles. We plot the mean angle and norm values with standard deviation (black error bar). The results show that any two fine-tuned weights have layer-wise consistent angle and norm with extremely low standard deviation.\n\n<!-- image -->\n\nwhere l ( k ) and θ ( k ) are constants that describe the magnitude and angle between weights at k -th layer, respectively. Henceforth, to simplify notation, we will omit the superscript ( k ) indicating the layer index.\n\nInterestingly, these consistencies in angle and norm are observed 1) across diverse setups and 2) both during and after training. Fig. 2 shows this consistency over 50 fine-tuned CLIP ViT-B/32 3 [30] models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 11,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-12",
      "content": "tingly, these consistencies in angle and norm are observed 1) across diverse setups and 2) both during and after training. Fig. 2 shows this consistency over 50 fine-tuned CLIP ViT-B/32 3 [30] models. It illustrates that the layer-wise norm and angle of these models exhibit almost constant values with extremely minimal error. While this figure depicts a specific model ( i.e ., CLIP ViT-B/32), we establish that such regularity is not confined to a single model or setting but is consistent across various CLIP fine-tuning scenarios. We conjecture this holds irrespective of networks (ViT [5], Hybrid-ViT [5], ResNet [9], ConvNext [24]), optimizers (SGD, AdamW [25]), data augmentations (RRC [35], RandAug [4]), datasets (CIFAR [17], ImageNet [34]), or initialization of the classifier (zeroshot, LP-FT [18]). Remarkably, this regularity also holds for fine-tuned weights at each step during training as well as after training.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 12,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-13",
      "content": "[17], ImageNet [34]), or initialization of the classifier (zeroshot, LP-FT [18]). Remarkably, this regularity also holds for fine-tuned weights at each step during training as well as after training. A comprehensive analysis supporting these findings is presented in the Appendix A.\n\nBased on the observation, we presume the distribution of the fine-tuned weights. The center of the fine-tuned weights is defined as µ = lim N →∞ 1 N ∑ N i =1 w i . We then deduce the following properties among fine-tuned weights: (i) ∥ w i -µ ∥ = constant, indicating a thin shell distribution; (ii) ( w 0 -µ ) ⊥ ( w i -µ ) ; and (iii) ( w i -µ ) ⊥ ( w j -µ ) for all i, j ∈ [1 , N ] . These properties are depicted in Fig. 5 for better understanding. The detailed proof is in the Appendix B.\n\n3 https://github.com/openai/CLIP",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 13,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-14",
      "content": "and (iii) ( w i -µ ) ⊥ ( w j -µ ) for all i, j ∈ [1 , N ] . These properties are depicted in Fig. 5 for better understanding. The detailed proof is in the Appendix B.\n\n3 https://github.com/openai/CLIP\n\nTable 1: Distance from the center ( i.e ., ∥ w -µ ∥ ) vs. performance . We report the ImageNet and distribution shift performance with the distance from the center of weights µ for fine-tuned and averaged models. We observe 1) both models consistently maintain a nearly constant distance from µ with remarkably small standard deviation; 2) averaging more models approaches µ , boosting ID and OOD performance.\n\n|                | ∥ w - µ ∥     |   ImageNet |   Avg. shifts |\n|----------------|---------------|------------|---------------|\n| Fine-tuned     | 13.133 ± .004 |      79.72 |         46.37 |\n| w (2) avr      | 9.192 ± .003  |      80.24 |         47.76 |\n| w (3) avr      | 7.439 ± .025  |      80.35 |         48.18 |\n| w (5) avr      | 5.633 ± .014  |      80.47 |         48.53 |\n| w",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 14,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-15",
      "content": "37 |\n| w (2) avr      | 9.192 ± .003  |      80.24 |         47.76 |\n| w (3) avr      | 7.439 ± .025  |      80.35 |         48.18 |\n| w (5) avr      | 5.633 ± .014  |      80.47 |         48.53 |\n| w (50) avr ≃ µ | ∼ 0           |      80.59 |         48.85 |\n\nFig. 3: Test error landscape. We visualize a test error landscape with three weight points: a pre-trained model ( w 0 ), a fine-tuned model ( w 1 ), and the averaged weights of 50 fine-tuned models ( w 50 avr ). w 50 avr locates near the lowest error basin. A better solution ( w H ) with lower error than w 1 can be easily found utilizing those three weights (§3).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.1 Geometric Properties Between Weights",
        "chunkIndex": 15,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-16",
      "content": "We proceed to explore the relationship between the proximity to the center of fine-tuned weights and their performance on ID and OOD datasets. Given that computing the exact center is infeasible, we approximate it by averaging differently seeded 50 fine-tuned weights, using it as a pseudo-center ( i.e . µ ≃ w (50) avr ).",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.2 Center of Weights and Performance",
        "chunkIndex": 16,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-17",
      "content": "Table 1 offers quantitative observations about the fine-tuned weights and their performance using CLIP ViT-B/32. The results include distances from the weight center ( µ ) of fine-tuned models and their averaged counterparts with their ID and OOD performances. Going closer to the center by averaging the weights leads to improving both performances. Interestingly, the standard deviation of the distances is less than 0.1% of the mean distance, suggesting highly consistent fine-tuned weight distances from the center across different weights. This suggests that fine-tuned weights occupy a thin shell as discussed in §2.1.\n\nObservation 3: Fine-tuned weights occupy local minima edges in the test error landscape. We present an additional observation regarding the test error landscape, which relates to the performance around the center of weights. Fig. 3 depicts the test error landscape on the ImageNet test dataset within a two-dimensional plane.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 17,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-18",
      "content": "regarding the test error landscape, which relates to the performance around the center of weights. Fig. 3 depicts the test error landscape on the ImageNet test dataset within a two-dimensional plane. This plane includes a pre-trained model's weight ( w 0 ), a single fine-tuned model ( w 1 ), and the pseudo-center ( w (50) avr ). This landscape\n\nFig. 4: Weights closer to the center enjoy higher performances. We show the ImageNet accuracy of weights with their distance from the center ( µ ), which is approximated with w (50) avr . Averaged weights are closer to the center than individual weights, and their accuracy increases as the averaging number of models ( N ) increases. Gray circles are the weights randomly sampled from the Gaussian distribution centered at w (50) avr . Even the random weights also achieve higher performance as they reach the center. The results indicate the critical role of proximity to the center on performance.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 18,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-19",
      "content": "ered at w (50) avr . Even the random weights also achieve higher performance as they reach the center. The results indicate the critical role of proximity to the center on performance.\n\n<!-- image -->\n\nreveals that a fine-tuned model typically occupies the boundary of test error regions. On the other hand, centered near pseudo-center ( w (50) avr ), the test error is the lowest and gets higher as the weights get far from the center. Interestingly, along the line segment w 0 w 1 ( i.e ., a WiSE-FT curve [41]), the fine-tuned weight w 1 is neither the point closest to the pseudo-center nor the one with the lowest test error. We will connect this observation in §3 to find w H , the weight on the line closest to the center.\n\nObservation 4: Randomly perturbed weights nearing the center also merit high performance.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 19,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-20",
      "content": "est error. We will connect this observation in §3 to find w H , the weight on the line closest to the center.\n\nObservation 4: Randomly perturbed weights nearing the center also merit high performance. To further investigate the impact of proximity to the center on performance, we conduct a toy experiment to measure ImageNet performance using random weights generated by adding layer-wise noise to the weight at the center. The standard deviation of the noise for each layer is adjusted to align with the distribution of fine-tuned weights. Fig. 4 presents a scatter plot correlating the distance of model weights from the distribution's center with corresponding ImageNet accuracies. Remarkably, randomly sampled weights demonstrate performance comparable to fine-tuned and averaged models, highlighting the importance of center proximity.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 20,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-21",
      "content": "nter with corresponding ImageNet accuracies. Remarkably, randomly sampled weights demonstrate performance comparable to fine-tuned and averaged models, highlighting the importance of center proximity.\n\nFinally, the above observations naturally raise a question: Why do fine-tuned weights through optimization not reach the center, staying constantly close instead? Previous studies [3, 15] tell us that optimization steps might struggle to guide fine-tuned weights to the center of the weight distribution due to the many stationary points in the loss surface. Alternatively, averaging independently finetuned models is a unique solution but both laborious and resource-intensive. It appears that there are no better alternatives for getting closer to the center, as\n\nFig. 5: Comprehensive illustration of the geometric dynamics of fine-tuned weights. This figure illustrates the behavior of fine-tuned weights that lie on a thin shell, supporting our Gaussian distribution hypothesis.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 21,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-22",
      "content": "sive illustration of the geometric dynamics of fine-tuned weights. This figure illustrates the behavior of fine-tuned weights that lie on a thin shell, supporting our Gaussian distribution hypothesis. Each sphere represents a thin shell that fine-tuned models lie on at each training step, with w 0 denoting the pretrained model. The curved lines trace the fine-tuning trajectory from w 0 , while the red vectors indicate that fine-tuned models at each time step are equidistant from w 0 .\n\n<!-- image -->\n\noptimization proves ineffective near these flat local minima. Could there be a faster approach to reaching the center? This question will be addressed in §3.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Observation 2: Distance from the center of weights and performance.",
        "chunkIndex": 22,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-23",
      "content": "The observed geometric patterns in weight distributions closely align with mathematical properties of Gaussian distributions, represented as N ( µ , Σ ) . Therefore, a plausible reason for the particular geometric pattern of fine-tuned weights could be the influence of Gaussian noise within the weight space. In highdimensional spaces, vectors sampled from such distributions tend to have nearly identical norms, specifically √ | µ | 2 + trace ( Σ ) and consistent in-between angles, due to the concentration of measure phenomenon [19]. The likelihood of the squared norm significantly deviating from this expected value is exponentially negligible in high-dimensional spaces, like a weight space.\n\nIn other words, the vectors sampled from high-dimensional Gaussian distribution lie on a very thin shell with the radius ≈ √ trace ( Σ ) around the center µ . Consequently, we hypothesize that fine-tuned weights follow a Gaussian distribution in a layer-wise manner.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.3 Our Hypothesis",
        "chunkIndex": 23,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-24",
      "content": "distribution lie on a very thin shell with the radius ≈ √ trace ( Σ ) around the center µ . Consequently, we hypothesize that fine-tuned weights follow a Gaussian distribution in a layer-wise manner. While not necessary for our observation, this hypothesis provides a sufficient condition for understanding the geometric dynamics of fine-tuned weights. For example, it aids in the intuitive understanding of the fact that the distance of w ( N ) avr from the weight center is proportional to 1 / √ N , signifying the reduction of variance. Fig. 5 comprehensively illustrates the observations and our hypothesis discussed in §2.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "2.3 Our Hypothesis",
        "chunkIndex": 24,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-25",
      "content": "This section introduces our method, Model Stock, a cost-efficient weight merging method. As discussed in §2.2, getting closer to the center of weights µ induces improved model performance. A straightforward method to approximate\n\nFig. 6: Schematic concept of our Model Stock. We present two scenarios with a small angle (left) and a large angle (right). Given a pre-trained weight ( w 0 ) and two fine-tuned weights ( w 1 , and w 2 ), we visualize a gray triangle representing the span of three weights. We consider this triangle area our search space spanned by three weights. We aim to find the best weight point on the triangle nearest the ideal center µ . We find that the perpendicular foot w H from point µ to the plane is the nearest point, which can be specified solely using the angle between the fine-tuned models, even without knowing the exact position of the center µ . We utilize w H as the merged weight of Model Stock.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 25,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-26",
      "content": "earest point, which can be specified solely using the angle between the fine-tuned models, even without knowing the exact position of the center µ . We utilize w H as the merged weight of Model Stock. Intuitively, when the angle θ is large ( i.e ., two weights are diverse), as in the right figure, w H will rely more on the pre-trained weight( w 0 ), vice versa.\n\n<!-- image -->\n\nµ is averaging multiple model weights, which can be computationally expensive. We propose an efficient alternative method by leveraging the pre-trained model weights , an aspect previously neglected by existing weight-merging methods. A pre-trained model usually possesses general knowledge and shows robust and reliable performance in out-of-distribution (OOD) cases [30]. Therefore, a pretrained model can become a robust anchor point . As shown in Fig. 3, we could readily identify a weight ( w H ) that is closer to the center-and thus better-by interpolating with the anchor.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 26,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-27",
      "content": "ore, a pretrained model can become a robust anchor point . As shown in Fig. 3, we could readily identify a weight ( w H ) that is closer to the center-and thus better-by interpolating with the anchor. Building on this concept, we propose a method to approximate the center of weights more accurately with only a few fine-tuned weights. Again, for readability, we omit the layer notation ( k ) , but the subsequent method is applied layer-wise.\n\nOn two fine-tuned models. We observed in §2 that two fine-tuned models with different random seeds have almost constant norms and the angle between them. Based on this, we define a plane connecting the pre-trained model and two fine-tuned models as shown in Fig. 6 (depicted as a gray triangular area). Any weight vector on this plane can be expressed as a linear combination of the pretrained model and two fine-tuned models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 27,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-28",
      "content": "o fine-tuned models as shown in Fig. 6 (depicted as a gray triangular area). Any weight vector on this plane can be expressed as a linear combination of the pretrained model and two fine-tuned models. Our goal is to find the weight closest to the center of fine-tuned weights on this plane, which is the perpendicular foot ( w H ) from the center of distribution ( µ ) to the plane.\n\nEven without knowing the exact position of the center µ , w H can be specified solely using the angle between the fine-tuned models with the following two conditions that µ must satisfy. First, as mentioned in §2.1, ( w 1 -µ ) ⊥ ( w 2 -µ ) and ∥ w i -µ ∥ = ∥ w 2 -µ ∥ hold, implying that △ w 1 µ w 2 forms an isosceles right triangle. In Fig. 6, µ should lie on the dotted hyper-circle. Another condition is that the condition ( w 0 -µ ) ⊥ ( w 12 -µ ) must be satisfied, where w 12 = w 1 + w 2 2 . This condition arises from the second property in §2.1, where ( w 0 -µ ) ⊥ ( w i -µ ) .",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 28,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-29",
      "content": ". Another condition is that the condition ( w 0 -µ ) ⊥ ( w 12 -µ ) must be satisfied, where w 12 = w 1 + w 2 2 . This condition arises from the second property in §2.1, where ( w 0 -µ ) ⊥ ( w i -µ ) . Combining these two conditions, µ is at the point where the line starting from w 0\n\nis tangent to the hyper-circle. We precisely determine the position of µ within a 3D volume slice that encompasses both µ and △ w 0 w 1 w 2 . Consequently, we can find the position of the closest weight w H to the distribution's center on the plane using straightforward geometric principles. The position of the perpendicular foot is determined as follows:\n\n<!-- formula-not-decoded -->\n\nFor more detailed proof, please refer to the Appendix D. Note that the interpolation ratio t = 2 cos θ 1+cos θ is solely determined by the angle θ between two fine-tuned models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 29,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-30",
      "content": "rmula-not-decoded -->\n\nFor more detailed proof, please refer to the Appendix D. Note that the interpolation ratio t = 2 cos θ 1+cos θ is solely determined by the angle θ between two fine-tuned models. Crucially, unlike previous methods, determining t does not require extra training [22, 36, 37, 40] or heuristic hyper-parameter settings [6, 40], thereby simplifying the process and enhancing its accessibility and efficiency.\n\nAs the θ decreases, the pre-trained model is less utilized for merging, as shown in Fig 6 (left). Coupled with the observation in Fig. 2, this indicates that bias layers rely less on pre-trained models and focus more on fine-tuned models, whereas weight layers depend more on pre-trained models. This observation extends the findings of previous works such as BitFit [43] and LP-FT [18].",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 30,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-31",
      "content": "trained models and focus more on fine-tuned models, whereas weight layers depend more on pre-trained models. This observation extends the findings of previous works such as BitFit [43] and LP-FT [18]. In the case of BitFit and LP ( i.e ., the first step of LP-FT), bias and classifier layers fully utilize fine-tuning, while other weight layers (attention and MLP) rely on pre-trained models. We present an additional analysis in the Appendix F.\n\nOn N fine-tuned models. We further extend the previous derivation to N fine-tuned models to move even closer to the weight center. Let us denote w ( N ) avr as the N -averaged weight, ∑ N i =1 w i /N , and w ( N ) H as the weight in the span ( w 0 , w 1 , . . . , w N ) closest to the distribution's center. Then, we derive the position of w ( N ) H as:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 31,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-32",
      "content": "∑ N i =1 w i /N , and w ( N ) H as the weight in the span ( w 0 , w 1 , . . . , w N ) closest to the distribution's center. Then, we derive the position of w ( N ) H as:\n\n<!-- formula-not-decoded -->\n\nDetailed proof is in the Appendix D. Similar to the case of two fine-tuned models, the interpolation ratio t depends solely on the angle θ between the pre-trained model and the N fine-tuned models.\n\nPeriodic merging. To move one step forward with our method, we propose periodic merging , which is performed between the fine-tuned models and the pre-trained model during training. As the geometric properties of weights are also applicable to weights during training (refer to Appendix A.2 for more details), we fully utilize this phenomenon here.\n\nFig. 7: Periodic merging for Model Stock . Weights are merged every epoch.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 32,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-33",
      "content": "to weights during training (refer to Appendix A.2 for more details), we fully utilize this phenomenon here.\n\nFig. 7: Periodic merging for Model Stock . Weights are merged every epoch.\n\n<!-- image -->\n\nWe strategically merge weights at the end of every epoch, allowing for the fine-tuning process to be parallelized with merging. The interpolation ratio for merging is determined by the angle θ between the pre-trained model and the fine-tuned models at the current epoch. Fig. 7 visualizes this periodic merging\n\nprocess. We argue that employing a periodic merging can approximate the center of weights more accurately. In §4.3, we empirically show that the periodic merging yields superior performance and achieves a closer distance to the center.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "3 Method",
        "chunkIndex": 33,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-34",
      "content": "We present the key experimental results in this section. We first provide our experimental setups in §4.1. Then, we present the main results in §4.2, and ablation studies in §4.3. More detailed experimental setups, analysis, and further results are in the Appendix G. We will release our codes and weights publicly.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4 Experiment",
        "chunkIndex": 34,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-35",
      "content": "Models. We conduct the experiments on CLIP ViT-B/32, CLIP ViT-B/16, and CLIP ViT-L/14 models. We set the number of fine-tuning models for Model Stock as two. We compare Model Stock against various fine-tuning techniques, including Model Soups [40], LP-FT [18], CAR-FT [27], FTP [37], and FLYP [7]. We use CLIP ViT-B/32 for ablation studies.\n\nDatasets. We fine-tune models on the ImageNet-1K [34] training dataset. We report the ImageNet-1K [34] top-1 accuracy for evaluating in-distribution (ID) performance. For distribution shift scenarios, we consider five out-of-distribution (OOD) benchmarks including ImageNet-V2 [33], ImageNet-R [10], ImageNetSketch [39], ImageNet-A [11], and ObjectNet [1]. Since previous methods, except Model Soups [40], have not been evaluated on ObjectNet, we omit the ObjectNet results when comparing against them.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.1 Experimental Setup",
        "chunkIndex": 35,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-36",
      "content": "ageNetSketch [39], ImageNet-A [11], and ObjectNet [1]. Since previous methods, except Model Soups [40], have not been evaluated on ObjectNet, we omit the ObjectNet results when comparing against them.\n\nTraining setup. We initialize the classifier weights ( e.g ., 1000 classes for ImageNet) using the text encoder of CLIP and text prompts following Model Soup [40]. We use AdamW [25] with batch size 512 and weight decay 0.1 for all the experiments, including vanilla fine-tuning, Model Soup reproduction, and Model Stock 4 . We employ two training setups for comparisons: (1) training 10 epochs with minimal data augmentation, following Model Soup's zero-shot initialization setup, and (2) training 16 epochs with strong data augmentation, following Model Soup's LP initialization setup [40], denoted with ⋆ . These setups enable a balanced comparison of Model Stock against zero-shot and LP-initialized Model Soups. Detailed hyper-parameters are in the Appendix G.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.1 Experimental Setup",
        "chunkIndex": 36,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-37",
      "content": "d with ⋆ . These setups enable a balanced comparison of Model Stock against zero-shot and LP-initialized Model Soups. Detailed hyper-parameters are in the Appendix G.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.1 Experimental Setup",
        "chunkIndex": 37,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-38",
      "content": "CLIP ViT-B/32. Table 2 shows the results of Model Stock on the pre-trained CLIP ViT-B/32 model by comparing it with Model Soups. 'Avg. shifts' denotes the average accuracy of the five OOD benchmark scores. Our Model Stock and Model Stock ⋆ show competitive performance with Model Soups. Furthermore, Model Stock ⋆ achieves state-of-the-art performance on ImageNet with 81.19 %\n\n4 We reduce the batch size to 64 on CLIP ViT-L/14 due to memory limitation.\n\nTable 2: Comparison against Model Soups [40] on CLIP ViT-B/32. We report the performance and relative fine-tuning costs on the CLIP ViT-B/32 scenario. α denotes the cost for LP initialization. Model Stock shows comparable performance with Model Soups with significantly reduced training costs.\n\n| Method                                                                        |   ImageNet |   Avg.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 38,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-39",
      "content": "el Stock shows comparable performance with Model Soups with significantly reduced training costs.\n\n| Method                                                                        |   ImageNet |   Avg. shifts | Cost   |\n|-------------------------------------------------------------------------------|------------|---------------|--------|\n| Comparing with Model Soups from zero-shot init. CLIP zero-shot Initialization |      63.34 |         48.51 | 0      |\n| Vanilla FT                                                                    |      78.35 |         47.03 | 1      |\n| Uniform Model Soup (from zero-shot)                                           |      79.76 |         52.08 | 48     |\n| Greedy Model Soup (from zero-shot)                                            |      80.42 |         50.83 | 48     |\n| Model Stock                                                                   |      79.89 |         50.99 | 2      |\n| Comparing with Model Soups from LP init.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 39,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-40",
      "content": "80.42 |         50.83 | 48     |\n| Model Stock                                                                   |      79.89 |         50.99 | 2      |\n| Comparing with Model Soups from LP init. CLIP LP initialization               |      75.57 |         47.21 | α      |\n| Vanilla FT ⋆                                                                  |      79.72 |         46.37 | 1      |\n| Uniform Model Soup (from LP init)                                             |      79.97 |         51.45 | 71+ α  |\n| Greedy Model Soup (from LP init)                                              |      81.03 |         50.75 | 71+ α  |\n| Model Stock ⋆                                                                 |      81.19 |         48.69 | 2      |\n\nTable 3: Model Stock on CLIP ViT-B/16. Model Stock shows competitive performance against previous fine-tuning methods on ImageNet and distribution shifts.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 40,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-41",
      "content": "|      81.19 |         48.69 | 2      |\n\nTable 3: Model Stock on CLIP ViT-B/16. Model Stock shows competitive performance against previous fine-tuning methods on ImageNet and distribution shifts.\n\n|               |          | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   |\n|---------------|----------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Method        | ImageNet | Avg. shifts           | IN-V2                 | IN-R                  | IN-A                  | IN-Sketch             |\n| Zero-shot     | 68.3     | 59.5                  | 62.0                  | 77.7                  | 49.9                  | 48.3                  |\n| Vanilla FT    | 82.8     | 57.7                  | 72.9                  | 66.4                  | 43.7                  | 48.0                  |\n| Vanilla FT ⋆  | 83.7     | 57.4                  |",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 41,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-42",
      "content": "Vanilla FT    | 82.8     | 57.7                  | 72.9                  | 66.4                  | 43.7                  | 48.0                  |\n| Vanilla FT ⋆  | 83.7     | 57.4                  | 73.5                  | 67.6                  | 40.0                  | 48.6                  |\n| LP [18]       | 79.7     | 48.1                  | 71.5                  | 52.4                  | 27.8                  | 40.5                  |\n| LP-FT [18]    | 81.7     | 60.5                  | 71.6                  | 72.9                  | 49.1                  | 48.4                  |\n| CAR-FT [27]   | 83.2     | 59.4                  | 73.0                  | 71.3                  | 43.7                  | 49.5                  |\n| FTP [37]      | 84.2     | 49.7                  | 74.6                  | 47.2                  | 26.5                  | 50.2                  |\n| FLYP [7]      | 82.6     | 60.5                  | 73.0                  | 71.4                  | 48.1",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 42,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-43",
      "content": "| 47.2                  | 26.5                  | 50.2                  |\n| FLYP [7]      | 82.6     | 60.5                  | 73.0                  | 71.4                  | 48.1                  | 49.6                  |\n| Model Stock   | 84.1     | 62.4                  | 74.8                  | 71.8                  | 51.2                  | 51.8                  |\n| Model Stock ⋆ | 85.2     | 60.1                  | 75.3                  | 68.7                  | 45.0                  | 51.3                  |\n\ntop-1 accuracy. As described in Fig. 1, Model Stock with WiSE-FT [41] enjoys a superior ID-OOD performance curve compared to Model Soup and its WiSE-FT curves. Note that Model Soups require dozens of fine-tuned models ( e.g ., zeroshot and LP-init Model Soups use 48 and 71 models, respectively), highlighting the effectiveness of Model Stock along with efficiency utilizing only two models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 43,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-44",
      "content": "ozens of fine-tuned models ( e.g ., zeroshot and LP-init Model Soups use 48 and 71 models, respectively), highlighting the effectiveness of Model Stock along with efficiency utilizing only two models. We provide further comparison results with WiSE-FT [41] curves on LP-init Model Soups in the Appendix H.\n\nCLIP ViT-B/16. Table 3 presents a comprehensive comparison of different fine-tuning methods applied to CLIP ViT-B/16. Previous works [18,36,37] lack ObjectNet [1] results; therefore, we omit ObjectNet and report the other four OOD benchmarks. Complete results with ObjectNet and ImageNet-Real [2] are in the Appendix H.2. The results show Model Stock exhibits exceptional performance on the ImageNet accuracy, e.g ., Model Stock ⋆ achieves 85.2% top-1\n\nTable 5: Impact of the number of fine-tuning models ( N ) on Model Stock. IN denote ImageNet accuracy.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 44,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-45",
      "content": "bits exceptional performance on the ImageNet accuracy, e.g ., Model Stock ⋆ achieves 85.2% top-1\n\nTable 5: Impact of the number of fine-tuning models ( N ) on Model Stock. IN denote ImageNet accuracy.\n\n|      |   IN |   Avg. Shifts |   ∥ w - µ ∥ |\n|------|------|---------------|-------------|\n| FT   | 79.7 |          46.7 |       13.13 |\n| N =2 | 80.1 |          48.8 |       10.01 |\n| N =3 | 80.2 |          48.8 |        9.05 |\n| N =4 | 80.4 |          48.9 |        8.45 |\n\nTable 6: Impact of merging period on Model Stock. IN denotes ImageNet accuracy.\n\n| Period     |   IN |   Avg. Shifts |\n|------------|------|---------------|\n| 1000 iters | 79.8 |          48.7 |\n| 5000 iters | 79.9 |          48.5 |\n| 1 epoch    | 80.1 |          48.8 |\n\naccuracy on ImageNet, which is a state-of-the-art level. Model Stock also shows robust performance across diverse distribution shift scenarios.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 45,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-46",
      "content": "|          48.5 |\n| 1 epoch    | 80.1 |          48.8 |\n\naccuracy on ImageNet, which is a state-of-the-art level. Model Stock also shows robust performance across diverse distribution shift scenarios.\n\nCLIP ViT-L/14. Table 4 shows the results of Model Stock on the CLIP ViT-L/14 model. The results show that Model Stock can push the limit of benchmark scores with large-size backbone architecture. We remark that Model Stock ⋆ achieves state-of-the-art performance with 87.7% ImageNet top-1 accuracy, implying that Model Stock is still effective in a scaleup scenario. The results consistently demonstrate the high efficacy and robustness of Model Stock across diverse scales of models and vari-\n\nTable 4: Model Stock on CLIP ViT-L/14.\n\n|               |   IN |   Avg. shifts |\n|---------------|------|---------------|\n| Zero-shot     | 75   |          63   |\n| Vanilla FT    | 85.8 |          66.8 |\n| Vanilla FT ⋆  | 87.1 |          68   |\n| TPGM [36]     | 87   |          69.4 |\n| CAR-FT [27]   |",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 46,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-47",
      "content": "-|---------------|\n| Zero-shot     | 75   |          63   |\n| Vanilla FT    | 85.8 |          66.8 |\n| Vanilla FT ⋆  | 87.1 |          68   |\n| TPGM [36]     | 87   |          69.4 |\n| CAR-FT [27]   | 87.1 |          67.8 |\n| Model Stock   | 87   |          71.6 |\n| Model Stock ⋆ | 87.7 |          73.5 |\n\nous benchmark scenarios, reaffirming its potential in practical applications.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.2 Main Results",
        "chunkIndex": 47,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-48",
      "content": "We conduct ablation studies on CLIP ViT-B/32. We train vanilla fine-tuned models and Model Stock for 16 and 8 epochs, respectively; thus, the training cost of Model Stock matches with a single fine-tuning process.\n\nExperiments on the number of fine-tuned models N . Table 5 shows the effect of the number of fine-tuned models. The results show that Model Stock obtains enhanced performance and closer distance from the (pseudo-) center ( ∥ w -µ ∥ ) as the number of merging models increases. Considering the trade-off between the performance and training cost induced by increased N , our setting ( N =2) shows the best for Model Stock.\n\nStudy on the merging period of Model Stock. Table 6 shows the results of various merging periods, including 1000 and 5000 iterations settings. Note that 1 epoch is ∼ 2500 iterations in our experiment. Model Stock shows consistent performance with various periods.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.3 Ablation studies and analysis of Model Stock",
        "chunkIndex": 48,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-49",
      "content": "esults of various merging periods, including 1000 and 5000 iterations settings. Note that 1 epoch is ∼ 2500 iterations in our experiment. Model Stock shows consistent performance with various periods.\n\nThe post-training merging strategy of Model Stock. We study an alternative of Model Stock that merges fine-tuned weights only once after each finetuning process is finished, similar to Model Soups [40]. We denote it as Model\n\nTable 7: Post-training merging strategy of Model Stock. We present ImageNet accuracy, distribution shifts, and distance from the center with the results of uniform averaging, a straightforward baseline.\n\n| Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Model Stock (post-training)   | Model Stock (post-training)   | Model Stock (post-training)   |\n|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.3 Ablation studies and analysis of Model Stock",
        "chunkIndex": 49,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-50",
      "content": "Stock (post-training)   | Model Stock (post-training)   |\n|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n|                               | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     |\n| N =2                          | 80.2                          | 47.8                          | 9.19                          | 80.3 (+0.1)                   | 50.4 (+2.6)                   | 7.62 (-1.57)                  |\n| N =3                          | 80.4                          | 48.2                          | 7.44                          | 80.4 (+0.0)                   | 50.2 (+2.0)                   | 6.49 (-0.95)                  |\n| N =4                          | 80.5",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.3 Ablation studies and analysis of Model Stock",
        "chunkIndex": 50,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-51",
      "content": "| 48.2                          | 7.44                          | 80.4 (+0.0)                   | 50.2 (+2.0)                   | 6.49 (-0.95)                  |\n| N =4                          | 80.5                          | 48.5                          | 5.63                          | 80.5 (+0.0)                   | 49.8 (+1.4)                   | 5.16 (-0.47)                  |\n\nStock (post-training). We utilize the individually fine-tuned weights as we conducted in §2, using the same training settings with different random seeds for each model. We report the performance and distance from the pseudo-center of Model Stock (post-training) in Table 7. On the left side of the table, we provide the performance of its counterpart, a uniform averaging of N models ( w N avg ). The improvements from the uniform averaging to Model Stock (post-training) are denoted in the table's parentheses.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.3 Ablation studies and analysis of Model Stock",
        "chunkIndex": 51,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-52",
      "content": "vide the performance of its counterpart, a uniform averaging of N models ( w N avg ). The improvements from the uniform averaging to Model Stock (post-training) are denoted in the table's parentheses. The results show that Model Stock (posttraining) archives improved distribution shift scores with closer distances toward the center than its counterpart.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "4.3 Ablation studies and analysis of Model Stock",
        "chunkIndex": 52,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-53",
      "content": "We discuss related works and highlight how our method differs and contributes to the existing works.\n\nModel Soups [40] is a straightforward weight averaging method that merges weights from various fine-tuned models trained with different hyper-parameters. It demonstrates improved in-distribution (ID) and out-of-distribution (OOD) performance. While effective, model soups typically require a large number of fine-tuned models. Our method aims to achieve similar or superior performance improvements more efficiently, utilizing significantly fewer fine-tuning costs. We provide further discussion about Model Soups with our new interpretation in Appendix E.\n\nRobust Fine-tuning. When fine-tuning generalist models like CLIP [30], we often observe the fine-tuned models lose the generalization ability of the original ones, with decreased OOD performance. To address this issue, several robust fine-tuning approaches have been proposed.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "5 Related Work",
        "chunkIndex": 53,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-54",
      "content": "ten observe the fine-tuned models lose the generalization ability of the original ones, with decreased OOD performance. To address this issue, several robust fine-tuning approaches have been proposed. LP-FT [18] attempts to preserve pretrained weights by initially training only a linear probing layer. WiSE-FT [41] improves OOD performance through linear interpolation between fine-tuned and pre-trained weights. While our method shares similarities with WiSE-FT in using pre-trained weights, our method determines interpolation ratios layer-wise based on geometric properties. Focusing on OOD performance, methods suggesting improved training objectives [7,27-29,36,37] have been proposed. Our approach differs from these methods as we do not propose a new fine-tuning loss. Instead, we perform two fine-tunings and achieve robust performance by merging them.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "5 Related Work",
        "chunkIndex": 54,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-55",
      "content": "-29,36,37] have been proposed. Our approach differs from these methods as we do not propose a new fine-tuning loss. Instead, we perform two fine-tunings and achieve robust performance by merging them.\n\nWeight Center and Flat minima. Recent machine learning research has extensively explored the significance of finding flat minima for improved generalization [3, 15, 21, 26]. Keskar et al . [16] and Hochreiter &amp; Schmidhuber [12] demonstrated that sharp optima by large batch SGD have steep, harmful directions, while broader optima enhance generalization. Stochastic Weight Averaging (SWA) [15] targets the center of flat minima, enhancing robustness against shifts in the loss landscape between training and test datasets. SWAG [26] builds on SWA by incorporating Bayesian model averaging with a Gaussian posterior to further boost performance. SWAD [3] found that the generalization gap between flat and sharp minima is more pronounced in OOD scenarios than in ID ones.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "5 Related Work",
        "chunkIndex": 55,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-56",
      "content": "an model averaging with a Gaussian posterior to further boost performance. SWAD [3] found that the generalization gap between flat and sharp minima is more pronounced in OOD scenarios than in ID ones. Our method theoretically extends these approaches by efficiently identifying the center of flat minima with novel geometric properties, leading to significant improvements in both ID and OOD performances.\n\nModel Weight Merging. Recent research has explored merging models finetuned on various tasks. Methods such as Task Arithmetic [14] and TIES [42] have been proposed. They are also based on the difference between fine-tuned and pre-trained weights (often referred to as the 'task vector'). However, our method distinguishes itself through geometric analysis for weight merging. In the domain of Large Language Models (LLMs), merging techniques like WARM [31] and WARP [32] have emerged. Our method has the potential for extension to these areas, offering new avenues for future research.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "5 Related Work",
        "chunkIndex": 56,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-57",
      "content": "Large Language Models (LLMs), merging techniques like WARM [31] and WARP [32] have emerged. Our method has the potential for extension to these areas, offering new avenues for future research.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "5 Related Work",
        "chunkIndex": 57,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-58",
      "content": "Our study illuminated the fine-tuning process in machine learning, revealing that fine-tuned models' weights generally exhibit the properties of a Gaussian distribution. The proximity of these models to the center of weights was crucial for improved performance in target domains like ImageNet and under diverse distribution shifts. Utilizing a pre-trained model as a robust anchor point, we efficiently minimized the variance with fewer fine-tuned models, eliminating the need for additional training to find the optimal interpolation ratio.\n\nAdditionally, our findings suggested further knowledge and applicability to the models near flat minima and will offer new insights on model weight merging methods. As the pretraining-finetuning paradigm gains more prominence, our insights will provide a foundation for better understanding and optimizing the fine-tuning process in both academia and industry.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 58,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-59",
      "content": "ods. As the pretraining-finetuning paradigm gains more prominence, our insights will provide a foundation for better understanding and optimizing the fine-tuning process in both academia and industry.\n\nLimitation. Due to resource limitations, we could not conduct larger-scale models such as ViT-G. Exploring this will be part of our future work.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 59,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-60",
      "content": "We thank the researchers at NAVER AI Lab for their valuable comments. This work was supervised by Sangdoo Yun and Dongyoon Han. Dong-Hwan Jang is currently at Samsung Advanced Institute of Technology (SAIT).",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "7 Acknowledgment",
        "chunkIndex": 60,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-61",
      "content": "1. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.: Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. NeurIPS (2019)\n2. Beyer, L., Hénaff, O.J., Kolesnikov, A., Zhai, X., Oord, A.v.d.: Are we done with imagenet? arXiv preprint arXiv:2006.07159 (2020)\n3. Cha, J., Chun, S., Lee, K., Cho, H.C., Park, S., Lee, Y., Park, S.: Swad: Domain generalization by seeking flat minima. NeurIPS 34 , 22405-22418 (2021)\n4. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: CVPRW. pp. 702-703 (2020)\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)\n6.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-62",
      "content": "hai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)\n6. Gouk, H., Hospedales, T.M., Pontil, M.: Distance-based regularisation of deep networks for fine-tuning. arXiv preprint arXiv:2002.08253 (2020)\n7. Goyal, S., Kumar, A., Garg, S., Kolter, Z., Raghunathan, A.: Finetune like you pretrain: Improved finetuning of zero-shot vision models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1933819347 (2023)\n8. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., Neubig, G.: Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 (2021)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770-778 (2016)\n10.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-63",
      "content": "of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 (2021)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770-778 (2016)\n10. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al.: The many faces of robustness: A critical analysis of out-of-distribution generalization. In: ICCV. pp. 8340-8349 (2021)\n11. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial examples. In: CVPR. pp. 15262-15271 (2021)\n12. Hochreiter, S., Schmidhuber, J.: Flat minima. Neural computation 9 (1), 1-42 (1997)\n13. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n14. Ilharco, G., Ribeiro, M.T., Wortsman, M., Schmidt, L., Hajishirzi, H., Farhadi, A.: Editing models with task arithmetic.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-64",
      "content": "adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n14. Ilharco, G., Ribeiro, M.T., Wortsman, M., Schmidt, L., Hajishirzi, H., Farhadi, A.: Editing models with task arithmetic. In: The Eleventh International Conference on Learning Representations (2023), https://openreview.net/forum?id=6t0Kwf8jrj\n15. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.G.: Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 (2018)\n16. Keskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M., Tang, P.T.P.: On largebatch training for deep learning: Generalization gap and sharp minima. In: International Conference on Learning Representations (2017), https://openreview. net/forum?id=H1oyRlYgg\n17. Krizhevsky, A.: Learning multiple layers of features from tiny images. In: Tech Report (2009)\n18.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-65",
      "content": "national Conference on Learning Representations (2017), https://openreview. net/forum?id=H1oyRlYgg\n17. Krizhevsky, A.: Learning multiple layers of features from tiny images. In: Tech Report (2009)\n18. Kumar, A., Raghunathan, A., Jones, R., Ma, T., Liang, P.: Fine-tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054 (2022)\n\n19. Ledoux, M.: The concentration of measure phenomenon. No. 89, American Mathematical Soc. (2001)\n20. Lee, Y., Chen, A.S., Tajwar, F., Kumar, A., Yao, H., Liang, P., Finn, C.: Surgical fine-tuning improves adaptation to distribution shifts. In: ICLR (2022)\n21. Li, H., Xu, Z., Taylor, G., Studer, C., Goldstein, T.: Visualizing the loss landscape of neural nets. NeurIPS 31 (2018)\n22. Li, T., Huang, Z., Tao, Q., Wu, Y., Huang, X.: Trainable weight averaging: Efficient training by optimizing historical solutions. In: ICLR (2022)\n23.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-66",
      "content": "ss landscape of neural nets. NeurIPS 31 (2018)\n22. Li, T., Huang, Z., Tao, Q., Wu, Y., Huang, X.: Trainable weight averaging: Efficient training by optimizing historical solutions. In: ICLR (2022)\n23. Lian, D., Zhou, D., Feng, J., Wang, X.: Scaling &amp; shifting your features: A new baseline for efficient model tuning. NeurIPS 35 , 109-123 (2022)\n24. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: CVPR. pp. 11976-11986 (2022)\n25. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n26. Maddox, W.J., Izmailov, P., Garipov, T., Vetrov, D.P., Wilson, A.G.: A simple baseline for bayesian uncertainty in deep learning. NeurIPS 32 (2019)\n27. Mao, X., Chen, Y., Jia, X., Zhang, R., Xue, H., Li, Z.: Context-aware robust finetuning. IJCV (12 2023). https://doi.org/10.1007/s11263-023-01951-2\n28.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-67",
      "content": "ertainty in deep learning. NeurIPS 32 (2019)\n27. Mao, X., Chen, Y., Jia, X., Zhang, R., Xue, H., Li, Z.: Context-aware robust finetuning. IJCV (12 2023). https://doi.org/10.1007/s11263-023-01951-2\n28. Nam, G., Heo, B., Lee, J.: Lipsum-ft: Robust fine-tuning of zero-shot models using random text guidance. In: The Twelfth International Conference on Learning Representations (2024)\n29. Oh, C., Lim, H., Kim, M., Han, D., Yun, S., Choo, J., Hauptmann, A., Cheng, Z.Q., Song, K.: Towards calibrated robust fine-tuning of vision-language models (2024), https://arxiv.org/abs/2311.01723\n30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n31. Rame, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem, O., Ferret, J.: WARM: On the benefits of weight averaged reward models.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-68",
      "content": "from natural language supervision. In: ICML (2021)\n31. Rame, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem, O., Ferret, J.: WARM: On the benefits of weight averaged reward models. In: Forty-first International Conference on Machine Learning (2024), https://openreview.net/ forum?id=s7RDnNUJy6\n32. Ramé, A., Ferret, J., Vieillard, N., Dadashi, R., Hussenot, L., Cedoz, P.L., Sessa, P.G., Girgin, S., Douillard, A., Bachem, O.: Warp: On the benefits of weight averaged rewarded policies (2024), https://arxiv.org/abs/2406.16768\n33. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML (2019)\n34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition challenge. IJCV 115 (3), 211-252 (2015)\n35.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-69",
      "content": ", H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition challenge. IJCV 115 (3), 211-252 (2015)\n35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. pp. 19 (2015)\n36. Tian, J., He, Z., Dai, X., Ma, C.Y., Liu, Y.C., Kira, Z.: Trainable projected gradient method for robust fine-tuning. In: CVPR. pp. 7836-7845 (2023)\n37. Tian, J., Liu, Y.C., Smith, J.S., Kira, Z.: Fast trainable projection for robust finetuning. In: NeurIPS (2023)\n38. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers &amp;distillation through attention. In: ICML. vol. 139, pp. 10347-10357 (July 2021)\n39. Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning robust global representations by penalizing local predictive power. In: NeurIPS (2019)",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-70",
      "content": "attention. In: ICML. vol. 139, pp. 10347-10357 (July 2021)\n39. Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning robust global representations by penalizing local predictive power. In: NeurIPS (2019)\n\n40. Wortsman, M., Ilharco, G., Gadre, S.Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A.S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.: Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In: ICML. pp. 23965-23998. PMLR (2022)\n41. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.: Robust fine-tuning of zeroshot models. In: CVPR. pp. 7959-7971 (2022)\n42. Yadav, P., Tam, D., Choshen, L., Raffel, C., Bansal, M.: TIES-merging: Resolving interference when merging models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id= xtaX3WyCj1\n43.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-71",
      "content": "C., Bansal, M.: TIES-merging: Resolving interference when merging models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id= xtaX3WyCj1\n43. Zaken, E.B., Ravfogel, S., Goldberg, Y.: Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021)\n44. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. ICLR (2018)",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-72",
      "content": "In this Appendix, we provide in-depth analysis and additional insights to complement the main text of our study on Model Stock, our novel approach to fine-tuning and weight merging. The contents are summarized as follows:\n\n- -We examine the angle norm consistency of fine-tuned weights across various settings in §A, extending the observations discussed in §2.1.\n- -We provide detailed proofs of geometric properties of fine-tuned weights in §B.\n- -We study the importance of reducing variance for performance in out-ofdistribution scenarios in §C, showcasing the test error landscape across various datasets and elaborating on the explanations in §2.2.\n- -We provide detailed proofs in §D for the optimal interpolation ratio in our method §3.\n- -We discuss prior studies through the lens of our findings in §E.\n- -We provide an additional analysis of the interpolation ratio in §F.\n- -We present experimental settings of §4 in §G.\n- -We present additional experiments of Model Stock in §H",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Appendix",
        "chunkIndex": 72,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-73",
      "content": "of our findings in §E.\n- -We provide an additional analysis of the interpolation ratio in §F.\n- -We present experimental settings of §4 in §G.\n- -We present additional experiments of Model Stock in §H\n\nEach section aims to offer a comprehensive understanding of our method's underlying principles and its broad applicability in machine learning.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "Appendix",
        "chunkIndex": 73,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-74",
      "content": "We argue that, as discussed in §2.1, angles and norms of fine-tuned weights would remain consistent across fine-tuned models, independent of various factors. These factors include architecture type (ViTs [5], ResNet [9], ConvNeXt [24]), optimizers (SGD, AdamW [25]), augmentations (RRC [35], RandAug [4]), datasets (CIFAR [17], ImageNet [34]), or the initialization of the classifier (zero-shot, LP as in LP-FT [18]). We depict the layer-wise angle and norm of 5 fine-tuned weights for each category based on different random seeds. We give detailed illustrations for each setting at the end of the Appendix to enhance readability (refer to Fig. H-O). Across all these settings, the angle and norm of weights exhibit a surprising level of consistency.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A Angle and Norm Consistency",
        "chunkIndex": 74,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-75",
      "content": "The layer-wise angle and norm across various settings are shown in Fig. HL. We visualize with every weight of attentions/convolutions (Attention/Conv), multi-layer perceptrons (MLP), normalizations (LayerNorm and BatchNorm), a classifier (Classifier), individual bias (Bias), and the remaining layers ( i.e ., the patchification layer, positional embedding, class embedding, and projection layer). We further display All in each figure, which denotes the concatenation of the weights of entire layers.\n\nThe layer-wise analysis reveals an interesting trend: Bias and classifier layers demonstrate smaller angles than attention and MLP layers. In other words, bias and classifier layers exhibit lower randomness and more reliable updates than attention and MLP layers. It is important to note that as the angle decreases, the pre-trained model is less utilized for merging (refer to Eq. (2).",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.1 Analysis on layer-wise tendency",
        "chunkIndex": 75,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-76",
      "content": "bit lower randomness and more reliable updates than attention and MLP layers. It is important to note that as the angle decreases, the pre-trained model is less utilized for merging (refer to Eq. (2). This indicates that bias and classifier layers focus more on fine-tuned models and rely less on the pre-trained model, whereas attention and MLP layers depend less on the finetuned model ( i.e ., t bias , t clf &gt; t attn , t mlp ). This observation extends the findings of previous works such as BitFit [43] and LP-FT [18]. In the case of BitFit and LP ( i.e ., the first step of LP-FT), bias and classifier layers fully utilize fine-tuning, while other layers (attention and MLP) rely on pre-trained models.\n\nThese traits could offer new insights into parameter-efficient transfer learning (PETL) [8,13,23,43] and layer-wise fine-tuning [18, 20, 36, 37].",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.1 Analysis on layer-wise tendency",
        "chunkIndex": 76,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-77",
      "content": "ayers (attention and MLP) rely on pre-trained models.\n\nThese traits could offer new insights into parameter-efficient transfer learning (PETL) [8,13,23,43] and layer-wise fine-tuning [18, 20, 36, 37]. Maintaining weights with high randomness (higher angles) while updating on biases and classifier weights with lower randomness and fewer parameters would be an efficient fine-tuning strategy. PETL has been exploring this direction but has not yet provided solid reasons why certain layers are more effective than others. Our analysis suggests that one reason could be the lower randomness (or variance) of these layers, as indicated by the angle trend per layer.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.1 Analysis on layer-wise tendency",
        "chunkIndex": 77,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-78",
      "content": "We further argue that the consistency we observed is maintained while training progresses, as illustrated by multiple thin shells in Fig. 5. To demonstrate that the angle and norm of fine-tuned models remain consistent during the entire training process, we plot their relationship across weights for every epoch in Fig. M. Please note that the angle is consistent across differently seeded models at the same timestamp ( i.e ., w 1 | t = t 1 and w 2 | t = t 1 ), not across models at different timestamps ( i.e ., w 1 | t = t 1 and w 1 | t = t 2 ). The observed trend is as follows: as training progresses, the angle between weights steadily decreases. This analysis uses the CLIP ViT-B/32 model fine-tuned on ImageNet-1K with five random seeds.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.2 Maintaining consistency during training",
        "chunkIndex": 78,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-79",
      "content": "Li et al . [21] showed that when evaluating the robustness of a neural network by adding random noise to certain weights, performance analysis based on adding filter-wise noise ( i.e ., adding noise for each row in all weight matrices) aligns more closely with the generalization performance than adding layer-wise noise does. Inspired by this observation, we investigate the possibility that the weight distribution may follow a filter-wise Gaussian distribution and adapt this concept to our method (see the performance analysis in §H.5). Fig. N illustrates the angle distribution filter-wise. The angle exhibits much larger standard deviations than the layer-wise distribution. This could be attributed to the reduction in dimensionality. As the number of dimensions decreases, it becomes challenging to approximate the norm as a constant value.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.3 Filter-wise analysis of weights",
        "chunkIndex": 79,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-80",
      "content": "To verify if this key observation also applies to non-CLIP models, we analyze the geometric patterns of fine-tuned weights trained using the DeiT [38] method ( i.e ., pre-trained on ImageNet-21K). Fig. O displays the angle and norm of 10 DeiT-base models first pre-trained on ImageNet-21K [34] and then fine-tuned on ImageNet-1K. We find that weights pre-trained with ImageNet-21K also exhibit consistent angle and norm , indicating that our observation may be valid beyond CLIP fine-tuning scenarios as well.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "A.4 Analysis on non-CLIP models",
        "chunkIndex": 80,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-81",
      "content": "For all indices i, j within the set [1 , N ] , where N denotes the sufficiently large number of fine-tuned weights, we derive one lemma and three propositions based on the foundational observation described in Eq. (1):\n\nLemma: w i · µ = µ · µ = l 2 cos θ .\n\nProof:\n\n<!-- formula-not-decoded -->\n\nSimilarly,\n\n<!-- formula-not-decoded -->\n\nProposition 1: ∥ w i -µ ∥ = constant. Proof:\n\n<!-- formula-not-decoded -->\n\nProposition 2: ( w 0 -µ ) ⊥ ( w i -µ ) .\n\nProof:\n\n<!-- formula-not-decoded -->\n\nProposition 3: ( w i -µ ) ⊥ ( w j -µ ) .\n\nFig. A: Test error landscape on OOD datasets. We depict the test error landscape on ImageNet-V2, -Sketch, ObjectNet, ImageNet-R, and -A (from left to right, from top to bottom, respectively) on the plane containing pre-trained model ( w 0 ), fine-tuned model ( w 1 ), and the pseudo-center of fine-tuned weights ( w (50) avr ). The local optima for the OOD datasets always lie on the line segment w 0 w (50) avr .\n\n<!-- image -->\n\nProof:",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "B Detailed Proof for Geometric Properties of Fine-tuned Weights",
        "chunkIndex": 81,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-82",
      "content": "0 ), fine-tuned model ( w 1 ), and the pseudo-center of fine-tuned weights ( w (50) avr ). The local optima for the OOD datasets always lie on the line segment w 0 w (50) avr .\n\n<!-- image -->\n\nProof:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "B Detailed Proof for Geometric Properties of Fine-tuned Weights",
        "chunkIndex": 82,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-83",
      "content": "In demonstrating the significance of variance reduction for robustness in out-ofdistribution (OOD) scenarios, we analyze the test error landscape as in §2.2. As shown in Fig. A, we examine the error landscape across various OOD datasets, including ImageNet-V2, ImageNet-Sketch, ObjectNet, ImageNet-R, and ImageNetA (from top to bottom). This landscape is plotted on a plane defined by the weights of a pre-trained model ( w 0 ), a fine-tuned model ( w 1 ), and the center of the fine-tuned weights, which is approximated by averaging 50 fine-tuned weights ( w (50) avr ). A notable pattern emerges where the local optima for these datasets consistently align with the line segment connecting w 0 and w (50) avr .\n\nThough the exact location of local minima differs depending on the dataset type, it has a common point that the minima are aligned on the line between the weight center and pre-trained model rather than the line between the fine-tuned weight and pre-trained model.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "C Importance of Reducing Weight Variance on Performance under Distribution Shifts",
        "chunkIndex": 83,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-84",
      "content": "he dataset type, it has a common point that the minima are aligned on the line between the weight center and pre-trained model rather than the line between the fine-tuned weight and pre-trained model. Consequently, not only does the averaged weight exhibit higher performance on distribution shifts compared to the fine-tuned model, but the WiSE-FT [41] curves corresponding to the averaged weights also demonstrate better ID/OOD trade-off than the WiSE-FT curve of the fine-tuned model, as illustrated in Fig. B. This indicates the importance of getting closer to the weight center, even for OOD datasets.\n\nFig. B: ID vs. OOD accuracy along WiSE-FT [41] curves for averaged models. As the number of weights used for averaging increases, the corresponding WiSE-FT curves demonstrate improvements in the ID-OOD trade-off.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "C Importance of Reducing Weight Variance on Performance under Distribution Shifts",
        "chunkIndex": 84,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-85",
      "content": "along WiSE-FT [41] curves for averaged models. As the number of weights used for averaging increases, the corresponding WiSE-FT curves demonstrate improvements in the ID-OOD trade-off.\n\n<!-- image -->\n\nAnother interesting point is that depending on the traits of datasets, the position of local minima differs. ImageNet-V2 has a similar dataset distribution to ImageNet since it shares the same data collection and categorization policy, and its local optima lies close to that of ImageNet. On the other hand, on the datasets with harsh variations ( e.g ., ImageNet-A), the local minima are positioned much closer to the pre-trained model than the original ImageNet or ImageNet-V2. This loss landscape gives an intuitive insight into the similarity between OOD datasets and ImageNet.\n\nIn conclusion, there is no universal interpolation ratio optimal for every distribution shift. However, all the local minima lie on the line between the weight center and the pre-trained model.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "C Importance of Reducing Weight Variance on Performance under Distribution Shifts",
        "chunkIndex": 85,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-86",
      "content": "eNet.\n\nIn conclusion, there is no universal interpolation ratio optimal for every distribution shift. However, all the local minima lie on the line between the weight center and the pre-trained model. This implies the importance of proximity to the weight center in achieving a better WiSE-FT line.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "C Importance of Reducing Weight Variance on Performance under Distribution Shifts",
        "chunkIndex": 86,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-87",
      "content": "Here, we present detailed proof of Model Stock introduced in §3. We first show the case with two fine-tuned models and extend our proof toward N fune-tuned models.\n\nOn two fine-tuned models. We will prove step-by-step how the optimal interpolation ratio t in Eq. (2) in the main paper is derived. Using the same notation as in §3, we denote the magnitude and the angle between the fine-tuned weights as l and θ , respectively. Starting from the fact that △ µ w 1 w 2 is a right isosceles triangle, we can derive the following relations from Fig. C:\n\nFig. C: Model Stock with two fine-tuned models. We reference the illustration in Fig. 6 to more understandably substantiate merging two fine-tuned models.\n\n<!-- image -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n⊔ ⊓\n\nInterestingly, w H is located at an orthocenter of the triangle △ w 0 w 1 w 2 with the given optimal ratio t .",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "D Detailed Proof of Model Stock",
        "chunkIndex": 87,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-88",
      "content": "formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n⊔ ⊓\n\nInterestingly, w H is located at an orthocenter of the triangle △ w 0 w 1 w 2 with the given optimal ratio t .\n\nFig. D: Model Stock with N fine-tuned models and Interpolation Ratio Variation. (a) We visualize a special case of N = 3 (tetrahedron) for better understanding. (b) The trend towards t = 1 with increasing N illustrates that w ( N ) H on the N -dimensional simplex approaches w ( N ) avr , reflecting a growing dependence on the number of fine-tuned models.\n\n<!-- image -->\n\nOn N fine-tuned models. Similarly, we can derive a more generalized interpolation ratio for N ≥ 2 . Our goal is to find the weight w ( N ) avr that is on the hyper-plane spanned by w 0 , w 1 , . . . , w N and closest to the weight center µ , as described in Fig. Da. Again, for simplicity, we treat w 0 as the origin O .\n\nBased on the observation, we presume that the following two conditions hold:",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "D Detailed Proof of Model Stock",
        "chunkIndex": 88,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-89",
      "content": ", w N and closest to the weight center µ , as described in Fig. Da. Again, for simplicity, we treat w 0 as the origin O .\n\nBased on the observation, we presume that the following two conditions hold:\n\n<!-- formula-not-decoded -->\n\nThe first condition comes from the symmetry of an N -simplex structure, and the second condition holds since the orthogonal projection is the minimal distance from µ . Then, we can derive t as follows:\n\nBy substituting the first condition into the second condition from Eq. (8),\n\n<!-- formula-not-decoded -->\n\nFig. E: Ensembling impact disappears when interpolating between two averaged weights. We plot the ImageNet performance of interpolated weights between two selected fine-tuned models in Model Soup [40] (left) and between their corresponding weight centers (right).\n\n<!-- image -->\n\nNote that the norm of the N -averaged fine-tuned weights can be derived as follows:\n\n<!-- formula-not-decoded -->\n\nwhile the term µ · w ( N ) avr can be simplified as",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "D Detailed Proof of Model Stock",
        "chunkIndex": 89,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-90",
      "content": "centers (right).\n\n<!-- image -->\n\nNote that the norm of the N -averaged fine-tuned weights can be derived as follows:\n\n<!-- formula-not-decoded -->\n\nwhile the term µ · w ( N ) avr can be simplified as\n\n<!-- formula-not-decoded -->\n\nBy substituting Eq. (10) and Eq. (11) into Eq. (9), we can finally derive the optimal interpolation ratio t as follows:\n\n<!-- formula-not-decoded -->\n\nFig. Db displays how the optimal interpolation ratio t varies as a function of θ with different numbers of fine-tuned models. As N increases, t trends towards 1, indicating that w ( N ) H on the N -dimensional simplex gets closer to w ( N ) avr . This shows increasing dependence on fine-tuned models as their number grows.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "D Detailed Proof of Model Stock",
        "chunkIndex": 90,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-91",
      "content": "In this section, we extend our findings to reinterpret the underlying mechanics in prior studies, WiSE-FT [41] and Model Soups [40], through a consistent rationale to illuminate their effectiveness.\n\nWiSE-FT [41] is a state-of-the-art robust fine-tuning method for CLIP-based models. It demonstrates that linearly combining weights of the pre-trained and fine-tuned models achieves a significant accuracy gain on distribution shifts. We argue that the WiSE-FT model's superiority over a fine-tuned model can be interpreted by its weights being closer to the center of the corresponding weight distribution. Fig. 3 already showed fine-tuned models typically lie on the periphery of flat minima. Given that the angle ∠ w 0 w (50) avr w 1 is nearly a right angle, along the line w 0 w 1 , multiple weight points are closer to the center than a single fine-tuned model, thereby enhancing performance. Note that w H is the closest to the center among the line w 0 w 1 .",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 91,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-92",
      "content": "ng the line w 0 w 1 , multiple weight points are closer to the center than a single fine-tuned model, thereby enhancing performance. Note that w H is the closest to the center among the line w 0 w 1 . More discussions on performance boosts observed in distribution shifts are provided in the Appendix C.\n\nModel Soup [40] merges various fine-tuned models' weights trained from varied hyper-parameters. It has been credited with delivering enhanced performance across ImageNet and distribution shifts. Here, we interpret the performance improvements of Model Soup as the result of the proximity to the center of weight distribution. Consider two weight vectors, w A and w B , fine-tuned with different hyper-parameters and following Gaussian distribution N ( µ A , Σ A ) and N ( µ B , Σ B ) respectively. Then, the interpolated weight vector w AB = t · w A + (1 -t ) · w B also follows a Gaussian distribution N ( µ AB , Σ AB ) .",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 92,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-93",
      "content": "ing Gaussian distribution N ( µ A , Σ A ) and N ( µ B , Σ B ) respectively. Then, the interpolated weight vector w AB = t · w A + (1 -t ) · w B also follows a Gaussian distribution N ( µ AB , Σ AB ) . The expected squared distance from the interpolated weight vector to its mean µ AB is minimized to trace ( Σ A ) trace ( Σ B ) trace ( Σ A )+ trace ( Σ B ) when t is chosen to trace ( Σ B ) trace ( Σ A )+ trace ( Σ B ) , indicating the reduction of variance through weight interpolation ( i.e ., the distance between w AB and µ AB might be closer than each weight's distance). For example, if trace ( Σ B ) is equal to trace ( Σ A ) , this minimum squared distance is exactly half of the sum of the individual traces when t = 0 . 5 . This insight suggests that the performance gains realized by Model Soup could be due to reduced variance resulting from merging numerous weights.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 93,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-94",
      "content": "f of the sum of the individual traces when t = 0 . 5 . This insight suggests that the performance gains realized by Model Soup could be due to reduced variance resulting from merging numerous weights.\n\nWe set up a toy experiment to evaluate the effect of variance reduction in the Model Soup scenario by comparing the interpolation of fine-tuned weights with the interpolation of their corresponding weight centers, when N = 2 . In the former case, variance reduction exists along with the effect of merging diverse hyper-parameters, while in the latter case, performance gain would only come from hyper-parameter diversity. If the diversity of hyper-parameters is a major factor, the performance gain from interpolation of central weights should remain the same. To test this, we assessed the ImageNet performance of interpolated weights between pairs of fine-tuned models within Greedy Model Soup 5 [40] and compared it to interpolations between their central weights, calculated as the average of",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 94,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-95",
      "content": "et performance of interpolated weights between pairs of fine-tuned models within Greedy Model Soup 5 [40] and compared it to interpolations between their central weights, calculated as the average of 20 differently seeded models. Fig. E shows that, unlike interpolations between individual models, using the centers does not significantly improve performance. This suggests that proximity to the center of the weight distribution may play a more critical role than hyper-parameter diversity in weight ensemble methods in this case.\n\n5 We opt for Greedy Model Soup to show that even the interpolation of models from the best merging combination does not benefit from the impact of weight diversity.\n\nFig. F: Trend of interpolation ratio t during Model Stock training.\n\n<!-- image -->\n\nIt is also worth noting that µ AB always surpasses the performance of w AB for the same interpolation ratio t , indicating that the importance of proximity to the center remains consistent for interpolated weights.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 95,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-96",
      "content": "th noting that µ AB always surpasses the performance of w AB for the same interpolation ratio t , indicating that the importance of proximity to the center remains consistent for interpolated weights. With extensive future research, this understanding could provide valuable insights for developing more generalizable and effective weight-merging techniques.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "E Discussion - Rethinking Pivotal Prior Studies",
        "chunkIndex": 96,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-97",
      "content": "We analyze the interpolation ratio t = 2 cos θ 1+cos θ in a layer-wise manner. During a Model Stock experiment on CLIP ViT-B/32 with 16 epoch training, we log the layer-wise merge ratios at every merging period. Figure F visualizes the averaged interpolation ratio during Model Stock training. We plot two trends of the interpolation ratio for the layer depth and training step. Our overall observation indicates the bias layers have high merge ratios t ( ≃ 1 ) with small angles θ ( ≃ 0 ), implying that the bias layers do not need to enjoy the pre-trained model, similar to our discussion in §2 and §3. Focusing on the weight layers, Figure Fa shows a U-shape tendency as the layer depth increases, implying the weights of intermediate layers can be more diverse ( i.e ., larger angle θ ) than those of early and later layers.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "F Analysis of the interpolation ratio t",
        "chunkIndex": 97,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-98",
      "content": "layers, Figure Fa shows a U-shape tendency as the layer depth increases, implying the weights of intermediate layers can be more diverse ( i.e ., larger angle θ ) than those of early and later layers. Our intuition here is that since the early and later layers are directly connected to input data and output labels, respectively, they may not demand the advantage of the pre-trained weight. Figure Fb presents that the models at the early training stage are more diverse and they enjoy the pre-trained weights more than those of the later training stage. As the model approaches convergence, the diversity of fine-tuning models decreases ( i.e ., smaller angle θ ).",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "F Analysis of the interpolation ratio t",
        "chunkIndex": 98,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-99",
      "content": "Here, we present detailed setups for the experiments in §4. We utilize AdamW optimizer [25] with a weight decay of 0.1. We employ two training setups for Model Stock. The first is training Model Stock with a learning rate of 3 × 10 -5 in 10 epochs with minimal data augmentation. The minimal data augmentation utilizes random resize crop augmentation with a minimum crop ratio of 0 . 9 ,\n\nFig. G: Results on LP initialization . We plot in-distribution ImageNet accuracy (x-axis) and distribution shift results (y-axis) with individual fine-tuned models (gray circles) and Model Soups [40]. Note that Model Stock has much smaller ( 35 × smaller) computational costs than Model Soups, leveraging 71 various fine-tuned models as in the original paper.\n\n<!-- image -->\n\nmixup [44] augmentation with β =0.5, following Model Soup's 'standard grid search' setting. The other is training Model Stock with a learning rate of 2 × 10 -5 in 16 epochs with strong data augmentation.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "G Experimental setup",
        "chunkIndex": 99,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-100",
      "content": "up [44] augmentation with β =0.5, following Model Soup's 'standard grid search' setting. The other is training Model Stock with a learning rate of 2 × 10 -5 in 16 epochs with strong data augmentation. The strong data augmentation utilizes random resize crop augmentation with a minimum crop ratio of 0 . 08 and random augmentation [4] ( N = 2 , M = 10 ) following Model Soup's 'random search' setting. When experimenting with the ViT-B/16 and ViT-L/14 models, we adjusted the learning rate and batch size to accommodate the GPU memory constraints.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "G Experimental setup",
        "chunkIndex": 100,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-101",
      "content": "We present additional experimental studies to verify the effectiveness and applicability of Model Stock.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H Additional Experiments",
        "chunkIndex": 101,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-102",
      "content": "We conduct Model Stock with LP initialization and compare it with Model Soups that are initialized from LP. The results are in Fig. G. In this experiment, we use the 16-epoch training setup with strong data augmentation for training Model Stock. As shown in Fig. G, Model Stock outperforms the individual fine-tuned models 6 (gray dots) on ImageNet accuracy. Model Stock also demonstrates competitive performance against Model Soups considering WiSE-FT curves. Note that Model Stock is much more efficient ( 35 × ) than Model Soups, which utilize 71 models in this experiment.\n\n6 All the individual model checkpoints are from the official Model Soup repository.\n\nTable A: Complete results of Table 3 with ObjectNet [1] and ImageNet-ReaL [2].\n\n| Method         | In-distribution   | In-distribution   | Distribution   | Distribution   | Distribution   | Distribution   | Distribution   |\n|----------------|-------------------|-------------------|----------------|----------------|----------------|----",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.1 Experiments with LP initialization",
        "chunkIndex": 102,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-103",
      "content": "| Distribution   | Distribution   | Distribution   | Distribution   | Distribution   |\n|----------------|-------------------|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Method         | ImageNet          | IN-ReaL           | IN-V2          | IN-R           | IN-A           | IN-Sketch      | ObjectNet      |\n| Zero-shot      | 68.3              | 75.1              | 62.0           | 77.7           | 49.9           | 48.3           | 54.2           |\n| Vanilla FT     | 82.8              | 87.8              | 72.9           | 66.4           | 43.7           | 48.0           | 51.8           |\n| Vanilla FT ∗   | 83.7              | 87.8              | 73.5           | 67.6           | 40.0           | 48.6           | 50.1           |\n| LP [18]        | 79.7              | -                 | 71.5           | 52.4           | 27.8           | 40.5           | -              |\n| LP-FT [18]     | 81.7              | -",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.1 Experiments with LP initialization",
        "chunkIndex": 103,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-104",
      "content": "|\n| LP [18]        | 79.7              | -                 | 71.5           | 52.4           | 27.8           | 40.5           | -              |\n| LP-FT [18]     | 81.7              | -                 | 71.6           | 72.9           | 49.1           | 48.4           | -              |\n| CAR-FT [27]    | 83.2              | -                 | 73.0           | 71.3           | 43.7           | 49.5           | -              |\n| FTP [37]       | 84.2              | -                 | 74.6           | 47.2           | 26.5           | 50.2           | -              |\n| FLYP [7]       | 82.6              | -                 | 73.0           | 71.4           | 48.1           | 49.6           | 58.7           |\n| Lipsum-FT [28] | 83.3              | -                 | 73.6           | 75.9           | 49.9           | 51.4           | 54.4           |\n| CaRot [29]     | 83.1              | -                 | 74.1           | 77.7           | 51.6           | 52.7           | 56",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.1 Experiments with LP initialization",
        "chunkIndex": 104,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-105",
      "content": "| 75.9           | 49.9           | 51.4           | 54.4           |\n| CaRot [29]     | 83.1              | -                 | 74.1           | 77.7           | 51.6           | 52.7           | 56.6           |\n| Model Stock    | 84.1              | 88.8              | 74.8           | 71.8           | 51.2           | 51.8           | 55.0           |\n| Model Stock ⋆  | 85.2              | 89.1              | 75.3           | 68.7           | 45.0           | 51.3           | 52.3           |\n\nTable B: Comparison against Model Soups [40] on CLIP ViT-B/16. Model Stock shows comparable performance with Model Soups.\n\n| Method               |   ImageNet Avg. shifts |   ImageNet Avg. shifts |\n|----------------------|------------------------|------------------------|\n| CLIP zero-shot Init. |                   68.3 |                   58.4 |\n| Vanilla FT           |                   82.8 |                   56.6 |\n| Vanilla FT ⋆         |                   83.7 |                   55.9",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.1 Experiments with LP initialization",
        "chunkIndex": 105,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-106",
      "content": "|                   68.3 |                   58.4 |\n| Vanilla FT           |                   82.8 |                   56.6 |\n| Vanilla FT ⋆         |                   83.7 |                   55.9 |\n| Uniform Model Soup   |                   84.4 |                   62.7 |\n| Greedy Model Soup    |                   84.3 |                   60.4 |\n| Model Stock          |                   84.1 |                   61   |\n| Model Stock ⋆        |                   85.2 |                   58.5 |",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.1 Experiments with LP initialization",
        "chunkIndex": 106,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-107",
      "content": "In the main paper, we omit the results of ObjectNet [1] on CLIP ViT-B/16 experiments since the comparison methods such as LP-FT [18], FTP [37] have not evaluated on ObjectNet benchmark. We here show the results with ObjectNet [1] and ImageNet-ReaL [2] of CLIP ViT-B/16 in Table A. We additionally compare Model Stock with recent fine-tuning methods including FLYP [7], Lipsum-FT [28], and CaRot [29] Model Stock consistently demonstrates its effectiveness with ObjectNet and ImageNet-ReaL as well.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.2 Complete comparison results on CLIP ViT-B/16",
        "chunkIndex": 107,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-108",
      "content": "Table B shows the performance of Model Stock on the pretrained CLIP ViT-B/16 model. Since the original Model Soups paper [40] only provides CLIP ViT-B/32 models, we replicate Model Soups experiments on CLIP ViT-B/16. We finetuned 48 models from CLIP ViT-B/16 initialization following the standard grid hyper-parameter sweep ( i.e ., zero-shot initialization setting). Model Stock shows\n\nTable C: Model Stock with different hyper-parameters on CLIP ViT-B/32 .\n\n| Method                                    | ImageNet     | Avg. shifts   |\n|-------------------------------------------|--------------|---------------|\n| Model Stock                               | 79.89        | 50.99         |\n| Model Stock w/ different hyper-parameters | 79.75 ± 0.45 | 50.40 ± 0.84  |\n\nTable D: Performance comparison of merging units in Model Stock.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.3 Model Stock vs. Model Soups on CLIP ViT-B/16",
        "chunkIndex": 108,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-109",
      "content": "| 79.89        | 50.99         |\n| Model Stock w/ different hyper-parameters | 79.75 ± 0.45 | 50.40 ± 0.84  |\n\nTable D: Performance comparison of merging units in Model Stock. This table presents the overall performance of Model Stock using different merging units: entire weight merging, entire weight merging based on transformer block angle, layerwise merging, and filter-wise merging. It highlights the effectiveness of each strategy in approaching the weight center and their impact on the model's performance.\n\n| Merging Unit                      | Target   | Target   | Avg. Shifts   |\n|-----------------------------------|----------|----------|---------------|\n|                                   | IN       | IN-ReaL  |               |\n| Entire weights                    | 79.69    | 85.39    | 46.40         |\n| Entire weights (rep.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.3 Model Stock vs. Model Soups on CLIP ViT-B/16",
        "chunkIndex": 109,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-110",
      "content": "--------|---------------|\n|                                   | IN       | IN-ReaL  |               |\n| Entire weights                    | 79.69    | 85.39    | 46.40         |\n| Entire weights (rep. blocks only) | 79.64    | 85.38    | 48.28         |\n| Layer-wise (ours)                 | 80.12    | 85.65    | 48.84         |\n| Filter-wise                       | 80.10    | 85.67    | 48.72         |\n\ncomparable performance against Model soups. Note that Model Soups requires 24 × more training cost than Model Stock.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.3 Model Stock vs. Model Soups on CLIP ViT-B/16",
        "chunkIndex": 110,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-111",
      "content": "To verify the validity of Model Stock beyond the setup of the main paper ( i.e ., different random seeds with the same hyper-parameters), we conduct Model Stock with different hyper-parameters. In detail, when we fine-tune two models for Model Stock, we choose different hyper-parameter for each model ( e.g ., learning rate, data augmentation.). To ensure the basic assumption of Model Stock, we use the same batch size and training epochs. C shows the experimental results on CLIP ViT-B/32. We repeat 5 runs and report accuracy with standard deviation. Model Stock with different hyper-parameters shows comparable performance to the original one.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.4 Model Stock with different hyper-parameters",
        "chunkIndex": 111,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-112",
      "content": "We investigate the efficacy of different merging units within our method, Model Stock. Our default approach employs layer-wise merging, but alternatives include merging based on the angle between 1) entire weights, 2) weights of the entire repetitive transformer blocks following [40], or 3) using a filter-wise approach as discussed in §A.3. The results of these ablations are summarized in Table D, where we assess the overall performance based on the chosen merging unit.\n\nOur analysis reveals that the accuracy of noise distribution estimation is critical in approaching the weight center. When assuming weight noise across the\n\nentire model, our method does not approximate the weight center as effectively as it does with layer-wise merging, leading to suboptimal overall performance. Similarly, the merging performance based on the angle of transformer blocks was insufficient. Conversely, while filter-wise noise demonstrates a larger standard deviation in angle, as depicted in Fig.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 112,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-113",
      "content": "Similarly, the merging performance based on the angle of transformer blocks was insufficient. Conversely, while filter-wise noise demonstrates a larger standard deviation in angle, as depicted in Fig. N, this increased variance results in a more significant error in Gaussian distribution approximation. Consequently, the overall performance under filter-wise merging is slightly inferior to layerwise one.\n\nThese findings underscore the importance of accurately modeling noise distribution in enhancing the performance of Model Stock. As our understanding and ability to model this noise distribution improve, we anticipate further increases in the efficacy and robustness of our approach.\n\n<!-- image -->\n\n(c)\n\nOpenCLIP ConvNeXt\n\nFig. H: Layer-wise angle and norm across different model architectures. The angle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are displayed from top to bottom.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 113,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-114",
      "content": "OpenCLIP ConvNeXt\n\nFig. H: Layer-wise angle and norm across different model architectures. The angle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are displayed from top to bottom. These metrics demonstrate consistency regardless of the model type from left (first layer) to right (last layer). It is important to note that we also depict the error bars for each layer in all figures, but they are not visible in most layers due to the small standard deviation.\n\n<!-- image -->\n\n(b)\n\nSGD optimizer with momentum\n\nFig. I: Layer-wise angle and norm across different optimizers. Displayed from top to bottom are the angle and norm for models trained with SGD and SGD with momentum, respectively. These metrics demonstrate consistency regardless of the optimization strategy from left (first layer) to right (last layer).\n\n<!-- image -->\n\n(c)\n\n+ RRC",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 114,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-115",
      "content": "d with SGD and SGD with momentum, respectively. These metrics demonstrate consistency regardless of the optimization strategy from left (first layer) to right (last layer).\n\n<!-- image -->\n\n(c)\n\n+ RRC\n\nFig. J: Layer-wise angle and norm across different augmentations. Displayed from top to bottom are the angle and norm for the vanilla model (10 epochs + no augmentation), +longer epochs (16 epochs), and +RRC. Each augmentation is applied incrementally. These metrics demonstrate consistency regardless of the augmentations from left (first layer) to right (last layer).\n\n<!-- image -->\n\n0.0000\n\nFig. K: Layer-wise angle and norm across different datasets. The angle and norm for models trained on different datasets, including CIFAR [17] are displayed from top to bottom. These metrics demonstrate consistency regardless of the dataset type from left (first layer) to right (last layer).\n\n<!-- image -->\n\n0.0000",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 115,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-116",
      "content": "sets, including CIFAR [17] are displayed from top to bottom. These metrics demonstrate consistency regardless of the dataset type from left (first layer) to right (last layer).\n\n<!-- image -->\n\n0.0000\n\nFig. L: Layer-wise angle and norm across different classifier initializations. The angle and norm for models trained with differently initialized networks following the LP-FT [18] method are displayed from top to bottom. These metrics demonstrate consistency regardless of the initialization method from left (first layer) to right (last layer).\n\nFig. M: Layer-wise angle during training. Displayed are the overlapped angles across models trained with different random seeds at each timestamp. Even during training, the angle remains highly consistent, decreasing as training progresses.\n\n<!-- image -->\n\n<!-- image -->\n\n(c)\n\nFilter-wise angle between attention weights in the second transformer block of ViT-B/32",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 116,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-117",
      "content": "g, the angle remains highly consistent, decreasing as training progresses.\n\n<!-- image -->\n\n<!-- image -->\n\n(c)\n\nFilter-wise angle between attention weights in the second transformer block of ViT-B/32\n\nFig. N: Filter-wise angle for attention and MLP layers in ViT-B/32. We display filter-wise angles for each layer. Each bar represents each row ( i.e ., filter) in the given layer. Interestingly, the angles between the filters of the fine-tuned weights exhibit similar values, while the standard deviation between each filter is notably larger than that of the angle between each layer. Due to the large number of layers, only representative layers are selected for display.\n\n<!-- image -->\n\nFig. O: Layer-wise angle and norm for DeiT. The angle and norm for DeiTbase models are displayed, each trained with different random seeds. These models are initially pre-trained on ImageNet-21K [34] and then fine-tuned on ImageNet-1K.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 117,
        "totalChunks": 119
      }
    },
    {
      "id": "2403.19522v2-chunk-118",
      "content": "or DeiT. The angle and norm for DeiTbase models are displayed, each trained with different random seeds. These models are initially pre-trained on ImageNet-21K [34] and then fine-tuned on ImageNet-1K. The consistency observed in the metrics is maintained even in the DeiT training setting.",
      "metadata": {
        "source": "arxiv:2403.19522v2",
        "title": "Model Stock: All we need is just a few fine-tuned models",
        "authors": [
          "Dong-Hwan Jang",
          "Sangdoo Yun",
          "Dongyoon Han"
        ],
        "section": "H.5 Ablation study on merging unit",
        "chunkIndex": 118,
        "totalChunks": 119
      }
    }
  ],
  "fullText": "## Model Stock: All we need is just a few fine-tuned models\n\nDong-Hwan Jang ⋆ Sangdoo Yun\n\nNAVER AI Lab\n\nAbstract. This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-ofdistribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two finetuned models, applicable during or after training. Our innovative layerwise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock , highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. Wedemonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands. Our code and pre-trained models are available at https://github.com/naver-ai/model-stock .\n\n## 1 Introduction\n\nPre-train/fine-tune paradigm [18,22,37,40,41] has proven to be a strong framework for training models to reach state-of-the-art performance. This approach, especially pivotal in fine-tuning pre-trained models, involves models acquiring general knowledge during pre-training and task-specific knowledge during finetuning. How we perform a fine-tuning stage is crucial, affecting task performance and robustness against distribution shifts.\n\nRecent advancements, notably Model Soup [40], which merges weights from multiple fine-tuned models trained under different training setups, have shown impressive performance without increasing inference costs. This method is believed to be effective because these models often reside in the same loss basin, and their merging results in a lower and flat loss basin. However, Model Soup's requirement for multiple fine-tuned models (more than dozens) raises concerns about efficiency and practicality in general scenarios where the models need to\n\n⋆ Work done during an internship at NAVER AI Lab.\n\nDongyoon Han\n\nFig. 1: Model Stock vs. Model Soup. Model Stock consistently enjoys improved accuracy on ImageNet (x-axis) and distribution shift benchmarks (y-axis) against individual fine-tuned models (gray circles). We plot WiSE-FT [41] curves to each fine-tuned model, highlighting Model Stock's better performance on distribution shifts compared to Model Soup [40]. Note that Model Stock has much smaller computational costs than Model Soups (24 × smaller), i.e ., Model Stock requires two fine-tuning procedures, whereas Model Soups are leveraging 48 various fine-tuned models in this experiment.\n\n<!-- image -->\n\nbe prepared from scratch. Thus, our question is: Is there an efficient way to achieve an effective merged weight from very few fine-tuned models?\n\nWe initially explore the dynamics of fine-tuned weights under the simplest scenario: varying the random seeds while maintaining the other training setups. It reveals that the fine-tuned weights with different random seeds reside on a very thin shell layer-wise during and after training. We then delve into the impact of a model-soup-like weight averaging approach. Our findings show that the closer proximity of the averaged weights correlates with improved In-Distribution (ID) and Out-Of-Distribution (OOD) performance.\n\nBuilding upon the findings, we propose a novel approach of fine-tuning method coined Model Stock , analogous to chicken stock in cooking, distinguishing it from what Model Soup intended. Now, the answer to our question is indeed affirmative: Model Stock approximates the merged weight using just a few fine-tuned models, leveraging the weight space's geometric properties and a pre-trained model's anchoring effect. This strategy offers a more computationally efficient alternative to the labor-intensive averaging of fine-tuned models, streamlining the process while enhancing model performance. Fig. 1 illustrates our brief comparison of Model Stock vs. Model Soup [40]. We reproduce the Model Soup experiments 1 based on the CLIP ViT-B/32 initialization by fine-tuning 48 models with various hyper-parameters, which is defined as zero-shot initialization setting. Fig. 1 shows that Model Stock outperforms Model Soups with much smaller computational costs.\n\n1 We follow the standard grid hyper-parameter sweep [40], fine-tuning for 10 epochs. Our results align with those presented in Fig. D.1 (right) of the original paper [40].\n\nOur comprehensive experiments demonstrate the effectiveness of Model Stock. We achieve performance comparable to, or even surpassing, that of the more resource-intensive methods such as Model Soup [40], using only a fraction of the models. Specifically, our method achieves 87.8% ImageNet top-1 accuracy (ID) and averaged 74.9% in five distribution shift benchmarks (OOD) on ViTL/14 fairly compared with the prior arts using the CLIP pre-trained weight. We believe that our study not only underscores Model Stock's practicability but also opens new directions in the pre-train/fine-tune paradigm for superior performance across various tasks.\n\n## 2 Analyzing Fine-tuned Weights\n\nOur study is driven by two fundamental findings related to the performance and robustness of fine-tuned models. The first one is that model weights are fine-tuned on different random seeds 2 lie on thin shell in weight space layerwise. The second posits that closer proximity to the center of this thin shell is beneficial for improving performance across the ImageNet and distribution shift benchmarks. The substantiation and implications of these observations are discussed in the subsequent sections.\n\n## 2.1 Geometric Properties Between Weights\n\nAngle and norm of weights. We begin by examining the intrinsic properties of the weights in fine-tuned models. We define the weight vector of the finetuned model at k -th layer as w ( k ) ∈ R n ( k ) where n ( k ) is the number of weight parameters at k -th layer, and the origin 0 as the pre-trained model weight w ( k ) 0 at k -th layer. Then the angle θ ( k ) between two weight w ( k ) 1 and w ( k ) 2 is defined by: θ ( k ) = arccos ( w ( k ) 1 · w ( k ) 2 ∥ w ( k ) 1 ∥∥ w ( k ) 2 ∥ ) , where the Euclidean l 2 -norm ∥ w ( k ) ∥ of a n ( k ) defined as ∥ w ( k ) ∥ = √ ∑ n ( k ) i =1 w ( k ) i 2 . Angle and norm will provide a geometric view of the weights at k -th layer between fine-tuned models.\n\nObservation 1: Angle and norm consistency among fine-tuned weights. We investigate the weight space of models fine-tuned on ImageNet from a pretrained model with various random seeds. Our first observation is that both angle θ ( k ) between two different models and norm ∥ w ( k ) ∥ of a weight exhibit consistent values with very low standard deviations, as shown in Fig. 2. This consistency can be mathematically represented as follows: For all i and j ∈ [1 , N ] when the number of fine-tuned weights, N , is sufficiently large, the following holds:\n\n<!-- formula-not-decoded -->\n\n2 Random seed influences training randomness, such as training data shuffling and data augmentation parameters.\n\n)\n\n(\n\nFig. 2: Layer-wise angle and norm of fine-tuned models. We measure the angle θ ( k ) (degree) and norm ∥ w ( k ) ∥ √ n ( k ) for 50 distinct weights, fine-tuned under different random seeds. We separately visualize weight layers (red bars) and bias layers (blue bars), where bias layers have much smaller angles. We plot the mean angle and norm values with standard deviation (black error bar). The results show that any two fine-tuned weights have layer-wise consistent angle and norm with extremely low standard deviation.\n\n<!-- image -->\n\nwhere l ( k ) and θ ( k ) are constants that describe the magnitude and angle between weights at k -th layer, respectively. Henceforth, to simplify notation, we will omit the superscript ( k ) indicating the layer index.\n\nInterestingly, these consistencies in angle and norm are observed 1) across diverse setups and 2) both during and after training. Fig. 2 shows this consistency over 50 fine-tuned CLIP ViT-B/32 3 [30] models. It illustrates that the layer-wise norm and angle of these models exhibit almost constant values with extremely minimal error. While this figure depicts a specific model ( i.e ., CLIP ViT-B/32), we establish that such regularity is not confined to a single model or setting but is consistent across various CLIP fine-tuning scenarios. We conjecture this holds irrespective of networks (ViT [5], Hybrid-ViT [5], ResNet [9], ConvNext [24]), optimizers (SGD, AdamW [25]), data augmentations (RRC [35], RandAug [4]), datasets (CIFAR [17], ImageNet [34]), or initialization of the classifier (zeroshot, LP-FT [18]). Remarkably, this regularity also holds for fine-tuned weights at each step during training as well as after training. A comprehensive analysis supporting these findings is presented in the Appendix A.\n\nBased on the observation, we presume the distribution of the fine-tuned weights. The center of the fine-tuned weights is defined as µ = lim N →∞ 1 N ∑ N i =1 w i . We then deduce the following properties among fine-tuned weights: (i) ∥ w i -µ ∥ = constant, indicating a thin shell distribution; (ii) ( w 0 -µ ) ⊥ ( w i -µ ) ; and (iii) ( w i -µ ) ⊥ ( w j -µ ) for all i, j ∈ [1 , N ] . These properties are depicted in Fig. 5 for better understanding. The detailed proof is in the Appendix B.\n\n3 https://github.com/openai/CLIP\n\nTable 1: Distance from the center ( i.e ., ∥ w -µ ∥ ) vs. performance . We report the ImageNet and distribution shift performance with the distance from the center of weights µ for fine-tuned and averaged models. We observe 1) both models consistently maintain a nearly constant distance from µ with remarkably small standard deviation; 2) averaging more models approaches µ , boosting ID and OOD performance.\n\n|                | ∥ w - µ ∥     |   ImageNet |   Avg. shifts |\n|----------------|---------------|------------|---------------|\n| Fine-tuned     | 13.133 ± .004 |      79.72 |         46.37 |\n| w (2) avr      | 9.192 ± .003  |      80.24 |         47.76 |\n| w (3) avr      | 7.439 ± .025  |      80.35 |         48.18 |\n| w (5) avr      | 5.633 ± .014  |      80.47 |         48.53 |\n| w (50) avr ≃ µ | ∼ 0           |      80.59 |         48.85 |\n\nFig. 3: Test error landscape. We visualize a test error landscape with three weight points: a pre-trained model ( w 0 ), a fine-tuned model ( w 1 ), and the averaged weights of 50 fine-tuned models ( w 50 avr ). w 50 avr locates near the lowest error basin. A better solution ( w H ) with lower error than w 1 can be easily found utilizing those three weights (§3).\n\n<!-- image -->\n\n## 2.2 Center of Weights and Performance\n\nWe proceed to explore the relationship between the proximity to the center of fine-tuned weights and their performance on ID and OOD datasets. Given that computing the exact center is infeasible, we approximate it by averaging differently seeded 50 fine-tuned weights, using it as a pseudo-center ( i.e . µ ≃ w (50) avr ).\n\n## Observation 2: Distance from the center of weights and performance.\n\nTable 1 offers quantitative observations about the fine-tuned weights and their performance using CLIP ViT-B/32. The results include distances from the weight center ( µ ) of fine-tuned models and their averaged counterparts with their ID and OOD performances. Going closer to the center by averaging the weights leads to improving both performances. Interestingly, the standard deviation of the distances is less than 0.1% of the mean distance, suggesting highly consistent fine-tuned weight distances from the center across different weights. This suggests that fine-tuned weights occupy a thin shell as discussed in §2.1.\n\nObservation 3: Fine-tuned weights occupy local minima edges in the test error landscape. We present an additional observation regarding the test error landscape, which relates to the performance around the center of weights. Fig. 3 depicts the test error landscape on the ImageNet test dataset within a two-dimensional plane. This plane includes a pre-trained model's weight ( w 0 ), a single fine-tuned model ( w 1 ), and the pseudo-center ( w (50) avr ). This landscape\n\nFig. 4: Weights closer to the center enjoy higher performances. We show the ImageNet accuracy of weights with their distance from the center ( µ ), which is approximated with w (50) avr . Averaged weights are closer to the center than individual weights, and their accuracy increases as the averaging number of models ( N ) increases. Gray circles are the weights randomly sampled from the Gaussian distribution centered at w (50) avr . Even the random weights also achieve higher performance as they reach the center. The results indicate the critical role of proximity to the center on performance.\n\n<!-- image -->\n\nreveals that a fine-tuned model typically occupies the boundary of test error regions. On the other hand, centered near pseudo-center ( w (50) avr ), the test error is the lowest and gets higher as the weights get far from the center. Interestingly, along the line segment w 0 w 1 ( i.e ., a WiSE-FT curve [41]), the fine-tuned weight w 1 is neither the point closest to the pseudo-center nor the one with the lowest test error. We will connect this observation in §3 to find w H , the weight on the line closest to the center.\n\nObservation 4: Randomly perturbed weights nearing the center also merit high performance. To further investigate the impact of proximity to the center on performance, we conduct a toy experiment to measure ImageNet performance using random weights generated by adding layer-wise noise to the weight at the center. The standard deviation of the noise for each layer is adjusted to align with the distribution of fine-tuned weights. Fig. 4 presents a scatter plot correlating the distance of model weights from the distribution's center with corresponding ImageNet accuracies. Remarkably, randomly sampled weights demonstrate performance comparable to fine-tuned and averaged models, highlighting the importance of center proximity.\n\nFinally, the above observations naturally raise a question: Why do fine-tuned weights through optimization not reach the center, staying constantly close instead? Previous studies [3, 15] tell us that optimization steps might struggle to guide fine-tuned weights to the center of the weight distribution due to the many stationary points in the loss surface. Alternatively, averaging independently finetuned models is a unique solution but both laborious and resource-intensive. It appears that there are no better alternatives for getting closer to the center, as\n\nFig. 5: Comprehensive illustration of the geometric dynamics of fine-tuned weights. This figure illustrates the behavior of fine-tuned weights that lie on a thin shell, supporting our Gaussian distribution hypothesis. Each sphere represents a thin shell that fine-tuned models lie on at each training step, with w 0 denoting the pretrained model. The curved lines trace the fine-tuning trajectory from w 0 , while the red vectors indicate that fine-tuned models at each time step are equidistant from w 0 .\n\n<!-- image -->\n\noptimization proves ineffective near these flat local minima. Could there be a faster approach to reaching the center? This question will be addressed in §3.\n\n## 2.3 Our Hypothesis\n\nThe observed geometric patterns in weight distributions closely align with mathematical properties of Gaussian distributions, represented as N ( µ , Σ ) . Therefore, a plausible reason for the particular geometric pattern of fine-tuned weights could be the influence of Gaussian noise within the weight space. In highdimensional spaces, vectors sampled from such distributions tend to have nearly identical norms, specifically √ | µ | 2 + trace ( Σ ) and consistent in-between angles, due to the concentration of measure phenomenon [19]. The likelihood of the squared norm significantly deviating from this expected value is exponentially negligible in high-dimensional spaces, like a weight space.\n\nIn other words, the vectors sampled from high-dimensional Gaussian distribution lie on a very thin shell with the radius ≈ √ trace ( Σ ) around the center µ . Consequently, we hypothesize that fine-tuned weights follow a Gaussian distribution in a layer-wise manner. While not necessary for our observation, this hypothesis provides a sufficient condition for understanding the geometric dynamics of fine-tuned weights. For example, it aids in the intuitive understanding of the fact that the distance of w ( N ) avr from the weight center is proportional to 1 / √ N , signifying the reduction of variance. Fig. 5 comprehensively illustrates the observations and our hypothesis discussed in §2.\n\n## 3 Method\n\nThis section introduces our method, Model Stock, a cost-efficient weight merging method. As discussed in §2.2, getting closer to the center of weights µ induces improved model performance. A straightforward method to approximate\n\nFig. 6: Schematic concept of our Model Stock. We present two scenarios with a small angle (left) and a large angle (right). Given a pre-trained weight ( w 0 ) and two fine-tuned weights ( w 1 , and w 2 ), we visualize a gray triangle representing the span of three weights. We consider this triangle area our search space spanned by three weights. We aim to find the best weight point on the triangle nearest the ideal center µ . We find that the perpendicular foot w H from point µ to the plane is the nearest point, which can be specified solely using the angle between the fine-tuned models, even without knowing the exact position of the center µ . We utilize w H as the merged weight of Model Stock. Intuitively, when the angle θ is large ( i.e ., two weights are diverse), as in the right figure, w H will rely more on the pre-trained weight( w 0 ), vice versa.\n\n<!-- image -->\n\nµ is averaging multiple model weights, which can be computationally expensive. We propose an efficient alternative method by leveraging the pre-trained model weights , an aspect previously neglected by existing weight-merging methods. A pre-trained model usually possesses general knowledge and shows robust and reliable performance in out-of-distribution (OOD) cases [30]. Therefore, a pretrained model can become a robust anchor point . As shown in Fig. 3, we could readily identify a weight ( w H ) that is closer to the center-and thus better-by interpolating with the anchor. Building on this concept, we propose a method to approximate the center of weights more accurately with only a few fine-tuned weights. Again, for readability, we omit the layer notation ( k ) , but the subsequent method is applied layer-wise.\n\nOn two fine-tuned models. We observed in §2 that two fine-tuned models with different random seeds have almost constant norms and the angle between them. Based on this, we define a plane connecting the pre-trained model and two fine-tuned models as shown in Fig. 6 (depicted as a gray triangular area). Any weight vector on this plane can be expressed as a linear combination of the pretrained model and two fine-tuned models. Our goal is to find the weight closest to the center of fine-tuned weights on this plane, which is the perpendicular foot ( w H ) from the center of distribution ( µ ) to the plane.\n\nEven without knowing the exact position of the center µ , w H can be specified solely using the angle between the fine-tuned models with the following two conditions that µ must satisfy. First, as mentioned in §2.1, ( w 1 -µ ) ⊥ ( w 2 -µ ) and ∥ w i -µ ∥ = ∥ w 2 -µ ∥ hold, implying that △ w 1 µ w 2 forms an isosceles right triangle. In Fig. 6, µ should lie on the dotted hyper-circle. Another condition is that the condition ( w 0 -µ ) ⊥ ( w 12 -µ ) must be satisfied, where w 12 = w 1 + w 2 2 . This condition arises from the second property in §2.1, where ( w 0 -µ ) ⊥ ( w i -µ ) . Combining these two conditions, µ is at the point where the line starting from w 0\n\nis tangent to the hyper-circle. We precisely determine the position of µ within a 3D volume slice that encompasses both µ and △ w 0 w 1 w 2 . Consequently, we can find the position of the closest weight w H to the distribution's center on the plane using straightforward geometric principles. The position of the perpendicular foot is determined as follows:\n\n<!-- formula-not-decoded -->\n\nFor more detailed proof, please refer to the Appendix D. Note that the interpolation ratio t = 2 cos θ 1+cos θ is solely determined by the angle θ between two fine-tuned models. Crucially, unlike previous methods, determining t does not require extra training [22, 36, 37, 40] or heuristic hyper-parameter settings [6, 40], thereby simplifying the process and enhancing its accessibility and efficiency.\n\nAs the θ decreases, the pre-trained model is less utilized for merging, as shown in Fig 6 (left). Coupled with the observation in Fig. 2, this indicates that bias layers rely less on pre-trained models and focus more on fine-tuned models, whereas weight layers depend more on pre-trained models. This observation extends the findings of previous works such as BitFit [43] and LP-FT [18]. In the case of BitFit and LP ( i.e ., the first step of LP-FT), bias and classifier layers fully utilize fine-tuning, while other weight layers (attention and MLP) rely on pre-trained models. We present an additional analysis in the Appendix F.\n\nOn N fine-tuned models. We further extend the previous derivation to N fine-tuned models to move even closer to the weight center. Let us denote w ( N ) avr as the N -averaged weight, ∑ N i =1 w i /N , and w ( N ) H as the weight in the span ( w 0 , w 1 , . . . , w N ) closest to the distribution's center. Then, we derive the position of w ( N ) H as:\n\n<!-- formula-not-decoded -->\n\nDetailed proof is in the Appendix D. Similar to the case of two fine-tuned models, the interpolation ratio t depends solely on the angle θ between the pre-trained model and the N fine-tuned models.\n\nPeriodic merging. To move one step forward with our method, we propose periodic merging , which is performed between the fine-tuned models and the pre-trained model during training. As the geometric properties of weights are also applicable to weights during training (refer to Appendix A.2 for more details), we fully utilize this phenomenon here.\n\nFig. 7: Periodic merging for Model Stock . Weights are merged every epoch.\n\n<!-- image -->\n\nWe strategically merge weights at the end of every epoch, allowing for the fine-tuning process to be parallelized with merging. The interpolation ratio for merging is determined by the angle θ between the pre-trained model and the fine-tuned models at the current epoch. Fig. 7 visualizes this periodic merging\n\nprocess. We argue that employing a periodic merging can approximate the center of weights more accurately. In §4.3, we empirically show that the periodic merging yields superior performance and achieves a closer distance to the center.\n\n## 4 Experiment\n\nWe present the key experimental results in this section. We first provide our experimental setups in §4.1. Then, we present the main results in §4.2, and ablation studies in §4.3. More detailed experimental setups, analysis, and further results are in the Appendix G. We will release our codes and weights publicly.\n\n## 4.1 Experimental Setup\n\nModels. We conduct the experiments on CLIP ViT-B/32, CLIP ViT-B/16, and CLIP ViT-L/14 models. We set the number of fine-tuning models for Model Stock as two. We compare Model Stock against various fine-tuning techniques, including Model Soups [40], LP-FT [18], CAR-FT [27], FTP [37], and FLYP [7]. We use CLIP ViT-B/32 for ablation studies.\n\nDatasets. We fine-tune models on the ImageNet-1K [34] training dataset. We report the ImageNet-1K [34] top-1 accuracy for evaluating in-distribution (ID) performance. For distribution shift scenarios, we consider five out-of-distribution (OOD) benchmarks including ImageNet-V2 [33], ImageNet-R [10], ImageNetSketch [39], ImageNet-A [11], and ObjectNet [1]. Since previous methods, except Model Soups [40], have not been evaluated on ObjectNet, we omit the ObjectNet results when comparing against them.\n\nTraining setup. We initialize the classifier weights ( e.g ., 1000 classes for ImageNet) using the text encoder of CLIP and text prompts following Model Soup [40]. We use AdamW [25] with batch size 512 and weight decay 0.1 for all the experiments, including vanilla fine-tuning, Model Soup reproduction, and Model Stock 4 . We employ two training setups for comparisons: (1) training 10 epochs with minimal data augmentation, following Model Soup's zero-shot initialization setup, and (2) training 16 epochs with strong data augmentation, following Model Soup's LP initialization setup [40], denoted with ⋆ . These setups enable a balanced comparison of Model Stock against zero-shot and LP-initialized Model Soups. Detailed hyper-parameters are in the Appendix G.\n\n## 4.2 Main Results\n\nCLIP ViT-B/32. Table 2 shows the results of Model Stock on the pre-trained CLIP ViT-B/32 model by comparing it with Model Soups. 'Avg. shifts' denotes the average accuracy of the five OOD benchmark scores. Our Model Stock and Model Stock ⋆ show competitive performance with Model Soups. Furthermore, Model Stock ⋆ achieves state-of-the-art performance on ImageNet with 81.19 %\n\n4 We reduce the batch size to 64 on CLIP ViT-L/14 due to memory limitation.\n\nTable 2: Comparison against Model Soups [40] on CLIP ViT-B/32. We report the performance and relative fine-tuning costs on the CLIP ViT-B/32 scenario. α denotes the cost for LP initialization. Model Stock shows comparable performance with Model Soups with significantly reduced training costs.\n\n| Method                                                                        |   ImageNet |   Avg. shifts | Cost   |\n|-------------------------------------------------------------------------------|------------|---------------|--------|\n| Comparing with Model Soups from zero-shot init. CLIP zero-shot Initialization |      63.34 |         48.51 | 0      |\n| Vanilla FT                                                                    |      78.35 |         47.03 | 1      |\n| Uniform Model Soup (from zero-shot)                                           |      79.76 |         52.08 | 48     |\n| Greedy Model Soup (from zero-shot)                                            |      80.42 |         50.83 | 48     |\n| Model Stock                                                                   |      79.89 |         50.99 | 2      |\n| Comparing with Model Soups from LP init. CLIP LP initialization               |      75.57 |         47.21 | α      |\n| Vanilla FT ⋆                                                                  |      79.72 |         46.37 | 1      |\n| Uniform Model Soup (from LP init)                                             |      79.97 |         51.45 | 71+ α  |\n| Greedy Model Soup (from LP init)                                              |      81.03 |         50.75 | 71+ α  |\n| Model Stock ⋆                                                                 |      81.19 |         48.69 | 2      |\n\nTable 3: Model Stock on CLIP ViT-B/16. Model Stock shows competitive performance against previous fine-tuning methods on ImageNet and distribution shifts.\n\n|               |          | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   |\n|---------------|----------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Method        | ImageNet | Avg. shifts           | IN-V2                 | IN-R                  | IN-A                  | IN-Sketch             |\n| Zero-shot     | 68.3     | 59.5                  | 62.0                  | 77.7                  | 49.9                  | 48.3                  |\n| Vanilla FT    | 82.8     | 57.7                  | 72.9                  | 66.4                  | 43.7                  | 48.0                  |\n| Vanilla FT ⋆  | 83.7     | 57.4                  | 73.5                  | 67.6                  | 40.0                  | 48.6                  |\n| LP [18]       | 79.7     | 48.1                  | 71.5                  | 52.4                  | 27.8                  | 40.5                  |\n| LP-FT [18]    | 81.7     | 60.5                  | 71.6                  | 72.9                  | 49.1                  | 48.4                  |\n| CAR-FT [27]   | 83.2     | 59.4                  | 73.0                  | 71.3                  | 43.7                  | 49.5                  |\n| FTP [37]      | 84.2     | 49.7                  | 74.6                  | 47.2                  | 26.5                  | 50.2                  |\n| FLYP [7]      | 82.6     | 60.5                  | 73.0                  | 71.4                  | 48.1                  | 49.6                  |\n| Model Stock   | 84.1     | 62.4                  | 74.8                  | 71.8                  | 51.2                  | 51.8                  |\n| Model Stock ⋆ | 85.2     | 60.1                  | 75.3                  | 68.7                  | 45.0                  | 51.3                  |\n\ntop-1 accuracy. As described in Fig. 1, Model Stock with WiSE-FT [41] enjoys a superior ID-OOD performance curve compared to Model Soup and its WiSE-FT curves. Note that Model Soups require dozens of fine-tuned models ( e.g ., zeroshot and LP-init Model Soups use 48 and 71 models, respectively), highlighting the effectiveness of Model Stock along with efficiency utilizing only two models. We provide further comparison results with WiSE-FT [41] curves on LP-init Model Soups in the Appendix H.\n\nCLIP ViT-B/16. Table 3 presents a comprehensive comparison of different fine-tuning methods applied to CLIP ViT-B/16. Previous works [18,36,37] lack ObjectNet [1] results; therefore, we omit ObjectNet and report the other four OOD benchmarks. Complete results with ObjectNet and ImageNet-Real [2] are in the Appendix H.2. The results show Model Stock exhibits exceptional performance on the ImageNet accuracy, e.g ., Model Stock ⋆ achieves 85.2% top-1\n\nTable 5: Impact of the number of fine-tuning models ( N ) on Model Stock. IN denote ImageNet accuracy.\n\n|      |   IN |   Avg. Shifts |   ∥ w - µ ∥ |\n|------|------|---------------|-------------|\n| FT   | 79.7 |          46.7 |       13.13 |\n| N =2 | 80.1 |          48.8 |       10.01 |\n| N =3 | 80.2 |          48.8 |        9.05 |\n| N =4 | 80.4 |          48.9 |        8.45 |\n\nTable 6: Impact of merging period on Model Stock. IN denotes ImageNet accuracy.\n\n| Period     |   IN |   Avg. Shifts |\n|------------|------|---------------|\n| 1000 iters | 79.8 |          48.7 |\n| 5000 iters | 79.9 |          48.5 |\n| 1 epoch    | 80.1 |          48.8 |\n\naccuracy on ImageNet, which is a state-of-the-art level. Model Stock also shows robust performance across diverse distribution shift scenarios.\n\nCLIP ViT-L/14. Table 4 shows the results of Model Stock on the CLIP ViT-L/14 model. The results show that Model Stock can push the limit of benchmark scores with large-size backbone architecture. We remark that Model Stock ⋆ achieves state-of-the-art performance with 87.7% ImageNet top-1 accuracy, implying that Model Stock is still effective in a scaleup scenario. The results consistently demonstrate the high efficacy and robustness of Model Stock across diverse scales of models and vari-\n\nTable 4: Model Stock on CLIP ViT-L/14.\n\n|               |   IN |   Avg. shifts |\n|---------------|------|---------------|\n| Zero-shot     | 75   |          63   |\n| Vanilla FT    | 85.8 |          66.8 |\n| Vanilla FT ⋆  | 87.1 |          68   |\n| TPGM [36]     | 87   |          69.4 |\n| CAR-FT [27]   | 87.1 |          67.8 |\n| Model Stock   | 87   |          71.6 |\n| Model Stock ⋆ | 87.7 |          73.5 |\n\nous benchmark scenarios, reaffirming its potential in practical applications.\n\n## 4.3 Ablation studies and analysis of Model Stock\n\nWe conduct ablation studies on CLIP ViT-B/32. We train vanilla fine-tuned models and Model Stock for 16 and 8 epochs, respectively; thus, the training cost of Model Stock matches with a single fine-tuning process.\n\nExperiments on the number of fine-tuned models N . Table 5 shows the effect of the number of fine-tuned models. The results show that Model Stock obtains enhanced performance and closer distance from the (pseudo-) center ( ∥ w -µ ∥ ) as the number of merging models increases. Considering the trade-off between the performance and training cost induced by increased N , our setting ( N =2) shows the best for Model Stock.\n\nStudy on the merging period of Model Stock. Table 6 shows the results of various merging periods, including 1000 and 5000 iterations settings. Note that 1 epoch is ∼ 2500 iterations in our experiment. Model Stock shows consistent performance with various periods.\n\nThe post-training merging strategy of Model Stock. We study an alternative of Model Stock that merges fine-tuned weights only once after each finetuning process is finished, similar to Model Soups [40]. We denote it as Model\n\nTable 7: Post-training merging strategy of Model Stock. We present ImageNet accuracy, distribution shifts, and distance from the center with the results of uniform averaging, a straightforward baseline.\n\n| Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Model Stock (post-training)   | Model Stock (post-training)   | Model Stock (post-training)   |\n|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n|                               | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     |\n| N =2                          | 80.2                          | 47.8                          | 9.19                          | 80.3 (+0.1)                   | 50.4 (+2.6)                   | 7.62 (-1.57)                  |\n| N =3                          | 80.4                          | 48.2                          | 7.44                          | 80.4 (+0.0)                   | 50.2 (+2.0)                   | 6.49 (-0.95)                  |\n| N =4                          | 80.5                          | 48.5                          | 5.63                          | 80.5 (+0.0)                   | 49.8 (+1.4)                   | 5.16 (-0.47)                  |\n\nStock (post-training). We utilize the individually fine-tuned weights as we conducted in §2, using the same training settings with different random seeds for each model. We report the performance and distance from the pseudo-center of Model Stock (post-training) in Table 7. On the left side of the table, we provide the performance of its counterpart, a uniform averaging of N models ( w N avg ). The improvements from the uniform averaging to Model Stock (post-training) are denoted in the table's parentheses. The results show that Model Stock (posttraining) archives improved distribution shift scores with closer distances toward the center than its counterpart.\n\n## 5 Related Work\n\nWe discuss related works and highlight how our method differs and contributes to the existing works.\n\nModel Soups [40] is a straightforward weight averaging method that merges weights from various fine-tuned models trained with different hyper-parameters. It demonstrates improved in-distribution (ID) and out-of-distribution (OOD) performance. While effective, model soups typically require a large number of fine-tuned models. Our method aims to achieve similar or superior performance improvements more efficiently, utilizing significantly fewer fine-tuning costs. We provide further discussion about Model Soups with our new interpretation in Appendix E.\n\nRobust Fine-tuning. When fine-tuning generalist models like CLIP [30], we often observe the fine-tuned models lose the generalization ability of the original ones, with decreased OOD performance. To address this issue, several robust fine-tuning approaches have been proposed. LP-FT [18] attempts to preserve pretrained weights by initially training only a linear probing layer. WiSE-FT [41] improves OOD performance through linear interpolation between fine-tuned and pre-trained weights. While our method shares similarities with WiSE-FT in using pre-trained weights, our method determines interpolation ratios layer-wise based on geometric properties. Focusing on OOD performance, methods suggesting improved training objectives [7,27-29,36,37] have been proposed. Our approach differs from these methods as we do not propose a new fine-tuning loss. Instead, we perform two fine-tunings and achieve robust performance by merging them.\n\nWeight Center and Flat minima. Recent machine learning research has extensively explored the significance of finding flat minima for improved generalization [3, 15, 21, 26]. Keskar et al . [16] and Hochreiter &amp; Schmidhuber [12] demonstrated that sharp optima by large batch SGD have steep, harmful directions, while broader optima enhance generalization. Stochastic Weight Averaging (SWA) [15] targets the center of flat minima, enhancing robustness against shifts in the loss landscape between training and test datasets. SWAG [26] builds on SWA by incorporating Bayesian model averaging with a Gaussian posterior to further boost performance. SWAD [3] found that the generalization gap between flat and sharp minima is more pronounced in OOD scenarios than in ID ones. Our method theoretically extends these approaches by efficiently identifying the center of flat minima with novel geometric properties, leading to significant improvements in both ID and OOD performances.\n\nModel Weight Merging. Recent research has explored merging models finetuned on various tasks. Methods such as Task Arithmetic [14] and TIES [42] have been proposed. They are also based on the difference between fine-tuned and pre-trained weights (often referred to as the 'task vector'). However, our method distinguishes itself through geometric analysis for weight merging. In the domain of Large Language Models (LLMs), merging techniques like WARM [31] and WARP [32] have emerged. Our method has the potential for extension to these areas, offering new avenues for future research.\n\n## 6 Conclusion\n\nOur study illuminated the fine-tuning process in machine learning, revealing that fine-tuned models' weights generally exhibit the properties of a Gaussian distribution. The proximity of these models to the center of weights was crucial for improved performance in target domains like ImageNet and under diverse distribution shifts. Utilizing a pre-trained model as a robust anchor point, we efficiently minimized the variance with fewer fine-tuned models, eliminating the need for additional training to find the optimal interpolation ratio.\n\nAdditionally, our findings suggested further knowledge and applicability to the models near flat minima and will offer new insights on model weight merging methods. As the pretraining-finetuning paradigm gains more prominence, our insights will provide a foundation for better understanding and optimizing the fine-tuning process in both academia and industry.\n\nLimitation. Due to resource limitations, we could not conduct larger-scale models such as ViT-G. Exploring this will be part of our future work.\n\n## 7 Acknowledgment\n\nWe thank the researchers at NAVER AI Lab for their valuable comments. This work was supervised by Sangdoo Yun and Dongyoon Han. Dong-Hwan Jang is currently at Samsung Advanced Institute of Technology (SAIT).\n\n## References\n\n1. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.: Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. NeurIPS (2019)\n2. Beyer, L., Hénaff, O.J., Kolesnikov, A., Zhai, X., Oord, A.v.d.: Are we done with imagenet? arXiv preprint arXiv:2006.07159 (2020)\n3. Cha, J., Chun, S., Lee, K., Cho, H.C., Park, S., Lee, Y., Park, S.: Swad: Domain generalization by seeking flat minima. NeurIPS 34 , 22405-22418 (2021)\n4. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: CVPRW. pp. 702-703 (2020)\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)\n6. Gouk, H., Hospedales, T.M., Pontil, M.: Distance-based regularisation of deep networks for fine-tuning. arXiv preprint arXiv:2002.08253 (2020)\n7. Goyal, S., Kumar, A., Garg, S., Kolter, Z., Raghunathan, A.: Finetune like you pretrain: Improved finetuning of zero-shot vision models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1933819347 (2023)\n8. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., Neubig, G.: Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366 (2021)\n9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770-778 (2016)\n10. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al.: The many faces of robustness: A critical analysis of out-of-distribution generalization. In: ICCV. pp. 8340-8349 (2021)\n11. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial examples. In: CVPR. pp. 15262-15271 (2021)\n12. Hochreiter, S., Schmidhuber, J.: Flat minima. Neural computation 9 (1), 1-42 (1997)\n13. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)\n14. Ilharco, G., Ribeiro, M.T., Wortsman, M., Schmidt, L., Hajishirzi, H., Farhadi, A.: Editing models with task arithmetic. In: The Eleventh International Conference on Learning Representations (2023), https://openreview.net/forum?id=6t0Kwf8jrj\n15. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.G.: Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407 (2018)\n16. Keskar, N.S., Mudigere, D., Nocedal, J., Smelyanskiy, M., Tang, P.T.P.: On largebatch training for deep learning: Generalization gap and sharp minima. In: International Conference on Learning Representations (2017), https://openreview. net/forum?id=H1oyRlYgg\n17. Krizhevsky, A.: Learning multiple layers of features from tiny images. In: Tech Report (2009)\n18. Kumar, A., Raghunathan, A., Jones, R., Ma, T., Liang, P.: Fine-tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054 (2022)\n\n19. Ledoux, M.: The concentration of measure phenomenon. No. 89, American Mathematical Soc. (2001)\n20. Lee, Y., Chen, A.S., Tajwar, F., Kumar, A., Yao, H., Liang, P., Finn, C.: Surgical fine-tuning improves adaptation to distribution shifts. In: ICLR (2022)\n21. Li, H., Xu, Z., Taylor, G., Studer, C., Goldstein, T.: Visualizing the loss landscape of neural nets. NeurIPS 31 (2018)\n22. Li, T., Huang, Z., Tao, Q., Wu, Y., Huang, X.: Trainable weight averaging: Efficient training by optimizing historical solutions. In: ICLR (2022)\n23. Lian, D., Zhou, D., Feng, J., Wang, X.: Scaling &amp; shifting your features: A new baseline for efficient model tuning. NeurIPS 35 , 109-123 (2022)\n24. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: CVPR. pp. 11976-11986 (2022)\n25. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)\n26. Maddox, W.J., Izmailov, P., Garipov, T., Vetrov, D.P., Wilson, A.G.: A simple baseline for bayesian uncertainty in deep learning. NeurIPS 32 (2019)\n27. Mao, X., Chen, Y., Jia, X., Zhang, R., Xue, H., Li, Z.: Context-aware robust finetuning. IJCV (12 2023). https://doi.org/10.1007/s11263-023-01951-2\n28. Nam, G., Heo, B., Lee, J.: Lipsum-ft: Robust fine-tuning of zero-shot models using random text guidance. In: The Twelfth International Conference on Learning Representations (2024)\n29. Oh, C., Lim, H., Kim, M., Han, D., Yun, S., Choo, J., Hauptmann, A., Cheng, Z.Q., Song, K.: Towards calibrated robust fine-tuning of vision-language models (2024), https://arxiv.org/abs/2311.01723\n30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: ICML (2021)\n31. Rame, A., Vieillard, N., Hussenot, L., Dadashi, R., Cideron, G., Bachem, O., Ferret, J.: WARM: On the benefits of weight averaged reward models. In: Forty-first International Conference on Machine Learning (2024), https://openreview.net/ forum?id=s7RDnNUJy6\n32. Ramé, A., Ferret, J., Vieillard, N., Dadashi, R., Hussenot, L., Cedoz, P.L., Sessa, P.G., Girgin, S., Douillard, A., Bachem, O.: Warp: On the benefits of weight averaged rewarded policies (2024), https://arxiv.org/abs/2406.16768\n33. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML (2019)\n34. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: Imagenet large scale visual recognition challenge. IJCV 115 (3), 211-252 (2015)\n35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. pp. 19 (2015)\n36. Tian, J., He, Z., Dai, X., Ma, C.Y., Liu, Y.C., Kira, Z.: Trainable projected gradient method for robust fine-tuning. In: CVPR. pp. 7836-7845 (2023)\n37. Tian, J., Liu, Y.C., Smith, J.S., Kira, Z.: Fast trainable projection for robust finetuning. In: NeurIPS (2023)\n38. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers &amp;distillation through attention. In: ICML. vol. 139, pp. 10347-10357 (July 2021)\n39. Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning robust global representations by penalizing local predictive power. In: NeurIPS (2019)\n\n40. Wortsman, M., Ilharco, G., Gadre, S.Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A.S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.: Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In: ICML. pp. 23965-23998. PMLR (2022)\n41. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.: Robust fine-tuning of zeroshot models. In: CVPR. pp. 7959-7971 (2022)\n42. Yadav, P., Tam, D., Choshen, L., Raffel, C., Bansal, M.: TIES-merging: Resolving interference when merging models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id= xtaX3WyCj1\n43. Zaken, E.B., Ravfogel, S., Goldberg, Y.: Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021)\n44. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. ICLR (2018)\n\n## Appendix\n\nIn this Appendix, we provide in-depth analysis and additional insights to complement the main text of our study on Model Stock, our novel approach to fine-tuning and weight merging. The contents are summarized as follows:\n\n- -We examine the angle norm consistency of fine-tuned weights across various settings in §A, extending the observations discussed in §2.1.\n- -We provide detailed proofs of geometric properties of fine-tuned weights in §B.\n- -We study the importance of reducing variance for performance in out-ofdistribution scenarios in §C, showcasing the test error landscape across various datasets and elaborating on the explanations in §2.2.\n- -We provide detailed proofs in §D for the optimal interpolation ratio in our method §3.\n- -We discuss prior studies through the lens of our findings in §E.\n- -We provide an additional analysis of the interpolation ratio in §F.\n- -We present experimental settings of §4 in §G.\n- -We present additional experiments of Model Stock in §H\n\nEach section aims to offer a comprehensive understanding of our method's underlying principles and its broad applicability in machine learning.\n\n## A Angle and Norm Consistency\n\nWe argue that, as discussed in §2.1, angles and norms of fine-tuned weights would remain consistent across fine-tuned models, independent of various factors. These factors include architecture type (ViTs [5], ResNet [9], ConvNeXt [24]), optimizers (SGD, AdamW [25]), augmentations (RRC [35], RandAug [4]), datasets (CIFAR [17], ImageNet [34]), or the initialization of the classifier (zero-shot, LP as in LP-FT [18]). We depict the layer-wise angle and norm of 5 fine-tuned weights for each category based on different random seeds. We give detailed illustrations for each setting at the end of the Appendix to enhance readability (refer to Fig. H-O). Across all these settings, the angle and norm of weights exhibit a surprising level of consistency.\n\n## A.1 Analysis on layer-wise tendency\n\nThe layer-wise angle and norm across various settings are shown in Fig. HL. We visualize with every weight of attentions/convolutions (Attention/Conv), multi-layer perceptrons (MLP), normalizations (LayerNorm and BatchNorm), a classifier (Classifier), individual bias (Bias), and the remaining layers ( i.e ., the patchification layer, positional embedding, class embedding, and projection layer). We further display All in each figure, which denotes the concatenation of the weights of entire layers.\n\nThe layer-wise analysis reveals an interesting trend: Bias and classifier layers demonstrate smaller angles than attention and MLP layers. In other words, bias and classifier layers exhibit lower randomness and more reliable updates than attention and MLP layers. It is important to note that as the angle decreases, the pre-trained model is less utilized for merging (refer to Eq. (2). This indicates that bias and classifier layers focus more on fine-tuned models and rely less on the pre-trained model, whereas attention and MLP layers depend less on the finetuned model ( i.e ., t bias , t clf &gt; t attn , t mlp ). This observation extends the findings of previous works such as BitFit [43] and LP-FT [18]. In the case of BitFit and LP ( i.e ., the first step of LP-FT), bias and classifier layers fully utilize fine-tuning, while other layers (attention and MLP) rely on pre-trained models.\n\nThese traits could offer new insights into parameter-efficient transfer learning (PETL) [8,13,23,43] and layer-wise fine-tuning [18, 20, 36, 37]. Maintaining weights with high randomness (higher angles) while updating on biases and classifier weights with lower randomness and fewer parameters would be an efficient fine-tuning strategy. PETL has been exploring this direction but has not yet provided solid reasons why certain layers are more effective than others. Our analysis suggests that one reason could be the lower randomness (or variance) of these layers, as indicated by the angle trend per layer.\n\n## A.2 Maintaining consistency during training\n\nWe further argue that the consistency we observed is maintained while training progresses, as illustrated by multiple thin shells in Fig. 5. To demonstrate that the angle and norm of fine-tuned models remain consistent during the entire training process, we plot their relationship across weights for every epoch in Fig. M. Please note that the angle is consistent across differently seeded models at the same timestamp ( i.e ., w 1 | t = t 1 and w 2 | t = t 1 ), not across models at different timestamps ( i.e ., w 1 | t = t 1 and w 1 | t = t 2 ). The observed trend is as follows: as training progresses, the angle between weights steadily decreases. This analysis uses the CLIP ViT-B/32 model fine-tuned on ImageNet-1K with five random seeds.\n\n## A.3 Filter-wise analysis of weights\n\nLi et al . [21] showed that when evaluating the robustness of a neural network by adding random noise to certain weights, performance analysis based on adding filter-wise noise ( i.e ., adding noise for each row in all weight matrices) aligns more closely with the generalization performance than adding layer-wise noise does. Inspired by this observation, we investigate the possibility that the weight distribution may follow a filter-wise Gaussian distribution and adapt this concept to our method (see the performance analysis in §H.5). Fig. N illustrates the angle distribution filter-wise. The angle exhibits much larger standard deviations than the layer-wise distribution. This could be attributed to the reduction in dimensionality. As the number of dimensions decreases, it becomes challenging to approximate the norm as a constant value.\n\n## A.4 Analysis on non-CLIP models\n\nTo verify if this key observation also applies to non-CLIP models, we analyze the geometric patterns of fine-tuned weights trained using the DeiT [38] method ( i.e ., pre-trained on ImageNet-21K). Fig. O displays the angle and norm of 10 DeiT-base models first pre-trained on ImageNet-21K [34] and then fine-tuned on ImageNet-1K. We find that weights pre-trained with ImageNet-21K also exhibit consistent angle and norm , indicating that our observation may be valid beyond CLIP fine-tuning scenarios as well.\n\n## B Detailed Proof for Geometric Properties of Fine-tuned Weights\n\nFor all indices i, j within the set [1 , N ] , where N denotes the sufficiently large number of fine-tuned weights, we derive one lemma and three propositions based on the foundational observation described in Eq. (1):\n\nLemma: w i · µ = µ · µ = l 2 cos θ .\n\nProof:\n\n<!-- formula-not-decoded -->\n\nSimilarly,\n\n<!-- formula-not-decoded -->\n\nProposition 1: ∥ w i -µ ∥ = constant. Proof:\n\n<!-- formula-not-decoded -->\n\nProposition 2: ( w 0 -µ ) ⊥ ( w i -µ ) .\n\nProof:\n\n<!-- formula-not-decoded -->\n\nProposition 3: ( w i -µ ) ⊥ ( w j -µ ) .\n\nFig. A: Test error landscape on OOD datasets. We depict the test error landscape on ImageNet-V2, -Sketch, ObjectNet, ImageNet-R, and -A (from left to right, from top to bottom, respectively) on the plane containing pre-trained model ( w 0 ), fine-tuned model ( w 1 ), and the pseudo-center of fine-tuned weights ( w (50) avr ). The local optima for the OOD datasets always lie on the line segment w 0 w (50) avr .\n\n<!-- image -->\n\nProof:\n\n<!-- formula-not-decoded -->\n\n## C Importance of Reducing Weight Variance on Performance under Distribution Shifts\n\nIn demonstrating the significance of variance reduction for robustness in out-ofdistribution (OOD) scenarios, we analyze the test error landscape as in §2.2. As shown in Fig. A, we examine the error landscape across various OOD datasets, including ImageNet-V2, ImageNet-Sketch, ObjectNet, ImageNet-R, and ImageNetA (from top to bottom). This landscape is plotted on a plane defined by the weights of a pre-trained model ( w 0 ), a fine-tuned model ( w 1 ), and the center of the fine-tuned weights, which is approximated by averaging 50 fine-tuned weights ( w (50) avr ). A notable pattern emerges where the local optima for these datasets consistently align with the line segment connecting w 0 and w (50) avr .\n\nThough the exact location of local minima differs depending on the dataset type, it has a common point that the minima are aligned on the line between the weight center and pre-trained model rather than the line between the fine-tuned weight and pre-trained model. Consequently, not only does the averaged weight exhibit higher performance on distribution shifts compared to the fine-tuned model, but the WiSE-FT [41] curves corresponding to the averaged weights also demonstrate better ID/OOD trade-off than the WiSE-FT curve of the fine-tuned model, as illustrated in Fig. B. This indicates the importance of getting closer to the weight center, even for OOD datasets.\n\nFig. B: ID vs. OOD accuracy along WiSE-FT [41] curves for averaged models. As the number of weights used for averaging increases, the corresponding WiSE-FT curves demonstrate improvements in the ID-OOD trade-off.\n\n<!-- image -->\n\nAnother interesting point is that depending on the traits of datasets, the position of local minima differs. ImageNet-V2 has a similar dataset distribution to ImageNet since it shares the same data collection and categorization policy, and its local optima lies close to that of ImageNet. On the other hand, on the datasets with harsh variations ( e.g ., ImageNet-A), the local minima are positioned much closer to the pre-trained model than the original ImageNet or ImageNet-V2. This loss landscape gives an intuitive insight into the similarity between OOD datasets and ImageNet.\n\nIn conclusion, there is no universal interpolation ratio optimal for every distribution shift. However, all the local minima lie on the line between the weight center and the pre-trained model. This implies the importance of proximity to the weight center in achieving a better WiSE-FT line.\n\n## D Detailed Proof of Model Stock\n\nHere, we present detailed proof of Model Stock introduced in §3. We first show the case with two fine-tuned models and extend our proof toward N fune-tuned models.\n\nOn two fine-tuned models. We will prove step-by-step how the optimal interpolation ratio t in Eq. (2) in the main paper is derived. Using the same notation as in §3, we denote the magnitude and the angle between the fine-tuned weights as l and θ , respectively. Starting from the fact that △ µ w 1 w 2 is a right isosceles triangle, we can derive the following relations from Fig. C:\n\nFig. C: Model Stock with two fine-tuned models. We reference the illustration in Fig. 6 to more understandably substantiate merging two fine-tuned models.\n\n<!-- image -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n⊔ ⊓\n\nInterestingly, w H is located at an orthocenter of the triangle △ w 0 w 1 w 2 with the given optimal ratio t .\n\nFig. D: Model Stock with N fine-tuned models and Interpolation Ratio Variation. (a) We visualize a special case of N = 3 (tetrahedron) for better understanding. (b) The trend towards t = 1 with increasing N illustrates that w ( N ) H on the N -dimensional simplex approaches w ( N ) avr , reflecting a growing dependence on the number of fine-tuned models.\n\n<!-- image -->\n\nOn N fine-tuned models. Similarly, we can derive a more generalized interpolation ratio for N ≥ 2 . Our goal is to find the weight w ( N ) avr that is on the hyper-plane spanned by w 0 , w 1 , . . . , w N and closest to the weight center µ , as described in Fig. Da. Again, for simplicity, we treat w 0 as the origin O .\n\nBased on the observation, we presume that the following two conditions hold:\n\n<!-- formula-not-decoded -->\n\nThe first condition comes from the symmetry of an N -simplex structure, and the second condition holds since the orthogonal projection is the minimal distance from µ . Then, we can derive t as follows:\n\nBy substituting the first condition into the second condition from Eq. (8),\n\n<!-- formula-not-decoded -->\n\nFig. E: Ensembling impact disappears when interpolating between two averaged weights. We plot the ImageNet performance of interpolated weights between two selected fine-tuned models in Model Soup [40] (left) and between their corresponding weight centers (right).\n\n<!-- image -->\n\nNote that the norm of the N -averaged fine-tuned weights can be derived as follows:\n\n<!-- formula-not-decoded -->\n\nwhile the term µ · w ( N ) avr can be simplified as\n\n<!-- formula-not-decoded -->\n\nBy substituting Eq. (10) and Eq. (11) into Eq. (9), we can finally derive the optimal interpolation ratio t as follows:\n\n<!-- formula-not-decoded -->\n\nFig. Db displays how the optimal interpolation ratio t varies as a function of θ with different numbers of fine-tuned models. As N increases, t trends towards 1, indicating that w ( N ) H on the N -dimensional simplex gets closer to w ( N ) avr . This shows increasing dependence on fine-tuned models as their number grows.\n\n## E Discussion - Rethinking Pivotal Prior Studies\n\nIn this section, we extend our findings to reinterpret the underlying mechanics in prior studies, WiSE-FT [41] and Model Soups [40], through a consistent rationale to illuminate their effectiveness.\n\nWiSE-FT [41] is a state-of-the-art robust fine-tuning method for CLIP-based models. It demonstrates that linearly combining weights of the pre-trained and fine-tuned models achieves a significant accuracy gain on distribution shifts. We argue that the WiSE-FT model's superiority over a fine-tuned model can be interpreted by its weights being closer to the center of the corresponding weight distribution. Fig. 3 already showed fine-tuned models typically lie on the periphery of flat minima. Given that the angle ∠ w 0 w (50) avr w 1 is nearly a right angle, along the line w 0 w 1 , multiple weight points are closer to the center than a single fine-tuned model, thereby enhancing performance. Note that w H is the closest to the center among the line w 0 w 1 . More discussions on performance boosts observed in distribution shifts are provided in the Appendix C.\n\nModel Soup [40] merges various fine-tuned models' weights trained from varied hyper-parameters. It has been credited with delivering enhanced performance across ImageNet and distribution shifts. Here, we interpret the performance improvements of Model Soup as the result of the proximity to the center of weight distribution. Consider two weight vectors, w A and w B , fine-tuned with different hyper-parameters and following Gaussian distribution N ( µ A , Σ A ) and N ( µ B , Σ B ) respectively. Then, the interpolated weight vector w AB = t · w A + (1 -t ) · w B also follows a Gaussian distribution N ( µ AB , Σ AB ) . The expected squared distance from the interpolated weight vector to its mean µ AB is minimized to trace ( Σ A ) trace ( Σ B ) trace ( Σ A )+ trace ( Σ B ) when t is chosen to trace ( Σ B ) trace ( Σ A )+ trace ( Σ B ) , indicating the reduction of variance through weight interpolation ( i.e ., the distance between w AB and µ AB might be closer than each weight's distance). For example, if trace ( Σ B ) is equal to trace ( Σ A ) , this minimum squared distance is exactly half of the sum of the individual traces when t = 0 . 5 . This insight suggests that the performance gains realized by Model Soup could be due to reduced variance resulting from merging numerous weights.\n\nWe set up a toy experiment to evaluate the effect of variance reduction in the Model Soup scenario by comparing the interpolation of fine-tuned weights with the interpolation of their corresponding weight centers, when N = 2 . In the former case, variance reduction exists along with the effect of merging diverse hyper-parameters, while in the latter case, performance gain would only come from hyper-parameter diversity. If the diversity of hyper-parameters is a major factor, the performance gain from interpolation of central weights should remain the same. To test this, we assessed the ImageNet performance of interpolated weights between pairs of fine-tuned models within Greedy Model Soup 5 [40] and compared it to interpolations between their central weights, calculated as the average of 20 differently seeded models. Fig. E shows that, unlike interpolations between individual models, using the centers does not significantly improve performance. This suggests that proximity to the center of the weight distribution may play a more critical role than hyper-parameter diversity in weight ensemble methods in this case.\n\n5 We opt for Greedy Model Soup to show that even the interpolation of models from the best merging combination does not benefit from the impact of weight diversity.\n\nFig. F: Trend of interpolation ratio t during Model Stock training.\n\n<!-- image -->\n\nIt is also worth noting that µ AB always surpasses the performance of w AB for the same interpolation ratio t , indicating that the importance of proximity to the center remains consistent for interpolated weights. With extensive future research, this understanding could provide valuable insights for developing more generalizable and effective weight-merging techniques.\n\n## F Analysis of the interpolation ratio t\n\nWe analyze the interpolation ratio t = 2 cos θ 1+cos θ in a layer-wise manner. During a Model Stock experiment on CLIP ViT-B/32 with 16 epoch training, we log the layer-wise merge ratios at every merging period. Figure F visualizes the averaged interpolation ratio during Model Stock training. We plot two trends of the interpolation ratio for the layer depth and training step. Our overall observation indicates the bias layers have high merge ratios t ( ≃ 1 ) with small angles θ ( ≃ 0 ), implying that the bias layers do not need to enjoy the pre-trained model, similar to our discussion in §2 and §3. Focusing on the weight layers, Figure Fa shows a U-shape tendency as the layer depth increases, implying the weights of intermediate layers can be more diverse ( i.e ., larger angle θ ) than those of early and later layers. Our intuition here is that since the early and later layers are directly connected to input data and output labels, respectively, they may not demand the advantage of the pre-trained weight. Figure Fb presents that the models at the early training stage are more diverse and they enjoy the pre-trained weights more than those of the later training stage. As the model approaches convergence, the diversity of fine-tuning models decreases ( i.e ., smaller angle θ ).\n\n## G Experimental setup\n\nHere, we present detailed setups for the experiments in §4. We utilize AdamW optimizer [25] with a weight decay of 0.1. We employ two training setups for Model Stock. The first is training Model Stock with a learning rate of 3 × 10 -5 in 10 epochs with minimal data augmentation. The minimal data augmentation utilizes random resize crop augmentation with a minimum crop ratio of 0 . 9 ,\n\nFig. G: Results on LP initialization . We plot in-distribution ImageNet accuracy (x-axis) and distribution shift results (y-axis) with individual fine-tuned models (gray circles) and Model Soups [40]. Note that Model Stock has much smaller ( 35 × smaller) computational costs than Model Soups, leveraging 71 various fine-tuned models as in the original paper.\n\n<!-- image -->\n\nmixup [44] augmentation with β =0.5, following Model Soup's 'standard grid search' setting. The other is training Model Stock with a learning rate of 2 × 10 -5 in 16 epochs with strong data augmentation. The strong data augmentation utilizes random resize crop augmentation with a minimum crop ratio of 0 . 08 and random augmentation [4] ( N = 2 , M = 10 ) following Model Soup's 'random search' setting. When experimenting with the ViT-B/16 and ViT-L/14 models, we adjusted the learning rate and batch size to accommodate the GPU memory constraints.\n\n## H Additional Experiments\n\nWe present additional experimental studies to verify the effectiveness and applicability of Model Stock.\n\n## H.1 Experiments with LP initialization\n\nWe conduct Model Stock with LP initialization and compare it with Model Soups that are initialized from LP. The results are in Fig. G. In this experiment, we use the 16-epoch training setup with strong data augmentation for training Model Stock. As shown in Fig. G, Model Stock outperforms the individual fine-tuned models 6 (gray dots) on ImageNet accuracy. Model Stock also demonstrates competitive performance against Model Soups considering WiSE-FT curves. Note that Model Stock is much more efficient ( 35 × ) than Model Soups, which utilize 71 models in this experiment.\n\n6 All the individual model checkpoints are from the official Model Soup repository.\n\nTable A: Complete results of Table 3 with ObjectNet [1] and ImageNet-ReaL [2].\n\n| Method         | In-distribution   | In-distribution   | Distribution   | Distribution   | Distribution   | Distribution   | Distribution   |\n|----------------|-------------------|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Method         | ImageNet          | IN-ReaL           | IN-V2          | IN-R           | IN-A           | IN-Sketch      | ObjectNet      |\n| Zero-shot      | 68.3              | 75.1              | 62.0           | 77.7           | 49.9           | 48.3           | 54.2           |\n| Vanilla FT     | 82.8              | 87.8              | 72.9           | 66.4           | 43.7           | 48.0           | 51.8           |\n| Vanilla FT ∗   | 83.7              | 87.8              | 73.5           | 67.6           | 40.0           | 48.6           | 50.1           |\n| LP [18]        | 79.7              | -                 | 71.5           | 52.4           | 27.8           | 40.5           | -              |\n| LP-FT [18]     | 81.7              | -                 | 71.6           | 72.9           | 49.1           | 48.4           | -              |\n| CAR-FT [27]    | 83.2              | -                 | 73.0           | 71.3           | 43.7           | 49.5           | -              |\n| FTP [37]       | 84.2              | -                 | 74.6           | 47.2           | 26.5           | 50.2           | -              |\n| FLYP [7]       | 82.6              | -                 | 73.0           | 71.4           | 48.1           | 49.6           | 58.7           |\n| Lipsum-FT [28] | 83.3              | -                 | 73.6           | 75.9           | 49.9           | 51.4           | 54.4           |\n| CaRot [29]     | 83.1              | -                 | 74.1           | 77.7           | 51.6           | 52.7           | 56.6           |\n| Model Stock    | 84.1              | 88.8              | 74.8           | 71.8           | 51.2           | 51.8           | 55.0           |\n| Model Stock ⋆  | 85.2              | 89.1              | 75.3           | 68.7           | 45.0           | 51.3           | 52.3           |\n\nTable B: Comparison against Model Soups [40] on CLIP ViT-B/16. Model Stock shows comparable performance with Model Soups.\n\n| Method               |   ImageNet Avg. shifts |   ImageNet Avg. shifts |\n|----------------------|------------------------|------------------------|\n| CLIP zero-shot Init. |                   68.3 |                   58.4 |\n| Vanilla FT           |                   82.8 |                   56.6 |\n| Vanilla FT ⋆         |                   83.7 |                   55.9 |\n| Uniform Model Soup   |                   84.4 |                   62.7 |\n| Greedy Model Soup    |                   84.3 |                   60.4 |\n| Model Stock          |                   84.1 |                   61   |\n| Model Stock ⋆        |                   85.2 |                   58.5 |\n\n## H.2 Complete comparison results on CLIP ViT-B/16\n\nIn the main paper, we omit the results of ObjectNet [1] on CLIP ViT-B/16 experiments since the comparison methods such as LP-FT [18], FTP [37] have not evaluated on ObjectNet benchmark. We here show the results with ObjectNet [1] and ImageNet-ReaL [2] of CLIP ViT-B/16 in Table A. We additionally compare Model Stock with recent fine-tuning methods including FLYP [7], Lipsum-FT [28], and CaRot [29] Model Stock consistently demonstrates its effectiveness with ObjectNet and ImageNet-ReaL as well.\n\n## H.3 Model Stock vs. Model Soups on CLIP ViT-B/16\n\nTable B shows the performance of Model Stock on the pretrained CLIP ViT-B/16 model. Since the original Model Soups paper [40] only provides CLIP ViT-B/32 models, we replicate Model Soups experiments on CLIP ViT-B/16. We finetuned 48 models from CLIP ViT-B/16 initialization following the standard grid hyper-parameter sweep ( i.e ., zero-shot initialization setting). Model Stock shows\n\nTable C: Model Stock with different hyper-parameters on CLIP ViT-B/32 .\n\n| Method                                    | ImageNet     | Avg. shifts   |\n|-------------------------------------------|--------------|---------------|\n| Model Stock                               | 79.89        | 50.99         |\n| Model Stock w/ different hyper-parameters | 79.75 ± 0.45 | 50.40 ± 0.84  |\n\nTable D: Performance comparison of merging units in Model Stock. This table presents the overall performance of Model Stock using different merging units: entire weight merging, entire weight merging based on transformer block angle, layerwise merging, and filter-wise merging. It highlights the effectiveness of each strategy in approaching the weight center and their impact on the model's performance.\n\n| Merging Unit                      | Target   | Target   | Avg. Shifts   |\n|-----------------------------------|----------|----------|---------------|\n|                                   | IN       | IN-ReaL  |               |\n| Entire weights                    | 79.69    | 85.39    | 46.40         |\n| Entire weights (rep. blocks only) | 79.64    | 85.38    | 48.28         |\n| Layer-wise (ours)                 | 80.12    | 85.65    | 48.84         |\n| Filter-wise                       | 80.10    | 85.67    | 48.72         |\n\ncomparable performance against Model soups. Note that Model Soups requires 24 × more training cost than Model Stock.\n\n## H.4 Model Stock with different hyper-parameters\n\nTo verify the validity of Model Stock beyond the setup of the main paper ( i.e ., different random seeds with the same hyper-parameters), we conduct Model Stock with different hyper-parameters. In detail, when we fine-tune two models for Model Stock, we choose different hyper-parameter for each model ( e.g ., learning rate, data augmentation.). To ensure the basic assumption of Model Stock, we use the same batch size and training epochs. C shows the experimental results on CLIP ViT-B/32. We repeat 5 runs and report accuracy with standard deviation. Model Stock with different hyper-parameters shows comparable performance to the original one.\n\n## H.5 Ablation study on merging unit\n\nWe investigate the efficacy of different merging units within our method, Model Stock. Our default approach employs layer-wise merging, but alternatives include merging based on the angle between 1) entire weights, 2) weights of the entire repetitive transformer blocks following [40], or 3) using a filter-wise approach as discussed in §A.3. The results of these ablations are summarized in Table D, where we assess the overall performance based on the chosen merging unit.\n\nOur analysis reveals that the accuracy of noise distribution estimation is critical in approaching the weight center. When assuming weight noise across the\n\nentire model, our method does not approximate the weight center as effectively as it does with layer-wise merging, leading to suboptimal overall performance. Similarly, the merging performance based on the angle of transformer blocks was insufficient. Conversely, while filter-wise noise demonstrates a larger standard deviation in angle, as depicted in Fig. N, this increased variance results in a more significant error in Gaussian distribution approximation. Consequently, the overall performance under filter-wise merging is slightly inferior to layerwise one.\n\nThese findings underscore the importance of accurately modeling noise distribution in enhancing the performance of Model Stock. As our understanding and ability to model this noise distribution improve, we anticipate further increases in the efficacy and robustness of our approach.\n\n<!-- image -->\n\n(c)\n\nOpenCLIP ConvNeXt\n\nFig. H: Layer-wise angle and norm across different model architectures. The angle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are displayed from top to bottom. These metrics demonstrate consistency regardless of the model type from left (first layer) to right (last layer). It is important to note that we also depict the error bars for each layer in all figures, but they are not visible in most layers due to the small standard deviation.\n\n<!-- image -->\n\n(b)\n\nSGD optimizer with momentum\n\nFig. I: Layer-wise angle and norm across different optimizers. Displayed from top to bottom are the angle and norm for models trained with SGD and SGD with momentum, respectively. These metrics demonstrate consistency regardless of the optimization strategy from left (first layer) to right (last layer).\n\n<!-- image -->\n\n(c)\n\n+ RRC\n\nFig. J: Layer-wise angle and norm across different augmentations. Displayed from top to bottom are the angle and norm for the vanilla model (10 epochs + no augmentation), +longer epochs (16 epochs), and +RRC. Each augmentation is applied incrementally. These metrics demonstrate consistency regardless of the augmentations from left (first layer) to right (last layer).\n\n<!-- image -->\n\n0.0000\n\nFig. K: Layer-wise angle and norm across different datasets. The angle and norm for models trained on different datasets, including CIFAR [17] are displayed from top to bottom. These metrics demonstrate consistency regardless of the dataset type from left (first layer) to right (last layer).\n\n<!-- image -->\n\n0.0000\n\nFig. L: Layer-wise angle and norm across different classifier initializations. The angle and norm for models trained with differently initialized networks following the LP-FT [18] method are displayed from top to bottom. These metrics demonstrate consistency regardless of the initialization method from left (first layer) to right (last layer).\n\nFig. M: Layer-wise angle during training. Displayed are the overlapped angles across models trained with different random seeds at each timestamp. Even during training, the angle remains highly consistent, decreasing as training progresses.\n\n<!-- image -->\n\n<!-- image -->\n\n(c)\n\nFilter-wise angle between attention weights in the second transformer block of ViT-B/32\n\nFig. N: Filter-wise angle for attention and MLP layers in ViT-B/32. We display filter-wise angles for each layer. Each bar represents each row ( i.e ., filter) in the given layer. Interestingly, the angles between the filters of the fine-tuned weights exhibit similar values, while the standard deviation between each filter is notably larger than that of the angle between each layer. Due to the large number of layers, only representative layers are selected for display.\n\n<!-- image -->\n\nFig. O: Layer-wise angle and norm for DeiT. The angle and norm for DeiTbase models are displayed, each trained with different random seeds. These models are initially pre-trained on ImageNet-21K [34] and then fine-tuned on ImageNet-1K. The consistency observed in the metrics is maintained even in the DeiT training setting.",
  "tables": [
    {
      "index": 0,
      "markdown": "|                | ∥ w - µ ∥     |   ImageNet |   Avg. shifts |\n|----------------|---------------|------------|---------------|\n| Fine-tuned     | 13.133 ± .004 |      79.72 |         46.37 |\n| w (2) avr      | 9.192 ± .003  |      80.24 |         47.76 |\n| w (3) avr      | 7.439 ± .025  |      80.35 |         48.18 |\n| w (5) avr      | 5.633 ± .014  |      80.47 |         48.53 |\n| w (50) avr ≃ µ | ∼ 0           |      80.59 |         48.85 |"
    },
    {
      "index": 1,
      "markdown": "| Method                                                                        |   ImageNet |   Avg. shifts | Cost   |\n|-------------------------------------------------------------------------------|------------|---------------|--------|\n| Comparing with Model Soups from zero-shot init. CLIP zero-shot Initialization |      63.34 |         48.51 | 0      |\n| Vanilla FT                                                                    |      78.35 |         47.03 | 1      |\n| Uniform Model Soup (from zero-shot)                                           |      79.76 |         52.08 | 48     |\n| Greedy Model Soup (from zero-shot)                                            |      80.42 |         50.83 | 48     |\n| Model Stock                                                                   |      79.89 |         50.99 | 2      |\n| Comparing with Model Soups from LP init. CLIP LP initialization               |      75.57 |         47.21 | α      |\n| Vanilla FT ⋆                                                                  |      79.72 |         46.37 | 1      |\n| Uniform Model Soup (from LP init)                                             |      79.97 |         51.45 | 71+ α  |\n| Greedy Model Soup (from LP init)                                              |      81.03 |         50.75 | 71+ α  |\n| Model Stock ⋆                                                                 |      81.19 |         48.69 | 2      |"
    },
    {
      "index": 2,
      "markdown": "|               |          | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   | Distribution shifts   |\n|---------------|----------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Method        | ImageNet | Avg. shifts           | IN-V2                 | IN-R                  | IN-A                  | IN-Sketch             |\n| Zero-shot     | 68.3     | 59.5                  | 62.0                  | 77.7                  | 49.9                  | 48.3                  |\n| Vanilla FT    | 82.8     | 57.7                  | 72.9                  | 66.4                  | 43.7                  | 48.0                  |\n| Vanilla FT ⋆  | 83.7     | 57.4                  | 73.5                  | 67.6                  | 40.0                  | 48.6                  |\n| LP [18]       | 79.7     | 48.1                  | 71.5                  | 52.4                  | 27.8                  | 40.5                  |\n| LP-FT [18]    | 81.7     | 60.5                  | 71.6                  | 72.9                  | 49.1                  | 48.4                  |\n| CAR-FT [27]   | 83.2     | 59.4                  | 73.0                  | 71.3                  | 43.7                  | 49.5                  |\n| FTP [37]      | 84.2     | 49.7                  | 74.6                  | 47.2                  | 26.5                  | 50.2                  |\n| FLYP [7]      | 82.6     | 60.5                  | 73.0                  | 71.4                  | 48.1                  | 49.6                  |\n| Model Stock   | 84.1     | 62.4                  | 74.8                  | 71.8                  | 51.2                  | 51.8                  |\n| Model Stock ⋆ | 85.2     | 60.1                  | 75.3                  | 68.7                  | 45.0                  | 51.3                  |"
    },
    {
      "index": 3,
      "markdown": "|      |   IN |   Avg. Shifts |   ∥ w - µ ∥ |\n|------|------|---------------|-------------|\n| FT   | 79.7 |          46.7 |       13.13 |\n| N =2 | 80.1 |          48.8 |       10.01 |\n| N =3 | 80.2 |          48.8 |        9.05 |\n| N =4 | 80.4 |          48.9 |        8.45 |"
    },
    {
      "index": 4,
      "markdown": "| Period     |   IN |   Avg. Shifts |\n|------------|------|---------------|\n| 1000 iters | 79.8 |          48.7 |\n| 5000 iters | 79.9 |          48.5 |\n| 1 epoch    | 80.1 |          48.8 |"
    },
    {
      "index": 5,
      "markdown": "|               |   IN |   Avg. shifts |\n|---------------|------|---------------|\n| Zero-shot     | 75   |          63   |\n| Vanilla FT    | 85.8 |          66.8 |\n| Vanilla FT ⋆  | 87.1 |          68   |\n| TPGM [36]     | 87   |          69.4 |\n| CAR-FT [27]   | 87.1 |          67.8 |\n| Model Stock   | 87   |          71.6 |\n| Model Stock ⋆ | 87.7 |          73.5 |"
    },
    {
      "index": 6,
      "markdown": "| Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Uniform averaging ( w N avg   | Model Stock (post-training)   | Model Stock (post-training)   | Model Stock (post-training)   |\n|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|\n|                               | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     | ImageNet                      | Avg. Shifts                   | ∥ w - µ ∥                     |\n| N =2                          | 80.2                          | 47.8                          | 9.19                          | 80.3 (+0.1)                   | 50.4 (+2.6)                   | 7.62 (-1.57)                  |\n| N =3                          | 80.4                          | 48.2                          | 7.44                          | 80.4 (+0.0)                   | 50.2 (+2.0)                   | 6.49 (-0.95)                  |\n| N =4                          | 80.5                          | 48.5                          | 5.63                          | 80.5 (+0.0)                   | 49.8 (+1.4)                   | 5.16 (-0.47)                  |"
    },
    {
      "index": 7,
      "markdown": "| Method         | In-distribution   | In-distribution   | Distribution   | Distribution   | Distribution   | Distribution   | Distribution   |\n|----------------|-------------------|-------------------|----------------|----------------|----------------|----------------|----------------|\n| Method         | ImageNet          | IN-ReaL           | IN-V2          | IN-R           | IN-A           | IN-Sketch      | ObjectNet      |\n| Zero-shot      | 68.3              | 75.1              | 62.0           | 77.7           | 49.9           | 48.3           | 54.2           |\n| Vanilla FT     | 82.8              | 87.8              | 72.9           | 66.4           | 43.7           | 48.0           | 51.8           |\n| Vanilla FT ∗   | 83.7              | 87.8              | 73.5           | 67.6           | 40.0           | 48.6           | 50.1           |\n| LP [18]        | 79.7              | -                 | 71.5           | 52.4           | 27.8           | 40.5           | -              |\n| LP-FT [18]     | 81.7              | -                 | 71.6           | 72.9           | 49.1           | 48.4           | -              |\n| CAR-FT [27]    | 83.2              | -                 | 73.0           | 71.3           | 43.7           | 49.5           | -              |\n| FTP [37]       | 84.2              | -                 | 74.6           | 47.2           | 26.5           | 50.2           | -              |\n| FLYP [7]       | 82.6              | -                 | 73.0           | 71.4           | 48.1           | 49.6           | 58.7           |\n| Lipsum-FT [28] | 83.3              | -                 | 73.6           | 75.9           | 49.9           | 51.4           | 54.4           |\n| CaRot [29]     | 83.1              | -                 | 74.1           | 77.7           | 51.6           | 52.7           | 56.6           |\n| Model Stock    | 84.1              | 88.8              | 74.8           | 71.8           | 51.2           | 51.8           | 55.0           |\n| Model Stock ⋆  | 85.2              | 89.1              | 75.3           | 68.7           | 45.0           | 51.3           | 52.3           |"
    },
    {
      "index": 8,
      "markdown": "| Method               |   ImageNet Avg. shifts |   ImageNet Avg. shifts |\n|----------------------|------------------------|------------------------|\n| CLIP zero-shot Init. |                   68.3 |                   58.4 |\n| Vanilla FT           |                   82.8 |                   56.6 |\n| Vanilla FT ⋆         |                   83.7 |                   55.9 |\n| Uniform Model Soup   |                   84.4 |                   62.7 |\n| Greedy Model Soup    |                   84.3 |                   60.4 |\n| Model Stock          |                   84.1 |                   61   |\n| Model Stock ⋆        |                   85.2 |                   58.5 |"
    },
    {
      "index": 9,
      "markdown": "| Method                                    | ImageNet     | Avg. shifts   |\n|-------------------------------------------|--------------|---------------|\n| Model Stock                               | 79.89        | 50.99         |\n| Model Stock w/ different hyper-parameters | 79.75 ± 0.45 | 50.40 ± 0.84  |"
    },
    {
      "index": 10,
      "markdown": "| Merging Unit                      | Target   | Target   | Avg. Shifts   |\n|-----------------------------------|----------|----------|---------------|\n|                                   | IN       | IN-ReaL  |               |\n| Entire weights                    | 79.69    | 85.39    | 46.40         |\n| Entire weights (rep. blocks only) | 79.64    | 85.38    | 48.28         |\n| Layer-wise (ours)                 | 80.12    | 85.65    | 48.84         |\n| Filter-wise                       | 80.10    | 85.67    | 48.72         |"
    }
  ],
  "stats": {
    "pages": 36,
    "chunksCreated": 119,
    "totalCharacters": 79570,
    "totalWords": 12249,
    "numTables": 11,
    "processingTimeMs": 36949
  }
}