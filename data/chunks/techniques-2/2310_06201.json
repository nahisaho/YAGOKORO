{
  "paper": {
    "id": "2310.06201v1",
    "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
    "abstract": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.",
    "authors": [
      "Yucheng Li",
      "Bo Dong",
      "Chenghua Lin",
      "Frank Guerin"
    ],
    "published": "2023-10-09T23:03:24.000Z",
    "updated": "2023-10-09T23:03:24.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.06201v1",
    "absUrl": "https://arxiv.org/abs/2310.06201v1"
  },
  "chunks": [
    {
      "id": "2310.06201v1-chunk-0",
      "content": "Yucheng Li 1 , Bo Dong 1 , Chenghua Lin 2 ∗ , Frank Guerin 1\n\n1 University of Surrey 2 University of Manchester\n\n{yucheng.li, bd00531, f.guerin}@surrey.ac.uk chenghua.lin@manchester.ac.uk",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-1",
      "content": "Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-2",
      "content": "tal results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance. Code and data are available at https://github.com/ liyucheng09/Selective\\_Context .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-3",
      "content": "Large language models (LLMs) have demonstrated remarkable power and impressive generalisation abilities across a wide range of natural language processing tasks, as well as real-life applications (Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023). However, a major challenge for existing LLMs is processing longer context. Dealing with longer context with LLMs is fundamen-\n\n∗ Corresponding author\n\nContext: Large Languages Models (LLMs) have shown their ability to perform new tasks, resulting in a line of work that focuses on further scaling these models. These efforts are based on the assumption { that more parameters will lead to better performance. }\n\nQuery: What's the assumption behind the efforts to further scale LLMs?\n\nLLMs: Further scaling Large Language Models will lead to better performance on a wide range of tasks.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-4",
      "content": "to better performance. }\n\nQuery: What's the assumption behind the efforts to further scale LLMs?\n\nLLMs: Further scaling Large Language Models will lead to better performance on a wide range of tasks.\n\nFigure 1: Some context is redundant because LLMs have learned that knowledge. LLMs can generate the correct answer even when these redundancies are deleted.\n\ntal in scenarios such as having long conversations, document summarisation, and question answering given long documents. However, it is very computationally expensive, particularly with Transformer based LLMs, due to the quadratic growth of memory and computation associated with the 2-D attention matrix (Vaswani et al., 2017). This makes LLMs less accessible and sometimes leads to context truncation during inference. Moreover, due to the above limitation, existing LLMs were usually pre-trained with fixed-context windows, which further constrains their capability in processing longer context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-5",
      "content": "tion during inference. Moreover, due to the above limitation, existing LLMs were usually pre-trained with fixed-context windows, which further constrains their capability in processing longer context.\n\nThere are active attempts in reducing the computation and memory cost of the Transformer architecture with sparse attention (Child et al., 2019) or local dense attention (Beltagy et al., 2020). There are also efforts to learn soft prompts with further distillation to save context cost during inference (Mu et al., 2023; Chevalier et al., 2023). In contrast to existing approaches that primarily focus on architectures or distillations, we introduce a fresh perspective to tackle the redundancy in the input context itself, thus proposing a complementary, modelagnostic approach that can be potentially combined with other architecture optimisation methods to further enhance inference efficiency.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-6",
      "content": "the input context itself, thus proposing a complementary, modelagnostic approach that can be potentially combined with other architecture optimisation methods to further enhance inference efficiency.\n\nThe proposed method is motivated by the potential redundancy and repetition in human language, which has two main sources. The first is the inherent redundancy of natural language. For example, in the conversation \"A: Did you get the chance to pick up groceries today?\" , \"B: Yes, I did get the groceries.\" , the underlined part can be seen as a common redundancy in communication. Linguistic studies suggest redundancy is ubiquitous in language (Wit and Gillette, 1999). The other type of input redundancy is from the overlap with training material. As the example in Fig. 1 shows, if some parts of input have already been included in the pre-training stage of LLMs, then it is safe to delete them and the model can still generate the correct answer.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-7",
      "content": "As the example in Fig. 1 shows, if some parts of input have already been included in the pre-training stage of LLMs, then it is safe to delete them and the model can still generate the correct answer. In summary, redundancy in the input context, while beneficial for human comprehension, can be extraneous for LLMs and might lead to unnecessary computational expense.\n\nIn this paper, we propose Selective Context , which prunes redundant content in a given input context, thereby reducing the computational cost and making better use of the fixed context length in LLMs. Selective Context evaluates informativeness of lexical units (i.e., tokens, phrases, or sentences) with self-information (Shannon, 1948) computed by a base causal language model. By selectively retaining content with higher self-information, our method provides a more compact and efficient context representation for LLMs to process without compromising their performance on various applications.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-8",
      "content": "aining content with higher self-information, our method provides a more compact and efficient context representation for LLMs to process without compromising their performance on various applications.\n\nWe evaluate the effectiveness and different settings of Selective Context on arXiv papers, BBC News, and real conversation on ShareGPT.com with four NLP tasks: summarisation, question answering, original context reconstruction, and conversation. Experimental results demonstrate that our proposed method can significantly enhance context efficiency of LLMs during inference while maintaining comparable performance compared to that achieved when full context is used.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-9",
      "content": "Self-information, also known as surprisal or information content , is a fundamental concept in information theory that quantifies the amount of information conveyed by an event given a distribution (Shannon, 1948). In the context of language modelling, the event can be regarded as one step of generation (i.e., a token) and the distribution corresponds to its output distribution. So the selfinformation of a token can be defined as the negative log likelihood:\n\n<!-- formula-not-decoded -->\n\nwhere I ( x ) represents the self-information of token x and P ( x ) denotes its output probability.\n\nIn information theory, self-information measures the level of surprise or uncertainty associated with an event; rare events convey more information and thus have higher self-information, while common events convey less information and have lower self-information.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "2 Self-Information",
        "chunkIndex": 9,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-10",
      "content": "urprise or uncertainty associated with an event; rare events convey more information and thus have higher self-information, while common events convey less information and have lower self-information. In the context of language modelling, self-information can be used to assess the informativeness of lexical units, e.g., words, phrases, or sentences. Lexical units with lower self-information are less informative and thus are more likely to be inferred from the context. As a result, we may treat these parts of input as redundant during LLM inference.\n\nIn NLP, self-information has been used to measure surprise in creative language artefacts (Bunescu and Uduehi, 2022). In addition, related concepts of self-information such as entropy and perplexity are widely used in language model optimisation and evaluation.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "2 Self-Information",
        "chunkIndex": 10,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-11",
      "content": "addition, related concepts of self-information such as entropy and perplexity are widely used in language model optimisation and evaluation.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the entropy H ( S ) of the sentence S = ( x 0 , ..., x n ) is the average self-information of words in the sentence, and perplexity PP ( S ) of the sentence can be calculated with entropy. The property of self-information that is especially relevant to our method is the additivity.\n\n<!-- formula-not-decoded -->\n\nThis means we can calculate the self-information of a lexical unit by simply summing the selfinformation of the tokens in it.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "2 Self-Information",
        "chunkIndex": 11,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-12",
      "content": "Selective Context optimises the input context by filtering out redundant or non-essential content to reduce computational cost and make better use of the limited context window. In implementation,\n\nOriginal: INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [ To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) . ] . Ideal CL models in the real world should be deal with domain shifts , researchers have recently started to sample tasks from two different datasets . For instance , proposed to train and evaluate a model on Imagenet first and then challenge its performance on the Places365 dataset . considers more scenarios , starting with Imagenet or Places365 , and then moving on to the VOC/CUB/Scenes datasets .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3 Method",
        "chunkIndex": 12,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-13",
      "content": "del on Imagenet first and then challenge its performance on the Places365 dataset . considers more scenarios , starting with Imagenet or Places365 , and then moving on to the VOC/CUB/Scenes datasets . Few works propose more advanced scenarios built on top of more than two datasets .\n\nFiltered: INTRODUCTION Continual Learning ( a promising learning paradigm to design models have to how across overTo uniform the language and enhance adopt the unique term continual learning Ideal CL models in should deal domain shifts researchers recently started sample tasks two different datasets For instance proposed to train and evaluate on Imagenet first challenge Places365considers more scenarios starting Imagenet or Places365 the VOC/CUB/Scenes datasetsFew works propose more advanced scenarios built top more than two datasets\n\nFigure 2: A visualisation of selective context. Darker colour indicates larger value of self-information.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3 Method",
        "chunkIndex": 13,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-14",
      "content": "CUB/Scenes datasetsFew works propose more advanced scenarios built top more than two datasets\n\nFigure 2: A visualisation of selective context. Darker colour indicates larger value of self-information.\n\nwe first 1) employ a causal language model such as GPT (Radford et al., 2019; Brown et al., 2020), OPT (Zhang et al., 2022), or LLaMA (Touvron et al., 2023), computing self-information for each token. We then 2) merge tokens, along with their corresponding self-information values, into lexical units, which can be phrases or sentences. This step is optional if tokens are being used as the basic units. Finally, 3) we eliminate content that is deemed least necessary to render the input more compact.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3 Method",
        "chunkIndex": 14,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-15",
      "content": "Given a context C = x 0 , x 1 , ..., x n , where x i denotes a token, we use a base language model M to compute the self-information for each token x t as follows:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3.1 Computing Self-Information",
        "chunkIndex": 15,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-16",
      "content": "If the content filtering of selective context is directly performed on the token level, it might lead to very disjoint context. Therefore apart from token level filtering, we also conduct the filtering procedure on phrase and sentence level. We call a basic unit in our filtering a lexical unit , which could be a token, a phrase or a sentence in our setting.\n\nTo enable selective context to work on phrases and sentences, we merge tokens and their selfinformation into lexical units. Each lexical unit u consists of multiple tokens ( x t , ..., x t + α ) , and we can calculate its self-information by summing the self-information of its individual tokens according to the additivity property of self-information:\n\n<!-- formula-not-decoded -->\n\nThe NLTK sentence tokenizer is employed to obtain sentence level lexical units. And we use spacy 1 to merge tokens into noun phrases. We do not merge verb phrases as it might produce very long phrases.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3.2 Merging into Lexical Units",
        "chunkIndex": 16,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-17",
      "content": "With the self-information of each lexical unit computed, we can now evaluate their informativeness. Instead of using a fixed threshold or retaining a fixed number of top k lexical units, we design a percentile-based filtering approach to adaptively select the most informative content.\n\nFirst, we rank the lexical units based on their self-information values in descending order. Then, we compute the p -th percentile of self-information values among all lexical units.\n\n<!-- formula-not-decoded -->\n\nNext, we selectively retain lexical units with selfinformation values greater than or equal to the p -th percentile, constructing a filtered context C ′ :\n\n<!-- formula-not-decoded -->\n\nThe percentile-based filtering is a more flexible approach to retain the most informative content depending on the distribution of self-information values in the given context. In Figure 2, we present\n\n1 https://spacy.io/api/ pipeline-functions#merge\\_noun\\_chunks",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3.3 Selective Retention of Informative Context",
        "chunkIndex": 17,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-18",
      "content": "tain the most informative content depending on the distribution of self-information values in the given context. In Figure 2, we present\n\n1 https://spacy.io/api/ pipeline-functions#merge\\_noun\\_chunks\n\nan example on phrase level where p is set to 50, which means half of phrases are filtered out. In this case, the context after processing by selective context only retains 57.2% of tokens, which saves 42.7% of context length.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "3.3 Selective Retention of Informative Context",
        "chunkIndex": 18,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-19",
      "content": "The goal of Selective Context is to reduce the redundancy in the input context without compromising the generation quality of LLMs. As a result, we are expecting the answers given both selective context and the original context to be as close as possible. We take the generated answer given full context as the reference answer, and compare to the generated answer given the selective context in our experiments.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4 Experiments",
        "chunkIndex": 19,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-20",
      "content": "Selective Context prunes redundancy in the input context to allow very long context processing for LLMs. However, existing benchmarks for LLMs, such as MMLU (Hendrycks et al., 2020) and ARC (Clark et al., 2018), are mostly single round question answering and are thus not suitable to evaluate our proposed method. Therefore, we collect three test sets consisting of long documents and conversations to evaluate Selective Context. Statistics in detail are presented in Table 4.\n\nBBC News: Adataset containing news articles collected from the British Broadcasting Corporation (BBC). This dataset covers a wide range of topics, including politics, business, sports, and technology. We use the full content of each news article in our experiments.\n\narXiv Articles: A dataset consisting of latest academic papers, spaning various scientific disciplines, such as computer science, physics, and mathematics.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.1 Datasets",
        "chunkIndex": 20,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-21",
      "content": "nt of each news article in our experiments.\n\narXiv Articles: A dataset consisting of latest academic papers, spaning various scientific disciplines, such as computer science, physics, and mathematics. As arXiv articles can be quite long, we only process the first two sections (usually introduction and background) for each paper in our experiments. ShareGPT.com: ShareGPT.com is a platform where ChatGPT users share their surprising and interesting conversation with ChatGPT. This datasets consists of conversations in different languages and in various scenarios (e.g., coding, chitchat, writing assistant, etc.). We use the ShareGPT dataset for the conversation task in our experiments.\n\nThe three evaluation datasets were created carefully to avoid data contamination . Data samples in the BBC News, arXiv, and ShareGPT.com datasets were all created after March 2023, which is after the release of all LLMs in our experiments.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.1 Datasets",
        "chunkIndex": 21,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-22",
      "content": "ted carefully to avoid data contamination . Data samples in the BBC News, arXiv, and ShareGPT.com datasets were all created after March 2023, which is after the release of all LLMs in our experiments. Considering some of baseline models are continually being updated, we employ the latest version released before 30 March 2023 to make sure models have never seen our test set in their pre-training and fine-tuning stage. In addition, as some of LLMs in our experiment has a max\\_length of 2048 tokens, we do not include articles or conversations exceeding this length.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.1 Datasets",
        "chunkIndex": 22,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-23",
      "content": "We test Selective Context on the following models: GPT-3.5, GPT-4: GPT-3.5 also known as ChatGPT, which is likely to be further fine-tuned from GPT-3 and InstructGPT. GPT-4 is the latest model from OpenAI, which has demonstrated substantially improved capability on complex reasoning compared to its predecessor. GPT-3.5 and GPT4 are unfortunately not open-source, we can only access these models via web api 2 .\n\nLLaMA-7, 13, 30B: LLaMA is a family of opensource language models released by Meta, which is reported to outperform GPT-3 with less parameters. The LLaMA family includes models with size ranging from 7B to 65B. To investigate the effect of scaling law to Selective Context, we experiment with LLaMA with 7B, 13B, and 30B parameters.\n\nVicuna-7, 13B: Vicuna (Chiang et al., 2023) is a family of open-source language models instructtuned from LLaMA. According to their technical report, Vicuna model perform quite well on a list of multitasking benchmarks.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.2 Models",
        "chunkIndex": 23,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-24",
      "content": "We evaluate Selective Context on four tasks:\n\nOriginal Context Reconstruction: Given a compressed context produced by Selective Context, this task aims to evaluate whether models are able to reconstruct the original context. This task assesses how well the filtered context retains the essential information from the original context. In our experiments, the compressed contexts are used as input, and the original contexts are used as reference answers.\n\nSummarisation: Given a context, the task is to generate a summary that captures the main points of the document. This task aims to evaluate whether Selective Context affects the overall understanding of models on the input contexts. In\n\n2 https://platform.openai.com/docs/ api-reference\n\nour experiments, the input and output are the compressed context and the summaries generated based on the compressed contexts. Summaries based on the original (full) contexts are treated as the reference answers.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.3 Tasks and Metrics",
        "chunkIndex": 24,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-25",
      "content": "nts, the input and output are the compressed context and the summaries generated based on the compressed contexts. Summaries based on the original (full) contexts are treated as the reference answers.\n\nQuestion Answering (QA): Given a document and a set of questions, the task is to generate answers based on the information available in the document. This task aims to evaluate models' understanding towards a specific query. Here we first generate questions and answers based on the original context, where these answers are treated as reference answers, and then ask LLMs to answer these questions with selective context.\n\nConversation: This task is only for the ShareGPT dataset. Given a conversation history and a user query, the task is to generate a response to the query based on the previous conversation history. This task aims to evaluate selective context's performance on conversation.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.3 Tasks and Metrics",
        "chunkIndex": 25,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-26",
      "content": "sation history and a user query, the task is to generate a response to the query based on the previous conversation history. This task aims to evaluate selective context's performance on conversation. Specifically, we ask LLMs to answer users' last query of ShareGPT conversation instances with selective context applied on the previous conversation history.\n\nWe employ four metrics to assess the performance of our models on the tasks: BLEU, METEOR, ROUGE, and BERTScore. BLEU (Papineni et al., 2002) calculates n-gram precision, which is the proportion of n-grams in the generated text that are also present in the reference text. METEOR (Banerjee and Lavie, 2005) take additional features such as synonymy, stemming and word order into consideration, which leads to more comprehensive evaluation. ROUGE (Lin, 2004) focuses on how much of the important information in the reference text is present in the generated summary.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.3 Tasks and Metrics",
        "chunkIndex": 26,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-27",
      "content": "rd order into consideration, which leads to more comprehensive evaluation. ROUGE (Lin, 2004) focuses on how much of the important information in the reference text is present in the generated summary. BERTScore (Zhang et al., 2019) leverages contextualised embeddings from pre-trained language models like BERT, computing the cosine similarity between the generated text and reference text embeddings to capture semantic similarity more effectively than traditional n-gram-based metrics.\n\nAs mentioned before, we use the generated answers given the full contexts as the reference answers. When testing the deterministic decoding strategy ( greedy decoding), we take one single run on full context as the reference answer. When testing non-deterministic decoding strategy ( temperature = 0.7 ), we run multiple times on full context to obtain multiple reference answers to address the randomness in decoding. The metrics are computed based on the set of reference answers.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.3 Tasks and Metrics",
        "chunkIndex": 27,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-28",
      "content": "( temperature = 0.7 ), we run multiple times on full context to obtain multiple reference answers to address the randomness in decoding. The metrics are computed based on the set of reference answers. In our experiment, we set the number of reference answers to 4.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.3 Tasks and Metrics",
        "chunkIndex": 28,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-29",
      "content": "We use smaller base causal language model for selfinformation computing in our experiments. For the LLaMA family and vicuna family, we employ LLaMA-7B to compute self-information. For the OpenAI family, we use a smaller GPT-3 variant curie for self-information computing, which is available on OpenAI web API. In self-information computing, we do not process the entire context at once. This is due to our observation on the tendency of LLMs to give later lexical units lower selfinformation. Instead, we compute self-information sentence by sentence in our experiments.\n\nIn our experiments, we compare the two different dimensions that are adjustable in Selective Context.\n\nCompression Ratios: We experiment with different content reduction ratios in Selective Context: 0.2, 0.35, 0.5, 0.65, and 0.8. These ratios determine the proportion of content to be filtered out, allowing us to study the trade-off between efficiency and performance as the amount of retained information varies.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.4 Experimental Settings",
        "chunkIndex": 29,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-30",
      "content": "0.65, and 0.8. These ratios determine the proportion of content to be filtered out, allowing us to study the trade-off between efficiency and performance as the amount of retained information varies.\n\nLexical Units: Lexical units are the basic element of content reduction in Selective Context. It can be sentence, phrases, or tokens. As mentioned in §3.2, we remove the redundancy in input context by a specific lexical unit level.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "4.4 Experimental Settings",
        "chunkIndex": 30,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-31",
      "content": "Except §5.5, all results of selective context presented are at the phrase level (the optimal).",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5 Results",
        "chunkIndex": 31,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-32",
      "content": "In Table 1, we first compare the performance of Selective Context against the Original Context to see how well Selective Context preserves useful information when reducing context cost. The metrics are averaged across all models mentioned in §4.2. The performance drop is shown in parentheses.\n\nAs demonstrated in the table, using Selective Context only leads to a marginal drop when the reduction ratio is set to 0.2 or 0.35, despite it significantly reducing the context cost. The BLEU score drops by only 0.05 when 20% of the content is reduced. And the number is even smaller when it comes to ROUGE-1, where the drop is just 0.03. This indicate a high level of consistency be-\n\nTable 1: Comparing Selective Context to the Original Context when temperature set to 0.7.\n\n|                   |       |            |            | ROUGE        | ROUGE      | ROUGE      | BERTScore   | BERTScore   | BERTScore     |\n|-------------------|-------|------------|------------|--------------|------------|---",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 32,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-33",
      "content": "|            |            | ROUGE        | ROUGE      | ROUGE      | BERTScore   | BERTScore   | BERTScore     |\n|-------------------|-------|------------|------------|--------------|------------|------------|-------------|-------------|---------------|\n| Method            | Ratio | BLEU       | METEOR     | rouge1       | rouge2     | rougeL     | Precision   | Recall      | F1            |\n| Original          | -     | .347       | .496       | .571         | .383       | .471       | .910        | .909        | .909          |\n| Selective Context | 0.2   | .295 (.05) | .460 (.04) | .540 ( .03 ) | .346 (.04) | .438 (.03) | .905 (.005) | .900 (.009) | .902 ( .007 ) |\n| Selective Context | 0.35  | .243 (.10) | .421 (.08) | .504 ( .07 ) | .294 (.09) | .396 (.07) | .900 (.010) | .894 (.015) | .897 ( .013 ) |\n| Selective Context | 0.5   | .179 (.17) | .362 (.13) | .449 (.12)   | .237 (.15) | .344 (.13) | .893 (.018) | .882 (.027) | .887 (.023)   |\n| Selective Context | 0.65  | .127 (.22)",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 33,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-34",
      "content": "| .897 ( .013 ) |\n| Selective Context | 0.5   | .179 (.17) | .362 (.13) | .449 (.12)   | .237 (.15) | .344 (.13) | .893 (.018) | .882 (.027) | .887 (.023)   |\n| Selective Context | 0.65  | .127 (.22) | .299 (.20) | .391 (.18)   | .178 (.21) | .287 (.18) | .885 (.025) | .870 (.039) | .877 (.032)   |\n| Selective Context | 0.8   | .070 (.28) | .224 (.27) | .311 (.26)   | .122 (.26) | .225 (.25) | .874 (.036) | .852 (.057) | .863 (.047)   |\n\nTable 2: Comparing Selective Context to the random deletion baseline when using greedy decoding.\n\n|                   |       |       |        | ROUGE   | ROUGE   | ROUGE   | BERTScore   | BERTScore   | BERTScore   |\n|-------------------|-------|-------|--------|---------|---------|---------|-------------|-------------|-------------|\n|                   | Ratio | BLEU  | METEOR | rouge1  | rouge2  | rougeL  | Precision   | Recall      | F1          |\n| Random            | 0.20  | 0.437 | 0.578  | 0.666   | 0.503   | 0.566   | 0.892       | 0.909",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 34,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-35",
      "content": "Ratio | BLEU  | METEOR | rouge1  | rouge2  | rougeL  | Precision   | Recall      | F1          |\n| Random            | 0.20  | 0.437 | 0.578  | 0.666   | 0.503   | 0.566   | 0.892       | 0.909       | 0.899       |\n|                   | 0.35  | 0.360 | 0.514  | 0.629   | 0.423   | 0.502   | 0.879       | 0.895       | 0.886       |\n|                   | 0.50  | 0.283 | 0.443  | 0.576   | 0.346   | 0.432   | 0.867       | 0.881       | 0.873       |\n|                   | 0.65  | 0.210 | 0.378  | 0.522   | 0.279   | 0.371   | 0.855       | 0.868       | 0.860       |\n|                   | 0.80  | 0.156 | 0.314  | 0.450   | 0.219   | 0.310   | 0.843       | 0.853       | 0.847       |\n| Selective Context | 0.20  | 0.527 | 0.643  | 0.714   | 0.585   | 0.631   | 0.930       | 0.932       | 0.931       |\n|                   | 0.35  | 0.446 | 0.588  | 0.679   | 0.508   | 0.570   | 0.915       | 0.916       | 0.915       |\n|                   | 0.50  | 0.350 | 0.528  | 0.642   | 0.425   | 0.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 35,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-36",
      "content": "931       |\n|                   | 0.35  | 0.446 | 0.588  | 0.679   | 0.508   | 0.570   | 0.915       | 0.916       | 0.915       |\n|                   | 0.50  | 0.350 | 0.528  | 0.642   | 0.425   | 0.501   | 0.900       | 0.902       | 0.900       |\n|                   | 0.65  | 0.244 | 0.418  | 0.557   | 0.315   | 0.404   | 0.886       | 0.877       | 0.881       |\n|                   | 0.80  | 0.160 | 0.328  | 0.464   | 0.223   | 0.319   | 0.875       | 0.858       | 0.866       |\n\ntween answers given selective contexts and original contexts when the reduction ratio is 0.2. Selective Context also yields impressive results when 35% of the content is reduced, with BERT scores around 0.9 and ROUGE-1 scores over 0.5. The drops become noticeable as the reduction ratio rises to 0.5, where the average BLEU score drops 0.17 and the average ROUGE-1 drops 0.12. A reduction ratio of 0.65 and 0.8 tends to be less valuable, as shown by the 0.18 drop on ROUGE-1 and 0.32 drop on BERTScore-F1.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 36,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-37",
      "content": "e the average BLEU score drops 0.17 and the average ROUGE-1 drops 0.12. A reduction ratio of 0.65 and 0.8 tends to be less valuable, as shown by the 0.18 drop on ROUGE-1 and 0.32 drop on BERTScore-F1.\n\nWe then compare Selective Context against the Random compression baseline as shown in Table 2. We observe that using Selective Context allows LLMs to generate very similar answers to the reference answers (answers given full context) although we significantly reduce the context cost. Selective Context maintains BERTScore-F1 above 0.9 when the compression ratio is 0.5 or lower, which shows a high similarity with the reference answers. ROUGE demonstrates the same trend: ROUGE-1 continue to be above 0.64 and ROUGE-L keeps above 0.5 when the ratio is under 0.5. We also notice that Selective Context is significantly more effective than the random baseline: Selective Context with compression ratio of 0.5 shows a better\n\nTable 3: Faithfulness test on gpt-3.5-turbo using selective context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 37,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-38",
      "content": "e Context is significantly more effective than the random baseline: Selective Context with compression ratio of 0.5 shows a better\n\nTable 3: Faithfulness test on gpt-3.5-turbo using selective context.\n\n| Ratio   |   #Sorry |   Answer len. | Unfaithfulness   |\n|---------|----------|---------------|------------------|\n| Full    |        0 |         160.3 | -                |\n| 0.2     |        0 |         156.5 | .027             |\n| 0.35    |        6 |         136   | .050             |\n| 0.5     |        4 |         140.2 | .038             |\n| 0.65    |       19 |         131.2 | .051             |\n| 0.8     |       27 |         103.7 | .086             |\n\noverlapping with the reference answer than Random baseline with only 20% content compression.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.1 Overview",
        "chunkIndex": 38,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-39",
      "content": "To evaluate to what extent selective context affects the faithfulness of LLMs generated content, we perform manual tests on our question answering results based on the idea of (Wang et al., 2020). We evaluate 1000 question answering pairs (200 for each ratio) with the following procedure: 1) We first extract OpenIE tuples from the answers of selective context, and then 2) manually evaluate whether each tuple is entailed by the reference answer. If the model's answer is \"Sorry, I don't know\", we treat it as \"Sorry\" cases and do not consider it as unfaithfulness.\n\nAs shown in the Table 3, we find that gpt-3.5\n\nFigure 3: Performance of selective context on different tasks. x-axix represents compression ratios (same below).\n\n<!-- image -->\n\nFigure 4: Acceptance rate of generated summaries.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.2 Faithfulness",
        "chunkIndex": 39,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-40",
      "content": ".5\n\nFigure 3: Performance of selective context on different tasks. x-axix represents compression ratios (same below).\n\n<!-- image -->\n\nFigure 4: Acceptance rate of generated summaries.\n\n<!-- image -->\n\ntend to generate shorter answers or refuse to answer the questions if it fails to identify necessary evidence in the given selective context. With a compress ratio of 0.65, gpt-3.5 refuses to answer 19 questions (9% of 200), and the answers are 35% shorter than the reference answer (131 tokens in average). However, selective context doesn't significantly affect the faithfulness across all compression ratios. About 3.8% of all tuples are not entailed by the reference answer when the compression ratio is 0.5, and this number rises slightly to 5.1% as the compression ratio increase to 0.65.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.2 Faithfulness",
        "chunkIndex": 40,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-41",
      "content": "In this part, we break down and analyse the performances of Selective Context into the four different NLP tasks: summarisation, question answering, original context reconstruction, and conversation. The results are as shown in Fig. 3. First, the results on the Original Context Reconstruction task (RC) show the steepest drop with increasing compression ratio, however, Selective Context allows LLMs to preserve most of the key points in the\n\nFigure 5: Effects of lexical units.\n\n<!-- image -->\n\noriginal context when the reduction ratio is lower than 0.5, as demonstrated by a rather high ROUGE score. Second, we notice that the curves of question answering and summarisation decrease gradually and are continually higher that the other two tasks evaluated by BERTScore. We could say Selective Context is especially suitable for tasks of summarisation and answer generation.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.3 Tasks",
        "chunkIndex": 41,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-42",
      "content": "We perform human evaluation to explore the effect of model scales and supervised instruct-tuning on Selective Context. We ask three college students to evaluate 1150 generated summaries from llama and vicuna (about 55 per model and ratio) and record whether they accept the generation as a reasonable summary. As shown in Figure 4, we find no specific trends between the scales and generation quality given Selective Context. The vicuna family demonstrates similar summarisation capability with 7b and 13b parameters. And so does the llama family, larger models do not show stronger robustness towards Selective Context. But instruct-tuned model vicuna demonstrates significant superior performance than\n\nOriginal Context , CUDA Memory = 77,695 MB; Time = 110.8 ms/token\n\nPlease see the original document and summary given full context in Appendix B.\n\nSelective Context , Ratio: 0.5, CUDA Memory = 61,885 MB, Time = 76.3 ms/token, Time to construct selective context = 46.1 ms",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.4 Scaling and Instruct-Tuning",
        "chunkIndex": 42,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-43",
      "content": "se see the original document and summary given full context in Appendix B.\n\nSelective Context , Ratio: 0.5, CUDA Memory = 61,885 MB, Time = 76.3 ms/token, Time to construct selective context = 46.1 ms\n\n[1] The above paragraph discusses the use of probabilistic methods, safety distance-based control methods, and trajectory prediction methods for assisting vehicles in avoiding collisions. [1] It mentions that CNN has made outstanding contributions in vision tasks, particularly in road condition inspection, due to its excellent regional feature extraction capabilities. However, the performance of CNN-based models for vision tasks is inferior to EfficientNets RepVGG, due to the huge differences between sequential tasks in NLP and image tasks in CV, the difficulty in keeping the original information of inputs after RNN layers, and the computational and memory requirements of switching layers.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.4 Scaling and Instruct-Tuning",
        "chunkIndex": 43,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-44",
      "content": "between sequential tasks in NLP and image tasks in CV, the difficulty in keeping the original information of inputs after RNN layers, and the computational and memory requirements of switching layers. The paragraph introduces a new network structure called Sequential Spatial Network (SSN) blocks, which overcomes the limitations of traditional CNN-based models. [2] The SSN block consists of convolutional layers, Upsampling Convolutional Decreasing (UCD) blocks, and Reinforcement Region Unit and Fast MultiHead SelfAttention (FMHSA) to enhance local information and improve normalization capability. The paragraph concludes by stating that the SSN network outperforms existing methods on the Lykit dataset and can be easily transferred for image classification tasks.\n\nFigure 6: Comparing the summary generated by vicuna\\_13b given original context and selective context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.4 Scaling and Instruct-Tuning",
        "chunkIndex": 44,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-45",
      "content": "sting methods on the Lykit dataset and can be easily transferred for image classification tasks.\n\nFigure 6: Comparing the summary generated by vicuna\\_13b given original context and selective context.\n\nllama models given selective context indicating instruct-tuning might help the model to be more robustness towards context compression. Given selective context, llama models often fail to follow instructions and go wild very easily.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.4 Scaling and Instruct-Tuning",
        "chunkIndex": 45,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-46",
      "content": "We test the effect of Selective Context based on different lexical units: tokens, phrases, and sentences via BERTScore-F1. As shown in Table 5, employing phrase as the basic lexical units in Selective Context is the optimal approach, consistently outperforming the other two variants, followed by token-level Selective Context. Removing redundancy at sentence-level is a rather unstable implementation compared to the token and phrase-level. This experiment indicates that a reasonable granularity can be crucial in Selective Context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.5 Lexical Units",
        "chunkIndex": 46,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-47",
      "content": "To have a straightforward impression on how well LLMs generate with selective context, we present two summaries given the full and selective context respectively in Figure 6. The original document and processing to obtain selective context are presented at Appendix B.\n\nWe first found that preparing selective context is extremely efficient. It takes a one-time cost of 46.1 ms to build selective context for the example paragraph, which includes computing self-information and performing lexical unit tokenisation. This ensures that the initial stage of establishing a selective context incurs very little overhead. Secondly, it shows selective context significantly reduces the memory usage of the GPU and accelerates the generation process. With compression ratio of 0.5, selective context reduces about 36% of the memory cost in inference and makes generation 1.32 times faster (per token).",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.6 Case Study",
        "chunkIndex": 47,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-48",
      "content": "f the GPU and accelerates the generation process. With compression ratio of 0.5, selective context reduces about 36% of the memory cost in inference and makes generation 1.32 times faster (per token). By comparing the content of the two summaries, we see that the summary given selective context missed relevant information about the research background (as denoted by the [1] marker), such as the use of machine learning in autonomous driving technology and instead starts with the different methods directly. This is due to the background parts not being selected and removed as redundancy before feeding to vicuna . We try to ask vicuna \"what is the background of this study?\" given the selective context, and obtain a decent answer: \"the research background of this paper is likely to be situated in the domain of autonomous driving technology and the application of artificial intelligence (AI) for improving vehicle safety and decision-making capabilities.\" .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.6 Case Study",
        "chunkIndex": 48,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-49",
      "content": "this paper is likely to be situated in the domain of autonomous driving technology and the application of artificial intelligence (AI) for improving vehicle safety and decision-making capabilities.\" . This demonstrates that LLMs are likely to be able to infer the deleted parts of background information in the selective context. Selective context also affects vicuna 's decision on what information should be included in the summary as the second summary includes details about for example FMHSA and\n\nUCD block (as denoted by the [2] marker) which are not covered in the summary generated with the full context. We find no factual errors in the summary given selective context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "5.6 Case Study",
        "chunkIndex": 49,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-50",
      "content": "We introduced Selective Context to improve the context efficiency of LLMs in inference by deleting redundant content measured by self-information. Our extensive experiments on arXiv papers, BBC news articles, and conversation transcripts showed that our proposed method can significantly reduce GPU memory cost, accelerate generation with minor performance decrease, and potentially enable LLMs to handle long documents and extended conversations without the risk of context truncation.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 50,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-51",
      "content": "Selective Context demonstrates promising results, but it is still necessary to note a couple of potential limitations. Firstly, our approach is somewhat influenced by the phrase boundary detection procedure. We employ the noun phrase tokenisation algorithm provided by spacy in our experiments. However, we do not consider verb phrases as there is no mature solution for verb phrase tokenisation. We speculate that we can achieve better compression performance with dependency tree-based filtering procedure which might lead to better boundary identification of lexical units. Secondly, in the experiment section, we use percentile to control the pruning process. However, the optimal compression percentile varies based on specific tasks and context. Developing a tool to find the optimal threshold can further enhance the effectiveness of selective context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "7 Limitations",
        "chunkIndex": 51,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-52",
      "content": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization , pages 65-72.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 52,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-53",
      "content": "ke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 .\n\nRazvan Bunescu and Oseremen O Uduehi. 2022. Distribution-based measures of surprise for creative language: Experiments with humor and metaphor. In Proceedings of the 3rd Workshop on Figurative Language Processing (FLP) , pages 68-78.\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-54",
      "content": "tbot impressing gpt-4 with 90%* chatgpt quality.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 .\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv , abs/2009.03300.\n\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pages 74-81.\n\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-55",
      "content": "ation of summaries. In Text summarization branches out , pages 74-81.\n\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .\n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311-318.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9.\n\nClaude E Shannon. 1948. A mathematical theory of communication. The Bell system technical journal , 27(3):379-423.\n\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-56",
      "content": "il, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems , 30.\n- Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. ArXiv , abs/2004.04228.\n- EC Wit and Marie Gillette. 1999. What is linguistic redundancy. University of Chicago .\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-57",
      "content": "usan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .\n- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 .",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-58",
      "content": "Table 4: Statistics of the three datasets. #Sent, #Phrase, #Token are averaged per document.\n\n| Dataset   |   #Doc |   #Sent |   #Phrase |   #Token |\n|-----------|--------|---------|-----------|----------|\n| Arxiv     |    408 |   28.2  |    514.55 |   864.85 |\n| ShareGPT  |    470 |   27.35 |    389.42 |   689.32 |\n| BBC       |    294 |   25.63 |    523.96 |   732.54 |",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "A Dataset statistics",
        "chunkIndex": 58,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-59",
      "content": "Here we present an example of selective context on a rather long context. The original paragraphs is from https://arxiv.org/abs/ 2303.07352 . The original paragraphs is shown in Fig. 7. The resulting context is shown in Fig. 8. The reference summary is given in Fig. 9.\n\nFigure 7: Selective context on the introduction of https://arxiv.org/abs/2303.07352\n\n<!-- image -->\n\nIn researchers how turn vehicles assisted driving more intelligent autonomous drivingDue the iteration intelligent hardware the improvement chip computing power collected quickly converted fed models In the driving process the safety factor users researchers Therefore how AV should avoid collisions has Concepts such probabilistic methods ( eg . : ) , safety distance-based control methods , trajectory prediction methods designed cope complex traffic conditions In terms vision CNN made outstanding contributions and applied road condition inspection tasks due its excellent regional feature extraction capabilities The local",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 59,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-60",
      "content": "ned cope complex traffic conditions In terms vision CNN made outstanding contributions and applied road condition inspection tasks due its excellent regional feature extraction capabilities The local feature information obtained CNN will obstacle detection Secondly because the motion trajectory planned for AV the relationship each local feature obtained CNN needs established Some strategies based CNN plus RNN so deal sequential graphs as input eg STDN . Although the above strategies performed vision tasks their performances inferior EfficientNets RepVGG . We believe this due the following aspects First the huge differences the sequential tasks NLP the image tasks CV ignored For example when the local feature information acquired compressed how achieve accurate mapping becomes Second difficult keep the original information of inputs since after RNN layers recover the dimension from one three Besides due the several transformations between different dimensions that process becomes especi",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 60,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-61",
      "content": "ult keep the original information of inputs since after RNN layers recover the dimension from one three Besides due the several transformations between different dimensions that process becomes especially our input size 224×224×5 Third , the computational and memory requirement switching layers are extremely heavy tasks also becomes a tricky point Higher hardware requirements as running arise the attention partIn CNN attention to vision tasksThe new network structure overcomes Sequential Spatial Network (SSN) blocks As shown Fig , input images first go the convolution stem for a stack SSN blocks The Upsampling Convolutional Decreasing (UCD) blocks introduced for local information enhancement deep convolution , in SSN block of features generated in can less loss of image resolution crucial the subsequent trajectory adjustment task In addition we adopt a staged architecture design using five convolutional layers steps gradually ( sequence length flexibly Such a design helps extract local",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 61,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-62",
      "content": "the subsequent trajectory adjustment task In addition we adopt a staged architecture design using five convolutional layers steps gradually ( sequence length flexibly Such a design helps extract local features different scales , since the first stage retains high resolution our design effectively reduce the output information atThe Reinforcement Region Unit and the Fast MultiHead Self-Attention (FMHSA in the SSN block can help obtain within the intermediate features and improve the normalization capability Finally average pooling better trajectory tuning Extensive experiments the lykit dataset demonstrate our SSN network In addition image classification SSN block can easily transferred and serve a versatile backbone.\n\nFigure 8: The resulting context\n\nGiven Original Context , CUDA Memory = 77,695 MB; Time = 110.8 ms/token",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 62,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-63",
      "content": "addition image classification SSN block can easily transferred and serve a versatile backbone.\n\nFigure 8: The resulting context\n\nGiven Original Context , CUDA Memory = 77,695 MB; Time = 110.8 ms/token\n\nThe paragraph discusses the development of autonomous driving technology and the use of artificial intelligence and machine learning to make decisions in the driving process. The safety of the vehicle is a top priority and methods such as probabilistic methods, safety distance-based control methods, and trajectory prediction methods have been developed to cope with complex traffic conditions. The use of convolutional neural networks (CNN) has been successful in image tasks such as obstacle detection, but the performance of these networks is still inferior to similar-sized convolutional neural networks counterparts.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 63,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-64",
      "content": "eural networks (CNN) has been successful in image tasks such as obstacle detection, but the performance of these networks is still inferior to similar-sized convolutional neural networks counterparts. This is due to the huge differences between the sequential tasks of NLP and the image tasks of CV, the difficulty of keeping the original information of inputs, and the heavy computational and memory requirement of switching between layers. The authors propose a new network structure based on CNN and attention to vision tasks in autonomous driving, using Sequential Spatial Network (SSN) blocks. The network structure overcomes these problems by using deep convolution and attention mechanisms to extract local and global features, and reducing the computational effort of subsequent layers. The proposed network structure is evaluated on the lykit dataset and shows superior performance in terms of accuracy. B.\n\nFigure 9: The reference summary generated by vicuna-13b given the full context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 64,
        "totalChunks": 66
      }
    },
    {
      "id": "2310.06201v1-chunk-65",
      "content": "roposed network structure is evaluated on the lykit dataset and shows superior performance in terms of accuracy. B.\n\nFigure 9: The reference summary generated by vicuna-13b given the full context.",
      "metadata": {
        "source": "arxiv:2310.06201v1",
        "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models",
        "authors": [
          "Yucheng Li",
          "Bo Dong",
          "Chenghua Lin",
          "Frank Guerin"
        ],
        "section": "B Example of selective context on long context",
        "chunkIndex": 65,
        "totalChunks": 66
      }
    }
  ],
  "fullText": "## Compressing Context to Enhance Inference Efficiency of Large Language Models\n\nYucheng Li 1 , Bo Dong 1 , Chenghua Lin 2 ∗ , Frank Guerin 1\n\n1 University of Surrey 2 University of Manchester\n\n{yucheng.li, bd00531, f.guerin}@surrey.ac.uk chenghua.lin@manchester.ac.uk\n\n## Abstract\n\nLarge language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage and a 32% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance. Code and data are available at https://github.com/ liyucheng09/Selective\\_Context .\n\n## 1 Introduction\n\nLarge language models (LLMs) have demonstrated remarkable power and impressive generalisation abilities across a wide range of natural language processing tasks, as well as real-life applications (Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023). However, a major challenge for existing LLMs is processing longer context. Dealing with longer context with LLMs is fundamen-\n\n∗ Corresponding author\n\nContext: Large Languages Models (LLMs) have shown their ability to perform new tasks, resulting in a line of work that focuses on further scaling these models. These efforts are based on the assumption { that more parameters will lead to better performance. }\n\nQuery: What's the assumption behind the efforts to further scale LLMs?\n\nLLMs: Further scaling Large Language Models will lead to better performance on a wide range of tasks.\n\nFigure 1: Some context is redundant because LLMs have learned that knowledge. LLMs can generate the correct answer even when these redundancies are deleted.\n\ntal in scenarios such as having long conversations, document summarisation, and question answering given long documents. However, it is very computationally expensive, particularly with Transformer based LLMs, due to the quadratic growth of memory and computation associated with the 2-D attention matrix (Vaswani et al., 2017). This makes LLMs less accessible and sometimes leads to context truncation during inference. Moreover, due to the above limitation, existing LLMs were usually pre-trained with fixed-context windows, which further constrains their capability in processing longer context.\n\nThere are active attempts in reducing the computation and memory cost of the Transformer architecture with sparse attention (Child et al., 2019) or local dense attention (Beltagy et al., 2020). There are also efforts to learn soft prompts with further distillation to save context cost during inference (Mu et al., 2023; Chevalier et al., 2023). In contrast to existing approaches that primarily focus on architectures or distillations, we introduce a fresh perspective to tackle the redundancy in the input context itself, thus proposing a complementary, modelagnostic approach that can be potentially combined with other architecture optimisation methods to further enhance inference efficiency.\n\nThe proposed method is motivated by the potential redundancy and repetition in human language, which has two main sources. The first is the inherent redundancy of natural language. For example, in the conversation \"A: Did you get the chance to pick up groceries today?\" , \"B: Yes, I did get the groceries.\" , the underlined part can be seen as a common redundancy in communication. Linguistic studies suggest redundancy is ubiquitous in language (Wit and Gillette, 1999). The other type of input redundancy is from the overlap with training material. As the example in Fig. 1 shows, if some parts of input have already been included in the pre-training stage of LLMs, then it is safe to delete them and the model can still generate the correct answer. In summary, redundancy in the input context, while beneficial for human comprehension, can be extraneous for LLMs and might lead to unnecessary computational expense.\n\nIn this paper, we propose Selective Context , which prunes redundant content in a given input context, thereby reducing the computational cost and making better use of the fixed context length in LLMs. Selective Context evaluates informativeness of lexical units (i.e., tokens, phrases, or sentences) with self-information (Shannon, 1948) computed by a base causal language model. By selectively retaining content with higher self-information, our method provides a more compact and efficient context representation for LLMs to process without compromising their performance on various applications.\n\nWe evaluate the effectiveness and different settings of Selective Context on arXiv papers, BBC News, and real conversation on ShareGPT.com with four NLP tasks: summarisation, question answering, original context reconstruction, and conversation. Experimental results demonstrate that our proposed method can significantly enhance context efficiency of LLMs during inference while maintaining comparable performance compared to that achieved when full context is used.\n\n## 2 Self-Information\n\nSelf-information, also known as surprisal or information content , is a fundamental concept in information theory that quantifies the amount of information conveyed by an event given a distribution (Shannon, 1948). In the context of language modelling, the event can be regarded as one step of generation (i.e., a token) and the distribution corresponds to its output distribution. So the selfinformation of a token can be defined as the negative log likelihood:\n\n<!-- formula-not-decoded -->\n\nwhere I ( x ) represents the self-information of token x and P ( x ) denotes its output probability.\n\nIn information theory, self-information measures the level of surprise or uncertainty associated with an event; rare events convey more information and thus have higher self-information, while common events convey less information and have lower self-information. In the context of language modelling, self-information can be used to assess the informativeness of lexical units, e.g., words, phrases, or sentences. Lexical units with lower self-information are less informative and thus are more likely to be inferred from the context. As a result, we may treat these parts of input as redundant during LLM inference.\n\nIn NLP, self-information has been used to measure surprise in creative language artefacts (Bunescu and Uduehi, 2022). In addition, related concepts of self-information such as entropy and perplexity are widely used in language model optimisation and evaluation.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere the entropy H ( S ) of the sentence S = ( x 0 , ..., x n ) is the average self-information of words in the sentence, and perplexity PP ( S ) of the sentence can be calculated with entropy. The property of self-information that is especially relevant to our method is the additivity.\n\n<!-- formula-not-decoded -->\n\nThis means we can calculate the self-information of a lexical unit by simply summing the selfinformation of the tokens in it.\n\n## 3 Method\n\nSelective Context optimises the input context by filtering out redundant or non-essential content to reduce computational cost and make better use of the limited context window. In implementation,\n\nOriginal: INTRODUCTION Continual Learning ( CL ) , also known as Lifelong Learning , is a promising learning paradigm to design models that have to learn how to perform multiple tasks across different environments over their lifetime [ To uniform the language and enhance the readability of the paper we adopt the unique term continual learning ( CL ) . ] . Ideal CL models in the real world should be deal with domain shifts , researchers have recently started to sample tasks from two different datasets . For instance , proposed to train and evaluate a model on Imagenet first and then challenge its performance on the Places365 dataset . considers more scenarios , starting with Imagenet or Places365 , and then moving on to the VOC/CUB/Scenes datasets . Few works propose more advanced scenarios built on top of more than two datasets .\n\nFiltered: INTRODUCTION Continual Learning ( a promising learning paradigm to design models have to how across overTo uniform the language and enhance adopt the unique term continual learning Ideal CL models in should deal domain shifts researchers recently started sample tasks two different datasets For instance proposed to train and evaluate on Imagenet first challenge Places365considers more scenarios starting Imagenet or Places365 the VOC/CUB/Scenes datasetsFew works propose more advanced scenarios built top more than two datasets\n\nFigure 2: A visualisation of selective context. Darker colour indicates larger value of self-information.\n\nwe first 1) employ a causal language model such as GPT (Radford et al., 2019; Brown et al., 2020), OPT (Zhang et al., 2022), or LLaMA (Touvron et al., 2023), computing self-information for each token. We then 2) merge tokens, along with their corresponding self-information values, into lexical units, which can be phrases or sentences. This step is optional if tokens are being used as the basic units. Finally, 3) we eliminate content that is deemed least necessary to render the input more compact.\n\n## 3.1 Computing Self-Information\n\nGiven a context C = x 0 , x 1 , ..., x n , where x i denotes a token, we use a base language model M to compute the self-information for each token x t as follows:\n\n<!-- formula-not-decoded -->\n\n## 3.2 Merging into Lexical Units\n\nIf the content filtering of selective context is directly performed on the token level, it might lead to very disjoint context. Therefore apart from token level filtering, we also conduct the filtering procedure on phrase and sentence level. We call a basic unit in our filtering a lexical unit , which could be a token, a phrase or a sentence in our setting.\n\nTo enable selective context to work on phrases and sentences, we merge tokens and their selfinformation into lexical units. Each lexical unit u consists of multiple tokens ( x t , ..., x t + α ) , and we can calculate its self-information by summing the self-information of its individual tokens according to the additivity property of self-information:\n\n<!-- formula-not-decoded -->\n\nThe NLTK sentence tokenizer is employed to obtain sentence level lexical units. And we use spacy 1 to merge tokens into noun phrases. We do not merge verb phrases as it might produce very long phrases.\n\n## 3.3 Selective Retention of Informative Context\n\nWith the self-information of each lexical unit computed, we can now evaluate their informativeness. Instead of using a fixed threshold or retaining a fixed number of top k lexical units, we design a percentile-based filtering approach to adaptively select the most informative content.\n\nFirst, we rank the lexical units based on their self-information values in descending order. Then, we compute the p -th percentile of self-information values among all lexical units.\n\n<!-- formula-not-decoded -->\n\nNext, we selectively retain lexical units with selfinformation values greater than or equal to the p -th percentile, constructing a filtered context C ′ :\n\n<!-- formula-not-decoded -->\n\nThe percentile-based filtering is a more flexible approach to retain the most informative content depending on the distribution of self-information values in the given context. In Figure 2, we present\n\n1 https://spacy.io/api/ pipeline-functions#merge\\_noun\\_chunks\n\nan example on phrase level where p is set to 50, which means half of phrases are filtered out. In this case, the context after processing by selective context only retains 57.2% of tokens, which saves 42.7% of context length.\n\n## 4 Experiments\n\nThe goal of Selective Context is to reduce the redundancy in the input context without compromising the generation quality of LLMs. As a result, we are expecting the answers given both selective context and the original context to be as close as possible. We take the generated answer given full context as the reference answer, and compare to the generated answer given the selective context in our experiments.\n\n## 4.1 Datasets\n\nSelective Context prunes redundancy in the input context to allow very long context processing for LLMs. However, existing benchmarks for LLMs, such as MMLU (Hendrycks et al., 2020) and ARC (Clark et al., 2018), are mostly single round question answering and are thus not suitable to evaluate our proposed method. Therefore, we collect three test sets consisting of long documents and conversations to evaluate Selective Context. Statistics in detail are presented in Table 4.\n\nBBC News: Adataset containing news articles collected from the British Broadcasting Corporation (BBC). This dataset covers a wide range of topics, including politics, business, sports, and technology. We use the full content of each news article in our experiments.\n\narXiv Articles: A dataset consisting of latest academic papers, spaning various scientific disciplines, such as computer science, physics, and mathematics. As arXiv articles can be quite long, we only process the first two sections (usually introduction and background) for each paper in our experiments. ShareGPT.com: ShareGPT.com is a platform where ChatGPT users share their surprising and interesting conversation with ChatGPT. This datasets consists of conversations in different languages and in various scenarios (e.g., coding, chitchat, writing assistant, etc.). We use the ShareGPT dataset for the conversation task in our experiments.\n\nThe three evaluation datasets were created carefully to avoid data contamination . Data samples in the BBC News, arXiv, and ShareGPT.com datasets were all created after March 2023, which is after the release of all LLMs in our experiments. Considering some of baseline models are continually being updated, we employ the latest version released before 30 March 2023 to make sure models have never seen our test set in their pre-training and fine-tuning stage. In addition, as some of LLMs in our experiment has a max\\_length of 2048 tokens, we do not include articles or conversations exceeding this length.\n\n## 4.2 Models\n\nWe test Selective Context on the following models: GPT-3.5, GPT-4: GPT-3.5 also known as ChatGPT, which is likely to be further fine-tuned from GPT-3 and InstructGPT. GPT-4 is the latest model from OpenAI, which has demonstrated substantially improved capability on complex reasoning compared to its predecessor. GPT-3.5 and GPT4 are unfortunately not open-source, we can only access these models via web api 2 .\n\nLLaMA-7, 13, 30B: LLaMA is a family of opensource language models released by Meta, which is reported to outperform GPT-3 with less parameters. The LLaMA family includes models with size ranging from 7B to 65B. To investigate the effect of scaling law to Selective Context, we experiment with LLaMA with 7B, 13B, and 30B parameters.\n\nVicuna-7, 13B: Vicuna (Chiang et al., 2023) is a family of open-source language models instructtuned from LLaMA. According to their technical report, Vicuna model perform quite well on a list of multitasking benchmarks.\n\n## 4.3 Tasks and Metrics\n\nWe evaluate Selective Context on four tasks:\n\nOriginal Context Reconstruction: Given a compressed context produced by Selective Context, this task aims to evaluate whether models are able to reconstruct the original context. This task assesses how well the filtered context retains the essential information from the original context. In our experiments, the compressed contexts are used as input, and the original contexts are used as reference answers.\n\nSummarisation: Given a context, the task is to generate a summary that captures the main points of the document. This task aims to evaluate whether Selective Context affects the overall understanding of models on the input contexts. In\n\n2 https://platform.openai.com/docs/ api-reference\n\nour experiments, the input and output are the compressed context and the summaries generated based on the compressed contexts. Summaries based on the original (full) contexts are treated as the reference answers.\n\nQuestion Answering (QA): Given a document and a set of questions, the task is to generate answers based on the information available in the document. This task aims to evaluate models' understanding towards a specific query. Here we first generate questions and answers based on the original context, where these answers are treated as reference answers, and then ask LLMs to answer these questions with selective context.\n\nConversation: This task is only for the ShareGPT dataset. Given a conversation history and a user query, the task is to generate a response to the query based on the previous conversation history. This task aims to evaluate selective context's performance on conversation. Specifically, we ask LLMs to answer users' last query of ShareGPT conversation instances with selective context applied on the previous conversation history.\n\nWe employ four metrics to assess the performance of our models on the tasks: BLEU, METEOR, ROUGE, and BERTScore. BLEU (Papineni et al., 2002) calculates n-gram precision, which is the proportion of n-grams in the generated text that are also present in the reference text. METEOR (Banerjee and Lavie, 2005) take additional features such as synonymy, stemming and word order into consideration, which leads to more comprehensive evaluation. ROUGE (Lin, 2004) focuses on how much of the important information in the reference text is present in the generated summary. BERTScore (Zhang et al., 2019) leverages contextualised embeddings from pre-trained language models like BERT, computing the cosine similarity between the generated text and reference text embeddings to capture semantic similarity more effectively than traditional n-gram-based metrics.\n\nAs mentioned before, we use the generated answers given the full contexts as the reference answers. When testing the deterministic decoding strategy ( greedy decoding), we take one single run on full context as the reference answer. When testing non-deterministic decoding strategy ( temperature = 0.7 ), we run multiple times on full context to obtain multiple reference answers to address the randomness in decoding. The metrics are computed based on the set of reference answers. In our experiment, we set the number of reference answers to 4.\n\n## 4.4 Experimental Settings\n\nWe use smaller base causal language model for selfinformation computing in our experiments. For the LLaMA family and vicuna family, we employ LLaMA-7B to compute self-information. For the OpenAI family, we use a smaller GPT-3 variant curie for self-information computing, which is available on OpenAI web API. In self-information computing, we do not process the entire context at once. This is due to our observation on the tendency of LLMs to give later lexical units lower selfinformation. Instead, we compute self-information sentence by sentence in our experiments.\n\nIn our experiments, we compare the two different dimensions that are adjustable in Selective Context.\n\nCompression Ratios: We experiment with different content reduction ratios in Selective Context: 0.2, 0.35, 0.5, 0.65, and 0.8. These ratios determine the proportion of content to be filtered out, allowing us to study the trade-off between efficiency and performance as the amount of retained information varies.\n\nLexical Units: Lexical units are the basic element of content reduction in Selective Context. It can be sentence, phrases, or tokens. As mentioned in §3.2, we remove the redundancy in input context by a specific lexical unit level.\n\n## 5 Results\n\nExcept §5.5, all results of selective context presented are at the phrase level (the optimal).\n\n## 5.1 Overview\n\nIn Table 1, we first compare the performance of Selective Context against the Original Context to see how well Selective Context preserves useful information when reducing context cost. The metrics are averaged across all models mentioned in §4.2. The performance drop is shown in parentheses.\n\nAs demonstrated in the table, using Selective Context only leads to a marginal drop when the reduction ratio is set to 0.2 or 0.35, despite it significantly reducing the context cost. The BLEU score drops by only 0.05 when 20% of the content is reduced. And the number is even smaller when it comes to ROUGE-1, where the drop is just 0.03. This indicate a high level of consistency be-\n\nTable 1: Comparing Selective Context to the Original Context when temperature set to 0.7.\n\n|                   |       |            |            | ROUGE        | ROUGE      | ROUGE      | BERTScore   | BERTScore   | BERTScore     |\n|-------------------|-------|------------|------------|--------------|------------|------------|-------------|-------------|---------------|\n| Method            | Ratio | BLEU       | METEOR     | rouge1       | rouge2     | rougeL     | Precision   | Recall      | F1            |\n| Original          | -     | .347       | .496       | .571         | .383       | .471       | .910        | .909        | .909          |\n| Selective Context | 0.2   | .295 (.05) | .460 (.04) | .540 ( .03 ) | .346 (.04) | .438 (.03) | .905 (.005) | .900 (.009) | .902 ( .007 ) |\n| Selective Context | 0.35  | .243 (.10) | .421 (.08) | .504 ( .07 ) | .294 (.09) | .396 (.07) | .900 (.010) | .894 (.015) | .897 ( .013 ) |\n| Selective Context | 0.5   | .179 (.17) | .362 (.13) | .449 (.12)   | .237 (.15) | .344 (.13) | .893 (.018) | .882 (.027) | .887 (.023)   |\n| Selective Context | 0.65  | .127 (.22) | .299 (.20) | .391 (.18)   | .178 (.21) | .287 (.18) | .885 (.025) | .870 (.039) | .877 (.032)   |\n| Selective Context | 0.8   | .070 (.28) | .224 (.27) | .311 (.26)   | .122 (.26) | .225 (.25) | .874 (.036) | .852 (.057) | .863 (.047)   |\n\nTable 2: Comparing Selective Context to the random deletion baseline when using greedy decoding.\n\n|                   |       |       |        | ROUGE   | ROUGE   | ROUGE   | BERTScore   | BERTScore   | BERTScore   |\n|-------------------|-------|-------|--------|---------|---------|---------|-------------|-------------|-------------|\n|                   | Ratio | BLEU  | METEOR | rouge1  | rouge2  | rougeL  | Precision   | Recall      | F1          |\n| Random            | 0.20  | 0.437 | 0.578  | 0.666   | 0.503   | 0.566   | 0.892       | 0.909       | 0.899       |\n|                   | 0.35  | 0.360 | 0.514  | 0.629   | 0.423   | 0.502   | 0.879       | 0.895       | 0.886       |\n|                   | 0.50  | 0.283 | 0.443  | 0.576   | 0.346   | 0.432   | 0.867       | 0.881       | 0.873       |\n|                   | 0.65  | 0.210 | 0.378  | 0.522   | 0.279   | 0.371   | 0.855       | 0.868       | 0.860       |\n|                   | 0.80  | 0.156 | 0.314  | 0.450   | 0.219   | 0.310   | 0.843       | 0.853       | 0.847       |\n| Selective Context | 0.20  | 0.527 | 0.643  | 0.714   | 0.585   | 0.631   | 0.930       | 0.932       | 0.931       |\n|                   | 0.35  | 0.446 | 0.588  | 0.679   | 0.508   | 0.570   | 0.915       | 0.916       | 0.915       |\n|                   | 0.50  | 0.350 | 0.528  | 0.642   | 0.425   | 0.501   | 0.900       | 0.902       | 0.900       |\n|                   | 0.65  | 0.244 | 0.418  | 0.557   | 0.315   | 0.404   | 0.886       | 0.877       | 0.881       |\n|                   | 0.80  | 0.160 | 0.328  | 0.464   | 0.223   | 0.319   | 0.875       | 0.858       | 0.866       |\n\ntween answers given selective contexts and original contexts when the reduction ratio is 0.2. Selective Context also yields impressive results when 35% of the content is reduced, with BERT scores around 0.9 and ROUGE-1 scores over 0.5. The drops become noticeable as the reduction ratio rises to 0.5, where the average BLEU score drops 0.17 and the average ROUGE-1 drops 0.12. A reduction ratio of 0.65 and 0.8 tends to be less valuable, as shown by the 0.18 drop on ROUGE-1 and 0.32 drop on BERTScore-F1.\n\nWe then compare Selective Context against the Random compression baseline as shown in Table 2. We observe that using Selective Context allows LLMs to generate very similar answers to the reference answers (answers given full context) although we significantly reduce the context cost. Selective Context maintains BERTScore-F1 above 0.9 when the compression ratio is 0.5 or lower, which shows a high similarity with the reference answers. ROUGE demonstrates the same trend: ROUGE-1 continue to be above 0.64 and ROUGE-L keeps above 0.5 when the ratio is under 0.5. We also notice that Selective Context is significantly more effective than the random baseline: Selective Context with compression ratio of 0.5 shows a better\n\nTable 3: Faithfulness test on gpt-3.5-turbo using selective context.\n\n| Ratio   |   #Sorry |   Answer len. | Unfaithfulness   |\n|---------|----------|---------------|------------------|\n| Full    |        0 |         160.3 | -                |\n| 0.2     |        0 |         156.5 | .027             |\n| 0.35    |        6 |         136   | .050             |\n| 0.5     |        4 |         140.2 | .038             |\n| 0.65    |       19 |         131.2 | .051             |\n| 0.8     |       27 |         103.7 | .086             |\n\noverlapping with the reference answer than Random baseline with only 20% content compression.\n\n## 5.2 Faithfulness\n\nTo evaluate to what extent selective context affects the faithfulness of LLMs generated content, we perform manual tests on our question answering results based on the idea of (Wang et al., 2020). We evaluate 1000 question answering pairs (200 for each ratio) with the following procedure: 1) We first extract OpenIE tuples from the answers of selective context, and then 2) manually evaluate whether each tuple is entailed by the reference answer. If the model's answer is \"Sorry, I don't know\", we treat it as \"Sorry\" cases and do not consider it as unfaithfulness.\n\nAs shown in the Table 3, we find that gpt-3.5\n\nFigure 3: Performance of selective context on different tasks. x-axix represents compression ratios (same below).\n\n<!-- image -->\n\nFigure 4: Acceptance rate of generated summaries.\n\n<!-- image -->\n\ntend to generate shorter answers or refuse to answer the questions if it fails to identify necessary evidence in the given selective context. With a compress ratio of 0.65, gpt-3.5 refuses to answer 19 questions (9% of 200), and the answers are 35% shorter than the reference answer (131 tokens in average). However, selective context doesn't significantly affect the faithfulness across all compression ratios. About 3.8% of all tuples are not entailed by the reference answer when the compression ratio is 0.5, and this number rises slightly to 5.1% as the compression ratio increase to 0.65.\n\n## 5.3 Tasks\n\nIn this part, we break down and analyse the performances of Selective Context into the four different NLP tasks: summarisation, question answering, original context reconstruction, and conversation. The results are as shown in Fig. 3. First, the results on the Original Context Reconstruction task (RC) show the steepest drop with increasing compression ratio, however, Selective Context allows LLMs to preserve most of the key points in the\n\nFigure 5: Effects of lexical units.\n\n<!-- image -->\n\noriginal context when the reduction ratio is lower than 0.5, as demonstrated by a rather high ROUGE score. Second, we notice that the curves of question answering and summarisation decrease gradually and are continually higher that the other two tasks evaluated by BERTScore. We could say Selective Context is especially suitable for tasks of summarisation and answer generation.\n\n## 5.4 Scaling and Instruct-Tuning\n\nWe perform human evaluation to explore the effect of model scales and supervised instruct-tuning on Selective Context. We ask three college students to evaluate 1150 generated summaries from llama and vicuna (about 55 per model and ratio) and record whether they accept the generation as a reasonable summary. As shown in Figure 4, we find no specific trends between the scales and generation quality given Selective Context. The vicuna family demonstrates similar summarisation capability with 7b and 13b parameters. And so does the llama family, larger models do not show stronger robustness towards Selective Context. But instruct-tuned model vicuna demonstrates significant superior performance than\n\nOriginal Context , CUDA Memory = 77,695 MB; Time = 110.8 ms/token\n\nPlease see the original document and summary given full context in Appendix B.\n\nSelective Context , Ratio: 0.5, CUDA Memory = 61,885 MB, Time = 76.3 ms/token, Time to construct selective context = 46.1 ms\n\n[1] The above paragraph discusses the use of probabilistic methods, safety distance-based control methods, and trajectory prediction methods for assisting vehicles in avoiding collisions. [1] It mentions that CNN has made outstanding contributions in vision tasks, particularly in road condition inspection, due to its excellent regional feature extraction capabilities. However, the performance of CNN-based models for vision tasks is inferior to EfficientNets RepVGG, due to the huge differences between sequential tasks in NLP and image tasks in CV, the difficulty in keeping the original information of inputs after RNN layers, and the computational and memory requirements of switching layers. The paragraph introduces a new network structure called Sequential Spatial Network (SSN) blocks, which overcomes the limitations of traditional CNN-based models. [2] The SSN block consists of convolutional layers, Upsampling Convolutional Decreasing (UCD) blocks, and Reinforcement Region Unit and Fast MultiHead SelfAttention (FMHSA) to enhance local information and improve normalization capability. The paragraph concludes by stating that the SSN network outperforms existing methods on the Lykit dataset and can be easily transferred for image classification tasks.\n\nFigure 6: Comparing the summary generated by vicuna\\_13b given original context and selective context.\n\nllama models given selective context indicating instruct-tuning might help the model to be more robustness towards context compression. Given selective context, llama models often fail to follow instructions and go wild very easily.\n\n## 5.5 Lexical Units\n\nWe test the effect of Selective Context based on different lexical units: tokens, phrases, and sentences via BERTScore-F1. As shown in Table 5, employing phrase as the basic lexical units in Selective Context is the optimal approach, consistently outperforming the other two variants, followed by token-level Selective Context. Removing redundancy at sentence-level is a rather unstable implementation compared to the token and phrase-level. This experiment indicates that a reasonable granularity can be crucial in Selective Context.\n\n## 5.6 Case Study\n\nTo have a straightforward impression on how well LLMs generate with selective context, we present two summaries given the full and selective context respectively in Figure 6. The original document and processing to obtain selective context are presented at Appendix B.\n\nWe first found that preparing selective context is extremely efficient. It takes a one-time cost of 46.1 ms to build selective context for the example paragraph, which includes computing self-information and performing lexical unit tokenisation. This ensures that the initial stage of establishing a selective context incurs very little overhead. Secondly, it shows selective context significantly reduces the memory usage of the GPU and accelerates the generation process. With compression ratio of 0.5, selective context reduces about 36% of the memory cost in inference and makes generation 1.32 times faster (per token). By comparing the content of the two summaries, we see that the summary given selective context missed relevant information about the research background (as denoted by the [1] marker), such as the use of machine learning in autonomous driving technology and instead starts with the different methods directly. This is due to the background parts not being selected and removed as redundancy before feeding to vicuna . We try to ask vicuna \"what is the background of this study?\" given the selective context, and obtain a decent answer: \"the research background of this paper is likely to be situated in the domain of autonomous driving technology and the application of artificial intelligence (AI) for improving vehicle safety and decision-making capabilities.\" . This demonstrates that LLMs are likely to be able to infer the deleted parts of background information in the selective context. Selective context also affects vicuna 's decision on what information should be included in the summary as the second summary includes details about for example FMHSA and\n\nUCD block (as denoted by the [2] marker) which are not covered in the summary generated with the full context. We find no factual errors in the summary given selective context.\n\n## 6 Conclusion\n\nWe introduced Selective Context to improve the context efficiency of LLMs in inference by deleting redundant content measured by self-information. Our extensive experiments on arXiv papers, BBC news articles, and conversation transcripts showed that our proposed method can significantly reduce GPU memory cost, accelerate generation with minor performance decrease, and potentially enable LLMs to handle long documents and extended conversations without the risk of context truncation.\n\n## 7 Limitations\n\nSelective Context demonstrates promising results, but it is still necessary to note a couple of potential limitations. Firstly, our approach is somewhat influenced by the phrase boundary detection procedure. We employ the noun phrase tokenisation algorithm provided by spacy in our experiments. However, we do not consider verb phrases as there is no mature solution for verb phrase tokenisation. We speculate that we can achieve better compression performance with dependency tree-based filtering procedure which might lead to better boundary identification of lexical units. Secondly, in the experiment section, we use percentile to control the pruning process. However, the optimal compression percentile varies based on specific tasks and context. Developing a tool to find the optimal threshold can further enhance the effectiveness of selective context.\n\n## References\n\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization , pages 65-72.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877-1901.\n\nSébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 .\n\nRazvan Bunescu and Oseremen O Uduehi. 2022. Distribution-based measures of surprise for creative language: Experiments with humor and metaphor. In Proceedings of the 3rd Workshop on Figurative Language Processing (FLP) , pages 68-78.\n\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 .\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457.\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv , abs/2009.03300.\n\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out , pages 74-81.\n\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .\n\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311-318.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9.\n\nClaude E Shannon. 1948. A mathematical theory of communication. The Bell system technical journal , 27(3):379-423.\n\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 .\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems , 30.\n- Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. ArXiv , abs/2004.04228.\n- EC Wit and Marie Gillette. 1999. What is linguistic redundancy. University of Chicago .\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .\n- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 .\n\n## A Dataset statistics\n\nTable 4: Statistics of the three datasets. #Sent, #Phrase, #Token are averaged per document.\n\n| Dataset   |   #Doc |   #Sent |   #Phrase |   #Token |\n|-----------|--------|---------|-----------|----------|\n| Arxiv     |    408 |   28.2  |    514.55 |   864.85 |\n| ShareGPT  |    470 |   27.35 |    389.42 |   689.32 |\n| BBC       |    294 |   25.63 |    523.96 |   732.54 |\n\n## B Example of selective context on long context\n\nHere we present an example of selective context on a rather long context. The original paragraphs is from https://arxiv.org/abs/ 2303.07352 . The original paragraphs is shown in Fig. 7. The resulting context is shown in Fig. 8. The reference summary is given in Fig. 9.\n\nFigure 7: Selective context on the introduction of https://arxiv.org/abs/2303.07352\n\n<!-- image -->\n\nIn researchers how turn vehicles assisted driving more intelligent autonomous drivingDue the iteration intelligent hardware the improvement chip computing power collected quickly converted fed models In the driving process the safety factor users researchers Therefore how AV should avoid collisions has Concepts such probabilistic methods ( eg . : ) , safety distance-based control methods , trajectory prediction methods designed cope complex traffic conditions In terms vision CNN made outstanding contributions and applied road condition inspection tasks due its excellent regional feature extraction capabilities The local feature information obtained CNN will obstacle detection Secondly because the motion trajectory planned for AV the relationship each local feature obtained CNN needs established Some strategies based CNN plus RNN so deal sequential graphs as input eg STDN . Although the above strategies performed vision tasks their performances inferior EfficientNets RepVGG . We believe this due the following aspects First the huge differences the sequential tasks NLP the image tasks CV ignored For example when the local feature information acquired compressed how achieve accurate mapping becomes Second difficult keep the original information of inputs since after RNN layers recover the dimension from one three Besides due the several transformations between different dimensions that process becomes especially our input size 224×224×5 Third , the computational and memory requirement switching layers are extremely heavy tasks also becomes a tricky point Higher hardware requirements as running arise the attention partIn CNN attention to vision tasksThe new network structure overcomes Sequential Spatial Network (SSN) blocks As shown Fig , input images first go the convolution stem for a stack SSN blocks The Upsampling Convolutional Decreasing (UCD) blocks introduced for local information enhancement deep convolution , in SSN block of features generated in can less loss of image resolution crucial the subsequent trajectory adjustment task In addition we adopt a staged architecture design using five convolutional layers steps gradually ( sequence length flexibly Such a design helps extract local features different scales , since the first stage retains high resolution our design effectively reduce the output information atThe Reinforcement Region Unit and the Fast MultiHead Self-Attention (FMHSA in the SSN block can help obtain within the intermediate features and improve the normalization capability Finally average pooling better trajectory tuning Extensive experiments the lykit dataset demonstrate our SSN network In addition image classification SSN block can easily transferred and serve a versatile backbone.\n\nFigure 8: The resulting context\n\nGiven Original Context , CUDA Memory = 77,695 MB; Time = 110.8 ms/token\n\nThe paragraph discusses the development of autonomous driving technology and the use of artificial intelligence and machine learning to make decisions in the driving process. The safety of the vehicle is a top priority and methods such as probabilistic methods, safety distance-based control methods, and trajectory prediction methods have been developed to cope with complex traffic conditions. The use of convolutional neural networks (CNN) has been successful in image tasks such as obstacle detection, but the performance of these networks is still inferior to similar-sized convolutional neural networks counterparts. This is due to the huge differences between the sequential tasks of NLP and the image tasks of CV, the difficulty of keeping the original information of inputs, and the heavy computational and memory requirement of switching between layers. The authors propose a new network structure based on CNN and attention to vision tasks in autonomous driving, using Sequential Spatial Network (SSN) blocks. The network structure overcomes these problems by using deep convolution and attention mechanisms to extract local and global features, and reducing the computational effort of subsequent layers. The proposed network structure is evaluated on the lykit dataset and shows superior performance in terms of accuracy. B.\n\nFigure 9: The reference summary generated by vicuna-13b given the full context.",
  "tables": [
    {
      "index": 0,
      "markdown": "|                   |       |            |            | ROUGE        | ROUGE      | ROUGE      | BERTScore   | BERTScore   | BERTScore     |\n|-------------------|-------|------------|------------|--------------|------------|------------|-------------|-------------|---------------|\n| Method            | Ratio | BLEU       | METEOR     | rouge1       | rouge2     | rougeL     | Precision   | Recall      | F1            |\n| Original          | -     | .347       | .496       | .571         | .383       | .471       | .910        | .909        | .909          |\n| Selective Context | 0.2   | .295 (.05) | .460 (.04) | .540 ( .03 ) | .346 (.04) | .438 (.03) | .905 (.005) | .900 (.009) | .902 ( .007 ) |\n| Selective Context | 0.35  | .243 (.10) | .421 (.08) | .504 ( .07 ) | .294 (.09) | .396 (.07) | .900 (.010) | .894 (.015) | .897 ( .013 ) |\n| Selective Context | 0.5   | .179 (.17) | .362 (.13) | .449 (.12)   | .237 (.15) | .344 (.13) | .893 (.018) | .882 (.027) | .887 (.023)   |\n| Selective Context | 0.65  | .127 (.22) | .299 (.20) | .391 (.18)   | .178 (.21) | .287 (.18) | .885 (.025) | .870 (.039) | .877 (.032)   |\n| Selective Context | 0.8   | .070 (.28) | .224 (.27) | .311 (.26)   | .122 (.26) | .225 (.25) | .874 (.036) | .852 (.057) | .863 (.047)   |"
    },
    {
      "index": 1,
      "markdown": "|                   |       |       |        | ROUGE   | ROUGE   | ROUGE   | BERTScore   | BERTScore   | BERTScore   |\n|-------------------|-------|-------|--------|---------|---------|---------|-------------|-------------|-------------|\n|                   | Ratio | BLEU  | METEOR | rouge1  | rouge2  | rougeL  | Precision   | Recall      | F1          |\n| Random            | 0.20  | 0.437 | 0.578  | 0.666   | 0.503   | 0.566   | 0.892       | 0.909       | 0.899       |\n|                   | 0.35  | 0.360 | 0.514  | 0.629   | 0.423   | 0.502   | 0.879       | 0.895       | 0.886       |\n|                   | 0.50  | 0.283 | 0.443  | 0.576   | 0.346   | 0.432   | 0.867       | 0.881       | 0.873       |\n|                   | 0.65  | 0.210 | 0.378  | 0.522   | 0.279   | 0.371   | 0.855       | 0.868       | 0.860       |\n|                   | 0.80  | 0.156 | 0.314  | 0.450   | 0.219   | 0.310   | 0.843       | 0.853       | 0.847       |\n| Selective Context | 0.20  | 0.527 | 0.643  | 0.714   | 0.585   | 0.631   | 0.930       | 0.932       | 0.931       |\n|                   | 0.35  | 0.446 | 0.588  | 0.679   | 0.508   | 0.570   | 0.915       | 0.916       | 0.915       |\n|                   | 0.50  | 0.350 | 0.528  | 0.642   | 0.425   | 0.501   | 0.900       | 0.902       | 0.900       |\n|                   | 0.65  | 0.244 | 0.418  | 0.557   | 0.315   | 0.404   | 0.886       | 0.877       | 0.881       |\n|                   | 0.80  | 0.160 | 0.328  | 0.464   | 0.223   | 0.319   | 0.875       | 0.858       | 0.866       |"
    },
    {
      "index": 2,
      "markdown": "| Ratio   |   #Sorry |   Answer len. | Unfaithfulness   |\n|---------|----------|---------------|------------------|\n| Full    |        0 |         160.3 | -                |\n| 0.2     |        0 |         156.5 | .027             |\n| 0.35    |        6 |         136   | .050             |\n| 0.5     |        4 |         140.2 | .038             |\n| 0.65    |       19 |         131.2 | .051             |\n| 0.8     |       27 |         103.7 | .086             |"
    },
    {
      "index": 3,
      "markdown": "| Dataset   |   #Doc |   #Sent |   #Phrase |   #Token |\n|-----------|--------|---------|-----------|----------|\n| Arxiv     |    408 |   28.2  |    514.55 |   864.85 |\n| ShareGPT  |    470 |   27.35 |    389.42 |   689.32 |\n| BBC       |    294 |   25.63 |    523.96 |   732.54 |"
    }
  ],
  "stats": {
    "pages": 12,
    "chunksCreated": 66,
    "totalCharacters": 44487,
    "totalWords": 6740,
    "numTables": 4,
    "processingTimeMs": 19901
  }
}