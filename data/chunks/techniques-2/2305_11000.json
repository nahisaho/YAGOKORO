{
  "paper": {
    "id": "2305.11000v2",
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
    "authors": [
      "Dong Zhang",
      "Shimin Li",
      "Xin Zhang",
      "Jun Zhan",
      "Pengyu Wang",
      "Yaqian Zhou",
      "Xipeng Qiu"
    ],
    "published": "2023-05-18T14:23:25.000Z",
    "updated": "2023-05-19T14:41:16.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2305.11000v2",
    "absUrl": "https://arxiv.org/abs/2305.11000v2"
  },
  "chunks": [
    {
      "id": "2305.11000v2-chunk-0",
      "content": "School of Computer Science, Fudan University Shanghai Key Laboratory of Intelligent Information Processing, Fudan University dongzhang22@m.fudan.edu.cn {smli20,zhouyaqian,xpqiu}@fudan.edu.cn\n\nhttps://github.com/0nutation/SpeechGPT",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou ∗ , Xipeng Qiu ∗",
        "chunkIndex": 0,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-1",
      "content": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multimodel content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pretraining, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-2",
      "content": "ing. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/ .",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-3",
      "content": "Large language models (OpenAI, 2023; Touvron et al., 2023) have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E (Driess et al., 2023), and LLaVA (Liu et al., 2023), have explored the ability of LLMs to understand multi-modal information. However, a significant gap exists between current LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and understand multi-modal content but cannot spontaneously generate multi-modal content. Second, continuous signals like images and speech cannot be adapted directly to LLMs that receive discrete tokens.\n\nThe current speech-language model mainly adopts a cascading paradigm (Huang et al., 2023a) i.e., the LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models are integrated to cover multiple aud",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-4",
      "content": "automatic speech recognition (ASR) model or a text-to-speech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models are integrated to cover multiple audio or speech tasks (Huang et al., 2023a; Shen et al., 2023). Some prior work on generative spoken language models involves encoding the speech signal into a discrete representation (Baevski et al., 2020; Hsu et al., 2021) and modeling it with language models (Lakhotia et al., 2021; Borsos et al., 2022; Zhang et al., 2023b; Wang et al., 2023).\n\n∗ Corresponding author\n\nFigure 1: SpeechGPT's capabilities to tackle multiple cross-modal tasks.\n\n<!-- image -->\n\nWhile capable of perceiving and generating speech, the existing cascading methods or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-5",
      "content": "f perceiving and generating speech, the existing cascading methods or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator. Since the representations of speech and text are not aligned, the LLM's knowledge cannot be transferred to the speech modality. Second, the cascade approach (Shen et al., 2023; Huang et al., 2023a) suffers from the loss of paralinguistic signals such as emotion and prosody. Third, existing spoken language models (Wang et al., 2023; Zhang et al., 2023b) only synthesize speech but fail to comprehend its semantic information, preventing them from achieving true cross-modal perception and generation.\n\nIn this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. We perform speech discretization with a self-supervised trained speech model to unify the modality between speech and text.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-6",
      "content": "ational abilities, capable of perceiving and generating multi-model content. We perform speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing the model with an inherent competence to perceive and generate the speech.\n\nTo provide the model with the capacity to handle multi-modal instructions, we build the first speechtext cross-modal instruction-following dataset SpeechInstruct. Specifically, we discretize the speech to discrete units (Hsu et al., 2021) and construct the cross-modal unit-text pair based on the existing ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to simulate actual user instructions as illustrated in Appendix B.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-7",
      "content": "modal unit-text pair based on the existing ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to simulate actual user instructions as illustrated in Appendix B. In addition, to further enhance the model's cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model receives the speech command, thinks about the process in text, and then outputs the response in speech.\n\nFor better cross-modal transfer and efficient training, SpeechGPT undergoes a three-stage training process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The first stage enables speech comprehension for SpeechGPT with the discrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the model's cross-modal capabilities. The third stage utilizes parameter-efficient LoRA (Hu et al., 2021) fine-tuning for further modality alignment.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-8",
      "content": "second stage employs the SpeechInstruct to improve the model's cross-modal capabilities. The third stage utilizes parameter-efficient LoRA (Hu et al., 2021) fine-tuning for further modality alignment.\n\nTo evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case analyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks, and spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for unimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.\n\nOur contributions include the following:\n\n- We build the first multi-modal large language model that can perceive and generate multi-modal contents.\n- We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instructionfollowing dataset.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-9",
      "content": "i-modal large language model that can perceive and generate multi-modal contents.\n- We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instructionfollowing dataset.\n\nFigure 2: Left : An overview of SpeechInstruct construction process. The SpeechInstruct dataset consists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. Template 1 is shown in 3.1. Template 2 is shown in Appendix C. Right : An illustration of SpeechGPT model structure.\n\n<!-- image -->\n\n- We build the first spoken dialogue LLM with strong human instruction following ability and spoken dialogue ability.\n- We show great potential to incorporate other modalities into LLMs through discrete representations.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-10",
      "content": "Multi-modal Large Language Model Current multi-modal LLMs predominantly focus on the visual domain, feeding continuous representations obtained from pre-trained visual encoders into LLMs, facilitating full-parameter or parameter-efficient training on visual-language data (OpenAI, 2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E (Driess et al., 2023) integrates the 540B PaLM (Chowdhery et al., 2022) and 22B Vision Transformer (Dosovitskiy et al., 2021) into the largest vision-language model. LLaVA (Liu et al., 2023) leverages pre-trained CLIP (Radford et al., 2021) visual encoder and LLaMA (Touvron et al., 2023) and conduct instruct tuning on GPT4-assisted visual instruction data. X-LLM (Chen et al., 2023) converts multi-modalities into representations with X2L interfaces as the inputs of the large language model. However, such structures only enable LLMs to process multi-modal input, without ability to generate multi-modal output.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "2 Related Work",
        "chunkIndex": 10,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-11",
      "content": "o representations with X2L interfaces as the inputs of the large language model. However, such structures only enable LLMs to process multi-modal input, without ability to generate multi-modal output. Diverging from prior studies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing it with the proficiency to accommodate both multi-modal input and output.\n\nGenerative Spoken Language Model Discrete self-supervised representation based spoken generative language modeling is making remarkable progress on large-scale speech dataset training (Nguyen et al., 2022). AudioLM (Borsos et al., 2022) proposes to model speech based on audio codecs together with semantic codes, which can synthesize speech in a textlesss setting. V ALL-E (Wang et al., 2023) builds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional generation task. However, these models are designed for a specific task and failed to benefit from LLMs.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "2 Related Work",
        "chunkIndex": 11,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-12",
      "content": "ds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional generation task. However, these models are designed for a specific task and failed to benefit from LLMs. SpeechGPT is built upon the foundation of LLM and transfers LLM's knowledge to speech modality, consequently obtaining better task generalization and human-instruction following ability.\n\nSpeech-Enabled LLM Interaction Following the emergence of ChatGPT, several studies have concentrated on the integration of expert speech models with LLMs to enable direct speech interaction with LLMs. HuggingGPT (Shen et al., 2023) facilitates task decomposition of human instructions by LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encompassing a range of automatic speech recognition (ASR) and text-to-speech models.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "2 Related Work",
        "chunkIndex": 12,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-13",
      "content": "n of human instructions by LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encompassing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT (Huang et al., 2023a) leverages a variety of audio foundation models to process complex audio information and connect LLMs with input/output interface (ASR, TTS) for speech conversations. However, these models exhibit increased complexity, demand extensive resources, and are prone to the unavoidable error accumulation problems. Our approach enables speech interaction with LLMs without relying on ASR or TTS systems, circumventing the aforementioned drawbacks.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "2 Related Work",
        "chunkIndex": 13,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-14",
      "content": "Due to the limitations in publicly available speech data and the lack of variety of speech-text tasks, we construct SpeechInstruct, a speech-text cross-modal instruction-following dataset. This dataset consists of two parts, the first part is called Cross-Modal Instruction, and the second part is called Chain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure 2.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3 SpeechInstruct Construction",
        "chunkIndex": 14,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-15",
      "content": "Data Collection We collect several large-scale English ASR datasets to construct Cross-Modal Instruction, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and LibriSpeech (Panayotov et al., 2015). We employ mHuBERT 2 as the speech tokenizer to discretize speech data into discrete units and remove the repetitive units of adjacent frames to get reduced units. Ultimately, we obtain 9 million unit-text data pairs.\n\nTask Description Generation We generate ASR and TTS task descriptions that are compatible with speech-text data pairs. Unlike the Self-Instruct method (Wang et al., 2022), we generate descriptions through a zero-shot approach. Specifically, we directly input the prompts shown in Appendix A into OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each task and some examples are shown in Appendix B.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3.1 Cross-modal Instruction",
        "chunkIndex": 15,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-16",
      "content": "e directly input the prompts shown in Appendix A into OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each task and some examples are shown in Appendix B.\n\nInstruction Formatting For a discrete unit sequence U and its associated transcription T , we determine whether it will be used for constructing an ASR task or a TTS task based on the probability p . Subsequently, we randomly select a description D from the corresponding task description. This results in a triplet consisting of the task description, discrete unit sequence, and transcription, denoted as ( D,U,T ) . Following this, the triplet is assembled into an instruction using the template: [Human]: { D } . This is input: { U } &lt;eoh&gt;.[SpeechGPT]: { T } &lt;eos&gt;. . To support multi-turn dialogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering to the maximum input length of the model.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3.1 Cross-modal Instruction",
        "chunkIndex": 16,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-17",
      "content": "i-turn dialogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering to the maximum input length of the model.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3.1 Cross-modal Instruction",
        "chunkIndex": 17,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-18",
      "content": "Speech Instruction Generation Due to the lack of instruction data with speech input and speech output, we trained a text-to-unit generator to convert text instruction data into speech instruction data. Specifically, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We trained it on LibriSpeech unit-text pairs in Cross-modal Instruction. We select 37,969 samples from the moss-002-sft-data dataset 3 whose response length is shorter than 35 words. And we convert both their instructions and responses into unit sequences through the text-to-unit generator. As a result, we obtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and speech responses, denoted as ( SpeechI, TextI, TextR, SpeechR ) .\n\nInstruction Formatting Using the above quadruplets, we could construct chain-of-thought style instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech Instruction-Text Response, Text Instruction",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3.2 Chain-of-Modality Instruction",
        "chunkIndex": 18,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-19",
      "content": "e above quadruplets, we could construct chain-of-thought style instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech Instruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response. Their corresponding templates can be found in Appendix C.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "3.2 Chain-of-Modality Instruction",
        "chunkIndex": 19,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-20",
      "content": "A unified framework is designed to provide architecture compatibility across different modalities. As shown in Figure 2, our model consists of three main components: discrete unit extractor , large language modal and unit vocoder . Under this architecture, LLM can perceive multi-modal inputs and generate multi-modal outputs.\n\nDiscrete Unit Extractor The discrete unit extractor utilizes the Hidden-unit BERT (HuBERT) model (Hsu et al., 2021) to transform continuous speech signals into a sequence of discrete units, .\n\n2 https://dl.fbaipublicfiles.com/hubert/mhubert\\_base\\_vp\\_en\\_es\\_fr\\_it3.pt\n\n3 https://huggingface.co/datasets/fnlp/moss-002-sft-data\n\nHuBERT is a self-supervised model that learns by predicting discrete labels for masked audio segments based on k-means clustering applied to the model's intermediate representations.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.1 Model Structure",
        "chunkIndex": 20,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-21",
      "content": "oss-002-sft-data\n\nHuBERT is a self-supervised model that learns by predicting discrete labels for masked audio segments based on k-means clustering applied to the model's intermediate representations. It features a combination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous intermediate representations, with a k-means model further converting these representations into a sequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a discrete units sequence represented as U = ( u 1 , u 2 , . . . , u T ) , u i ∈ 0 , 1 , . . . , K -1 , ∀ 1 ≤ i ≤ T , with K denoting the total number of clusters.\n\nLarge Language Model We employ the Meta AI LLaMA (Touvron et al., 2023) model as our Large Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an LMhead layer. The total number of parameters in LLaMA ranges from 7B to 65B.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.1 Model Structure",
        "chunkIndex": 21,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-22",
      "content": "on et al., 2023) model as our Large Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an LMhead layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from an extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance compared to the substantially larger 175B GPT-3 across various NLP benchmarks.\n\nUnit Vocoder Due to limition of single speaker unit vocoder in (Polyak et al., 2021), we train a multi-speaker unit HiFi-GAN to decode the speech signal from the discrete representation. The HiFi-GAN architecture consists of a generator G and multiple discriminators D . The generator uses look-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled by a series of blocks composed of transposed convolution and a residual block with dilated layers. The speaker embedding is concatenated to each frame in the up-sampled sequence.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.1 Model Structure",
        "chunkIndex": 22,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-23",
      "content": "uences are up-sampled by a series of blocks composed of transposed convolution and a residual block with dilated layers. The speaker embedding is concatenated to each frame in the up-sampled sequence. The discriminator features a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have the same architecture as (Polyak et al., 2021).",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.1 Model Structure",
        "chunkIndex": 23,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-24",
      "content": "To incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding embedding matrix first. We divide the training process into three stages. The first stage is ModalityAdaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction Fine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.\n\nExpanding Vocabulary Given original LLM vocabulary V of size | V | , to integrate speech discrete representations into LLM, we expand the vocabulary with an additional set of unit tokens V ′ , of size | V ′ | = K . The expanded vocabulary V ′′ is the union of the original vocabulary V and the new words V ′ :\n\n<!-- formula-not-decoded -->\n\nWe denote the original word embedding matrix as E ∈ R | V |× d , where d is the dimension of word embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized word embedding matrix E ′ ∈ R | V ′′ |× d .",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 24,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-25",
      "content": "ing matrix as E ∈ R | V |× d , where d is the dimension of word embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized word embedding matrix E ′ ∈ R | V ′′ |× d . We preserve the original word embeddings by copying the values of E to the first | V | rows of E ′ :\n\n<!-- formula-not-decoded -->\n\nFinally, we replace the original vocabulary and word embedding matrix with the new vocabulary V ′′ and the word embedding matrix E ′ .\n\nStage 1: Modality-Adaptation Pre-training To enable LLM to handle discrete units modality, we utilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns with the text pre-training objective of LLM. Given unlabeled speech corpus C consisting of speech U 1 , U 2 , . . . , U m and LLM denoted as L 1 , the negative log-likelihood loss can be formulated as:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 25,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-26",
      "content": "ctive of LLM. Given unlabeled speech corpus C consisting of speech U 1 , U 2 , . . . , U m and LLM denoted as L 1 , the negative log-likelihood loss can be formulated as:\n\n<!-- formula-not-decoded -->\n\nwhere m is the number of speech in dataset C , n j is the number of discrete unit token in speech U j , and u i,j represents the i-th unit token in the j-th speech.\n\nStage 2: Cross-modal Instruction Fine-Tuning In this stage, we align speech and text modalities utilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to\n\nInstruction\n\n: Can you transcribe the speech into a written format?\n\nInput\n\n: Speech clip (Transcripts: I'm afraid there are no signs here said he.)\n\nOutput\n\n: Text: I'm afraid there are no signs here said he.\n\nInstruction\n\n: Listen to the speech and write down its content.\n\nInput : Speech clip (Transcripts: Did anyone know that these proofs would be there no one saved the printer.)\n\nOutput",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 26,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-27",
      "content": "ns here said he.\n\nInstruction\n\n: Listen to the speech and write down its content.\n\nInput : Speech clip (Transcripts: Did anyone know that these proofs would be there no one saved the printer.)\n\nOutput\n\n: Text: Did anyone know that these proofs would be there no one saved the printer.\n\nInstruction : Would you mind speaking these words as naturally as possible?\n\nInput\n\n: Text: Today is a sunny day and I'm happy to be here.\n\nOutput\n\n: Speech clip (Transcripts: Today is a sunny day and I'm happy to be here.)\n\nInstruction\n\n: Would you please speed-read the following sentence?\n\nInput : Text: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.\n\nOutput : Speech clip (Transcripts: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.)\n\nTable 1: Cases of cross-modal instruction-following results",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 27,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-28",
      "content": "Speech clip (Transcripts: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.)\n\nTable 1: Cases of cross-modal instruction-following results\n\nderive mix dataset I , which consists of samples T 1 , T 2 , . . . , T x . We fine-tune the model L obtained from the first stage on I .\n\nEach sample T j consisting of t 1 , t 2 , . . . , t n j is formed by concatenating a prefix and a text. The training objective is to minimize the negative log-likelihood and the loss calculation only considers the text part, ignoring the prefix, which can be formated as:\n\n<!-- formula-not-decoded -->\n\nwhere x is the number of samples in corpus I , y j is the total number of tokens in sample T j , p j is the number of tokens in the prefix part of T j , and t i,j represents the i-th word in T j .",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 28,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-29",
      "content": "->\n\nwhere x is the number of samples in corpus I , y j is the total number of tokens in sample T j , p j is the number of tokens in the prefix part of T j , and t i,j represents the i-th word in T j .\n\nStage 3: Chain-of-Modality Instruction Fine-Tuning After obtaining the model in stage 2, we utilizes parameter-efficient Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune it on Chain-ofModality Instruction in SpeechInstruct. We add LoRA weights (adapters) to the attention mechanisms and train the newly added LoRA parameters. We adopt the same loss function as stage 2.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "4.2 Training",
        "chunkIndex": 29,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-30",
      "content": "Datasets For modality-adaption pre-training, we use LibriLight (Kahn et al., 2020) which contains 60K hours of unlabelled English audiobook speech. For cross-modal instruction fine-tuning stage, we use Gigaspeech (Chen et al., 2021), Common voice (Ardila et al., 2020) and LibriSpeech (Panayotov et al., 2015) dataset and moss-002-sft-data dataset, which is illustrated in detail in 3.1. For chain-ofmodality instruction fine-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail in 3.2.\n\nConfiguration We employ LLaMA-13B (Touvron et al., 2023) as our backbone model. For stage 1, we use 96 A100 gpu and train for 900 steps with batch size 768. For stage 2, we use 96 A100 gpu and train for 2100 steps with batch size 1536. For stage 3, we use 8 A100 gpu and train for 4200 steps with batch size 128. Details about training hyperparameters are shown in Appendix 3. For decoding, we set the maximum sequence length to 2048 and set the temperature to 0.8.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "5.1 Experimental Setups",
        "chunkIndex": 30,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-31",
      "content": "u and train for 4200 steps with batch size 128. Details about training hyperparameters are shown in Appendix 3. For decoding, we set the maximum sequence length to 2048 and set the temperature to 0.8. We use Topk sampling with k =60. We also use Topp sampling with p=0.8.\n\nTable 2: Cases of spoken dialogue results\n\n<!-- image -->\n\nEvaluation We evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction following ability and spoken dialogue ability. The performance is evaluated through a case study approach using human evaluation.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "5.1 Experimental Setups",
        "chunkIndex": 31,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-32",
      "content": "Cross-modal Instruction Following As shown in Table 1, when provided with various instructions, the model is capable of performing corresponding tasks and generating accurate outputs in accordance with these inputs.\n\nSpoken Dialogue Table 2 shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows that in interactions with humans, SpeechGPT is capable of comprehending speech instructions and responding accordingly in speech, while adhering to the HHH criteria (Harmless, Helpful, Honest) (Askell et al., 2021).",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "5.2 Main Results",
        "chunkIndex": 32,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-33",
      "content": "Despite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue abilities, it still presents certain limitations: 1) It does not consider paralinguistic information in speech, such as the inability to generate responses in different emotional tones, 2) It necessitates the generation of a text-based response prior to the production of a speech-based one, 3) Due to the context length limitation, it is incapable of supporting multi-turn dialogues.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "6 Limitation",
        "chunkIndex": 33,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-34",
      "content": "This work presents SpeechGPT, an inherent cross-modal multimodal large language model capable of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal instruction-following dataset contains cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a three-stage training paradigm to obtain the final SpeechGPT. Experimental results indicate that SpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate that combining discrete speech tokens into the language model is a promising direction.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "7 Conclusion",
        "chunkIndex": 34,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-35",
      "content": "- Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common voice: A massively-multilingual speech corpus, 2020.\n- Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. A general language assistant as a laboratory for alignment, 2021.\n- Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems , 33: 12449-12460, 2020.\n- Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 35,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-36",
      "content": "nformation Processing Systems , 33: 12449-12460, 2020.\n- Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio generation, 2022.\n- Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S. X., and Xu, B. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. 2023.\n- Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y., You, Z., and Yan, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio, 2021.\n- Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 36,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-37",
      "content": "an, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio, 2021.\n- Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 37,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-38",
      "content": "ewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022.\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 , 2023.\n- Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Self-supervised speech representation learning by masked prediction of hidden units.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 38,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-39",
      "content": "303.03378 , 2023.\n- Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 29:3451-3460, 2021.\n- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021.\n- Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y., Zhao, Z., and Watanabe, S. Audiogpt: Understanding and generating speech, music, sound, and talking head, 2023a.\n- Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 39,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-40",
      "content": "a.\n- Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F. Language is not all you need: Aligning perception with language models, 2023b.\n- Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In ICASSP 2020 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, may 2020. doi: 10.1109/icassp40776.2020.9052942. URL https://doi.org/10.1109% 2Ficassp40776.2020.9052942 .\n- Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 40,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-41",
      "content": ".9052942 .\n- Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics , 9:1336-1354, 2021.\n- Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485 , 2023.\n- Nguyen, T. A., Kharitonov, E., Copet, J., Adi, Y., Hsu, W.-N., Elkahky, A., Tomasello, P., Algayres, R., Sagot, B., Mohamed, A., and Dupoux, E. Generative spoken dialogue language modeling, 2022.\n- OpenAI. Gpt-4 technical report, 2023.\n- Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 5206-5210, 2015.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 41,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-42",
      "content": "y, D., and Khudanpur, S. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 5206-5210, 2015. doi: 10.1109/ICASSP.2015.7178964.\n- Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux, E. Speech resynthesis from discrete disentangled self-supervised representations, 2021.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\n- Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface, 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 42,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-43",
      "content": "ce, 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers, 2023.\n- Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions, 2022.\n\n- Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023a.\n- Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 43,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-44",
      "content": "Efficient fine-tuning of language models with zero-init attention, 2023a.\n- Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling, 2023b.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "References",
        "chunkIndex": 44,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-45",
      "content": "You are asked to come up with a set of 100 diverse task instructions about automatic speech recognition, which is about recognizing speech.\n\nHere are the requirements:\n\n1. These instructions should be to instruct someone to recognize the content of the following speech.\n\n2. Try not to repeat the verb for each instruction to maximize diversity.\n\n3. The language used for instruction also should be diverse. For example, you should combine questions with imperative instructions.\n\n4. The type of instructions should be diverse.\n\n5. The instructions should be in English.\n\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n\nList of 100 tasks:",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "ASR :",
        "chunkIndex": 45,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-46",
      "content": "You are asked to come up with a set of 100 diverse task instructions about text to speech, which is about recognizing speech .\n\nHere are the requirements:\n\n1. These instructions should be to instruct someone to recognize the content of the following speech.\n\n2. Try not to repeat the verb for each instruction to maximize diversity.\n\n3. The language used for instruction also should be diverse. For example, you should combine questions with imperative instructions.\n\n4. The type of instructions should be diverse.\n\n5. The instructions should be in English.\n\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n\nList of 100 tasks:",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "TTS :",
        "chunkIndex": 46,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-47",
      "content": "Begin by converting the spoken words into written text. Can you transcribe the speech into a written format? Focus on translating the audible content into text. Transcribe the speech by carefully listening to it. Would you kindly write down the content of the speech? Analyze the speech and create a written transcription. Engage with the speech to produce a text-based version. Can you document the speech in written form? Transform the spoken words into text accurately. How about putting the speech's content into writing?",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "ASR :",
        "chunkIndex": 47,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-48",
      "content": "Can you please read this sentence out loud?\n\nRecite the following words as if you were speaking normally.\n\nProject your voice to clearly articulate this statement.\n\nWould you mind speaking these words as naturally as possible?\n\nWhisper the given sentence softly.\n\nEnunciate each word in this sentence with precision. How would you express this sentence in a conversational tone?\n\nCould you please relay the message below verbally?\n\nEmphasize the key points while reading the sentence.\n\nSing the text provided in a melodic voice.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "TTS :",
        "chunkIndex": 48,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-49",
      "content": "[Human] : This is a speech instruction: {SpeechI}. And your response should be speech. You can do it step by step. You can first transcribe the instruction and get the text Instruction. Then you can think about the instruction and get the text response. Last, you should speak the response aloud &lt;eoh&gt;. [SpeechGPT] : [tq] {TextI}; [ta] {TextR}; [ua] {SpeechR}&lt;eoa&gt;.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Speech Instruction-Speech Response :",
        "chunkIndex": 49,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-50",
      "content": "[Human] : This is a speech instruction: {SpeechI}. And your response should be text. You can do it step by step. You can first transcribe the instruction and get the text instruction. Then you can think about the instruction and get the text response. &lt;eoh&gt;. [SpeechGPT] : [tq] {TextI}; [ta] {TextR}&lt;eoa&gt;.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Speech Instruction-Text Response :",
        "chunkIndex": 50,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-51",
      "content": "[Human] : This is a text instruction: {TextI}. And your response should be speech. You can do it step by step. You can think about the instruction and get the text response. Then you should speak the response aloud &lt;eoh&gt;. [SpeechGPT] : [ta] {TextR}; [ua] {SpeechR}&lt;eoa&gt;.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Text Instruction-Speech Response :",
        "chunkIndex": 51,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-52",
      "content": "[Human] : This is a text instruction: {TextI}. And your response should be text. You can think about the instruction and get the text response. [SpeechGPT] : [ta] {TextR}&lt;eoa&gt;.",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "Text Instruction-Text Response :",
        "chunkIndex": 52,
        "totalChunks": 54
      }
    },
    {
      "id": "2305.11000v2-chunk-53",
      "content": "Table 3: SpeechGPT training hyperparameters.\n\n|                      | Stage 1   | Stage 2   | Stage 3   |\n|----------------------|-----------|-----------|-----------|\n| Batch size           | 768       | 1536      | 128       |\n| Peak learning rate   | 2e-4      | 2e-4      | 2e-4      |\n| Max length           | 1024      | 512       | 1024      |\n| Training steps       | 900       | 4000      | 4200      |\n| LoRA rank            | -         | -         | 8         |\n| LoRA alpha           | -         | -         | 16        |\n| Trainable parameters | 13B       | 13B       | 6M        |\n| Training device      | 96 × A100 | 96 × A100 | 8 × A100  |",
      "metadata": {
        "source": "arxiv:2305.11000v2",
        "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
        "authors": [
          "Dong Zhang",
          "Shimin Li",
          "Xin Zhang",
          "Jun Zhan",
          "Pengyu Wang",
          "Yaqian Zhou",
          "Xipeng Qiu"
        ],
        "section": "D Hyperparameters",
        "chunkIndex": 53,
        "totalChunks": 54
      }
    }
  ],
  "fullText": "## SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities\n\n## Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou ∗ , Xipeng Qiu ∗\n\nSchool of Computer Science, Fudan University Shanghai Key Laboratory of Intelligent Information Processing, Fudan University dongzhang22@m.fudan.edu.cn {smli20,zhouyaqian,xpqiu}@fudan.edu.cn\n\nhttps://github.com/0nutation/SpeechGPT\n\n## Abstract\n\nMulti-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multimodel content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pretraining, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/ .\n\n## 1 Introduction\n\nLarge language models (OpenAI, 2023; Touvron et al., 2023) have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E (Driess et al., 2023), and LLaVA (Liu et al., 2023), have explored the ability of LLMs to understand multi-modal information. However, a significant gap exists between current LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and understand multi-modal content but cannot spontaneously generate multi-modal content. Second, continuous signals like images and speech cannot be adapted directly to LLMs that receive discrete tokens.\n\nThe current speech-language model mainly adopts a cascading paradigm (Huang et al., 2023a) i.e., the LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models are integrated to cover multiple audio or speech tasks (Huang et al., 2023a; Shen et al., 2023). Some prior work on generative spoken language models involves encoding the speech signal into a discrete representation (Baevski et al., 2020; Hsu et al., 2021) and modeling it with language models (Lakhotia et al., 2021; Borsos et al., 2022; Zhang et al., 2023b; Wang et al., 2023).\n\n∗ Corresponding author\n\nFigure 1: SpeechGPT's capabilities to tackle multiple cross-modal tasks.\n\n<!-- image -->\n\nWhile capable of perceiving and generating speech, the existing cascading methods or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator. Since the representations of speech and text are not aligned, the LLM's knowledge cannot be transferred to the speech modality. Second, the cascade approach (Shen et al., 2023; Huang et al., 2023a) suffers from the loss of paralinguistic signals such as emotion and prosody. Third, existing spoken language models (Wang et al., 2023; Zhang et al., 2023b) only synthesize speech but fail to comprehend its semantic information, preventing them from achieving true cross-modal perception and generation.\n\nIn this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. We perform speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing the model with an inherent competence to perceive and generate the speech.\n\nTo provide the model with the capacity to handle multi-modal instructions, we build the first speechtext cross-modal instruction-following dataset SpeechInstruct. Specifically, we discretize the speech to discrete units (Hsu et al., 2021) and construct the cross-modal unit-text pair based on the existing ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to simulate actual user instructions as illustrated in Appendix B. In addition, to further enhance the model's cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model receives the speech command, thinks about the process in text, and then outputs the response in speech.\n\nFor better cross-modal transfer and efficient training, SpeechGPT undergoes a three-stage training process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The first stage enables speech comprehension for SpeechGPT with the discrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the model's cross-modal capabilities. The third stage utilizes parameter-efficient LoRA (Hu et al., 2021) fine-tuning for further modality alignment.\n\nTo evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case analyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks, and spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for unimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.\n\nOur contributions include the following:\n\n- We build the first multi-modal large language model that can perceive and generate multi-modal contents.\n- We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instructionfollowing dataset.\n\nFigure 2: Left : An overview of SpeechInstruct construction process. The SpeechInstruct dataset consists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. Template 1 is shown in 3.1. Template 2 is shown in Appendix C. Right : An illustration of SpeechGPT model structure.\n\n<!-- image -->\n\n- We build the first spoken dialogue LLM with strong human instruction following ability and spoken dialogue ability.\n- We show great potential to incorporate other modalities into LLMs through discrete representations.\n\n## 2 Related Work\n\nMulti-modal Large Language Model Current multi-modal LLMs predominantly focus on the visual domain, feeding continuous representations obtained from pre-trained visual encoders into LLMs, facilitating full-parameter or parameter-efficient training on visual-language data (OpenAI, 2023; Huang et al., 2023b; Zhang et al., 2023a). Palm-E (Driess et al., 2023) integrates the 540B PaLM (Chowdhery et al., 2022) and 22B Vision Transformer (Dosovitskiy et al., 2021) into the largest vision-language model. LLaVA (Liu et al., 2023) leverages pre-trained CLIP (Radford et al., 2021) visual encoder and LLaMA (Touvron et al., 2023) and conduct instruct tuning on GPT4-assisted visual instruction data. X-LLM (Chen et al., 2023) converts multi-modalities into representations with X2L interfaces as the inputs of the large language model. However, such structures only enable LLMs to process multi-modal input, without ability to generate multi-modal output. Diverging from prior studies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing it with the proficiency to accommodate both multi-modal input and output.\n\nGenerative Spoken Language Model Discrete self-supervised representation based spoken generative language modeling is making remarkable progress on large-scale speech dataset training (Nguyen et al., 2022). AudioLM (Borsos et al., 2022) proposes to model speech based on audio codecs together with semantic codes, which can synthesize speech in a textlesss setting. V ALL-E (Wang et al., 2023) builds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional generation task. However, these models are designed for a specific task and failed to benefit from LLMs. SpeechGPT is built upon the foundation of LLM and transfers LLM's knowledge to speech modality, consequently obtaining better task generalization and human-instruction following ability.\n\nSpeech-Enabled LLM Interaction Following the emergence of ChatGPT, several studies have concentrated on the integration of expert speech models with LLMs to enable direct speech interaction with LLMs. HuggingGPT (Shen et al., 2023) facilitates task decomposition of human instructions by LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encompassing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT (Huang et al., 2023a) leverages a variety of audio foundation models to process complex audio information and connect LLMs with input/output interface (ASR, TTS) for speech conversations. However, these models exhibit increased complexity, demand extensive resources, and are prone to the unavoidable error accumulation problems. Our approach enables speech interaction with LLMs without relying on ASR or TTS systems, circumventing the aforementioned drawbacks.\n\n## 3 SpeechInstruct Construction\n\nDue to the limitations in publicly available speech data and the lack of variety of speech-text tasks, we construct SpeechInstruct, a speech-text cross-modal instruction-following dataset. This dataset consists of two parts, the first part is called Cross-Modal Instruction, and the second part is called Chain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure 2.\n\n## 3.1 Cross-modal Instruction\n\nData Collection We collect several large-scale English ASR datasets to construct Cross-Modal Instruction, including Gigaspeech (Chen et al., 2021), Common Voice (Ardila et al., 2020), and LibriSpeech (Panayotov et al., 2015). We employ mHuBERT 2 as the speech tokenizer to discretize speech data into discrete units and remove the repetitive units of adjacent frames to get reduced units. Ultimately, we obtain 9 million unit-text data pairs.\n\nTask Description Generation We generate ASR and TTS task descriptions that are compatible with speech-text data pairs. Unlike the Self-Instruct method (Wang et al., 2022), we generate descriptions through a zero-shot approach. Specifically, we directly input the prompts shown in Appendix A into OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each task and some examples are shown in Appendix B.\n\nInstruction Formatting For a discrete unit sequence U and its associated transcription T , we determine whether it will be used for constructing an ASR task or a TTS task based on the probability p . Subsequently, we randomly select a description D from the corresponding task description. This results in a triplet consisting of the task description, discrete unit sequence, and transcription, denoted as ( D,U,T ) . Following this, the triplet is assembled into an instruction using the template: [Human]: { D } . This is input: { U } &lt;eoh&gt;.[SpeechGPT]: { T } &lt;eos&gt;. . To support multi-turn dialogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering to the maximum input length of the model.\n\n## 3.2 Chain-of-Modality Instruction\n\nSpeech Instruction Generation Due to the lack of instruction data with speech input and speech output, we trained a text-to-unit generator to convert text instruction data into speech instruction data. Specifically, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We trained it on LibriSpeech unit-text pairs in Cross-modal Instruction. We select 37,969 samples from the moss-002-sft-data dataset 3 whose response length is shorter than 35 words. And we convert both their instructions and responses into unit sequences through the text-to-unit generator. As a result, we obtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and speech responses, denoted as ( SpeechI, TextI, TextR, SpeechR ) .\n\nInstruction Formatting Using the above quadruplets, we could construct chain-of-thought style instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech Instruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response. Their corresponding templates can be found in Appendix C.\n\n## 4 SpeechGPT\n\n## 4.1 Model Structure\n\nA unified framework is designed to provide architecture compatibility across different modalities. As shown in Figure 2, our model consists of three main components: discrete unit extractor , large language modal and unit vocoder . Under this architecture, LLM can perceive multi-modal inputs and generate multi-modal outputs.\n\nDiscrete Unit Extractor The discrete unit extractor utilizes the Hidden-unit BERT (HuBERT) model (Hsu et al., 2021) to transform continuous speech signals into a sequence of discrete units, .\n\n2 https://dl.fbaipublicfiles.com/hubert/mhubert\\_base\\_vp\\_en\\_es\\_fr\\_it3.pt\n\n3 https://huggingface.co/datasets/fnlp/moss-002-sft-data\n\nHuBERT is a self-supervised model that learns by predicting discrete labels for masked audio segments based on k-means clustering applied to the model's intermediate representations. It features a combination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous intermediate representations, with a k-means model further converting these representations into a sequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a discrete units sequence represented as U = ( u 1 , u 2 , . . . , u T ) , u i ∈ 0 , 1 , . . . , K -1 , ∀ 1 ≤ i ≤ T , with K denoting the total number of clusters.\n\nLarge Language Model We employ the Meta AI LLaMA (Touvron et al., 2023) model as our Large Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an LMhead layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from an extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance compared to the substantially larger 175B GPT-3 across various NLP benchmarks.\n\nUnit Vocoder Due to limition of single speaker unit vocoder in (Polyak et al., 2021), we train a multi-speaker unit HiFi-GAN to decode the speech signal from the discrete representation. The HiFi-GAN architecture consists of a generator G and multiple discriminators D . The generator uses look-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled by a series of blocks composed of transposed convolution and a residual block with dilated layers. The speaker embedding is concatenated to each frame in the up-sampled sequence. The discriminator features a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have the same architecture as (Polyak et al., 2021).\n\n## 4.2 Training\n\nTo incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding embedding matrix first. We divide the training process into three stages. The first stage is ModalityAdaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction Fine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.\n\nExpanding Vocabulary Given original LLM vocabulary V of size | V | , to integrate speech discrete representations into LLM, we expand the vocabulary with an additional set of unit tokens V ′ , of size | V ′ | = K . The expanded vocabulary V ′′ is the union of the original vocabulary V and the new words V ′ :\n\n<!-- formula-not-decoded -->\n\nWe denote the original word embedding matrix as E ∈ R | V |× d , where d is the dimension of word embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized word embedding matrix E ′ ∈ R | V ′′ |× d . We preserve the original word embeddings by copying the values of E to the first | V | rows of E ′ :\n\n<!-- formula-not-decoded -->\n\nFinally, we replace the original vocabulary and word embedding matrix with the new vocabulary V ′′ and the word embedding matrix E ′ .\n\nStage 1: Modality-Adaptation Pre-training To enable LLM to handle discrete units modality, we utilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns with the text pre-training objective of LLM. Given unlabeled speech corpus C consisting of speech U 1 , U 2 , . . . , U m and LLM denoted as L 1 , the negative log-likelihood loss can be formulated as:\n\n<!-- formula-not-decoded -->\n\nwhere m is the number of speech in dataset C , n j is the number of discrete unit token in speech U j , and u i,j represents the i-th unit token in the j-th speech.\n\nStage 2: Cross-modal Instruction Fine-Tuning In this stage, we align speech and text modalities utilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to\n\nInstruction\n\n: Can you transcribe the speech into a written format?\n\nInput\n\n: Speech clip (Transcripts: I'm afraid there are no signs here said he.)\n\nOutput\n\n: Text: I'm afraid there are no signs here said he.\n\nInstruction\n\n: Listen to the speech and write down its content.\n\nInput : Speech clip (Transcripts: Did anyone know that these proofs would be there no one saved the printer.)\n\nOutput\n\n: Text: Did anyone know that these proofs would be there no one saved the printer.\n\nInstruction : Would you mind speaking these words as naturally as possible?\n\nInput\n\n: Text: Today is a sunny day and I'm happy to be here.\n\nOutput\n\n: Speech clip (Transcripts: Today is a sunny day and I'm happy to be here.)\n\nInstruction\n\n: Would you please speed-read the following sentence?\n\nInput : Text: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.\n\nOutput : Speech clip (Transcripts: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.)\n\nTable 1: Cases of cross-modal instruction-following results\n\nderive mix dataset I , which consists of samples T 1 , T 2 , . . . , T x . We fine-tune the model L obtained from the first stage on I .\n\nEach sample T j consisting of t 1 , t 2 , . . . , t n j is formed by concatenating a prefix and a text. The training objective is to minimize the negative log-likelihood and the loss calculation only considers the text part, ignoring the prefix, which can be formated as:\n\n<!-- formula-not-decoded -->\n\nwhere x is the number of samples in corpus I , y j is the total number of tokens in sample T j , p j is the number of tokens in the prefix part of T j , and t i,j represents the i-th word in T j .\n\nStage 3: Chain-of-Modality Instruction Fine-Tuning After obtaining the model in stage 2, we utilizes parameter-efficient Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune it on Chain-ofModality Instruction in SpeechInstruct. We add LoRA weights (adapters) to the attention mechanisms and train the newly added LoRA parameters. We adopt the same loss function as stage 2.\n\n## 5 Experiments\n\n## 5.1 Experimental Setups\n\nDatasets For modality-adaption pre-training, we use LibriLight (Kahn et al., 2020) which contains 60K hours of unlabelled English audiobook speech. For cross-modal instruction fine-tuning stage, we use Gigaspeech (Chen et al., 2021), Common voice (Ardila et al., 2020) and LibriSpeech (Panayotov et al., 2015) dataset and moss-002-sft-data dataset, which is illustrated in detail in 3.1. For chain-ofmodality instruction fine-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail in 3.2.\n\nConfiguration We employ LLaMA-13B (Touvron et al., 2023) as our backbone model. For stage 1, we use 96 A100 gpu and train for 900 steps with batch size 768. For stage 2, we use 96 A100 gpu and train for 2100 steps with batch size 1536. For stage 3, we use 8 A100 gpu and train for 4200 steps with batch size 128. Details about training hyperparameters are shown in Appendix 3. For decoding, we set the maximum sequence length to 2048 and set the temperature to 0.8. We use Topk sampling with k =60. We also use Topp sampling with p=0.8.\n\nTable 2: Cases of spoken dialogue results\n\n<!-- image -->\n\nEvaluation We evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction following ability and spoken dialogue ability. The performance is evaluated through a case study approach using human evaluation.\n\n## 5.2 Main Results\n\nCross-modal Instruction Following As shown in Table 1, when provided with various instructions, the model is capable of performing corresponding tasks and generating accurate outputs in accordance with these inputs.\n\nSpoken Dialogue Table 2 shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows that in interactions with humans, SpeechGPT is capable of comprehending speech instructions and responding accordingly in speech, while adhering to the HHH criteria (Harmless, Helpful, Honest) (Askell et al., 2021).\n\n## 6 Limitation\n\nDespite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue abilities, it still presents certain limitations: 1) It does not consider paralinguistic information in speech, such as the inability to generate responses in different emotional tones, 2) It necessitates the generation of a text-based response prior to the production of a speech-based one, 3) Due to the context length limitation, it is incapable of supporting multi-turn dialogues.\n\n## 7 Conclusion\n\nThis work presents SpeechGPT, an inherent cross-modal multimodal large language model capable of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal instruction-following dataset contains cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a three-stage training paradigm to obtain the final SpeechGPT. Experimental results indicate that SpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate that combining discrete speech tokens into the language model is a promising direction.\n\n## References\n\n- Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common voice: A massively-multilingual speech corpus, 2020.\n- Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. A general language assistant as a laboratory for alignment, 2021.\n- Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems , 33: 12449-12460, 2020.\n- Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio generation, 2022.\n- Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S. X., and Xu, B. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. 2023.\n- Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y., You, Z., and Yan, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio, 2021.\n- Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways, 2022.\n- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n\n- Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378 , 2023.\n- Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing , 29:3451-3460, 2021.\n- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models, 2021.\n- Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y., Zhao, Z., and Watanabe, S. Audiogpt: Understanding and generating speech, music, sound, and talking head, 2023a.\n- Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., and Wei, F. Language is not all you need: Aligning perception with language models, 2023b.\n- Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In ICASSP 2020 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, may 2020. doi: 10.1109/icassp40776.2020.9052942. URL https://doi.org/10.1109% 2Ficassp40776.2020.9052942 .\n- Lakhotia, K., Kharitonov, E., Hsu, W.-N., Adi, Y., Polyak, A., Bolte, B., Nguyen, T.-A., Copet, J., Baevski, A., Mohamed, A., et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics , 9:1336-1354, 2021.\n- Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485 , 2023.\n- Nguyen, T. A., Kharitonov, E., Copet, J., Adi, Y., Hsu, W.-N., Elkahky, A., Tomasello, P., Algayres, R., Sagot, B., Mohamed, A., and Dupoux, E. Generative spoken dialogue language modeling, 2022.\n- OpenAI. Gpt-4 technical report, 2023.\n- Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 5206-5210, 2015. doi: 10.1109/ICASSP.2015.7178964.\n- Polyak, A., Adi, Y., Copet, J., Kharitonov, E., Lakhotia, K., Hsu, W.-N., Mohamed, A., and Dupoux, E. Speech resynthesis from discrete disentangled self-supervised representations, 2021.\n- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\n- Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface, 2023.\n- Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Neural codec language models are zero-shot text to speech synthesizers, 2023.\n- Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions, 2022.\n\n- Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023a.\n- Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling, 2023b.\n\n## A Prompts to Generate Task Description\n\n## ASR :\n\nYou are asked to come up with a set of 100 diverse task instructions about automatic speech recognition, which is about recognizing speech.\n\nHere are the requirements:\n\n1. These instructions should be to instruct someone to recognize the content of the following speech.\n\n2. Try not to repeat the verb for each instruction to maximize diversity.\n\n3. The language used for instruction also should be diverse. For example, you should combine questions with imperative instructions.\n\n4. The type of instructions should be diverse.\n\n5. The instructions should be in English.\n\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n\nList of 100 tasks:\n\n## TTS :\n\nYou are asked to come up with a set of 100 diverse task instructions about text to speech, which is about recognizing speech .\n\nHere are the requirements:\n\n1. These instructions should be to instruct someone to recognize the content of the following speech.\n\n2. Try not to repeat the verb for each instruction to maximize diversity.\n\n3. The language used for instruction also should be diverse. For example, you should combine questions with imperative instructions.\n\n4. The type of instructions should be diverse.\n\n5. The instructions should be in English.\n\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n\nList of 100 tasks:\n\n## B Examples of Task Description\n\n## ASR :\n\nBegin by converting the spoken words into written text. Can you transcribe the speech into a written format? Focus on translating the audible content into text. Transcribe the speech by carefully listening to it. Would you kindly write down the content of the speech? Analyze the speech and create a written transcription. Engage with the speech to produce a text-based version. Can you document the speech in written form? Transform the spoken words into text accurately. How about putting the speech's content into writing?\n\n## TTS :\n\nCan you please read this sentence out loud?\n\nRecite the following words as if you were speaking normally.\n\nProject your voice to clearly articulate this statement.\n\nWould you mind speaking these words as naturally as possible?\n\nWhisper the given sentence softly.\n\nEnunciate each word in this sentence with precision. How would you express this sentence in a conversational tone?\n\nCould you please relay the message below verbally?\n\nEmphasize the key points while reading the sentence.\n\nSing the text provided in a melodic voice.\n\n## C Chain-of-Modality Instructions Templates\n\n## Speech Instruction-Speech Response :\n\n[Human] : This is a speech instruction: {SpeechI}. And your response should be speech. You can do it step by step. You can first transcribe the instruction and get the text Instruction. Then you can think about the instruction and get the text response. Last, you should speak the response aloud &lt;eoh&gt;. [SpeechGPT] : [tq] {TextI}; [ta] {TextR}; [ua] {SpeechR}&lt;eoa&gt;.\n\n## Speech Instruction-Text Response :\n\n[Human] : This is a speech instruction: {SpeechI}. And your response should be text. You can do it step by step. You can first transcribe the instruction and get the text instruction. Then you can think about the instruction and get the text response. &lt;eoh&gt;. [SpeechGPT] : [tq] {TextI}; [ta] {TextR}&lt;eoa&gt;.\n\n## Text Instruction-Speech Response :\n\n[Human] : This is a text instruction: {TextI}. And your response should be speech. You can do it step by step. You can think about the instruction and get the text response. Then you should speak the response aloud &lt;eoh&gt;. [SpeechGPT] : [ta] {TextR}; [ua] {SpeechR}&lt;eoa&gt;.\n\n## Text Instruction-Text Response :\n\n[Human] : This is a text instruction: {TextI}. And your response should be text. You can think about the instruction and get the text response. [SpeechGPT] : [ta] {TextR}&lt;eoa&gt;.\n\n## D Hyperparameters\n\nTable 3: SpeechGPT training hyperparameters.\n\n|                      | Stage 1   | Stage 2   | Stage 3   |\n|----------------------|-----------|-----------|-----------|\n| Batch size           | 768       | 1536      | 128       |\n| Peak learning rate   | 2e-4      | 2e-4      | 2e-4      |\n| Max length           | 1024      | 512       | 1024      |\n| Training steps       | 900       | 4000      | 4200      |\n| LoRA rank            | -         | -         | 8         |\n| LoRA alpha           | -         | -         | 16        |\n| Trainable parameters | 13B       | 13B       | 6M        |\n| Training device      | 96 × A100 | 96 × A100 | 8 × A100  |",
  "tables": [
    {
      "index": 0,
      "markdown": "|                      | Stage 1   | Stage 2   | Stage 3   |\n|----------------------|-----------|-----------|-----------|\n| Batch size           | 768       | 1536      | 128       |\n| Peak learning rate   | 2e-4      | 2e-4      | 2e-4      |\n| Max length           | 1024      | 512       | 1024      |\n| Training steps       | 900       | 4000      | 4200      |\n| LoRA rank            | -         | -         | 8         |\n| LoRA alpha           | -         | -         | 16        |\n| Trainable parameters | 13B       | 13B       | 6M        |\n| Training device      | 96 × A100 | 96 × A100 | 8 × A100  |"
    }
  ],
  "stats": {
    "pages": 13,
    "chunksCreated": 54,
    "totalCharacters": 33897,
    "totalWords": 5179,
    "numTables": 1,
    "processingTimeMs": 13434
  }
}