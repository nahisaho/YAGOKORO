{
  "paper": {
    "id": "2303.17580v4",
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
    "authors": [
      "Yongliang Shen",
      "Kaitao Song",
      "Xu Tan",
      "Dongsheng Li",
      "Weiming Lu",
      "Yueting Zhuang"
    ],
    "published": "2023-03-30T17:48:28.000Z",
    "updated": "2023-12-03T18:17:21.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2303.17580v4",
    "absUrl": "https://arxiv.org/abs/2303.17580v4"
  },
  "chunks": [
    {
      "id": "2303.17580v4-chunk-0",
      "content": "Yongliang Shen 1 , 2 , ∗ , Kaitao Song 2 , ∗ , † , Xu Tan 2 , Dongsheng Li 2 , Weiming Lu 1 , † , Yueting Zhuang 1 , †\n\nZhejiang University 1 , Microsoft Research Asia\n\n2\n\n{syl, luwm, yzhuang}@zju.edu.cn , {kaitaosong, xuta, dongsli}@microsoft.com\n\nhttps://github.com/microsoft/JARVIS",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "chunkIndex": 0,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-1",
      "content": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and su",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-2",
      "content": "hatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-3",
      "content": "Large language models (LLMs) [1, 2, 3, 4, 5, 6], such as ChatGPT, have attracted substantial attention from both academia and industry, due to their remarkable performance on various natural language processing (NLP) tasks. Based on large-scale pre-training on massive text corpora and reinforcement learning from human feedback [2], LLMs can exhibit superior capabilities in language understanding, generation, and reasoning. The powerful capability of LLMs also drives many emergent research topics (e.g., in-context learning [1, 7, 8], instruction learning [9, 10, 11, 12, 13, 14], and chain-ofthought prompting [15, 16, 17, 18]) to further investigate the huge potential of LLMs, and brings unlimited possibilities for us for advancing artificial general intelligence.\n\nDespite these great successes, current LLM technologies are still imperfect and confront some urgent challenges on the way to building an advanced AI system.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-4",
      "content": "dvancing artificial general intelligence.\n\nDespite these great successes, current LLM technologies are still imperfect and confront some urgent challenges on the way to building an advanced AI system. We discuss them from these aspects: 1) Limited to the input and output forms of text generation, current LLMs lack the ability to process complex information such as vision and speech, regardless of their significant achievements in NLP\n\n* The first two authors have equal contributions. This work was done when the first author was an intern at Microsoft Research Asia.\n\n† Corresponding author.\n\nFigure 1: Language serves as an interface for LLMs (e.g., ChatGPT) to connect numerous AI models (e.g., those in Hugging Face) for solving complicated AI tasks. In this concept, an LLM acts as a controller, managing and organizing the cooperation of expert models. The LLM first plans a list of tasks based on the user request and then assigns expert models to each task.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-5",
      "content": "ncept, an LLM acts as a controller, managing and organizing the cooperation of expert models. The LLM first plans a list of tasks based on the user request and then assigns expert models to each task. After the experts execute the tasks, the LLM collects the results and responds to the user.\n\n<!-- image -->\n\ntasks; 2) In real-world scenarios, some complex tasks are usually composed of multiple sub-tasks, and thus require the scheduling and cooperation of multiple models, which are also beyond the capability of language models; 3) For some challenging tasks, LLMs demonstrate excellent results in zero-shot or few-shot settings, but they are still weaker than some experts (e.g., fine-tuned models). How to address these issues could be the critical step for LLMs toward artificial general intelligence.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-6",
      "content": "hot or few-shot settings, but they are still weaker than some experts (e.g., fine-tuned models). How to address these issues could be the critical step for LLMs toward artificial general intelligence.\n\nIn this paper, we point out that in order to handle complicated AI tasks, LLMs should be able to coordinate with external models to harness their powers. Hence, the pivotal question is how to choose suitable middleware to bridge the connections between LLMs and AI models. To tackle this issue, we notice that each AI model can be described in the form of language by summarizing its function. Therefore, we introduce a concept: ' Language as a generic interface for LLMs to collaborate with AI models '. In other words, by incorporating these model descriptions into prompts, LLMs can be considered as the brain to manage AI models such as planning, scheduling, and cooperation. As a result, this strategy empowers LLMs to invoke external models for solving AI tasks.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-7",
      "content": "prompts, LLMs can be considered as the brain to manage AI models such as planning, scheduling, and cooperation. As a result, this strategy empowers LLMs to invoke external models for solving AI tasks. However, when it comes to integrating multiple AI models into LLMs, another challenge emerges: solving numerous AI tasks needs collecting a large number of high-quality model descriptions, which in turn requires heavy prompt engineering. Coincidentally, we notice that some public ML communities usually offer a wide range of applicable models with well-defined model descriptions for solving specific AI tasks such as language, vision, and speech. These observations bring us some inspiration: Can we link LLMs (e.g., ChatGPT) with public ML communities (e.g., GitHub, Hugging Face 1 , etc) for solving complex AI tasks via a language-based interface?",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-8",
      "content": "se observations bring us some inspiration: Can we link LLMs (e.g., ChatGPT) with public ML communities (e.g., GitHub, Hugging Face 1 , etc) for solving complex AI tasks via a language-based interface?\n\nIn this paper, we propose an LLM-powered agent named HuggingGPT to autonomously tackle a wide range of complex AI tasks, which connects LLMs (i.e., ChatGPT) and the ML community (i.e., Hugging Face) and can process inputs from different modalities. More specifically, the LLM acts as a brain: on one hand, it disassembles tasks based on user requests, and on the other hand, assigns suitable models to the tasks according to the model description. By executing models and integrating results in the planned tasks, HuggingGPT can autonomously fulfill complex user requests. The whole process of HuggingGPT, illustrated in Figure 1, can be divided into four stages:",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-9",
      "content": "dels and integrating results in the planned tasks, HuggingGPT can autonomously fulfill complex user requests. The whole process of HuggingGPT, illustrated in Figure 1, can be divided into four stages:\n\n- Task Planning: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n- Model Selection: To solve the planned tasks, ChatGPT selects expert models that are hosted on Hugging Face based on model descriptions.\n- Task Execution: Invoke and execute each selected model, and return the results to ChatGPT.\n\n1 https://huggingface.co/models\n\nFigure 2: Overview of HuggingGPT. With an LLM (e.g., ChatGPT) as the core controller and the expert models as the executors, the workflow of HuggingGPT consists of four stages: 1) Task planning : LLM parses the user request into a task list and determines the execution order and resource dependencies among tasks; 2) Model selection : LLM assigns appropriate models to tasks based on th",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-10",
      "content": "anning : LLM parses the user request into a task list and determines the execution order and resource dependencies among tasks; 2) Model selection : LLM assigns appropriate models to tasks based on the description of expert models on Hugging Face; 3) Task execution : Expert models on hybrid endpoints execute the assigned tasks; 4) Response generation : LLM integrates the inference results of experts and generates a summary of workflow logs to respond to the user.\n\n<!-- image -->\n\n- Response Generation: Finally, ChatGPT is utilized to integrate the predictions from all models and generate responses for users.\n\nBenefiting from such a design, HuggingGPT can automatically generate plans from user requests and use external models, enabling it to integrate multimodal perceptual capabilities and tackle various complex AI tasks.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-11",
      "content": "such a design, HuggingGPT can automatically generate plans from user requests and use external models, enabling it to integrate multimodal perceptual capabilities and tackle various complex AI tasks. More notably, this pipeline allows HuggingGPT to continually absorb the powers from task-specific experts, facilitating the growth and scalability of AI capabilities.\n\nOverall, our contributions can be summarized as follows:\n\n1. To complement the advantages of large language models and expert models, we propose HuggingGPT with an inter-model cooperation protocol. HuggingGPT applies LLMs as the brain for planning and decision, and automatically invokes and executes expert models for each specific task, providing a new way for designing general AI solutions.\n\n2. By integrating the Hugging Face hub with numerous task-specific models around ChatGPT, HuggingGPT is able to tackle generalized AI tasks covering multiple modalities and domains.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 11,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-12",
      "content": "ral AI solutions.\n\n2. By integrating the Hugging Face hub with numerous task-specific models around ChatGPT, HuggingGPT is able to tackle generalized AI tasks covering multiple modalities and domains. Through the open collaboration of models, HuggingGPT can provide users with multimodal and reliable conversation services.\n3. We point out the importance of task planning and model selection in HuggingGPT (and autonomous agents), and formulate some experimental evaluations for measuring the capability of LLMs in planning and model selection.\n4. Extensive experiments on multiple challenging AI tasks across language, vision, speech, and cross-modality demonstrate the capability and huge potential of HuggingGPT in understanding and solving complex tasks from multiple modalities and domains.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 12,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-13",
      "content": "In recent years, the field of natural language processing (NLP) has been revolutionized by the emergence of large language models (LLMs) [1, 2, 3, 4, 5, 19, 6], exemplified by models such as GPT-3 [1], GPT-4 [20], PaLM [3], and LLaMa [6]. LLMs have demonstrated impressive capabilities in zero-shot and few-shot tasks, as well as more complex tasks such as mathematical problems and commonsense reasoning, due to their massive corpus and intensive training computation. To extend the scope of large language models (LLMs) beyond text generation, contemporary research can be divided into two branches: 1) Some works have devised unified multimodal language models for solving various AI tasks [21, 22, 23]. For example, Flamingo [21] combines frozen pre-trained vision and language models for perception and reasoning.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "2 Related Works",
        "chunkIndex": 13,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-14",
      "content": "ve devised unified multimodal language models for solving various AI tasks [21, 22, 23]. For example, Flamingo [21] combines frozen pre-trained vision and language models for perception and reasoning. BLIP-2 [22] utilizes a Q-former to harmonize linguistic and visual semantics, and Kosmos-1 [23] incorporates visual input into text sequences to amalgamate linguistic and visual inputs. 2) Recently, some researchers started to investigate the integration of using tools or models in LLMs [24, 25, 26, 27, 28]. Toolformer [24] is the pioneering work to introduce external API tags within text sequences, facilitating the ability of LLMs to access external tools. Consequently, numerous works have expanded LLMs to encompass the visual modality. Visual ChatGPT [26] fuses visual foundation models, such as BLIP [29] and ControlNet [30], with LLMs.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "2 Related Works",
        "chunkIndex": 14,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-15",
      "content": "external tools. Consequently, numerous works have expanded LLMs to encompass the visual modality. Visual ChatGPT [26] fuses visual foundation models, such as BLIP [29] and ControlNet [30], with LLMs. Visual Programming [31] and ViperGPT [25] apply LLMs to visual objects by employing programming languages, parsing visual queries into interpretable steps expressed as Python code. More discussions about related works are included in Appendix B.\n\nDistinct from these approaches, HuggingGPT advances towards more general AI capabilities in the following aspects: 1) HuggingGPT uses the LLM as the controller to route user requests to expert models, effectively combining the language comprehension capabilities of the LLM with the expertise of other expert models; 2) The mechanism of HuggingGPT allows it to address tasks in any modality or any domain by organizing cooperation among models through the LLM.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "2 Related Works",
        "chunkIndex": 15,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-16",
      "content": "es of the LLM with the expertise of other expert models; 2) The mechanism of HuggingGPT allows it to address tasks in any modality or any domain by organizing cooperation among models through the LLM. Benefiting from the design of task planning in HuggingGPT, our system can automatically and effectively generate task procedures and solve more complex problems; 3) HuggingGPT offers a more flexible approach to model selection, which assigns and orchestrates tasks based on model descriptions. By providing only the model descriptions, HuggingGPT can continuously and conveniently integrate diverse expert models from AI communities, without altering any structure or prompt settings. This open and continuous manner brings us one step closer to realizing artificial general intelligence.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "2 Related Works",
        "chunkIndex": 16,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-17",
      "content": "HuggingGPT is a collaborative system for solving AI tasks, composed of a large language model (LLM) and numerous expert models from ML communities. Its workflow includes four stages: task planning, model selection, task execution, and response generation, as shown in Figure 2. Given a user request, our HuggingGPT, which adopts an LLM as the controller, will automatically deploy the whole workflow, thereby coordinating and executing the expert models to fulfill the target. Table 1 presents the detailed prompt design in our HuggingGPT. In the following subsections, we will introduce the design of each stage.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3 HuggingGPT",
        "chunkIndex": 17,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-18",
      "content": "Generally, in real-world scenarios, user requests usually encompass some intricate intentions and thus need to orchestrate multiple sub-tasks to fulfill the target. Therefore, we formulate task planning\n\n|                     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 18,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-19",
      "content": "| Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 19,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-20",
      "content": "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 20,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-21",
      "content": "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                     | available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 21,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-22",
      "content": "| available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                     | Can you tell",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 22,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-23",
      "content": "|\n|                     | Can you tell me how many objects in e1.jpg?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Demonstrations [{ \"task\" : \"object-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e1.jpg\" }}]",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 23,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-24",
      "content": "| Demonstrations [{ \"task\" : \"object-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e1.jpg\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|                     | In e2.jpg, what's the animal and what's it doing?",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 24,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-25",
      "content": "| [{ \"task\" : \"image-to-text\", \"id\" : 0, \"dep\" :[-1], \"args\" : { \"im age\" : \"e2.jpg\" }}, { \"task\" :\"image-cls\", \"id\" : 1, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" :\"object-detection\", \"id\" : 2, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" : \"vi- sual-quesrion-answering\", \"id\" : 3, \"dep\" :[-1], \"args\" : { \"text\" : \"what's the animal doing?\", \"image\" : \"e2.jpg\" }}]",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 25,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-26",
      "content": "e2.jpg\" }}, { \"task\" : \"vi- sual-quesrion-answering\", \"id\" : 3, \"dep\" :[-1], \"args\" : { \"text\" : \"what's the animal doing?\", \"image\" : \"e2.jpg\" }}]                                                                                                                                                                                                                                                               |\n|                     | First generate a HED image of e3.jpg, then based on the HED image and a text 'a girl reading a book', create a new image as a response.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 26,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-27",
      "content": "| [{ \"task\" : \"pose-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e3.jpg\" }}, { \"task\" : \"pose-text-to-image\", \"id\" : 1, \"dep\" : [0], \"args\" : { \"text\" : \"a girl reading a book\", \"image\" : \"<re- source>-0\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Model Selection     | Prompt",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 27,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-28",
      "content": "|\n| Model Selection     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 28,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-29",
      "content": "pt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Model Selection     | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 29,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-30",
      "content": "call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 30,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-31",
      "content": "he user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            |\n| Model Selection     | Candidate Models",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 31,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-32",
      "content": "|\n| Model Selection     | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metada",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 32,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-33",
      "content": "| {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           |\n| Response Generation | Prompt",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 33,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-34",
      "content": "|\n| Response Generation | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 34,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-35",
      "content": "Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 35,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-36",
      "content": "Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. |\n\nTable 1: The details of the prompt design in HuggingGPT. In the prompts, we set some injectable slots such as {{ Demonstrations }} and {{ Candidate Models }}. These slots are uniformly replaced with the corresponding text before being fed into the LLM.\n\nas the first stage of HuggingGPT, which aims to use LLM to analyze the user request and then decompose it into a collection of structured tasks. Moreover, we require the LLM to determine dependencies and execution orders for these decomposed tasks, to build their connections. To enhance the efficacy of task planning in LLMs, HuggingGPT employs a prompt design, which consists of specification-based instruction and demonstration-based parsing.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 36,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-37",
      "content": "ks, to build their connections. To enhance the efficacy of task planning in LLMs, HuggingGPT employs a prompt design, which consists of specification-based instruction and demonstration-based parsing. We introduce these details in the following paragraphs.\n\nSpecification-based Instruction To better represent the expected tasks of user requests and use them in the subsequent stages, we expect the LLM to parse tasks by adhering to specific specifications (e.g., JSON format ). Therefore, we design a standardized template for tasks and instruct the LLM to\n\nconduct task parsing through slot filing. As shown in Table 1, the task parsing template comprises four slots ( \"task\" , \"id\" , \"dep\" , and \"args\" ) to represent the task name, unique identifier, dependencies and arguments. Additional details for each slot can be found in the template description (see the Appendix A.1.1).",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 37,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-38",
      "content": "d\" , \"dep\" , and \"args\" ) to represent the task name, unique identifier, dependencies and arguments. Additional details for each slot can be found in the template description (see the Appendix A.1.1). By adhering to these task specifications, HuggingGPT can automatically employ the LLM to analyze user requests and parse tasks accordingly.\n\nDemonstration-based Parsing To better understand the intention and criteria for task planning, HuggingGPT incorporates multiple demonstrations in the prompt. Each demonstration consists of a user request and its corresponding output, which represents the expected sequence of parsed tasks. By incorporating dependencies among tasks, these demonstrations aid HuggingGPT in understanding the logical connections between tasks, facilitating accurate determination of execution order and identification of resource dependencies. The details of our demonstrations is presented in Table 1.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 38,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-39",
      "content": "ng the logical connections between tasks, facilitating accurate determination of execution order and identification of resource dependencies. The details of our demonstrations is presented in Table 1.\n\nFurthermore, to support more complex scenarios (e.g., multi-turn dialogues), we include chat logs in the prompt by appending the following instruction: ' To assist with task planning, the chat history is available as {{ Chat Logs }}, where you can trace the user-mentioned resources and incorporate them into the task planning. '. Here {{ Chat Logs }} represents the previous chat logs. This design allows HuggingGPT to better manage context and respond to user requests in multi-turn dialogues.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.1 Task Planning",
        "chunkIndex": 39,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-40",
      "content": "Following task planning, HuggingGPT proceeds to the task of matching tasks with models, i.e., selecting the most appropriate model for each task in the parsed task list. To this end, we use model descriptions as the language interface to connect each model. More specifically, we first gather the descriptions of expert models from the ML community (e.g., Hugging Face) and then employ a dynamic in-context task-model assignment mechanism to choose models for the tasks. This strategy enables incremental model access (simply providing the description of the expert models) and can be more open and flexible to use ML communities. More details are introduced in the next paragraph.\n\nIn-context Task-model Assignment We formulate the task-model assignment as a single-choice problem, where available models are presented as options within a given context.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.2 Model Selection",
        "chunkIndex": 40,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-41",
      "content": "uced in the next paragraph.\n\nIn-context Task-model Assignment We formulate the task-model assignment as a single-choice problem, where available models are presented as options within a given context. Generally, based on the provided user instruction and task information in the prompt, HuggingGPT is able to select the most appropriate model for each parsed task. However, due to the limits of maximum context length, it is not feasible to encompass the information of all relevant models within one prompt. To mitigate this issue, we first filter out models based on their task type to select the ones that match the current task. Among these selected models, we rank them based on the number of downloads 2 on Hugging Face and then select the topK models as the candidates. This strategy can substantially reduce the token usage in the prompt and effectively select the appropriate models for each task.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.2 Model Selection",
        "chunkIndex": 41,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-42",
      "content": "antially reduce the token usage in the prompt and effectively select the appropriate models for each task.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.2 Model Selection",
        "chunkIndex": 42,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-43",
      "content": "Once a specific model is assigned to a parsed task, the next step is to execute the task (i.e., perform model inference). In this stage, HuggingGPT will automatically feed these task arguments into the models, execute these models to obtain the inference results, and then send them back to the LLM. It is necessary to emphasize the issue of resource dependencies at this stage. Since the outputs of the prerequisite tasks are dynamically produced, HuggingGPT also needs to dynamically specify the dependent resources for the task before launching it. Therefore, it is challenging to build the connections between tasks with resource dependencies at this stage.\n\nResource Dependency To address this issue, we use a unique symbol, ' &lt;resource&gt; ', to maintain resource dependencies. Specifically, HuggingGPT identifies the resources generated by the prerequisite task as &lt;resource&gt;-task\\_id , where task\\_id is the id of the prerequisite task.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.3 Task Execution",
        "chunkIndex": 43,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-44",
      "content": "o maintain resource dependencies. Specifically, HuggingGPT identifies the resources generated by the prerequisite task as &lt;resource&gt;-task\\_id , where task\\_id is the id of the prerequisite task. During the task planning stage, if some tasks are dependent on the outputs of previously executed tasks (e.g., task\\_id ), HuggingGPT sets this symbol (i.e., &lt;resource&gt;-task\\_id ) to the corresponding resource subfield in the arguments. Then in the task execution stage, HuggingGPT dynamically replaces this symbol with the resource generated by the prerequisite task. As a result, this strategy empowers HuggingGPT to efficiently handle resource dependencies during task execution.\n\n2 To some extent, we think the downloads can reflect the popularity and quality of the model.\n\nTable 2: Evaluation for task planning in different task types.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.3 Task Execution",
        "chunkIndex": 44,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-45",
      "content": "es during task execution.\n\n2 To some extent, we think the downloads can reflect the popularity and quality of the model.\n\nTable 2: Evaluation for task planning in different task types.\n\n<!-- image -->\n\nFurthermore, for the remaining tasks without any resource dependencies, we will execute these tasks directly in parallel to further improve inference efficiency. This means that multiple tasks can be executed simultaneously if they meet the prerequisite dependencies. Additionally, we offer a hybrid inference endpoint to deploy these models for speedup and computational stability. For more details, please refer to Appendix A.1.3.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.3 Task Execution",
        "chunkIndex": 45,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-46",
      "content": "After all task executions are completed, HuggingGPT needs to generate the final responses. As shown in Table 1, HuggingGPT integrates all the information from the previous three stages (task planning, model selection, and task execution) into a concise summary in this stage, including the list of planned tasks, the selected models for the tasks, and the inference results of the models.\n\nMost important among them are the inference results, which are the key points for HuggingGPT to make the final decisions. These inference results are presented in a structured format, such as bounding boxes with detection probabilities in the object detection model, answer distributions in the question-answering model, etc. HuggingGPT allows LLM to receive these structured inference results as input and generate responses in the form of friendly human language.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.4 Response Generation",
        "chunkIndex": 46,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-47",
      "content": ", answer distributions in the question-answering model, etc. HuggingGPT allows LLM to receive these structured inference results as input and generate responses in the form of friendly human language. Moreover, instead of simply aggregating the results, LLM generates responses that actively respond to user requests, providing a reliable decision with a confidence level.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "3.4 Response Generation",
        "chunkIndex": 47,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-48",
      "content": "In our experiments, we employed the gpt-3.5-turbo , text-davinci-003 and gpt-4 variants of the GPT models as the main LLMs, which are publicly accessible through the OpenAI API 3 . To enable more stable outputs of LLM, we set the decoding temperature to 0. In addition, to regulate the LLM output to satisfy the expected format (e.g., JSON format), we set the logit\\_bias to 0.2 on the format constraints (e.g., ' { ' and ' } '). We provide detailed prompts designed for the task planning, model selection, and response generation stages in Table 1, where {{ variable }} indicates the slot which needs to be populated with the corresponding text before being fed into the LLM.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.1 Settings",
        "chunkIndex": 48,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-49",
      "content": "Figure 1 and Figure 2 have shown two demonstrations of HuggingGPT. In Figure 1, the user request consists of two sub-tasks: describing the image and object counting. In response to the request, HuggingGPT planned three tasks: image classification, image captioning, and object detection, and launched the google/vit [32], nlpconnet/vit-gpt2-image-captioning [33], and facebook/detr-resnet-101 [34] models, respectively. Finally, HuggingGPT integrated the results of the model inference and generated responses (describing the image and providing the count of contained objects) to the user.\n\n3 https://platform.openai.com/\n\nA more detailed example is shown in Figure 2. In this case, the user's request included three tasks: detecting the pose of a person in an example image, generating a new image based on that pose and specified text, and creating a speech describing the image.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.2 Qualitative Results",
        "chunkIndex": 49,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-50",
      "content": ", the user's request included three tasks: detecting the pose of a person in an example image, generating a new image based on that pose and specified text, and creating a speech describing the image. HuggingGPT parsed these into six tasks, including pose detection, text-to-image conditional on pose, object detection, image classification, image captioning, and text-to-speech. We observed that HuggingGPT can correctly orchestrate the execution order and resource dependencies among tasks. For instance, the pose conditional text-to-image task had to follow pose detection and use its output as input. After this, HuggingGPT selected the appropriate model for each task and synthesized the results of the model execution into a final response. For more demonstrations, please refer to the Appendix A.3.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.2 Qualitative Results",
        "chunkIndex": 50,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-51",
      "content": "In HuggingGPT, task planning plays a pivotal role in the whole workflow, since it determines which tasks will be executed in the subsequent pipeline. Therefore, we deem that the quality of task planning can be utilized to measure the capability of LLMs as a controller in HuggingGPT. For this purpose, we conduct quantitative evaluations to measure the capability of LLMs. Here we simplified the evaluation by only considering the task\n\nTable 3: Evaluation for the single task. 'Acc' and 'Pre' represents Accuracy and Precision.\n\n| LLM       |   Acc ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|---------|---------|------------|--------|\n| Alpaca-7b |    6.48 |   35.6  |       6.64 |   4.88 |\n| Vicuna-7b |   23.86 |   45.51 |      26.51 |  29.44 |\n| GPT-3.5   |   52.62 |   62.12 |      52.62 |  54.45 |",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 51,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-52",
      "content": "---------|------------|--------|\n| Alpaca-7b |    6.48 |   35.6  |       6.64 |   4.88 |\n| Vicuna-7b |   23.86 |   45.51 |      26.51 |  29.44 |\n| GPT-3.5   |   52.62 |   62.12 |      52.62 |  54.45 |\n\ntype, without its associated arguments. To better conduct evaluations on task planning, we group tasks into three distinct categories (see Table 2) and formulate different metrics for them:\n\n- Single Task refers to a request that involves only one task. We consider the planning to be correct if and only if the task name (i.e., \"task\" ) and the predicted label are identically equal. In this context, we utilize F1 and accuracy as the evaluation metrics.\n- Sequential Task indicates that the user's request can be decomposed into a sequence of multiple sub-tasks. In this case, we employ F1 and normalized Edit Distance [35] as the evaluation metrics.\n- Graph Task indicates that user requests can be decomposed into directed acyclic graphs.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 52,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-53",
      "content": "of multiple sub-tasks. In this case, we employ F1 and normalized Edit Distance [35] as the evaluation metrics.\n- Graph Task indicates that user requests can be decomposed into directed acyclic graphs. Considering the possibility of multiple planning topologies within graph tasks, relying solely on the F1-score is not enough to reflect the LLM capability in planning. To address this, following Vicuna [36], we employed GPT-4 as a critic to evaluate the correctness of the planning. The accuracy is obtained by evaluating the judgment of GPT-4, referred to as the GPT-4 Score. Detailed information about the GPT-4 Score can be found in Appendix A.1.5.\n\nDataset To conduct our evaluation, we invite some annotators to submit some requests. We collect these data as the evaluation dataset. We use GPT-4 to generate task planning as the pseudo labels, which cover single, sequential, and graph tasks.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 53,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-54",
      "content": "some annotators to submit some requests. We collect these data as the evaluation dataset. We use GPT-4 to generate task planning as the pseudo labels, which cover single, sequential, and graph tasks. Furthermore, we invite some expert annotators to label task planning for some complex requests (46 examples) as a high-quality humanannotated dataset. We also plan to improve the\n\nTable 4: Evaluation for the sequential task. 'ED' means Edit Distance.\n\n| LLM       |   ED ↓ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|--------|---------|------------|--------|\n| Alpaca-7b |   0.83 |   22.27 |      23.35 |  22.8  |\n| Vicuna-7b |   0.8  |   19.15 |      28.45 |  22.89 |\n| GPT-3.5   |   0.54 |   61.09 |      45.15 |  51.92 |\n\nquality and quantity of this dataset to further assist in evaluating the LLM's planning capabilities, which remains a future work. More details about this dataset are in Appendix A.2.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 54,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-55",
      "content": "5.15 |  51.92 |\n\nquality and quantity of this dataset to further assist in evaluating the LLM's planning capabilities, which remains a future work. More details about this dataset are in Appendix A.2. Using this dataset, we conduct experimental evaluations on various LLMs, including Alpaca-7b [37], Vicuna-7b [36], and GPT models, for task planning.\n\nTable 5: Evaluation for the graph task.\n\n| LLM       |   GPT-4 Score ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|-----------------|---------|------------|--------|\n| Alpaca-7b |           13.14 |   16.18 |      28.33 |  20.59 |\n| Vicuna-7b |           19.17 |   13.97 |      28.08 |  18.66 |\n| GPT-3.5   |           50.48 |   54.9  |      49.23 |  51.91 |\n\nPerformance Tables 3, 4 and 5 show the planning capabilities of HuggingGPT on the three categories of GPT-4 annotated datasets, respectively.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 55,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-56",
      "content": "3.5   |           50.48 |   54.9  |      49.23 |  51.91 |\n\nPerformance Tables 3, 4 and 5 show the planning capabilities of HuggingGPT on the three categories of GPT-4 annotated datasets, respectively. We observed that GPT-3.5 exhibits more prominent planning capabilities, outperforming the open-source LLMs Alpaca-7b and Vicuna-7b in terms of all types of\n\nuser requests. Specifically, in more complex tasks (e.g., sequential and graph tasks), GPT-3.5 has shown absolute predominance over other LLMs. These results also demonstrate the evaluation of task planning can reflect the capability of LLMs as a controller. Therefore, we believe that developing technologies to improve the ability of LLMs in task planning is very important, and we leave it as a future research direction.\n\nTable 6: Evaluation on the human-annotated dataset.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 56,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-57",
      "content": "eve that developing technologies to improve the ability of LLMs in task planning is very important, and we leave it as a future research direction.\n\nTable 6: Evaluation on the human-annotated dataset.\n\n| LLM       | Sequential Task   | Sequential Task   | Graph Task   | Graph Task   |\n|-----------|-------------------|-------------------|--------------|--------------|\n|           | Acc ↑             | ED ↓              | Acc ↑        | F1 ↑         |\n| Alpaca-7b | 0                 | 0.96              | 4.17         | 4.17         |\n| Vicuna-7b | 7.45              | 0.89              | 10.12        | 7.84         |\n| GPT-3.5   | 18.18             | 0.76              | 20.83        | 16.45        |\n| GPT-4     | 41.36             | 0.61              | 58.33        | 49.28        |\n\nFurthermore, we conduct experiments on the high-quality human-annotated dataset to obtain a more precise evaluation. Table 6 reports the comparisons on the human-annotated dataset.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 57,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-58",
      "content": "| 49.28        |\n\nFurthermore, we conduct experiments on the high-quality human-annotated dataset to obtain a more precise evaluation. Table 6 reports the comparisons on the human-annotated dataset. These results align with the aforementioned conclusion, highlighting that more powerful LLMs demonstrate better performance in task planning. Moreover, we compare the results between human annotations and GPT-4 annotations. We find that even though GPT-4 outperforms other LLMs, there still remains a substantial gap when compared with human annotations. These observations further underscore the importance of enhancing the planning capabilities of LLMs.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.3 Quantitative Evaluation",
        "chunkIndex": 58,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-59",
      "content": "Table 7: Evaluation of task planning in terms of the variety of demonstrations. We refer to the variety of demonstrations as the number of different task types involved in the demonstrations.\n\n| Demo Variety (# task types)   | LLM     | Single Task   | Single Task   | Sequencial Task   | Sequencial Task   | Graph Task   |\n|-------------------------------|---------|---------------|---------------|-------------------|-------------------|--------------|\n| Demo Variety (# task types)   | LLM     | Acc ↑         | F1 ↑          | ED (%) ↓          | F1 ↑              | F1 ↑         |\n| 2                             | GPT-3.5 | 43.31         | 48.29         | 71.27             | 32.15             | 43.42        |\n| 2                             | GPT-4   | 65.59         | 67.08         | 47.17             | 55.13             | 53.96        |\n| 6                             | GPT-3.5 | 51.31         | 51.81         | 60.81             | 43.19             | 58.51        |\n| 6",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.4 Ablation Study",
        "chunkIndex": 59,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-60",
      "content": "| 55.13             | 53.96        |\n| 6                             | GPT-3.5 | 51.31         | 51.81         | 60.81             | 43.19             | 58.51        |\n| 6                             | GPT-4   | 66.83         | 68.14         | 42.20             | 58.18             | 64.34        |\n| 10                            | GPT-3.5 | 52.83         | 53.70         | 56.52             | 47.03             | 64.24        |\n| 10                            | GPT-4   | 67.52         | 71.05         | 39.32             | 60.80             | 66.90        |\n\nFigure 3: Evaluation of task planning with different numbers of demonstrations.\n\n<!-- image -->\n\nAs previously mentioned in our default setting, we apply few-shot demonstrations to enhance the capability of LLMs in understanding user intent and parsing task sequences.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.4 Ablation Study",
        "chunkIndex": 60,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-61",
      "content": "nstrations.\n\n<!-- image -->\n\nAs previously mentioned in our default setting, we apply few-shot demonstrations to enhance the capability of LLMs in understanding user intent and parsing task sequences. To better investigate the effect of demonstrations on our framework, we conducted a series of ablation studies from two perspectives: the number of demonstrations and the variety of demonstrations. Table 7 reports the planning results under the different variety of demonstrations. We observe that increasing the variety among demonstrations can moderately improve the performance of LLMs in conduct planning. Moreover, Figure 3 illustrates the results of task planning with different number of demonstrations. We can find that adding some demonstrations can slightly improve model performance but this improvement will be limited when the number is over 4 demonstrations. In the future, we will continue to explore more elements that can improve the capability of LLMs at different stages.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.4 Ablation Study",
        "chunkIndex": 61,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-62",
      "content": "nce but this improvement will be limited when the number is over 4 demonstrations. In the future, we will continue to explore more elements that can improve the capability of LLMs at different stages.\n\n| LLM        | Task Planning   | Task Planning   | Model Selection   | Model Selection   | Response       |\n|------------|-----------------|-----------------|-------------------|-------------------|----------------|\n| LLM        | Passing Rate ↑  | Rationality ↑   | Passing Rate ↑    | Rationality ↑     | Success Rate ↑ |\n| Alpaca-13b | 51.04           | 32.17           | -                 | -                 | 6.92           |\n| Vicuna-13b | 79.41           | 58.41           | -                 | -                 | 15.64          |\n| GPT-3.5    | 91.22           | 78.47           | 93.89             | 84.29             | 63.08          |",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.4 Ablation Study",
        "chunkIndex": 62,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-63",
      "content": "79.41           | 58.41           | -                 | -                 | 15.64          |\n| GPT-3.5    | 91.22           | 78.47           | 93.89             | 84.29             | 63.08          |\n\nTable 8: Human Evaluation on different LLMs. We report two metrics, passing rate (%) and rationality (%), in the task planning and model selection stages and report a straightforward success rate (%) to evaluate whether the request raised by the user is finally resolved.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.4 Ablation Study",
        "chunkIndex": 63,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-64",
      "content": "In addition to objective evaluations, we also invite human experts to conduct a subjective evaluation in our experiments. We collected 130 diverse requests to evaluate the performance of HuggingGPT at various stages, including task planning, model selection, and final response generation. We designed three evaluation metrics, namely passing rate, rationality, and success rate. The definitions of each metric can be found in Appendix A.1.6. The results are reported in Table 8. From Table 8, we can observe similar conclusions that GPT-3.5 can significantly outperform open-source LLMs like Alpaca-13b and Vicuna-13b by a large margin across different stages, from task planning to response generation stages. These results indicate that our objective evaluations are aligned with human evaluation and further demonstrate the necessity of a powerful LLM as a controller in the framework of autonomous agents.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "4.5 Human Evaluation",
        "chunkIndex": 64,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-65",
      "content": "HuggingGPT has presented a new paradigm for designing AI solutions, but we want to highlight that there still remain some limitations or improvement spaces: 1) Planning in HuggingGPT heavily relies on the capability of LLM. Consequently, we cannot ensure that the generated plan will always be feasible and optimal. Therefore, it is crucial to explore ways to optimize the LLM in order to enhance its planning abilities; 2) Efficiency poses a common challenge in our framework. To build such a collaborative system (i.e., HuggingGPT) with task automation, it heavily relies on a powerful controller (e.g., ChatGPT). However, HuggingGPT requires multiple interactions with LLMs throughout the whole workflow and thus brings increasing time costs for generating the response; 3) Token Lengths is another common problem when using LLM, since the maximum token length is always limited.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "5 Limitations",
        "chunkIndex": 65,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-66",
      "content": "ghout the whole workflow and thus brings increasing time costs for generating the response; 3) Token Lengths is another common problem when using LLM, since the maximum token length is always limited. Although some works have extended the maximum length to 32K, it is still insatiable for us if we want to connect numerous models. Therefore, how to briefly and effectively summarize model descriptions is also worthy of exploration; 4) Instability is mainly caused because LLMs are usually uncontrollable. Although LLM is skilled in generation, it still possibly fails to conform to instructions or give incorrect answers during the prediction, leading to exceptions in the program workflow. How to reduce these uncertainties during inference should be considered in designing systems.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "5 Limitations",
        "chunkIndex": 66,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-67",
      "content": "In this paper, we propose a system named HuggingGPT to solve AI tasks, with language as the interface to connect LLMs with AI models. The principle of our system is that an LLM can be viewed as a controller to manage AI models, and can utilize models from ML communities like Hugging Face to automatically solve different requests of users. By exploiting the advantages of LLMs in understanding and reasoning, HuggingGPT can dissect the intent of users and decompose it into multiple sub-tasks. And then, based on expert model descriptions, HuggingGPT is able to assign the most suitable models for each task and integrate results from different models to generate the final response. By utilizing the ability of numerous AI models from machine learning communities, HuggingGPT demonstrates immense potential in solving challenging AI tasks, thereby paving a new pathway towards achieving artificial general intelligence.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "6 Conclusion",
        "chunkIndex": 67,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-68",
      "content": "We appreciate the support of the Hugging Face team to help us in improving our GitHub project and web demo. Besides, we also appreciate the contributions of Bei Li , Kai Shen , Meiqi Chen , Qingyao Guo , Yichong Leng , Yuancheng Wang , Dingyao Yu for the data labeling and Wenqi Zhang , Wen Wang , Zeqi Tan for paper revision.\n\nThis work is partly supported by the Fundamental Research Funds for the Central Universities (No. 226-2023-00060), Key Research and Development Program of Zhejiang Province (No. 2023C01152), National Key Research and Development Project of China (No. 2018AAA0101900), and MOE Engineering Research Center of Digital Library.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Acknowledgement",
        "chunkIndex": 68,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-69",
      "content": "- [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In NeurIPS , 2020.\n- [2] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-70",
      "content": "cob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR , abs/2203.02155, 2022.\n- [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, and others. Palm: Scaling language modeling with pathways. ArXiv , abs/2204.02311, 2022.\n- [4] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open Pre-trained Transformer Language Models.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-71",
      "content": "ctoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open Pre-trained Transformer Language Models. ArXiv , abs/2205.01068, 2022.\n- [5] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An Open Bilingual Pre-trained Model. ICLR 2023 poster , 2023.\n- [6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and Efficient Foundation Language Models. ArXiv , abs/2302.13971, 2023.\n- [7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-72",
      "content": "Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and Efficient Foundation Language Models. ArXiv , abs/2302.13971, 2023.\n- [7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An Explanation of In-context Learning as Implicit Bayesian Inference. ICLR 2022 Poster , 2022.\n- [8] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2022.\n- [9] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-73",
      "content": "ational Linguistics, 2022.\n- [9] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations , 2022.\n- [10] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\n\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Virendrabhai Purohit, Ishani Mondal, Jacob William Anderson, Kirby C. Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, rushang karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-74",
      "content": "karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2022.\n\n- [11] S. Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. OptIML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-75",
      "content": "o, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. OptIML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. ArXiv , abs/2212.12017, 2022.\n- [12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022.\n- [13] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-76",
      "content": "Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022.\n- [13] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022.\n- [14] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. CoRR , abs/2301.13688, 2023.\n- [15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Conference on Neural Information Processing Systems (NeurIPS) , 2022.\n- [16] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-77",
      "content": "ce on Neural Information Processing Systems (NeurIPS) , 2022.\n- [16] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. In Conference on Neural Information Processing Systems (NeurIPS) , 2022.\n- [17] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided Language Models. ArXiv , abs/2211.10435, 2022.\n- [18] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023 poster , abs/2203.11171, 2023.\n- [19] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-78",
      "content": "rgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. CoRR , abs/2206.07682, 2022.\n- [20] OpenAI. Gpt-4 technical report, 2023.\n- [21] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n- [22] Junnan Li, Dongxu Li, S. Savarese, and Steven Hoi. Blip-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ArXiv , abs/2301.12597, 2023.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-79",
      "content": "g, 2022.\n- [22] Junnan Li, Dongxu Li, S. Savarese, and Steven Hoi. Blip-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ArXiv , abs/2301.12597, 2023.\n\n- [23] Shaohan Huang, Li Dong, Wenhui Wang, Y. Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, O. Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language Is Not All You Need: Aligning Perception with Language Models. ArXiv , abs/2302.14045, 2023.\n- [24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools. ArXiv , abs/2302.04761, 2023.\n- [25] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning, 2023.\n- [26] Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-80",
      "content": "ac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning, 2023.\n- [26] Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. arXiv , 2023.\n- [27] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis, 2023.\n- [28] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu,",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-81",
      "content": "ing Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.\n- [29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping LanguageImage Pre-training for Unified Vision-Language Understanding and Generation. In International Conference on Machine Learning (ICML) , pages 12888-12900, 2022.\n- [30] Lvmin Zhang and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. ArXiv , abs/2302.05543, 2023.\n- [31] Tanmay Gupta and Aniruddha Kembhavi. Visual Programming: Compositional visual reasoning without training. arXiv , abs/2211.11559, 2022.\n- [32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-82",
      "content": "11559, 2022.\n- [32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n- [33] Ankur Kumar. The illustrated image captioning using transformers. ankur3107.github.io , 2022.\n- [34] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020.\n- [35] A. Marzal and E. Vidal. Computation of normalized edit distance and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence , 15(9):926-932, 1993.\n- [36] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 82,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-83",
      "content": "ce , 15(9):926-932, 1993.\n- [36] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n- [37] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "References",
        "chunkIndex": 83,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-84",
      "content": "In this section, we will present more details about some designs of each stage in HuggingGPT.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1 More details",
        "chunkIndex": 84,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-85",
      "content": "To format the parsed task, we define the template [{ \"task\" : task, \"id\" , task\\_id, \"dep\" : depen-dency\\_task\\_ids, \"args\" : { \"text\" : text, \"image\" : URL, \"audio\" : URL, \"video\" : URL}}] with four slots: \"task\" , \"id\" , \"dep\" , and \"args\" . Table 9 presents the definitions of each slot.\n\nTable 9: Definitions for each slot for parsed tasks in the task planning.\n\n| Name   | Definitions                                                                                                                                                                                                                                                                                                                                                 |\n|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.1 Template for Task Planning",
        "chunkIndex": 85,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-86",
      "content": "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"task\" | It represents the type of the parsed task. It covers different tasks in language, visual, video, audio, etc. The currently supported task list of HuggingGPT is shown in Table 13.                                                                                                                                                                          |\n| \"id\"   | The unique identifier for task planning, which is used for references to dependent tasks and their generated resources.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.1 Template for Task Planning",
        "chunkIndex": 86,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-87",
      "content": "|\n| \"dep\"  | It defines the pre-requisite tasks required for execution. The task will be launched only when all the pre-requisite dependent tasks are finished.                                                                                                                                                                                                          |\n| \"args\" | It contains the list of required arguments for task execution. It contains three subfields populated with text, image, and audio resources according to the task type. They are resolved from either the user's request or the generated resources of the dependent tasks. The corresponding argument types for different task types are shown in Table 13. |",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.1 Template for Task Planning",
        "chunkIndex": 87,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-88",
      "content": "r the generated resources of the dependent tasks. The corresponding argument types for different task types are shown in Table 13. |",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.1 Template for Task Planning",
        "chunkIndex": 88,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-89",
      "content": "In general, the Hugging Face Hub hosts expert models that come with detailed model descriptions, typically provided by the developers. These descriptions encompass various aspects of the model, such as its function, architecture, supported languages and domains, licensing, and other relevant details. These comprehensive model descriptions play a crucial role in aiding the decision of HuggingGPT. By assessing the user's requests and comparing them with the model descriptions, HuggingGPT can effectively determine the most suitable model for the given task.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.2 Model Descriptions",
        "chunkIndex": 89,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-90",
      "content": "An ideal scenario is that we only use inference endpoints on cloud service (e.g., Hugging Face). However, in some cases, we have to deploy local inference endpoints, such as when inference endpoints for certain models do not exist, the inference is time-consuming, or network access is limited. To keep the stability and efficiency of the system, HuggingGPT allows us to pull and run some common or time-consuming models locally. The local inference endpoints are fast but cover fewer models, while the inference endpoints in the cloud service (e.g., Hugging Face) are the opposite. Therefore, local endpoints have higher priority than cloud inference endpoints. Only if the matched model is not deployed locally, HuggingGPT will run the model on the cloud endpoint like Hugging Face. Overall, we think that how to design and deploy systems with better stability for HuggingGPT or other autonomous agents will be very important in the future.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.3 Hybrid Endpoint in System Deployment",
        "chunkIndex": 90,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-91",
      "content": "Up to now, HuggingGPT has supported 24 AI tasks, which cover language, vision, speech and etc. Table 13 presents the detailed information of the supported task list in HuggingGPT.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.4 Task List",
        "chunkIndex": 91,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-92",
      "content": "Following the evaluation method used by Vicuna [36], we employed GPT-4 as an evaluator to assess the planning capabilities of LLMs. In more detail, we include the user request and the task list planned by LLM in the prompt, and then let GPT-4 judge whether the list of tasks is accurate and\n\nalso provide a rationale. To guide GPT-4 to make the correct judgments, we designed some task guidelines: 1) the tasks are in the supported task list (see Table 13); 2) the planned task list can reach the solution to the user request; 3) the logical relationship and order among the tasks are reasonable. In the prompt, we also supplement several positive and negative demonstrations of task planning to provide reference for GPT-4. The prompt for GPT-4 score is shown in Table 10. We further want to emphasize that GPT-4 score is not always correct although it has shown a high correlation. Therefore, we also expect to explore more confident metrics to evaluate the ability of LLMs in planning.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.5 GPT-4 Score",
        "chunkIndex": 92,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-93",
      "content": "t to emphasize that GPT-4 score is not always correct although it has shown a high correlation. Therefore, we also expect to explore more confident metrics to evaluate the ability of LLMs in planning.\n\nAs a critic, your task is to assess whether the AI assistant has properly planned the task based on the user's request. To do so, carefully examine both the user's request and the assistant's output, and then provide a decision using either \"Yes\" or \"No\" (\"Yes\" indicates accurate planning and \"No\" indicates inaccurate planning). Additionally, provide a rationale for your choice using the following structure: { \"choice\" : \"yes\"/\"no\", \"reason\" : \"Your reason for your choice\"}. Please adhere to the following guidelines: 1. The task must be selected from the following options: {{ Available Task List }}. 2. Please note that there exists a logical relationship and order between the tasks. 3. Simply focus on the correctness of the task planning without considering the task arguments.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.5 GPT-4 Score",
        "chunkIndex": 93,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-94",
      "content": "lable Task List }}. 2. Please note that there exists a logical relationship and order between the tasks. 3. Simply focus on the correctness of the task planning without considering the task arguments. Positive examples: {{ Positive Demos }} Negative examples: {{ Negative Demos }} Current user request: {{ Input }} AI assistant's output: {{ Output }} Your judgement:\n\nTable 10: The prompt design for GPT-4 Score.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.5 GPT-4 Score",
        "chunkIndex": 94,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-95",
      "content": "To better align human preferences, we invited three human experts to evaluate the different stages of HuggingGPT. First, we selected 3-5 tasks from the task list of Hugging Face and then manually created user requests based on the selected tasks. We will discard samples that cannot generate new requests from the selected tasks. Totally, we conduct random sampling by using different seeds, resulting in a collection of 130 diverse user requests. Based on the produced samples, we evaluate the performance of LLMs at different stages (e.g., task planning, model selection, and response generation). Here, we designed three evaluation metrics:\n\n- Passing Rate : to determine whether the planned task graph or selected model can be successfully executed;\n- Rationality : to assess whether the generated task sequence or selected tools align with user requests in a rational manner;\n- Success Rate : to verify if the final results satisfy the user's request.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.6 Human Evaluation",
        "chunkIndex": 95,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-96",
      "content": "Rationality : to assess whether the generated task sequence or selected tools align with user requests in a rational manner;\n- Success Rate : to verify if the final results satisfy the user's request.\n\nThree human experts were asked to annotate the provided data according to our well-designed metrics and then calculated the average values to obtain the final scores.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.1.6 Human Evaluation",
        "chunkIndex": 96,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-97",
      "content": "As aforementioned, we create two datasets for evaluating task planning. Here we provide more details about these datasets. In total, we gathered a diverse set of 3,497 user requests. Since labeling this dataset to obtain the task planning for each request is heavy, we employed the capabilities of GPT-4 to annotate them. Finally, these auto-labeled requests can be categorized into three types: single task (1,450 requests), sequence task (1,917 requests), and graph task (130 requests). For a more reliable evaluation, we also construct a human-annotated dataset. We invite some expert annotators to label some complex requests, which include 46 examples. Currently, the human-annotated dataset includes 24 sequential tasks and 22 graph tasks. Detailed statistics about the GPT-4-annotated and human-annotated datasets are shown in Table 11.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.2 Datasets for Task Planning Evaluation",
        "chunkIndex": 97,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-98",
      "content": "Through task planning and model selection, HuggingGPT, a multi-model collaborative system, empowers LLMs with an extended range of capabilities. Here, we extensively evaluate HuggingGPT across diverse multimodal tasks, and some selected cases are shown in Figures 4 and 5. With the cooperation of a powerful LLM and numerous expert models, HuggingGPT effectively tackles\n\nTable 11: Statistics on datasets for task planning evaluation.\n\n| Datasets        | Number of Requests by Type   | Number of Requests by Type   | Number of Requests by Type   | Request Length   | Request Length   | Number of Tasks   | Number of Tasks   |\n|-----------------|------------------------------|------------------------------|------------------------------|------------------|------------------|-------------------|-------------------|\n|                 | Single                       | Sequential                   | Graph                        | Max              | Average          | Max               | Average",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.1 Case Study on Various Tasks",
        "chunkIndex": 98,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-99",
      "content": "----------------|\n|                 | Single                       | Sequential                   | Graph                        | Max              | Average          | Max               | Average           |\n| GPT-4-annotated | 1,450                        | 1,917                        | 130                          | 52               | 13.26            | 13                | 1.82              |\n| Human-annotated | -                            | 24                           | 22                           | 95               | 10.20            | 12                | 2.00              |\n\ntasks spanning various modalities, including language, image, audio, and video. Its proficiency encompasses diverse task forms, such as detection, generation, classification, and question answering.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.1 Case Study on Various Tasks",
        "chunkIndex": 99,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-100",
      "content": "Sometimes, user requests may contain multiple implicit tasks or require multi-faceted information, in which case we cannot rely on a single expert model to solve them. To overcome this challenge, HuggingGPT organizes the collaboration of multiple models through task planning. As shown in Figures 6, 7 and 8, we conducted experiments to evaluate the effectiveness of HuggingGPT in the case of complex tasks:\n\n- Figure 6 demonstrates the ability of HuggingGPT to cope with complex tasks in a multi-round conversation scenario. The user splits a complex request into several steps and reaches the final goal through multiple rounds of interaction. We find that HuggingGPT can track the contextual state of user requests through the dialogue context management in the task planning stage. Moreover, HuggingGPT demonstrates the ability to access user-referenced resources and proficiently resolve dependencies between tasks in the dialogue scenario.\n- Figure 7 shows that for a simple request like \"descr",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.2 Case Study on Complex Tasks",
        "chunkIndex": 100,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-101",
      "content": "ingGPT demonstrates the ability to access user-referenced resources and proficiently resolve dependencies between tasks in the dialogue scenario.\n- Figure 7 shows that for a simple request like \"describe the image in as much detail as possible\" , HuggingGPT can decompose it into five related tasks, namely image captioning, image classification, object detection, segmentation, and visual question answering tasks. HuggingGPT assigns expert models to handle each task to gather information about the image from various perspectives. Finally, the LLM integrates this diverse information to deliver a comprehensive and detailed description to the user.\n- Figure 8 shows two cases where a user request can contain several tasks. In these cases, HuggingGPT first performs all the tasks requested by the user by orchestrating the work of multiple expert models, and then let the LLM aggregate the model inference results to respond to the user.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.2 Case Study on Complex Tasks",
        "chunkIndex": 101,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-102",
      "content": "s, HuggingGPT first performs all the tasks requested by the user by orchestrating the work of multiple expert models, and then let the LLM aggregate the model inference results to respond to the user.\n\nIn summary, HuggingGPT establishes the collaboration of LLM with external expert models and shows promising performance on various forms of complex tasks.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.2 Case Study on Complex Tasks",
        "chunkIndex": 102,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-103",
      "content": "We show more cases here to illustrate HuggingGPT's ability to handle realistic scenarios with task resource dependencies, multimodality, multiple resources, etc. To make clear the workflow of HuggingGPT, we also provide the results of the task planning and task execution stages.\n\n- Figure 9 illustrates the operational process of HuggingGPT in the presence of resource dependencies among tasks. In this case, HuggingGPT can parse out concrete tasks based on abstract requests from the user, including pose detection, image captioning, and pose conditional image generation tasks. Furthermore, HuggingGPT effectively recognizes the dependencies between task #3 and tasks #1, #2, and injected the inferred results of tasks #1 and #2 into the input arguments of task #3 after the dependency tasks were completed.\n- Figure 10 demonstrates the conversational ability of HuggingGPT on audio and video modalities.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.3 Case Study on More Scenarios",
        "chunkIndex": 103,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-104",
      "content": "sults of tasks #1 and #2 into the input arguments of task #3 after the dependency tasks were completed.\n- Figure 10 demonstrates the conversational ability of HuggingGPT on audio and video modalities. In the two cases, it shows HuggingGPT completes the user-requested text-to-audio and text-to-video tasks via the expert models, respectively. In the top one, the two models are executed in parallel (generating audio and generating video concurrently), and in the bottom one, the two models are executed serially (generating text from the image first, and then generating audio based on the text). This further validates that HuggingGPT can organize the cooperation between models and the resource dependencies between tasks.\n\n- Figure 11 shows HuggingGPT integrating multiple user-input resources to perform simple reasoning.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.3 Case Study on More Scenarios",
        "chunkIndex": 104,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-105",
      "content": "HuggingGPT can organize the cooperation between models and the resource dependencies between tasks.\n\n- Figure 11 shows HuggingGPT integrating multiple user-input resources to perform simple reasoning. We can find that HuggingGPT can break up the main task into multiple basic tasks even with multiple resources, and finally integrate the results of multiple inferences from multiple models to get the correct answer.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "A.3.3 Case Study on More Scenarios",
        "chunkIndex": 105,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-106",
      "content": "The emergence of ChatGPT and its subsequent variant GPT-4, has created a revolutionary technology wave in LLM and AI area. Especially in the past several weeks, we also have witnessed some experimental but also very interesting LLM applications, such as AutoGPT 4 , AgentGPT 5 , BabyAGI 6 , and etc. Therefore, we also give some discussions about these works and provide some comparisons from multiple dimensions, including scenarios, planning, tools, as shown in Table 12.\n\nScenarios Currently, these experimental agents (e.g., AutoGPT, AgentGPT and BabyAGI) are mainly used to solve daily requests. While for HuggingGPT, it focuses on solving tasks in the AI area (e.g., vision, language, speech, etc), by utilizing the powers of Hugging Face. Therefore, HuggingGPT can be considered as a more professional agent.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 106,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-107",
      "content": "ingGPT, it focuses on solving tasks in the AI area (e.g., vision, language, speech, etc), by utilizing the powers of Hugging Face. Therefore, HuggingGPT can be considered as a more professional agent. Generally speaking, users can choose the most suitable agent based on their requirements (e.g., daily requests or professional areas) or customize their own agent by defining knowledge, planning strategy and toolkits.\n\nTable 12: Comparision between HuggingGPT and other autonomous agents.\n\n| Name                     | Scenarios   | Planning           | Tools                              |\n|--------------------------|-------------|--------------------|------------------------------------|\n| BabyAGI AgentGPT AutoGPT | Daily       | Iterative Planning | - - Web Search, Code Executor, ... |\n| HuggingGPT               | AI area     | Global Planning    | Models in Hugging Face             |",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 107,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-108",
      "content": "BabyAGI AgentGPT AutoGPT | Daily       | Iterative Planning | - - Web Search, Code Executor, ... |\n| HuggingGPT               | AI area     | Global Planning    | Models in Hugging Face             |\n\nPlanning BabyAGI, AgentGPT and AutoGPT can all be considered as autonomous agents, which provide some solutions for task automation. For these agents, all of them adopt step-by-step thinking, which iteratively generates the next task by using LLMs. Besides, AutoGPT employs an addition reflexion module for each task generation, which is used to check whether the current predicted task is appropriate or not. Compared with these applications, HuggingGPT adopts a global planning strategy to obtain the entire task queue within one query. It is difficult to judge which one is better, since each one has its deficiencies and both of them heavily rely on the ability of LLMs, even though existing LLMs are not specifically designed for task planning.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 108,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-109",
      "content": "icult to judge which one is better, since each one has its deficiencies and both of them heavily rely on the ability of LLMs, even though existing LLMs are not specifically designed for task planning. For example, iterative planning combined with reflexion requires a huge amount of LLM queries, and if one step generates an error prediction, the entire workflow would possibly enter an endless loop. While for global planning, although it can always produce a solution for each user request within one query, it still cannot guarantee the correctness of each step or the optimality of the entire plan. Therefore, both iterative and global planning have their own merits and can borrow from each other to alleviate their shortcoming. Additionally, one notable point is that the difficulty of task planning is also linearly correlated to the task range. As the scope of tasks increases, it becomes more challenging for the controller to predict precise plans.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 109,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-110",
      "content": "e point is that the difficulty of task planning is also linearly correlated to the task range. As the scope of tasks increases, it becomes more challenging for the controller to predict precise plans. Consequently, optimizing the controller (i.e., LLM) for task planning will be crucial in building autonomous agents.\n\nTools Among these agents, AutoGPT is the main one to involve other tools for usage. More specifically, AutoGPT primarily uses some common tools (e.g., web search, code executor), while HuggingGPT utilizes the expert models of ML communities (e.g., Hugging Face). Therefore, AutoGPT has a broader task range but is not suitable for more professional problems, whereas HuggingGPT is more specialized and focuses on solving more complex AI tasks. Therefore, the\n\n4 https://github.com/Significant-Gravitas/Auto-GPT\n\n5 https://github.com/reworkd/AgentGPT\n\n6 https://github.com/yoheinakajima/babyagi",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 110,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-111",
      "content": "ized and focuses on solving more complex AI tasks. Therefore, the\n\n4 https://github.com/Significant-Gravitas/Auto-GPT\n\n5 https://github.com/reworkd/AgentGPT\n\n6 https://github.com/yoheinakajima/babyagi\n\nrange of tools used in LLMs will be a trade-off between task depth and task range. In addition, we also note some industry products for LLM applications (e.g., ChatGPT plugins 7 ) and developer tools (e.g., LangChain 8 , HuggingFace Transformer Agent 9 , Semantic Kernels 10 ) for LLM applications. We believe these rapid developments will also facilitate the community to explore how to better integrate LLMs with external tools.\n\nOverall, HuggingGPT can also be considered as an autonomous agent. Compared with these experimental agents, which mainly use GPT models to generate solutions for user requests, HuggingGPT systematically presents a clear pipeline with four stages: task planning, model selection, task execution and response generation.",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 111,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-112",
      "content": "ly use GPT models to generate solutions for user requests, HuggingGPT systematically presents a clear pipeline with four stages: task planning, model selection, task execution and response generation. Such a pipeline can effectively improve the success rate of solving user requests. Besides, HuggingGPT also introduces a global planning strategy to decompose user requests and thus accomplish task automation. Furthermore, HuggingGPT is a collaborative system, which fully utilizes the power of expert models from ML communities to solve AI tasks and present the huge potential of using external tools. Compared with these agents, HuggingGPT allows us to better solve tasks more effectively in professional areas and can be easily extended to any vertical domain. In the future, we will continue to enhance HuggingGPT with even more powerful abilities to develop a versatile autonomous agent with unlimited possibilities.\n\n7 https://openai.com/blog/chatgpt-plugins\n\n8 https://python.langchain.com/",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 112,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-113",
      "content": "o enhance HuggingGPT with even more powerful abilities to develop a versatile autonomous agent with unlimited possibilities.\n\n7 https://openai.com/blog/chatgpt-plugins\n\n8 https://python.langchain.com/\n\n9 https://huggingface.co/docs/transformers/transformers\\_agents\n\n10 https://github.com/microsoft/semantic-kernel\n\nFigure 4: Case study on various tasks (a).\n\n<!-- image -->\n\nFigure 5: Case study on various tasks (b).\n\n<!-- image -->\n\nTable 13: The task list used in HuggingGPT. The first and second columns are the name and arguments of the corresponding task. The third and fourth columns provide some examples of the candidate models and their model descriptions.\n\n| Task                 | Args         | Candidate Models                                        | Descriptions                                                                     |\n|----------------------|--------------|---------------------------------------------------------|-----------------------------------------------------",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 113,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-114",
      "content": "|\n|----------------------|--------------|---------------------------------------------------------|----------------------------------------------------------------------------------|\n| NLP Tasks            | NLP Tasks    | NLP Tasks                                               | NLP Tasks                                                                        |\n| Text-CLS             | text         | [ cardiffnlp/twitter-roberta- base-sentiment , ...]     | [' This is a RoBERTa-base model trained on 58M tweets ...', ...]                 |\n| Token-CLS            | text         | [ dslim/bert-base-NER , ...]                            | [' bert-base-NER is a fine-tuned BERT model that is ready to ...', ...]          |\n| Text2text-Generation | text         | [ google/flan-t5-xl , ...]                              | [' If you already know T5, FLAN-T5 is just better at everything ...', ...]       |\n| Summarization        | text         | [ bar",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 114,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-115",
      "content": "ext         | [ google/flan-t5-xl , ...]                              | [' If you already know T5, FLAN-T5 is just better at everything ...', ...]       |\n| Summarization        | text         | [ bart-large-cnn , ...]                                 | [ ' BART model pre-trained on English language, and fine-tuned ...', ...]        |\n| Translation          | text         | [ t5-base , ...]                                        | [' With T5, we propose reframing all NLP tasks into a unified ...', ...]         |\n| Question-Answering   | text         | [ deepset/roberta-base- squad2 , ...]                   | [' This is the roberta-base model, fine-tuned using the SQuAD2.0 ...', ...]      |\n| Conversation         | text         | [ PygmalionAI/pygmalion- 6b , ...]                      | [' Pymalion 6B is a proof-of-concept dialogue model based on ...', ...]          |\n| Text-Generation      | text         | [ gpt2 , ...]                                           | [' Pretrained model on",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 115,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-116",
      "content": "ymalion 6B is a proof-of-concept dialogue model based on ...', ...]          |\n| Text-Generation      | text         | [ gpt2 , ...]                                           | [' Pretrained model on English ...', ...]                                        |\n| Tabular-CLS          | text         | [ matth/flowformer , ...]                               | [' Automatic detection of blast cells in ALL data using transformers. ...', ...] |\n| CV Tasks             | CV Tasks     | CV Tasks                                                | CV Tasks                                                                         |\n| Image-to-Text        | image        | [ nlpconnect/vit-gpt2-image- captioning , ...]          | [' This is an image captioning model trained by @ydshieh in flax ...', ...]      |\n| Text-to-Image        | image        | [ runwayml/stable-diffusion- v1-5 , ...]                | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| VQA",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 116,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-117",
      "content": "|\n| Text-to-Image        | image        | [ runwayml/stable-diffusion- v1-5 , ...]                | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| VQA                  | text + image | [ dandelin/vilt-b32- finetuned-vqa , ...]               | [' Vision-and-Language Transformer (ViLT) model fine-tuned on ...', ...]         |\n| Segmentation         | image        | [ facebook/detr-resnet-50- panoptic , ...]              | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| DQA                  | text + image | [ impira/layoutlm- document-qa , ...]                   | [' This is a fine-tuned version of the multi-modal LayoutLM model ...', ...]     |\n| Image-CLS            | image        | [ microsoft/resnet-50 , ...]                            | [' ResNet model pre-trained on ...', ...]                                        |\n| Image-to-image       | image        | [ radames/stable-diffusion- v1-5-img2img , ...]",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 117,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-118",
      "content": "| [' ResNet model pre-trained on ...', ...]                                        |\n| Image-to-image       | image        | [ radames/stable-diffusion- v1-5-img2img , ...]         | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| Object-Detection     | image        | [ facebook/detr-resnet-50 , ...]                        | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| ControlNet-SD        | image        | [ lllyasviel/sd-controlnet- canny , ...]                | [' ControlNet is a neural network structure to control diffusion ...', ...]      |\n| Audio Tasks          | Audio Tasks  | Audio Tasks                                             | Audio Tasks                                                                      |\n| Text-to-Speech       | text         | [ espnet/kan- bayashi_ljspeech_vits , ...]              | [' his model was trained by kan-bayashi using ljspeech/tts1 recipe in",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 118,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-119",
      "content": "|\n| Text-to-Speech       | text         | [ espnet/kan- bayashi_ljspeech_vits , ...]              | [' his model was trained by kan-bayashi using ljspeech/tts1 recipe in ...', ...] |\n| Audio-CLS            | audio        | [ TalTechNLP/voxlingua107- epaca-tdnn , ...]            | [' This is a spoken language recognition model trained on the ...', ...]         |\n| ASR                  | audio        | [ jonatasgrosman/wav2vec2- large-xlsr-53-english , ...] | [' Fine-tuned XLSR-53 large model for speech recognition in English ...', ...]   |\n| Audio-to-Audio       | audio        | [ speechbrain/metricgan- plus-voicebank , ...]          | [' MetricGAN-trained model for Enhancement ...', ...]                            |\n| Video Tasks          | Video Tasks  | Video Tasks                                             | Video Tasks                                                                      |\n| Text-to-Video        | text         | [ damo-vilab/text-to-vi",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 119,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-120",
      "content": "o Tasks                                             | Video Tasks                                                                      |\n| Text-to-Video        | text         | [ damo-vilab/text-to-video- ms-1.7b , ...]              | [' his model is based on a multi-stage text-to-video generation ...', ...]       |\n| Video-CLS            | video        | [ MCG-NJU/videomae- base , ...]                         | [' VideoMAE model pre-trained on Kinetics-400 for 1600 epochs ...', ...]         |\n\nFigure 6: Case study on complex tasks (a).\n\n<!-- image -->\n\nFigure 7: Case study on complex tasks (b).\n\n<!-- image -->\n\nQuery: Can you tell me how many objects in the picture example1.jpg?\n\nResponse: There are 8 objects in the picture. I used the model facebook/detr-resnet-101 for this task which is trained on a popular object detection dataset called COCO, and it has the highest number of downloads. It generated an image with predicted boxes that is available under the path: /images/79f2.jpg .",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 120,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-121",
      "content": "s trained on a popular object detection dataset called COCO, and it has the highest number of downloads. It generated an image with predicted boxes that is available under the path: /images/79f2.jpg . As for image captioning, I selected the model nlpconnect/vit-gpt2-image-captioning which is more frequently used and works better in this case. The generated text from the image is ' a large jetliner sitting on top of an airport tarmac. '",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "B More Discussion about Related Works",
        "chunkIndex": 121,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-122",
      "content": "1: {\"task\": \"object-detection\", \"id\": 0, dep: [-1], \"args\": {\"image\":\n\n\"example1.jpg\"}},\n\n2: {\"task\": \"image-to-text\", \"id\": 1, dep: [-1], \"args\": {\"image\":\n\n\"example1.jpg\"}}",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Task Planning:",
        "chunkIndex": 122,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-123",
      "content": "1. [{\"generated image with predicted box\": \"/images/79f2.jpg\", \"predicted\":  [{\"box\": {\"xmax\": 463, \"xmin\": 373, \"ymax\": 267, \"ymin\": 199}, \"label\": \"bus\", \"score\": 0.9981155395507812}, {\"box\": {\"xmax\": 292, \"xmin\": 124, \"ymax\": 224, \"ymin\": 175}, \"label\": \"airplane\", \"score\": 0.9983609318733215}, {\"box\": {\"xmax\": 554, \"xmin\": 487, \"ymax\": 277, \"ymin\": 226}, \"label\": \"person\", \"score\": 0.9910836219787598}, {\"box\": {\"xmax\": 230, \"xmin\": 219, \"ymax\": 264, \"ymin\": 227}, \"label\": \"person\", \"score\": 0.998579740524292}, {\"box\": {\"xmax\": 361, \"xmin\": 330, \"ymax\": 231, \"ymin\": 220}, \"label\": \"truck\", \"score\": 0.9066138863563538}, {\"box\": {\"xmax\": 639, \"xmin\": 460, \"ymax\": 420, \"ymin\": 142}, \"label\": \"bus\", \"score\": 0.9987366795539856}, {\"box\": {\"xmax\": 363, \"xmin\": 279, \"ymax\": 222, \"ymin\": 192}, \"label\": \"airplane\", \"score\": 0.9976897239685059}, {\"box\": {\"xmax\": 317, \"xmin\": 287, \"ymax\": 230, \"ymin\": 219}, \"label\": \"truck\", \"score\": 0.9339178800582886}]},",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Execution Results:",
        "chunkIndex": 123,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-124",
      "content": "min\": 279, \"ymax\": 222, \"ymin\": 192}, \"label\": \"airplane\", \"score\": 0.9976897239685059}, {\"box\": {\"xmax\": 317, \"xmin\": 287, \"ymax\": 230, \"ymin\": 219}, \"label\": \"truck\", \"score\": 0.9339178800582886}]},\n\n2. {\"generated\\_text\": \"a large jetliner sitting on top of an airport tarmac \"}\n\nQuery: In example2.jpg, what's the animal and what's it doing?\n\nResponse: Sure, the animal in the image is a tiger and it's laying down. I performed 3 tasks including image-to-text, image-classification, and visual-question-answering on the image you provided. To accomplish this, I selected models nlpconnect/vit-gpt2-image-captioning , google/ vit-base-patch16-224 and dandelin/vilt-b32-finetuned-vqa because they are the perfect models for this use case respectively. According to the results, I have 88% confidence that the animal is a tiger and also have a 77.8% confidence that it's laying down .The generated image with predicted box is stored at the path: /images/d59a.jpg .",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Execution Results:",
        "chunkIndex": 124,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-125",
      "content": "e that the animal is a tiger and also have a 77.8% confidence that it's laying down .The generated image with predicted box is stored at the path: /images/d59a.jpg .",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Execution Results:",
        "chunkIndex": 125,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-126",
      "content": "1: {\"task\": \"image-classification\", \"id\": 0, dep: [-1], \"args\": {\"image\": \"example2.jpg\" }},\n\n2: {\"task\": \"visual-question-answering\", \"id\": 1, dep: [-1], \"args\": {\"text\": \"What is the animal doing?\",\n\n\"image\": \"example2.jpg\" }}\n\n3: {\"task\":\"image-to-text\", \"id\": 2, dep: [-1], \"args\": {\"image\": \"example2.jpg\" }}",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Task Planning:",
        "chunkIndex": 126,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-127",
      "content": "1. [{'score': 0.8858247399330139, 'label': 'tiger, Panthera tigris'}, {'score': 0.10940514504909515, 'label': 'tiger cat'}, {'score': 0.0006216467591002584, 'label': 'jaguar, panther, Panthera onca, Felis onca'}, {'score': 0.0004262701258994639, 'label': 'dhole, Cuon alpinus'}, {'score': 0.00030842673731967807, 'label': 'lion, king of beasts, Panthera leo'}]\n\n2. [{'answer': 'sitting', 'score': 0.8294351696968079}, {'answer': 'laying down', 'score': 0.7387925386428833}, {'answer': 'resting', 'score': 0.625247061252594}, {'answer': 'laying', 'score': 0.6114301681518555}, {'answer':\n\n'relaxing', 'score': 0.44740357995033264}]\n\n3. {'generated\\_text': 'a brown and white striped zebra laying on a rock'}\n\nFigure 8: Case study on complex tasks (c).\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 9: Qualitative analysis of multi-model cooperation with resource dependency.\n\n<!-- image -->\n\nFigure 10: Qualitative analysis of multi-model cooperation on video and audio modalities.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Execution Results:",
        "chunkIndex": 127,
        "totalChunks": 129
      }
    },
    {
      "id": "2303.17580v4-chunk-128",
      "content": "9: Qualitative analysis of multi-model cooperation with resource dependency.\n\n<!-- image -->\n\nFigure 10: Qualitative analysis of multi-model cooperation on video and audio modalities.\n\n<!-- image -->\n\nFigure 11: Qualitative analysis of multi-model cooperation with multiple sources.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2303.17580v4",
        "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
        "authors": [
          "Yongliang Shen",
          "Kaitao Song",
          "Xu Tan",
          "Dongsheng Li",
          "Weiming Lu",
          "Yueting Zhuang"
        ],
        "section": "Execution Results:",
        "chunkIndex": 128,
        "totalChunks": 129
      }
    }
  ],
  "fullText": "## HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face\n\nYongliang Shen 1 , 2 , ∗ , Kaitao Song 2 , ∗ , † , Xu Tan 2 , Dongsheng Li 2 , Weiming Lu 1 , † , Yueting Zhuang 1 , †\n\nZhejiang University 1 , Microsoft Research Asia\n\n2\n\n{syl, luwm, yzhuang}@zju.edu.cn , {kaitaosong, xuta, dongsli}@microsoft.com\n\nhttps://github.com/microsoft/JARVIS\n\n## Abstract\n\nSolving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.\n\n## 1 Introduction\n\nLarge language models (LLMs) [1, 2, 3, 4, 5, 6], such as ChatGPT, have attracted substantial attention from both academia and industry, due to their remarkable performance on various natural language processing (NLP) tasks. Based on large-scale pre-training on massive text corpora and reinforcement learning from human feedback [2], LLMs can exhibit superior capabilities in language understanding, generation, and reasoning. The powerful capability of LLMs also drives many emergent research topics (e.g., in-context learning [1, 7, 8], instruction learning [9, 10, 11, 12, 13, 14], and chain-ofthought prompting [15, 16, 17, 18]) to further investigate the huge potential of LLMs, and brings unlimited possibilities for us for advancing artificial general intelligence.\n\nDespite these great successes, current LLM technologies are still imperfect and confront some urgent challenges on the way to building an advanced AI system. We discuss them from these aspects: 1) Limited to the input and output forms of text generation, current LLMs lack the ability to process complex information such as vision and speech, regardless of their significant achievements in NLP\n\n* The first two authors have equal contributions. This work was done when the first author was an intern at Microsoft Research Asia.\n\n† Corresponding author.\n\nFigure 1: Language serves as an interface for LLMs (e.g., ChatGPT) to connect numerous AI models (e.g., those in Hugging Face) for solving complicated AI tasks. In this concept, an LLM acts as a controller, managing and organizing the cooperation of expert models. The LLM first plans a list of tasks based on the user request and then assigns expert models to each task. After the experts execute the tasks, the LLM collects the results and responds to the user.\n\n<!-- image -->\n\ntasks; 2) In real-world scenarios, some complex tasks are usually composed of multiple sub-tasks, and thus require the scheduling and cooperation of multiple models, which are also beyond the capability of language models; 3) For some challenging tasks, LLMs demonstrate excellent results in zero-shot or few-shot settings, but they are still weaker than some experts (e.g., fine-tuned models). How to address these issues could be the critical step for LLMs toward artificial general intelligence.\n\nIn this paper, we point out that in order to handle complicated AI tasks, LLMs should be able to coordinate with external models to harness their powers. Hence, the pivotal question is how to choose suitable middleware to bridge the connections between LLMs and AI models. To tackle this issue, we notice that each AI model can be described in the form of language by summarizing its function. Therefore, we introduce a concept: ' Language as a generic interface for LLMs to collaborate with AI models '. In other words, by incorporating these model descriptions into prompts, LLMs can be considered as the brain to manage AI models such as planning, scheduling, and cooperation. As a result, this strategy empowers LLMs to invoke external models for solving AI tasks. However, when it comes to integrating multiple AI models into LLMs, another challenge emerges: solving numerous AI tasks needs collecting a large number of high-quality model descriptions, which in turn requires heavy prompt engineering. Coincidentally, we notice that some public ML communities usually offer a wide range of applicable models with well-defined model descriptions for solving specific AI tasks such as language, vision, and speech. These observations bring us some inspiration: Can we link LLMs (e.g., ChatGPT) with public ML communities (e.g., GitHub, Hugging Face 1 , etc) for solving complex AI tasks via a language-based interface?\n\nIn this paper, we propose an LLM-powered agent named HuggingGPT to autonomously tackle a wide range of complex AI tasks, which connects LLMs (i.e., ChatGPT) and the ML community (i.e., Hugging Face) and can process inputs from different modalities. More specifically, the LLM acts as a brain: on one hand, it disassembles tasks based on user requests, and on the other hand, assigns suitable models to the tasks according to the model description. By executing models and integrating results in the planned tasks, HuggingGPT can autonomously fulfill complex user requests. The whole process of HuggingGPT, illustrated in Figure 1, can be divided into four stages:\n\n- Task Planning: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n- Model Selection: To solve the planned tasks, ChatGPT selects expert models that are hosted on Hugging Face based on model descriptions.\n- Task Execution: Invoke and execute each selected model, and return the results to ChatGPT.\n\n1 https://huggingface.co/models\n\nFigure 2: Overview of HuggingGPT. With an LLM (e.g., ChatGPT) as the core controller and the expert models as the executors, the workflow of HuggingGPT consists of four stages: 1) Task planning : LLM parses the user request into a task list and determines the execution order and resource dependencies among tasks; 2) Model selection : LLM assigns appropriate models to tasks based on the description of expert models on Hugging Face; 3) Task execution : Expert models on hybrid endpoints execute the assigned tasks; 4) Response generation : LLM integrates the inference results of experts and generates a summary of workflow logs to respond to the user.\n\n<!-- image -->\n\n- Response Generation: Finally, ChatGPT is utilized to integrate the predictions from all models and generate responses for users.\n\nBenefiting from such a design, HuggingGPT can automatically generate plans from user requests and use external models, enabling it to integrate multimodal perceptual capabilities and tackle various complex AI tasks. More notably, this pipeline allows HuggingGPT to continually absorb the powers from task-specific experts, facilitating the growth and scalability of AI capabilities.\n\nOverall, our contributions can be summarized as follows:\n\n1. To complement the advantages of large language models and expert models, we propose HuggingGPT with an inter-model cooperation protocol. HuggingGPT applies LLMs as the brain for planning and decision, and automatically invokes and executes expert models for each specific task, providing a new way for designing general AI solutions.\n\n2. By integrating the Hugging Face hub with numerous task-specific models around ChatGPT, HuggingGPT is able to tackle generalized AI tasks covering multiple modalities and domains. Through the open collaboration of models, HuggingGPT can provide users with multimodal and reliable conversation services.\n3. We point out the importance of task planning and model selection in HuggingGPT (and autonomous agents), and formulate some experimental evaluations for measuring the capability of LLMs in planning and model selection.\n4. Extensive experiments on multiple challenging AI tasks across language, vision, speech, and cross-modality demonstrate the capability and huge potential of HuggingGPT in understanding and solving complex tasks from multiple modalities and domains.\n\n## 2 Related Works\n\nIn recent years, the field of natural language processing (NLP) has been revolutionized by the emergence of large language models (LLMs) [1, 2, 3, 4, 5, 19, 6], exemplified by models such as GPT-3 [1], GPT-4 [20], PaLM [3], and LLaMa [6]. LLMs have demonstrated impressive capabilities in zero-shot and few-shot tasks, as well as more complex tasks such as mathematical problems and commonsense reasoning, due to their massive corpus and intensive training computation. To extend the scope of large language models (LLMs) beyond text generation, contemporary research can be divided into two branches: 1) Some works have devised unified multimodal language models for solving various AI tasks [21, 22, 23]. For example, Flamingo [21] combines frozen pre-trained vision and language models for perception and reasoning. BLIP-2 [22] utilizes a Q-former to harmonize linguistic and visual semantics, and Kosmos-1 [23] incorporates visual input into text sequences to amalgamate linguistic and visual inputs. 2) Recently, some researchers started to investigate the integration of using tools or models in LLMs [24, 25, 26, 27, 28]. Toolformer [24] is the pioneering work to introduce external API tags within text sequences, facilitating the ability of LLMs to access external tools. Consequently, numerous works have expanded LLMs to encompass the visual modality. Visual ChatGPT [26] fuses visual foundation models, such as BLIP [29] and ControlNet [30], with LLMs. Visual Programming [31] and ViperGPT [25] apply LLMs to visual objects by employing programming languages, parsing visual queries into interpretable steps expressed as Python code. More discussions about related works are included in Appendix B.\n\nDistinct from these approaches, HuggingGPT advances towards more general AI capabilities in the following aspects: 1) HuggingGPT uses the LLM as the controller to route user requests to expert models, effectively combining the language comprehension capabilities of the LLM with the expertise of other expert models; 2) The mechanism of HuggingGPT allows it to address tasks in any modality or any domain by organizing cooperation among models through the LLM. Benefiting from the design of task planning in HuggingGPT, our system can automatically and effectively generate task procedures and solve more complex problems; 3) HuggingGPT offers a more flexible approach to model selection, which assigns and orchestrates tasks based on model descriptions. By providing only the model descriptions, HuggingGPT can continuously and conveniently integrate diverse expert models from AI communities, without altering any structure or prompt settings. This open and continuous manner brings us one step closer to realizing artificial general intelligence.\n\n## 3 HuggingGPT\n\nHuggingGPT is a collaborative system for solving AI tasks, composed of a large language model (LLM) and numerous expert models from ML communities. Its workflow includes four stages: task planning, model selection, task execution, and response generation, as shown in Figure 2. Given a user request, our HuggingGPT, which adopts an LLM as the controller, will automatically deploy the whole workflow, thereby coordinating and executing the expert models to fulfill the target. Table 1 presents the detailed prompt design in our HuggingGPT. In the following subsections, we will introduce the design of each stage.\n\n## 3.1 Task Planning\n\nGenerally, in real-world scenarios, user requests usually encompass some intricate intentions and thus need to orchestrate multiple sub-tasks to fulfill the target. Therefore, we formulate task planning\n\n|                     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                     | available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                     | Can you tell me how many objects in e1.jpg?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Demonstrations [{ \"task\" : \"object-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e1.jpg\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|                     | In e2.jpg, what's the animal and what's it doing?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [{ \"task\" : \"image-to-text\", \"id\" : 0, \"dep\" :[-1], \"args\" : { \"im age\" : \"e2.jpg\" }}, { \"task\" :\"image-cls\", \"id\" : 1, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" :\"object-detection\", \"id\" : 2, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" : \"vi- sual-quesrion-answering\", \"id\" : 3, \"dep\" :[-1], \"args\" : { \"text\" : \"what's the animal doing?\", \"image\" : \"e2.jpg\" }}]                                                                                                                                                                                                                                                               |\n|                     | First generate a HED image of e3.jpg, then based on the HED image and a text 'a girl reading a book', create a new image as a response.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | [{ \"task\" : \"pose-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e3.jpg\" }}, { \"task\" : \"pose-text-to-image\", \"id\" : 1, \"dep\" : [0], \"args\" : { \"text\" : \"a girl reading a book\", \"image\" : \"<re- source>-0\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Model Selection     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Model Selection     | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            |\n| Model Selection     | Candidate Models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Candidate Models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Model Selection     | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           |\n| Response Generation | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Response Generation | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. |\n\nTable 1: The details of the prompt design in HuggingGPT. In the prompts, we set some injectable slots such as {{ Demonstrations }} and {{ Candidate Models }}. These slots are uniformly replaced with the corresponding text before being fed into the LLM.\n\nas the first stage of HuggingGPT, which aims to use LLM to analyze the user request and then decompose it into a collection of structured tasks. Moreover, we require the LLM to determine dependencies and execution orders for these decomposed tasks, to build their connections. To enhance the efficacy of task planning in LLMs, HuggingGPT employs a prompt design, which consists of specification-based instruction and demonstration-based parsing. We introduce these details in the following paragraphs.\n\nSpecification-based Instruction To better represent the expected tasks of user requests and use them in the subsequent stages, we expect the LLM to parse tasks by adhering to specific specifications (e.g., JSON format ). Therefore, we design a standardized template for tasks and instruct the LLM to\n\nconduct task parsing through slot filing. As shown in Table 1, the task parsing template comprises four slots ( \"task\" , \"id\" , \"dep\" , and \"args\" ) to represent the task name, unique identifier, dependencies and arguments. Additional details for each slot can be found in the template description (see the Appendix A.1.1). By adhering to these task specifications, HuggingGPT can automatically employ the LLM to analyze user requests and parse tasks accordingly.\n\nDemonstration-based Parsing To better understand the intention and criteria for task planning, HuggingGPT incorporates multiple demonstrations in the prompt. Each demonstration consists of a user request and its corresponding output, which represents the expected sequence of parsed tasks. By incorporating dependencies among tasks, these demonstrations aid HuggingGPT in understanding the logical connections between tasks, facilitating accurate determination of execution order and identification of resource dependencies. The details of our demonstrations is presented in Table 1.\n\nFurthermore, to support more complex scenarios (e.g., multi-turn dialogues), we include chat logs in the prompt by appending the following instruction: ' To assist with task planning, the chat history is available as {{ Chat Logs }}, where you can trace the user-mentioned resources and incorporate them into the task planning. '. Here {{ Chat Logs }} represents the previous chat logs. This design allows HuggingGPT to better manage context and respond to user requests in multi-turn dialogues.\n\n## 3.2 Model Selection\n\nFollowing task planning, HuggingGPT proceeds to the task of matching tasks with models, i.e., selecting the most appropriate model for each task in the parsed task list. To this end, we use model descriptions as the language interface to connect each model. More specifically, we first gather the descriptions of expert models from the ML community (e.g., Hugging Face) and then employ a dynamic in-context task-model assignment mechanism to choose models for the tasks. This strategy enables incremental model access (simply providing the description of the expert models) and can be more open and flexible to use ML communities. More details are introduced in the next paragraph.\n\nIn-context Task-model Assignment We formulate the task-model assignment as a single-choice problem, where available models are presented as options within a given context. Generally, based on the provided user instruction and task information in the prompt, HuggingGPT is able to select the most appropriate model for each parsed task. However, due to the limits of maximum context length, it is not feasible to encompass the information of all relevant models within one prompt. To mitigate this issue, we first filter out models based on their task type to select the ones that match the current task. Among these selected models, we rank them based on the number of downloads 2 on Hugging Face and then select the topK models as the candidates. This strategy can substantially reduce the token usage in the prompt and effectively select the appropriate models for each task.\n\n## 3.3 Task Execution\n\nOnce a specific model is assigned to a parsed task, the next step is to execute the task (i.e., perform model inference). In this stage, HuggingGPT will automatically feed these task arguments into the models, execute these models to obtain the inference results, and then send them back to the LLM. It is necessary to emphasize the issue of resource dependencies at this stage. Since the outputs of the prerequisite tasks are dynamically produced, HuggingGPT also needs to dynamically specify the dependent resources for the task before launching it. Therefore, it is challenging to build the connections between tasks with resource dependencies at this stage.\n\nResource Dependency To address this issue, we use a unique symbol, ' &lt;resource&gt; ', to maintain resource dependencies. Specifically, HuggingGPT identifies the resources generated by the prerequisite task as &lt;resource&gt;-task\\_id , where task\\_id is the id of the prerequisite task. During the task planning stage, if some tasks are dependent on the outputs of previously executed tasks (e.g., task\\_id ), HuggingGPT sets this symbol (i.e., &lt;resource&gt;-task\\_id ) to the corresponding resource subfield in the arguments. Then in the task execution stage, HuggingGPT dynamically replaces this symbol with the resource generated by the prerequisite task. As a result, this strategy empowers HuggingGPT to efficiently handle resource dependencies during task execution.\n\n2 To some extent, we think the downloads can reflect the popularity and quality of the model.\n\nTable 2: Evaluation for task planning in different task types.\n\n<!-- image -->\n\nFurthermore, for the remaining tasks without any resource dependencies, we will execute these tasks directly in parallel to further improve inference efficiency. This means that multiple tasks can be executed simultaneously if they meet the prerequisite dependencies. Additionally, we offer a hybrid inference endpoint to deploy these models for speedup and computational stability. For more details, please refer to Appendix A.1.3.\n\n## 3.4 Response Generation\n\nAfter all task executions are completed, HuggingGPT needs to generate the final responses. As shown in Table 1, HuggingGPT integrates all the information from the previous three stages (task planning, model selection, and task execution) into a concise summary in this stage, including the list of planned tasks, the selected models for the tasks, and the inference results of the models.\n\nMost important among them are the inference results, which are the key points for HuggingGPT to make the final decisions. These inference results are presented in a structured format, such as bounding boxes with detection probabilities in the object detection model, answer distributions in the question-answering model, etc. HuggingGPT allows LLM to receive these structured inference results as input and generate responses in the form of friendly human language. Moreover, instead of simply aggregating the results, LLM generates responses that actively respond to user requests, providing a reliable decision with a confidence level.\n\n## 4 Experiments\n\n## 4.1 Settings\n\nIn our experiments, we employed the gpt-3.5-turbo , text-davinci-003 and gpt-4 variants of the GPT models as the main LLMs, which are publicly accessible through the OpenAI API 3 . To enable more stable outputs of LLM, we set the decoding temperature to 0. In addition, to regulate the LLM output to satisfy the expected format (e.g., JSON format), we set the logit\\_bias to 0.2 on the format constraints (e.g., ' { ' and ' } '). We provide detailed prompts designed for the task planning, model selection, and response generation stages in Table 1, where {{ variable }} indicates the slot which needs to be populated with the corresponding text before being fed into the LLM.\n\n## 4.2 Qualitative Results\n\nFigure 1 and Figure 2 have shown two demonstrations of HuggingGPT. In Figure 1, the user request consists of two sub-tasks: describing the image and object counting. In response to the request, HuggingGPT planned three tasks: image classification, image captioning, and object detection, and launched the google/vit [32], nlpconnet/vit-gpt2-image-captioning [33], and facebook/detr-resnet-101 [34] models, respectively. Finally, HuggingGPT integrated the results of the model inference and generated responses (describing the image and providing the count of contained objects) to the user.\n\n3 https://platform.openai.com/\n\nA more detailed example is shown in Figure 2. In this case, the user's request included three tasks: detecting the pose of a person in an example image, generating a new image based on that pose and specified text, and creating a speech describing the image. HuggingGPT parsed these into six tasks, including pose detection, text-to-image conditional on pose, object detection, image classification, image captioning, and text-to-speech. We observed that HuggingGPT can correctly orchestrate the execution order and resource dependencies among tasks. For instance, the pose conditional text-to-image task had to follow pose detection and use its output as input. After this, HuggingGPT selected the appropriate model for each task and synthesized the results of the model execution into a final response. For more demonstrations, please refer to the Appendix A.3.\n\n## 4.3 Quantitative Evaluation\n\nIn HuggingGPT, task planning plays a pivotal role in the whole workflow, since it determines which tasks will be executed in the subsequent pipeline. Therefore, we deem that the quality of task planning can be utilized to measure the capability of LLMs as a controller in HuggingGPT. For this purpose, we conduct quantitative evaluations to measure the capability of LLMs. Here we simplified the evaluation by only considering the task\n\nTable 3: Evaluation for the single task. 'Acc' and 'Pre' represents Accuracy and Precision.\n\n| LLM       |   Acc ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|---------|---------|------------|--------|\n| Alpaca-7b |    6.48 |   35.6  |       6.64 |   4.88 |\n| Vicuna-7b |   23.86 |   45.51 |      26.51 |  29.44 |\n| GPT-3.5   |   52.62 |   62.12 |      52.62 |  54.45 |\n\ntype, without its associated arguments. To better conduct evaluations on task planning, we group tasks into three distinct categories (see Table 2) and formulate different metrics for them:\n\n- Single Task refers to a request that involves only one task. We consider the planning to be correct if and only if the task name (i.e., \"task\" ) and the predicted label are identically equal. In this context, we utilize F1 and accuracy as the evaluation metrics.\n- Sequential Task indicates that the user's request can be decomposed into a sequence of multiple sub-tasks. In this case, we employ F1 and normalized Edit Distance [35] as the evaluation metrics.\n- Graph Task indicates that user requests can be decomposed into directed acyclic graphs. Considering the possibility of multiple planning topologies within graph tasks, relying solely on the F1-score is not enough to reflect the LLM capability in planning. To address this, following Vicuna [36], we employed GPT-4 as a critic to evaluate the correctness of the planning. The accuracy is obtained by evaluating the judgment of GPT-4, referred to as the GPT-4 Score. Detailed information about the GPT-4 Score can be found in Appendix A.1.5.\n\nDataset To conduct our evaluation, we invite some annotators to submit some requests. We collect these data as the evaluation dataset. We use GPT-4 to generate task planning as the pseudo labels, which cover single, sequential, and graph tasks. Furthermore, we invite some expert annotators to label task planning for some complex requests (46 examples) as a high-quality humanannotated dataset. We also plan to improve the\n\nTable 4: Evaluation for the sequential task. 'ED' means Edit Distance.\n\n| LLM       |   ED ↓ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|--------|---------|------------|--------|\n| Alpaca-7b |   0.83 |   22.27 |      23.35 |  22.8  |\n| Vicuna-7b |   0.8  |   19.15 |      28.45 |  22.89 |\n| GPT-3.5   |   0.54 |   61.09 |      45.15 |  51.92 |\n\nquality and quantity of this dataset to further assist in evaluating the LLM's planning capabilities, which remains a future work. More details about this dataset are in Appendix A.2. Using this dataset, we conduct experimental evaluations on various LLMs, including Alpaca-7b [37], Vicuna-7b [36], and GPT models, for task planning.\n\nTable 5: Evaluation for the graph task.\n\n| LLM       |   GPT-4 Score ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|-----------------|---------|------------|--------|\n| Alpaca-7b |           13.14 |   16.18 |      28.33 |  20.59 |\n| Vicuna-7b |           19.17 |   13.97 |      28.08 |  18.66 |\n| GPT-3.5   |           50.48 |   54.9  |      49.23 |  51.91 |\n\nPerformance Tables 3, 4 and 5 show the planning capabilities of HuggingGPT on the three categories of GPT-4 annotated datasets, respectively. We observed that GPT-3.5 exhibits more prominent planning capabilities, outperforming the open-source LLMs Alpaca-7b and Vicuna-7b in terms of all types of\n\nuser requests. Specifically, in more complex tasks (e.g., sequential and graph tasks), GPT-3.5 has shown absolute predominance over other LLMs. These results also demonstrate the evaluation of task planning can reflect the capability of LLMs as a controller. Therefore, we believe that developing technologies to improve the ability of LLMs in task planning is very important, and we leave it as a future research direction.\n\nTable 6: Evaluation on the human-annotated dataset.\n\n| LLM       | Sequential Task   | Sequential Task   | Graph Task   | Graph Task   |\n|-----------|-------------------|-------------------|--------------|--------------|\n|           | Acc ↑             | ED ↓              | Acc ↑        | F1 ↑         |\n| Alpaca-7b | 0                 | 0.96              | 4.17         | 4.17         |\n| Vicuna-7b | 7.45              | 0.89              | 10.12        | 7.84         |\n| GPT-3.5   | 18.18             | 0.76              | 20.83        | 16.45        |\n| GPT-4     | 41.36             | 0.61              | 58.33        | 49.28        |\n\nFurthermore, we conduct experiments on the high-quality human-annotated dataset to obtain a more precise evaluation. Table 6 reports the comparisons on the human-annotated dataset. These results align with the aforementioned conclusion, highlighting that more powerful LLMs demonstrate better performance in task planning. Moreover, we compare the results between human annotations and GPT-4 annotations. We find that even though GPT-4 outperforms other LLMs, there still remains a substantial gap when compared with human annotations. These observations further underscore the importance of enhancing the planning capabilities of LLMs.\n\n## 4.4 Ablation Study\n\nTable 7: Evaluation of task planning in terms of the variety of demonstrations. We refer to the variety of demonstrations as the number of different task types involved in the demonstrations.\n\n| Demo Variety (# task types)   | LLM     | Single Task   | Single Task   | Sequencial Task   | Sequencial Task   | Graph Task   |\n|-------------------------------|---------|---------------|---------------|-------------------|-------------------|--------------|\n| Demo Variety (# task types)   | LLM     | Acc ↑         | F1 ↑          | ED (%) ↓          | F1 ↑              | F1 ↑         |\n| 2                             | GPT-3.5 | 43.31         | 48.29         | 71.27             | 32.15             | 43.42        |\n| 2                             | GPT-4   | 65.59         | 67.08         | 47.17             | 55.13             | 53.96        |\n| 6                             | GPT-3.5 | 51.31         | 51.81         | 60.81             | 43.19             | 58.51        |\n| 6                             | GPT-4   | 66.83         | 68.14         | 42.20             | 58.18             | 64.34        |\n| 10                            | GPT-3.5 | 52.83         | 53.70         | 56.52             | 47.03             | 64.24        |\n| 10                            | GPT-4   | 67.52         | 71.05         | 39.32             | 60.80             | 66.90        |\n\nFigure 3: Evaluation of task planning with different numbers of demonstrations.\n\n<!-- image -->\n\nAs previously mentioned in our default setting, we apply few-shot demonstrations to enhance the capability of LLMs in understanding user intent and parsing task sequences. To better investigate the effect of demonstrations on our framework, we conducted a series of ablation studies from two perspectives: the number of demonstrations and the variety of demonstrations. Table 7 reports the planning results under the different variety of demonstrations. We observe that increasing the variety among demonstrations can moderately improve the performance of LLMs in conduct planning. Moreover, Figure 3 illustrates the results of task planning with different number of demonstrations. We can find that adding some demonstrations can slightly improve model performance but this improvement will be limited when the number is over 4 demonstrations. In the future, we will continue to explore more elements that can improve the capability of LLMs at different stages.\n\n| LLM        | Task Planning   | Task Planning   | Model Selection   | Model Selection   | Response       |\n|------------|-----------------|-----------------|-------------------|-------------------|----------------|\n| LLM        | Passing Rate ↑  | Rationality ↑   | Passing Rate ↑    | Rationality ↑     | Success Rate ↑ |\n| Alpaca-13b | 51.04           | 32.17           | -                 | -                 | 6.92           |\n| Vicuna-13b | 79.41           | 58.41           | -                 | -                 | 15.64          |\n| GPT-3.5    | 91.22           | 78.47           | 93.89             | 84.29             | 63.08          |\n\nTable 8: Human Evaluation on different LLMs. We report two metrics, passing rate (%) and rationality (%), in the task planning and model selection stages and report a straightforward success rate (%) to evaluate whether the request raised by the user is finally resolved.\n\n## 4.5 Human Evaluation\n\nIn addition to objective evaluations, we also invite human experts to conduct a subjective evaluation in our experiments. We collected 130 diverse requests to evaluate the performance of HuggingGPT at various stages, including task planning, model selection, and final response generation. We designed three evaluation metrics, namely passing rate, rationality, and success rate. The definitions of each metric can be found in Appendix A.1.6. The results are reported in Table 8. From Table 8, we can observe similar conclusions that GPT-3.5 can significantly outperform open-source LLMs like Alpaca-13b and Vicuna-13b by a large margin across different stages, from task planning to response generation stages. These results indicate that our objective evaluations are aligned with human evaluation and further demonstrate the necessity of a powerful LLM as a controller in the framework of autonomous agents.\n\n## 5 Limitations\n\nHuggingGPT has presented a new paradigm for designing AI solutions, but we want to highlight that there still remain some limitations or improvement spaces: 1) Planning in HuggingGPT heavily relies on the capability of LLM. Consequently, we cannot ensure that the generated plan will always be feasible and optimal. Therefore, it is crucial to explore ways to optimize the LLM in order to enhance its planning abilities; 2) Efficiency poses a common challenge in our framework. To build such a collaborative system (i.e., HuggingGPT) with task automation, it heavily relies on a powerful controller (e.g., ChatGPT). However, HuggingGPT requires multiple interactions with LLMs throughout the whole workflow and thus brings increasing time costs for generating the response; 3) Token Lengths is another common problem when using LLM, since the maximum token length is always limited. Although some works have extended the maximum length to 32K, it is still insatiable for us if we want to connect numerous models. Therefore, how to briefly and effectively summarize model descriptions is also worthy of exploration; 4) Instability is mainly caused because LLMs are usually uncontrollable. Although LLM is skilled in generation, it still possibly fails to conform to instructions or give incorrect answers during the prediction, leading to exceptions in the program workflow. How to reduce these uncertainties during inference should be considered in designing systems.\n\n## 6 Conclusion\n\nIn this paper, we propose a system named HuggingGPT to solve AI tasks, with language as the interface to connect LLMs with AI models. The principle of our system is that an LLM can be viewed as a controller to manage AI models, and can utilize models from ML communities like Hugging Face to automatically solve different requests of users. By exploiting the advantages of LLMs in understanding and reasoning, HuggingGPT can dissect the intent of users and decompose it into multiple sub-tasks. And then, based on expert model descriptions, HuggingGPT is able to assign the most suitable models for each task and integrate results from different models to generate the final response. By utilizing the ability of numerous AI models from machine learning communities, HuggingGPT demonstrates immense potential in solving challenging AI tasks, thereby paving a new pathway towards achieving artificial general intelligence.\n\n## Acknowledgement\n\nWe appreciate the support of the Hugging Face team to help us in improving our GitHub project and web demo. Besides, we also appreciate the contributions of Bei Li , Kai Shen , Meiqi Chen , Qingyao Guo , Yichong Leng , Yuancheng Wang , Dingyao Yu for the data labeling and Wenqi Zhang , Wen Wang , Zeqi Tan for paper revision.\n\nThis work is partly supported by the Fundamental Research Funds for the Central Universities (No. 226-2023-00060), Key Research and Development Program of Zhejiang Province (No. 2023C01152), National Key Research and Development Project of China (No. 2018AAA0101900), and MOE Engineering Research Center of Digital Library.\n\n## References\n\n- [1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In NeurIPS , 2020.\n- [2] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR , abs/2203.02155, 2022.\n- [3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, and others. Palm: Scaling language modeling with pathways. ArXiv , abs/2204.02311, 2022.\n- [4] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open Pre-trained Transformer Language Models. ArXiv , abs/2205.01068, 2022.\n- [5] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. Glm-130b: An Open Bilingual Pre-trained Model. ICLR 2023 poster , 2023.\n- [6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and Efficient Foundation Language Models. ArXiv , abs/2302.13971, 2023.\n- [7] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An Explanation of In-context Learning as Implicit Bayesian Inference. ICLR 2022 Poster , 2022.\n- [8] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2022.\n- [9] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations , 2022.\n- [10] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\n\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Virendrabhai Purohit, Ishani Mondal, Jacob William Anderson, Kirby C. Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, rushang karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP) . Association for Computational Linguistics, 2022.\n\n- [11] S. Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. OptIML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization. ArXiv , abs/2212.12017, 2022.\n- [12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022.\n- [13] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions, 2022.\n- [14] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. CoRR , abs/2301.13688, 2023.\n- [15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Conference on Neural Information Processing Systems (NeurIPS) , 2022.\n- [16] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. In Conference on Neural Information Processing Systems (NeurIPS) , 2022.\n- [17] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided Language Models. ArXiv , abs/2211.10435, 2022.\n- [18] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023 poster , abs/2203.11171, 2023.\n- [19] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. CoRR , abs/2206.07682, 2022.\n- [20] OpenAI. Gpt-4 technical report, 2023.\n- [21] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.\n- [22] Junnan Li, Dongxu Li, S. Savarese, and Steven Hoi. Blip-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ArXiv , abs/2301.12597, 2023.\n\n- [23] Shaohan Huang, Li Dong, Wenhui Wang, Y. Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, O. Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language Is Not All You Need: Aligning Perception with Language Models. ArXiv , abs/2302.14045, 2023.\n- [24] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach Themselves to Use Tools. ArXiv , abs/2302.04761, 2023.\n- [25] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning, 2023.\n- [26] Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models. arXiv , 2023.\n- [27] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis, 2023.\n- [28] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.\n- [29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping LanguageImage Pre-training for Unified Vision-Language Understanding and Generation. In International Conference on Machine Learning (ICML) , pages 12888-12900, 2022.\n- [30] Lvmin Zhang and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. ArXiv , abs/2302.05543, 2023.\n- [31] Tanmay Gupta and Aniruddha Kembhavi. Visual Programming: Compositional visual reasoning without training. arXiv , abs/2211.11559, 2022.\n- [32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021.\n- [33] Ankur Kumar. The illustrated image captioning using transformers. ankur3107.github.io , 2022.\n- [34] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers, 2020.\n- [35] A. Marzal and E. Vidal. Computation of normalized edit distance and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence , 15(9):926-932, 1993.\n- [36] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n- [37] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.\n\n## A Appendix\n\n## A.1 More details\n\nIn this section, we will present more details about some designs of each stage in HuggingGPT.\n\n## A.1.1 Template for Task Planning\n\nTo format the parsed task, we define the template [{ \"task\" : task, \"id\" , task\\_id, \"dep\" : depen-dency\\_task\\_ids, \"args\" : { \"text\" : text, \"image\" : URL, \"audio\" : URL, \"video\" : URL}}] with four slots: \"task\" , \"id\" , \"dep\" , and \"args\" . Table 9 presents the definitions of each slot.\n\nTable 9: Definitions for each slot for parsed tasks in the task planning.\n\n| Name   | Definitions                                                                                                                                                                                                                                                                                                                                                 |\n|--------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"task\" | It represents the type of the parsed task. It covers different tasks in language, visual, video, audio, etc. The currently supported task list of HuggingGPT is shown in Table 13.                                                                                                                                                                          |\n| \"id\"   | The unique identifier for task planning, which is used for references to dependent tasks and their generated resources.                                                                                                                                                                                                                                     |\n| \"dep\"  | It defines the pre-requisite tasks required for execution. The task will be launched only when all the pre-requisite dependent tasks are finished.                                                                                                                                                                                                          |\n| \"args\" | It contains the list of required arguments for task execution. It contains three subfields populated with text, image, and audio resources according to the task type. They are resolved from either the user's request or the generated resources of the dependent tasks. The corresponding argument types for different task types are shown in Table 13. |\n\n## A.1.2 Model Descriptions\n\nIn general, the Hugging Face Hub hosts expert models that come with detailed model descriptions, typically provided by the developers. These descriptions encompass various aspects of the model, such as its function, architecture, supported languages and domains, licensing, and other relevant details. These comprehensive model descriptions play a crucial role in aiding the decision of HuggingGPT. By assessing the user's requests and comparing them with the model descriptions, HuggingGPT can effectively determine the most suitable model for the given task.\n\n## A.1.3 Hybrid Endpoint in System Deployment\n\nAn ideal scenario is that we only use inference endpoints on cloud service (e.g., Hugging Face). However, in some cases, we have to deploy local inference endpoints, such as when inference endpoints for certain models do not exist, the inference is time-consuming, or network access is limited. To keep the stability and efficiency of the system, HuggingGPT allows us to pull and run some common or time-consuming models locally. The local inference endpoints are fast but cover fewer models, while the inference endpoints in the cloud service (e.g., Hugging Face) are the opposite. Therefore, local endpoints have higher priority than cloud inference endpoints. Only if the matched model is not deployed locally, HuggingGPT will run the model on the cloud endpoint like Hugging Face. Overall, we think that how to design and deploy systems with better stability for HuggingGPT or other autonomous agents will be very important in the future.\n\n## A.1.4 Task List\n\nUp to now, HuggingGPT has supported 24 AI tasks, which cover language, vision, speech and etc. Table 13 presents the detailed information of the supported task list in HuggingGPT.\n\n## A.1.5 GPT-4 Score\n\nFollowing the evaluation method used by Vicuna [36], we employed GPT-4 as an evaluator to assess the planning capabilities of LLMs. In more detail, we include the user request and the task list planned by LLM in the prompt, and then let GPT-4 judge whether the list of tasks is accurate and\n\nalso provide a rationale. To guide GPT-4 to make the correct judgments, we designed some task guidelines: 1) the tasks are in the supported task list (see Table 13); 2) the planned task list can reach the solution to the user request; 3) the logical relationship and order among the tasks are reasonable. In the prompt, we also supplement several positive and negative demonstrations of task planning to provide reference for GPT-4. The prompt for GPT-4 score is shown in Table 10. We further want to emphasize that GPT-4 score is not always correct although it has shown a high correlation. Therefore, we also expect to explore more confident metrics to evaluate the ability of LLMs in planning.\n\nAs a critic, your task is to assess whether the AI assistant has properly planned the task based on the user's request. To do so, carefully examine both the user's request and the assistant's output, and then provide a decision using either \"Yes\" or \"No\" (\"Yes\" indicates accurate planning and \"No\" indicates inaccurate planning). Additionally, provide a rationale for your choice using the following structure: { \"choice\" : \"yes\"/\"no\", \"reason\" : \"Your reason for your choice\"}. Please adhere to the following guidelines: 1. The task must be selected from the following options: {{ Available Task List }}. 2. Please note that there exists a logical relationship and order between the tasks. 3. Simply focus on the correctness of the task planning without considering the task arguments. Positive examples: {{ Positive Demos }} Negative examples: {{ Negative Demos }} Current user request: {{ Input }} AI assistant's output: {{ Output }} Your judgement:\n\nTable 10: The prompt design for GPT-4 Score.\n\n## A.1.6 Human Evaluation\n\nTo better align human preferences, we invited three human experts to evaluate the different stages of HuggingGPT. First, we selected 3-5 tasks from the task list of Hugging Face and then manually created user requests based on the selected tasks. We will discard samples that cannot generate new requests from the selected tasks. Totally, we conduct random sampling by using different seeds, resulting in a collection of 130 diverse user requests. Based on the produced samples, we evaluate the performance of LLMs at different stages (e.g., task planning, model selection, and response generation). Here, we designed three evaluation metrics:\n\n- Passing Rate : to determine whether the planned task graph or selected model can be successfully executed;\n- Rationality : to assess whether the generated task sequence or selected tools align with user requests in a rational manner;\n- Success Rate : to verify if the final results satisfy the user's request.\n\nThree human experts were asked to annotate the provided data according to our well-designed metrics and then calculated the average values to obtain the final scores.\n\n## A.2 Datasets for Task Planning Evaluation\n\nAs aforementioned, we create two datasets for evaluating task planning. Here we provide more details about these datasets. In total, we gathered a diverse set of 3,497 user requests. Since labeling this dataset to obtain the task planning for each request is heavy, we employed the capabilities of GPT-4 to annotate them. Finally, these auto-labeled requests can be categorized into three types: single task (1,450 requests), sequence task (1,917 requests), and graph task (130 requests). For a more reliable evaluation, we also construct a human-annotated dataset. We invite some expert annotators to label some complex requests, which include 46 examples. Currently, the human-annotated dataset includes 24 sequential tasks and 22 graph tasks. Detailed statistics about the GPT-4-annotated and human-annotated datasets are shown in Table 11.\n\n## A.3 Case Study\n\n## A.3.1 Case Study on Various Tasks\n\nThrough task planning and model selection, HuggingGPT, a multi-model collaborative system, empowers LLMs with an extended range of capabilities. Here, we extensively evaluate HuggingGPT across diverse multimodal tasks, and some selected cases are shown in Figures 4 and 5. With the cooperation of a powerful LLM and numerous expert models, HuggingGPT effectively tackles\n\nTable 11: Statistics on datasets for task planning evaluation.\n\n| Datasets        | Number of Requests by Type   | Number of Requests by Type   | Number of Requests by Type   | Request Length   | Request Length   | Number of Tasks   | Number of Tasks   |\n|-----------------|------------------------------|------------------------------|------------------------------|------------------|------------------|-------------------|-------------------|\n|                 | Single                       | Sequential                   | Graph                        | Max              | Average          | Max               | Average           |\n| GPT-4-annotated | 1,450                        | 1,917                        | 130                          | 52               | 13.26            | 13                | 1.82              |\n| Human-annotated | -                            | 24                           | 22                           | 95               | 10.20            | 12                | 2.00              |\n\ntasks spanning various modalities, including language, image, audio, and video. Its proficiency encompasses diverse task forms, such as detection, generation, classification, and question answering.\n\n## A.3.2 Case Study on Complex Tasks\n\nSometimes, user requests may contain multiple implicit tasks or require multi-faceted information, in which case we cannot rely on a single expert model to solve them. To overcome this challenge, HuggingGPT organizes the collaboration of multiple models through task planning. As shown in Figures 6, 7 and 8, we conducted experiments to evaluate the effectiveness of HuggingGPT in the case of complex tasks:\n\n- Figure 6 demonstrates the ability of HuggingGPT to cope with complex tasks in a multi-round conversation scenario. The user splits a complex request into several steps and reaches the final goal through multiple rounds of interaction. We find that HuggingGPT can track the contextual state of user requests through the dialogue context management in the task planning stage. Moreover, HuggingGPT demonstrates the ability to access user-referenced resources and proficiently resolve dependencies between tasks in the dialogue scenario.\n- Figure 7 shows that for a simple request like \"describe the image in as much detail as possible\" , HuggingGPT can decompose it into five related tasks, namely image captioning, image classification, object detection, segmentation, and visual question answering tasks. HuggingGPT assigns expert models to handle each task to gather information about the image from various perspectives. Finally, the LLM integrates this diverse information to deliver a comprehensive and detailed description to the user.\n- Figure 8 shows two cases where a user request can contain several tasks. In these cases, HuggingGPT first performs all the tasks requested by the user by orchestrating the work of multiple expert models, and then let the LLM aggregate the model inference results to respond to the user.\n\nIn summary, HuggingGPT establishes the collaboration of LLM with external expert models and shows promising performance on various forms of complex tasks.\n\n## A.3.3 Case Study on More Scenarios\n\nWe show more cases here to illustrate HuggingGPT's ability to handle realistic scenarios with task resource dependencies, multimodality, multiple resources, etc. To make clear the workflow of HuggingGPT, we also provide the results of the task planning and task execution stages.\n\n- Figure 9 illustrates the operational process of HuggingGPT in the presence of resource dependencies among tasks. In this case, HuggingGPT can parse out concrete tasks based on abstract requests from the user, including pose detection, image captioning, and pose conditional image generation tasks. Furthermore, HuggingGPT effectively recognizes the dependencies between task #3 and tasks #1, #2, and injected the inferred results of tasks #1 and #2 into the input arguments of task #3 after the dependency tasks were completed.\n- Figure 10 demonstrates the conversational ability of HuggingGPT on audio and video modalities. In the two cases, it shows HuggingGPT completes the user-requested text-to-audio and text-to-video tasks via the expert models, respectively. In the top one, the two models are executed in parallel (generating audio and generating video concurrently), and in the bottom one, the two models are executed serially (generating text from the image first, and then generating audio based on the text). This further validates that HuggingGPT can organize the cooperation between models and the resource dependencies between tasks.\n\n- Figure 11 shows HuggingGPT integrating multiple user-input resources to perform simple reasoning. We can find that HuggingGPT can break up the main task into multiple basic tasks even with multiple resources, and finally integrate the results of multiple inferences from multiple models to get the correct answer.\n\n## B More Discussion about Related Works\n\nThe emergence of ChatGPT and its subsequent variant GPT-4, has created a revolutionary technology wave in LLM and AI area. Especially in the past several weeks, we also have witnessed some experimental but also very interesting LLM applications, such as AutoGPT 4 , AgentGPT 5 , BabyAGI 6 , and etc. Therefore, we also give some discussions about these works and provide some comparisons from multiple dimensions, including scenarios, planning, tools, as shown in Table 12.\n\nScenarios Currently, these experimental agents (e.g., AutoGPT, AgentGPT and BabyAGI) are mainly used to solve daily requests. While for HuggingGPT, it focuses on solving tasks in the AI area (e.g., vision, language, speech, etc), by utilizing the powers of Hugging Face. Therefore, HuggingGPT can be considered as a more professional agent. Generally speaking, users can choose the most suitable agent based on their requirements (e.g., daily requests or professional areas) or customize their own agent by defining knowledge, planning strategy and toolkits.\n\nTable 12: Comparision between HuggingGPT and other autonomous agents.\n\n| Name                     | Scenarios   | Planning           | Tools                              |\n|--------------------------|-------------|--------------------|------------------------------------|\n| BabyAGI AgentGPT AutoGPT | Daily       | Iterative Planning | - - Web Search, Code Executor, ... |\n| HuggingGPT               | AI area     | Global Planning    | Models in Hugging Face             |\n\nPlanning BabyAGI, AgentGPT and AutoGPT can all be considered as autonomous agents, which provide some solutions for task automation. For these agents, all of them adopt step-by-step thinking, which iteratively generates the next task by using LLMs. Besides, AutoGPT employs an addition reflexion module for each task generation, which is used to check whether the current predicted task is appropriate or not. Compared with these applications, HuggingGPT adopts a global planning strategy to obtain the entire task queue within one query. It is difficult to judge which one is better, since each one has its deficiencies and both of them heavily rely on the ability of LLMs, even though existing LLMs are not specifically designed for task planning. For example, iterative planning combined with reflexion requires a huge amount of LLM queries, and if one step generates an error prediction, the entire workflow would possibly enter an endless loop. While for global planning, although it can always produce a solution for each user request within one query, it still cannot guarantee the correctness of each step or the optimality of the entire plan. Therefore, both iterative and global planning have their own merits and can borrow from each other to alleviate their shortcoming. Additionally, one notable point is that the difficulty of task planning is also linearly correlated to the task range. As the scope of tasks increases, it becomes more challenging for the controller to predict precise plans. Consequently, optimizing the controller (i.e., LLM) for task planning will be crucial in building autonomous agents.\n\nTools Among these agents, AutoGPT is the main one to involve other tools for usage. More specifically, AutoGPT primarily uses some common tools (e.g., web search, code executor), while HuggingGPT utilizes the expert models of ML communities (e.g., Hugging Face). Therefore, AutoGPT has a broader task range but is not suitable for more professional problems, whereas HuggingGPT is more specialized and focuses on solving more complex AI tasks. Therefore, the\n\n4 https://github.com/Significant-Gravitas/Auto-GPT\n\n5 https://github.com/reworkd/AgentGPT\n\n6 https://github.com/yoheinakajima/babyagi\n\nrange of tools used in LLMs will be a trade-off between task depth and task range. In addition, we also note some industry products for LLM applications (e.g., ChatGPT plugins 7 ) and developer tools (e.g., LangChain 8 , HuggingFace Transformer Agent 9 , Semantic Kernels 10 ) for LLM applications. We believe these rapid developments will also facilitate the community to explore how to better integrate LLMs with external tools.\n\nOverall, HuggingGPT can also be considered as an autonomous agent. Compared with these experimental agents, which mainly use GPT models to generate solutions for user requests, HuggingGPT systematically presents a clear pipeline with four stages: task planning, model selection, task execution and response generation. Such a pipeline can effectively improve the success rate of solving user requests. Besides, HuggingGPT also introduces a global planning strategy to decompose user requests and thus accomplish task automation. Furthermore, HuggingGPT is a collaborative system, which fully utilizes the power of expert models from ML communities to solve AI tasks and present the huge potential of using external tools. Compared with these agents, HuggingGPT allows us to better solve tasks more effectively in professional areas and can be easily extended to any vertical domain. In the future, we will continue to enhance HuggingGPT with even more powerful abilities to develop a versatile autonomous agent with unlimited possibilities.\n\n7 https://openai.com/blog/chatgpt-plugins\n\n8 https://python.langchain.com/\n\n9 https://huggingface.co/docs/transformers/transformers\\_agents\n\n10 https://github.com/microsoft/semantic-kernel\n\nFigure 4: Case study on various tasks (a).\n\n<!-- image -->\n\nFigure 5: Case study on various tasks (b).\n\n<!-- image -->\n\nTable 13: The task list used in HuggingGPT. The first and second columns are the name and arguments of the corresponding task. The third and fourth columns provide some examples of the candidate models and their model descriptions.\n\n| Task                 | Args         | Candidate Models                                        | Descriptions                                                                     |\n|----------------------|--------------|---------------------------------------------------------|----------------------------------------------------------------------------------|\n| NLP Tasks            | NLP Tasks    | NLP Tasks                                               | NLP Tasks                                                                        |\n| Text-CLS             | text         | [ cardiffnlp/twitter-roberta- base-sentiment , ...]     | [' This is a RoBERTa-base model trained on 58M tweets ...', ...]                 |\n| Token-CLS            | text         | [ dslim/bert-base-NER , ...]                            | [' bert-base-NER is a fine-tuned BERT model that is ready to ...', ...]          |\n| Text2text-Generation | text         | [ google/flan-t5-xl , ...]                              | [' If you already know T5, FLAN-T5 is just better at everything ...', ...]       |\n| Summarization        | text         | [ bart-large-cnn , ...]                                 | [ ' BART model pre-trained on English language, and fine-tuned ...', ...]        |\n| Translation          | text         | [ t5-base , ...]                                        | [' With T5, we propose reframing all NLP tasks into a unified ...', ...]         |\n| Question-Answering   | text         | [ deepset/roberta-base- squad2 , ...]                   | [' This is the roberta-base model, fine-tuned using the SQuAD2.0 ...', ...]      |\n| Conversation         | text         | [ PygmalionAI/pygmalion- 6b , ...]                      | [' Pymalion 6B is a proof-of-concept dialogue model based on ...', ...]          |\n| Text-Generation      | text         | [ gpt2 , ...]                                           | [' Pretrained model on English ...', ...]                                        |\n| Tabular-CLS          | text         | [ matth/flowformer , ...]                               | [' Automatic detection of blast cells in ALL data using transformers. ...', ...] |\n| CV Tasks             | CV Tasks     | CV Tasks                                                | CV Tasks                                                                         |\n| Image-to-Text        | image        | [ nlpconnect/vit-gpt2-image- captioning , ...]          | [' This is an image captioning model trained by @ydshieh in flax ...', ...]      |\n| Text-to-Image        | image        | [ runwayml/stable-diffusion- v1-5 , ...]                | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| VQA                  | text + image | [ dandelin/vilt-b32- finetuned-vqa , ...]               | [' Vision-and-Language Transformer (ViLT) model fine-tuned on ...', ...]         |\n| Segmentation         | image        | [ facebook/detr-resnet-50- panoptic , ...]              | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| DQA                  | text + image | [ impira/layoutlm- document-qa , ...]                   | [' This is a fine-tuned version of the multi-modal LayoutLM model ...', ...]     |\n| Image-CLS            | image        | [ microsoft/resnet-50 , ...]                            | [' ResNet model pre-trained on ...', ...]                                        |\n| Image-to-image       | image        | [ radames/stable-diffusion- v1-5-img2img , ...]         | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| Object-Detection     | image        | [ facebook/detr-resnet-50 , ...]                        | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| ControlNet-SD        | image        | [ lllyasviel/sd-controlnet- canny , ...]                | [' ControlNet is a neural network structure to control diffusion ...', ...]      |\n| Audio Tasks          | Audio Tasks  | Audio Tasks                                             | Audio Tasks                                                                      |\n| Text-to-Speech       | text         | [ espnet/kan- bayashi_ljspeech_vits , ...]              | [' his model was trained by kan-bayashi using ljspeech/tts1 recipe in ...', ...] |\n| Audio-CLS            | audio        | [ TalTechNLP/voxlingua107- epaca-tdnn , ...]            | [' This is a spoken language recognition model trained on the ...', ...]         |\n| ASR                  | audio        | [ jonatasgrosman/wav2vec2- large-xlsr-53-english , ...] | [' Fine-tuned XLSR-53 large model for speech recognition in English ...', ...]   |\n| Audio-to-Audio       | audio        | [ speechbrain/metricgan- plus-voicebank , ...]          | [' MetricGAN-trained model for Enhancement ...', ...]                            |\n| Video Tasks          | Video Tasks  | Video Tasks                                             | Video Tasks                                                                      |\n| Text-to-Video        | text         | [ damo-vilab/text-to-video- ms-1.7b , ...]              | [' his model is based on a multi-stage text-to-video generation ...', ...]       |\n| Video-CLS            | video        | [ MCG-NJU/videomae- base , ...]                         | [' VideoMAE model pre-trained on Kinetics-400 for 1600 epochs ...', ...]         |\n\nFigure 6: Case study on complex tasks (a).\n\n<!-- image -->\n\nFigure 7: Case study on complex tasks (b).\n\n<!-- image -->\n\nQuery: Can you tell me how many objects in the picture example1.jpg?\n\nResponse: There are 8 objects in the picture. I used the model facebook/detr-resnet-101 for this task which is trained on a popular object detection dataset called COCO, and it has the highest number of downloads. It generated an image with predicted boxes that is available under the path: /images/79f2.jpg . As for image captioning, I selected the model nlpconnect/vit-gpt2-image-captioning which is more frequently used and works better in this case. The generated text from the image is ' a large jetliner sitting on top of an airport tarmac. '\n\n## Task Planning:\n\n1: {\"task\": \"object-detection\", \"id\": 0, dep: [-1], \"args\": {\"image\":\n\n\"example1.jpg\"}},\n\n2: {\"task\": \"image-to-text\", \"id\": 1, dep: [-1], \"args\": {\"image\":\n\n\"example1.jpg\"}}\n\n## Execution Results:\n\n1. [{\"generated image with predicted box\": \"/images/79f2.jpg\", \"predicted\":  [{\"box\": {\"xmax\": 463, \"xmin\": 373, \"ymax\": 267, \"ymin\": 199}, \"label\": \"bus\", \"score\": 0.9981155395507812}, {\"box\": {\"xmax\": 292, \"xmin\": 124, \"ymax\": 224, \"ymin\": 175}, \"label\": \"airplane\", \"score\": 0.9983609318733215}, {\"box\": {\"xmax\": 554, \"xmin\": 487, \"ymax\": 277, \"ymin\": 226}, \"label\": \"person\", \"score\": 0.9910836219787598}, {\"box\": {\"xmax\": 230, \"xmin\": 219, \"ymax\": 264, \"ymin\": 227}, \"label\": \"person\", \"score\": 0.998579740524292}, {\"box\": {\"xmax\": 361, \"xmin\": 330, \"ymax\": 231, \"ymin\": 220}, \"label\": \"truck\", \"score\": 0.9066138863563538}, {\"box\": {\"xmax\": 639, \"xmin\": 460, \"ymax\": 420, \"ymin\": 142}, \"label\": \"bus\", \"score\": 0.9987366795539856}, {\"box\": {\"xmax\": 363, \"xmin\": 279, \"ymax\": 222, \"ymin\": 192}, \"label\": \"airplane\", \"score\": 0.9976897239685059}, {\"box\": {\"xmax\": 317, \"xmin\": 287, \"ymax\": 230, \"ymin\": 219}, \"label\": \"truck\", \"score\": 0.9339178800582886}]},\n\n2. {\"generated\\_text\": \"a large jetliner sitting on top of an airport tarmac \"}\n\nQuery: In example2.jpg, what's the animal and what's it doing?\n\nResponse: Sure, the animal in the image is a tiger and it's laying down. I performed 3 tasks including image-to-text, image-classification, and visual-question-answering on the image you provided. To accomplish this, I selected models nlpconnect/vit-gpt2-image-captioning , google/ vit-base-patch16-224 and dandelin/vilt-b32-finetuned-vqa because they are the perfect models for this use case respectively. According to the results, I have 88% confidence that the animal is a tiger and also have a 77.8% confidence that it's laying down .The generated image with predicted box is stored at the path: /images/d59a.jpg .\n\n## Task Planning:\n\n1: {\"task\": \"image-classification\", \"id\": 0, dep: [-1], \"args\": {\"image\": \"example2.jpg\" }},\n\n2: {\"task\": \"visual-question-answering\", \"id\": 1, dep: [-1], \"args\": {\"text\": \"What is the animal doing?\",\n\n\"image\": \"example2.jpg\" }}\n\n3: {\"task\":\"image-to-text\", \"id\": 2, dep: [-1], \"args\": {\"image\": \"example2.jpg\" }}\n\n## Execution Results:\n\n1. [{'score': 0.8858247399330139, 'label': 'tiger, Panthera tigris'}, {'score': 0.10940514504909515, 'label': 'tiger cat'}, {'score': 0.0006216467591002584, 'label': 'jaguar, panther, Panthera onca, Felis onca'}, {'score': 0.0004262701258994639, 'label': 'dhole, Cuon alpinus'}, {'score': 0.00030842673731967807, 'label': 'lion, king of beasts, Panthera leo'}]\n\n2. [{'answer': 'sitting', 'score': 0.8294351696968079}, {'answer': 'laying down', 'score': 0.7387925386428833}, {'answer': 'resting', 'score': 0.625247061252594}, {'answer': 'laying', 'score': 0.6114301681518555}, {'answer':\n\n'relaxing', 'score': 0.44740357995033264}]\n\n3. {'generated\\_text': 'a brown and white striped zebra laying on a rock'}\n\nFigure 8: Case study on complex tasks (c).\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 9: Qualitative analysis of multi-model cooperation with resource dependency.\n\n<!-- image -->\n\nFigure 10: Qualitative analysis of multi-model cooperation on video and audio modalities.\n\n<!-- image -->\n\nFigure 11: Qualitative analysis of multi-model cooperation with multiple sources.\n\n<!-- image -->",
  "tables": [
    {
      "index": 0,
      "markdown": "|                     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                     | available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | available as {{ Chat Logs }}, where you can trace the user-mentioned into the task planning stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                     | Can you tell me how many objects in e1.jpg?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Demonstrations [{ \"task\" : \"object-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e1.jpg\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|                     | In e2.jpg, what's the animal and what's it doing?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [{ \"task\" : \"image-to-text\", \"id\" : 0, \"dep\" :[-1], \"args\" : { \"im age\" : \"e2.jpg\" }}, { \"task\" :\"image-cls\", \"id\" : 1, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" :\"object-detection\", \"id\" : 2, \"dep\" : [-1], \"args\" : { \"image\" : \"e2.jpg\" }}, { \"task\" : \"vi- sual-quesrion-answering\", \"id\" : 3, \"dep\" :[-1], \"args\" : { \"text\" : \"what's the animal doing?\", \"image\" : \"e2.jpg\" }}]                                                                                                                                                                                                                                                               |\n|                     | First generate a HED image of e3.jpg, then based on the HED image and a text 'a girl reading a book', create a new image as a response.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | [{ \"task\" : \"pose-detection\", \"id\" : 0, \"dep\" : [-1], \"args\" : { \"im age\" : \"e3.jpg\" }}, { \"task\" : \"pose-text-to-image\", \"id\" : 1, \"dep\" : [0], \"args\" : { \"text\" : \"a girl reading a book\", \"image\" : \"<re- source>-0\" }}]                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| Model Selection     | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Model Selection     | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            | #2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: { \"id\" : \"id\", \"reason\" : \"your detail reason for the choice\"}. We have a list of models for                                                                                                                                                                                                                                                            |\n| Model Selection     | Candidate Models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Candidate Models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Model Selection     | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           | {\"model_id\": model id #1, \"metadata\": meta-info #1, \"description\": description of model #1} {\"model_id\": model id #2, \"metadata\": meta-info #2, \"description\": description of model #2} · · · · · · · · · {\"model_id\": model id # K , \"metadata\": meta-info # K , \"description\": description of model # K }                                                                                                                                                                                                                                                                                                                                                           |\n| Response Generation | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Response Generation | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. | #4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it. |"
    },
    {
      "index": 1,
      "markdown": "| LLM       |   Acc ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|---------|---------|------------|--------|\n| Alpaca-7b |    6.48 |   35.6  |       6.64 |   4.88 |\n| Vicuna-7b |   23.86 |   45.51 |      26.51 |  29.44 |\n| GPT-3.5   |   52.62 |   62.12 |      52.62 |  54.45 |"
    },
    {
      "index": 2,
      "markdown": "| LLM       |   ED ↓ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|--------|---------|------------|--------|\n| Alpaca-7b |   0.83 |   22.27 |      23.35 |  22.8  |\n| Vicuna-7b |   0.8  |   19.15 |      28.45 |  22.89 |\n| GPT-3.5   |   0.54 |   61.09 |      45.15 |  51.92 |"
    },
    {
      "index": 3,
      "markdown": "| LLM       |   GPT-4 Score ↑ |   Pre ↑ |   Recall ↑ |   F1 ↑ |\n|-----------|-----------------|---------|------------|--------|\n| Alpaca-7b |           13.14 |   16.18 |      28.33 |  20.59 |\n| Vicuna-7b |           19.17 |   13.97 |      28.08 |  18.66 |\n| GPT-3.5   |           50.48 |   54.9  |      49.23 |  51.91 |"
    },
    {
      "index": 4,
      "markdown": "| LLM       | Sequential Task   | Sequential Task   | Graph Task   | Graph Task   |\n|-----------|-------------------|-------------------|--------------|--------------|\n|           | Acc ↑             | ED ↓              | Acc ↑        | F1 ↑         |\n| Alpaca-7b | 0                 | 0.96              | 4.17         | 4.17         |\n| Vicuna-7b | 7.45              | 0.89              | 10.12        | 7.84         |\n| GPT-3.5   | 18.18             | 0.76              | 20.83        | 16.45        |\n| GPT-4     | 41.36             | 0.61              | 58.33        | 49.28        |"
    },
    {
      "index": 5,
      "markdown": "| Demo Variety (# task types)   | LLM     | Single Task   | Single Task   | Sequencial Task   | Sequencial Task   | Graph Task   |\n|-------------------------------|---------|---------------|---------------|-------------------|-------------------|--------------|\n| Demo Variety (# task types)   | LLM     | Acc ↑         | F1 ↑          | ED (%) ↓          | F1 ↑              | F1 ↑         |\n| 2                             | GPT-3.5 | 43.31         | 48.29         | 71.27             | 32.15             | 43.42        |\n| 2                             | GPT-4   | 65.59         | 67.08         | 47.17             | 55.13             | 53.96        |\n| 6                             | GPT-3.5 | 51.31         | 51.81         | 60.81             | 43.19             | 58.51        |\n| 6                             | GPT-4   | 66.83         | 68.14         | 42.20             | 58.18             | 64.34        |\n| 10                            | GPT-3.5 | 52.83         | 53.70         | 56.52             | 47.03             | 64.24        |\n| 10                            | GPT-4   | 67.52         | 71.05         | 39.32             | 60.80             | 66.90        |"
    },
    {
      "index": 6,
      "markdown": "| LLM        | Task Planning   | Task Planning   | Model Selection   | Model Selection   | Response       |\n|------------|-----------------|-----------------|-------------------|-------------------|----------------|\n| LLM        | Passing Rate ↑  | Rationality ↑   | Passing Rate ↑    | Rationality ↑     | Success Rate ↑ |\n| Alpaca-13b | 51.04           | 32.17           | -                 | -                 | 6.92           |\n| Vicuna-13b | 79.41           | 58.41           | -                 | -                 | 15.64          |\n| GPT-3.5    | 91.22           | 78.47           | 93.89             | 84.29             | 63.08          |"
    },
    {
      "index": 7,
      "markdown": "| Name   | Definitions                                                                                                                                                                                                                                                                                                                                                 |\n|--------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"task\" | It represents the type of the parsed task. It covers different tasks in language, visual, video, audio, etc. The currently supported task list of HuggingGPT is shown in Table 13.                                                                                                                                                                          |\n| \"id\"   | The unique identifier for task planning, which is used for references to dependent tasks and their generated resources.                                                                                                                                                                                                                                     |\n| \"dep\"  | It defines the pre-requisite tasks required for execution. The task will be launched only when all the pre-requisite dependent tasks are finished.                                                                                                                                                                                                          |\n| \"args\" | It contains the list of required arguments for task execution. It contains three subfields populated with text, image, and audio resources according to the task type. They are resolved from either the user's request or the generated resources of the dependent tasks. The corresponding argument types for different task types are shown in Table 13. |"
    },
    {
      "index": 8,
      "markdown": "| Datasets        | Number of Requests by Type   | Number of Requests by Type   | Number of Requests by Type   | Request Length   | Request Length   | Number of Tasks   | Number of Tasks   |\n|-----------------|------------------------------|------------------------------|------------------------------|------------------|------------------|-------------------|-------------------|\n|                 | Single                       | Sequential                   | Graph                        | Max              | Average          | Max               | Average           |\n| GPT-4-annotated | 1,450                        | 1,917                        | 130                          | 52               | 13.26            | 13                | 1.82              |\n| Human-annotated | -                            | 24                           | 22                           | 95               | 10.20            | 12                | 2.00              |"
    },
    {
      "index": 9,
      "markdown": "| Name                     | Scenarios   | Planning           | Tools                              |\n|--------------------------|-------------|--------------------|------------------------------------|\n| BabyAGI AgentGPT AutoGPT | Daily       | Iterative Planning | - - Web Search, Code Executor, ... |\n| HuggingGPT               | AI area     | Global Planning    | Models in Hugging Face             |"
    },
    {
      "index": 10,
      "markdown": "| Task                 | Args         | Candidate Models                                        | Descriptions                                                                     |\n|----------------------|--------------|---------------------------------------------------------|----------------------------------------------------------------------------------|\n| NLP Tasks            | NLP Tasks    | NLP Tasks                                               | NLP Tasks                                                                        |\n| Text-CLS             | text         | [ cardiffnlp/twitter-roberta- base-sentiment , ...]     | [' This is a RoBERTa-base model trained on 58M tweets ...', ...]                 |\n| Token-CLS            | text         | [ dslim/bert-base-NER , ...]                            | [' bert-base-NER is a fine-tuned BERT model that is ready to ...', ...]          |\n| Text2text-Generation | text         | [ google/flan-t5-xl , ...]                              | [' If you already know T5, FLAN-T5 is just better at everything ...', ...]       |\n| Summarization        | text         | [ bart-large-cnn , ...]                                 | [ ' BART model pre-trained on English language, and fine-tuned ...', ...]        |\n| Translation          | text         | [ t5-base , ...]                                        | [' With T5, we propose reframing all NLP tasks into a unified ...', ...]         |\n| Question-Answering   | text         | [ deepset/roberta-base- squad2 , ...]                   | [' This is the roberta-base model, fine-tuned using the SQuAD2.0 ...', ...]      |\n| Conversation         | text         | [ PygmalionAI/pygmalion- 6b , ...]                      | [' Pymalion 6B is a proof-of-concept dialogue model based on ...', ...]          |\n| Text-Generation      | text         | [ gpt2 , ...]                                           | [' Pretrained model on English ...', ...]                                        |\n| Tabular-CLS          | text         | [ matth/flowformer , ...]                               | [' Automatic detection of blast cells in ALL data using transformers. ...', ...] |\n| CV Tasks             | CV Tasks     | CV Tasks                                                | CV Tasks                                                                         |\n| Image-to-Text        | image        | [ nlpconnect/vit-gpt2-image- captioning , ...]          | [' This is an image captioning model trained by @ydshieh in flax ...', ...]      |\n| Text-to-Image        | image        | [ runwayml/stable-diffusion- v1-5 , ...]                | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| VQA                  | text + image | [ dandelin/vilt-b32- finetuned-vqa , ...]               | [' Vision-and-Language Transformer (ViLT) model fine-tuned on ...', ...]         |\n| Segmentation         | image        | [ facebook/detr-resnet-50- panoptic , ...]              | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| DQA                  | text + image | [ impira/layoutlm- document-qa , ...]                   | [' This is a fine-tuned version of the multi-modal LayoutLM model ...', ...]     |\n| Image-CLS            | image        | [ microsoft/resnet-50 , ...]                            | [' ResNet model pre-trained on ...', ...]                                        |\n| Image-to-image       | image        | [ radames/stable-diffusion- v1-5-img2img , ...]         | [' Stable Diffusion is a latent text-to-image diffusion model ...', ...]         |\n| Object-Detection     | image        | [ facebook/detr-resnet-50 , ...]                        | [' DEtection TRansformer (DETR) model trained end-to-end on ...', ...]           |\n| ControlNet-SD        | image        | [ lllyasviel/sd-controlnet- canny , ...]                | [' ControlNet is a neural network structure to control diffusion ...', ...]      |\n| Audio Tasks          | Audio Tasks  | Audio Tasks                                             | Audio Tasks                                                                      |\n| Text-to-Speech       | text         | [ espnet/kan- bayashi_ljspeech_vits , ...]              | [' his model was trained by kan-bayashi using ljspeech/tts1 recipe in ...', ...] |\n| Audio-CLS            | audio        | [ TalTechNLP/voxlingua107- epaca-tdnn , ...]            | [' This is a spoken language recognition model trained on the ...', ...]         |\n| ASR                  | audio        | [ jonatasgrosman/wav2vec2- large-xlsr-53-english , ...] | [' Fine-tuned XLSR-53 large model for speech recognition in English ...', ...]   |\n| Audio-to-Audio       | audio        | [ speechbrain/metricgan- plus-voicebank , ...]          | [' MetricGAN-trained model for Enhancement ...', ...]                            |\n| Video Tasks          | Video Tasks  | Video Tasks                                             | Video Tasks                                                                      |\n| Text-to-Video        | text         | [ damo-vilab/text-to-video- ms-1.7b , ...]              | [' his model is based on a multi-stage text-to-video generation ...', ...]       |\n| Video-CLS            | video        | [ MCG-NJU/videomae- base , ...]                         | [' VideoMAE model pre-trained on Kinetics-400 for 1600 epochs ...', ...]         |"
    }
  ],
  "stats": {
    "pages": 27,
    "chunksCreated": 129,
    "totalCharacters": 90852,
    "totalWords": 11489,
    "numTables": 11,
    "processingTimeMs": 30965
  }
}