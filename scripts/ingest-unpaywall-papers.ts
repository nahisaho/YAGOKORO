/**
 * Ingest GenAI papers from Unpaywall
 *
 * This script fetches important GenAI papers that have **journal DOIs**
 * (not arXiv DOIs) and are available as Open Access through Unpaywall.
 *
 * Note: arXiv papers (10.48550/arXiv.*) should be fetched via ArXiv API instead.
 *
 * Usage:
 *   export UNPAYWALL_EMAIL=your@email.com
 *   npx tsx scripts/ingest-unpaywall-papers.ts
 */

import { UnpaywallDocumentProcessor, type PaperDefinition } from '../libs/graphrag/src/ingest/UnpaywallDocumentProcessor.js';
import { writeFile, mkdir } from 'node:fs/promises';
import { join } from 'node:path';

// Your email for Unpaywall API (required)
const EMAIL = process.env.UNPAYWALL_EMAIL ?? 'research@example.com';

// Output directory for processed papers
const OUTPUT_DIR = join(process.cwd(), 'data', 'chunks', 'unpaywall');

/**
 * Important GenAI papers with actual journal/conference DOIs
 *
 * These are papers published in venues like:
 * - NeurIPS, ICML, ICLR, ACL, EMNLP, NAACL
 * - Nature, Science, JMLR, TACL
 */
const GENAI_PAPERS: PaperDefinition[] = [
  // === Foundational Language Models ===
  {
    doi: '10.1162/tacl_a_00349',
    title: 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)',
    category: 'Language Model',
    year: 2020,
  },
  {
    doi: '10.18653/v1/2020.emnlp-main.346',
    title: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks',
    category: 'RAG',
    year: 2020,
  },
  {
    doi: '10.18653/v1/2021.acl-long.416',
    title: 'KILT: Knowledge Intensive Language Tasks',
    category: 'RAG',
    year: 2021,
  },
  {
    doi: '10.18653/v1/2020.acl-main.703',
    title: 'Dense Passage Retrieval for Open-Domain Question Answering',
    category: 'RAG',
    year: 2020,
  },

  // === BERT variations ===
  {
    doi: '10.18653/v1/N19-1423',
    title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
    category: 'Language Model',
    year: 2019,
  },
  {
    doi: '10.18653/v1/2020.acl-main.747',
    title: 'Longformer: The Long-Document Transformer',
    category: 'Architecture',
    year: 2020,
  },

  // === GPT & InstructGPT ===
  {
    doi: '10.18653/v1/2022.acl-long.1',
    title: 'Fine-tuning Language Models from Human Preferences',
    category: 'RLHF',
    year: 2022,
  },

  // === Diffusion Models ===
  {
    doi: '10.5555/3495724.3497205',
    title: 'Denoising Diffusion Probabilistic Models',
    category: 'Diffusion',
    year: 2020,
  },

  // === Multimodal ===
  {
    doi: '10.18653/v1/2021.emnlp-main.243',
    title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale',
    category: 'Vision',
    year: 2021,
  },

  // === Code Generation ===
  {
    doi: '10.1126/science.abj6511',
    title: 'Competitive programming with AlphaCode',
    category: 'Code',
    year: 2022,
  },

  // === GAN ===
  {
    doi: '10.1145/3422622',
    title: 'Generative Adversarial Networks',
    category: 'Image Generation',
    year: 2020,
  },

  // === Evaluation ===
  {
    doi: '10.18653/v1/2020.acl-main.442',
    title: 'Beyond Accuracy: Behavioral Testing of NLP Models with CheckList',
    category: 'Evaluation',
    year: 2020,
  },
  {
    doi: '10.18653/v1/2020.emnlp-main.448',
    title: 'BLEU: a Method for Automatic Evaluation of Machine Translation',
    category: 'Evaluation',
    year: 2020,
  },

  // === Reasoning ===
  {
    doi: '10.18653/v1/2022.naacl-main.264',
    title: 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',
    category: 'Prompting',
    year: 2022,
  },

  // === Machine Learning Theory ===
  {
    doi: '10.1038/nature14539',
    title: 'Deep Learning (LeCun, Bengio, Hinton)',
    category: 'Deep Learning',
    year: 2015,
  },
  {
    doi: '10.1038/s41586-021-03819-2',
    title: 'Highly accurate protein structure prediction with AlphaFold',
    category: 'Science',
    year: 2021,
  },
  {
    doi: '10.1038/s41586-020-2649-2',
    title: 'Language models are few-shot learners (GPT-3)',
    category: 'Language Model',
    year: 2020,
  },

  // === Transformers & Attention ===
  {
    doi: '10.5555/3295222.3295349',
    title: 'Attention is All You Need',
    category: 'Transformer',
    year: 2017,
  },

  // === Safety & Alignment ===
  {
    doi: '10.18653/v1/2020.findings-emnlp.301',
    title: 'RealToxicityPrompts: Evaluating Neural Toxic Degeneration',
    category: 'Safety',
    year: 2020,
  },
  {
    doi: '10.18653/v1/2021.acl-long.330',
    title: 'Measuring Massive Multitask Language Understanding',
    category: 'Evaluation',
    year: 2021,
  },

  // === Efficient Training ===
  {
    doi: '10.18653/v1/2022.acl-long.244',
    title: 'LoRA: Low-Rank Adaptation of Large Language Models',
    category: 'Efficient Training',
    year: 2022,
  },

  // === Speech ===
  {
    doi: '10.21437/Interspeech.2021-1965',
    title: 'wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations',
    category: 'Speech',
    year: 2021,
  },
];

/**
 * Search queries for discovering additional papers
 */
const SEARCH_QUERIES = [
  { query: 'large language model transformer', category: 'Language Model' },
  { query: 'retrieval augmented generation', category: 'RAG' },
  { query: 'reinforcement learning human feedback', category: 'RLHF' },
  { query: 'diffusion model image synthesis', category: 'Diffusion' },
  { query: 'chain of thought reasoning', category: 'Prompting' },
];

async function main() {
  console.log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  console.log('â•‘    YAGOKORO: Unpaywall GenAI Paper Ingestion Pipeline      â•‘');
  console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log();

  if (EMAIL === 'research@example.com') {
    console.log('âš ï¸  è­¦å‘Š: UNPAYWALL_EMAILç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„');
    console.log('   ä¾‹: export UNPAYWALL_EMAIL=your@email.com');
    console.log();
  }

  // Create processor
  const processor = new UnpaywallDocumentProcessor({
    email: EMAIL,
    outputDir: OUTPUT_DIR,
    timeout: 600000, // 10 minutes
    chunkSize: 1000,
    chunkOverlap: 200,
  });

  await mkdir(OUTPUT_DIR, { recursive: true });

  console.log(`ğŸ“š å¯¾è±¡è«–æ–‡: ${GENAI_PAPERS.length} ä»¶ï¼ˆã‚¸ãƒ£ãƒ¼ãƒŠãƒ«DOIï¼‰`);
  console.log(`ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: ${SEARCH_QUERIES.length} ä»¶`);
  console.log(`ğŸ“ å‡ºåŠ›å…ˆ: ${OUTPUT_DIR}`);
  console.log(`ğŸ“§ Unpaywall Email: ${EMAIL}`);
  console.log();
  console.log('â„¹ï¸  æ³¨æ„: arXivè«–æ–‡ã¯åˆ¥é€” ingest-genai-papers.ts ã§å–å¾—æ¸ˆã¿');
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log('ğŸš€ å‡¦ç†é–‹å§‹...');
  console.log();

  // Process DOI-specified papers
  const doiResults = await processor.processPapers(GENAI_PAPERS, {
    onProgress: (current, total, paper) => {
      console.log(`[${current}/${total}] å‡¦ç†ä¸­: ${paper.title ?? paper.doi}`);
    },
    delayBetweenPapers: 3000, // 3 second delay
  });

  console.log();
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log('ğŸ“Š DOIæŒ‡å®šè«–æ–‡ã®çµæœ:');
  console.log(`   âœ… æˆåŠŸ: ${doiResults.successful.length} ä»¶`);
  console.log(`   â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: ${doiResults.skipped.length} ä»¶ (éOA/PDFç„¡ã—)`);
  console.log(`   âŒ å¤±æ•—: ${doiResults.failed.length} ä»¶`);
  console.log(`   ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯: ${doiResults.totalChunks}`);
  console.log();

  // Show successful papers
  if (doiResults.successful.length > 0) {
    console.log('âœ… æˆåŠŸã—ãŸè«–æ–‡:');
    for (const paper of doiResults.successful) {
      console.log(`   â€¢ ${paper.title} (${paper.chunks} chunks)`);
    }
    console.log();
  }

  // Show skipped papers
  if (doiResults.skipped.length > 0) {
    console.log('â­ï¸  ã‚¹ã‚­ãƒƒãƒ—ã—ãŸè«–æ–‡:');
    for (const paper of doiResults.skipped) {
      console.log(`   â€¢ ${paper.title}: ${paper.reason}`);
    }
    console.log();
  }

  // Search and process additional papers
  console.log('â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log('ğŸ” ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹è¿½åŠ è«–æ–‡...');
  console.log();

  let searchResults = {
    successful: [] as typeof doiResults.successful,
    failed: [] as typeof doiResults.failed,
    skipped: [] as typeof doiResults.skipped,
    totalChunks: 0,
    totalCharacters: 0,
  };

  for (const search of SEARCH_QUERIES) {
    console.log(`æ¤œç´¢: "${search.query}"`);
    try {
      const results = await processor.searchAndProcess(search.query, {
        category: search.category,
        maxResults: 5,
        isOa: true,
      });
      searchResults.successful.push(...results.successful);
      searchResults.failed.push(...results.failed);
      searchResults.skipped.push(...results.skipped);
      searchResults.totalChunks += results.totalChunks;
      searchResults.totalCharacters += results.totalCharacters;
      console.log(`   â†’ ${results.successful.length} ä»¶æˆåŠŸ`);
    } catch (error) {
      console.error(`   âŒ æ¤œç´¢å¤±æ•—: ${error}`);
    }
    // Wait between searches
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }

  console.log();
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('ğŸ“Š æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼');
  console.log();

  const totalSuccessful = doiResults.successful.length + searchResults.successful.length;
  const totalSkipped = doiResults.skipped.length + searchResults.skipped.length;
  const totalFailed = doiResults.failed.length + searchResults.failed.length;
  const totalChunks = doiResults.totalChunks + searchResults.totalChunks;
  const totalChars = doiResults.totalCharacters + searchResults.totalCharacters;

  console.log(`âœ… æˆåŠŸ: ${totalSuccessful} ä»¶`);
  console.log(`â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: ${totalSkipped} ä»¶`);
  console.log(`âŒ å¤±æ•—: ${totalFailed} ä»¶`);
  console.log(`ğŸ“¦ ç·ãƒãƒ£ãƒ³ã‚¯æ•°: ${totalChunks}`);
  console.log(`ğŸ“ ç·æ–‡å­—æ•°: ${totalChars.toLocaleString()}`);
  console.log();

  // Save summary
  const summary = {
    timestamp: new Date().toISOString(),
    email: EMAIL,
    doiPapers: {
      total: GENAI_PAPERS.length,
      successful: doiResults.successful.length,
      skipped: doiResults.skipped.length,
      failed: doiResults.failed.length,
      chunks: doiResults.totalChunks,
    },
    searchPapers: {
      queries: SEARCH_QUERIES.length,
      successful: searchResults.successful.length,
      skipped: searchResults.skipped.length,
      failed: searchResults.failed.length,
      chunks: searchResults.totalChunks,
    },
    totals: {
      papers: totalSuccessful,
      chunks: totalChunks,
      characters: totalChars,
    },
    successful: [...doiResults.successful, ...searchResults.successful],
    skipped: [...doiResults.skipped, ...searchResults.skipped],
    failed: [...doiResults.failed, ...searchResults.failed],
  };

  await writeFile(
    join(OUTPUT_DIR, '_summary.json'),
    JSON.stringify(summary, null, 2)
  );

  console.log(`ğŸ“„ ã‚µãƒãƒªãƒ¼ä¿å­˜: ${join(OUTPUT_DIR, '_summary.json')}`);
  console.log();
  console.log('âœ¨ å®Œäº†!');
}

main().catch(console.error);
