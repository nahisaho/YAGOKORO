{
  "arxivId": "2112.04426",
  "title": "RETRO: Retrieval-Enhanced Transformers",
  "category": "RAGãƒ»çŸ¥è­˜çµ±åˆ",
  "year": 2021,
  "paper": {
    "id": "2112.04426v3",
    "title": "Improving language models by retrieving from trillions of tokens",
    "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
    "authors": [
      "Sebastian Borgeaud",
      "Arthur Mensch",
      "Jordan Hoffmann",
      "Trevor Cai",
      "Eliza Rutherford",
      "Katie Millican",
      "George van den Driessche",
      "Jean-Baptiste Lespiau",
      "Bogdan Damoc",
      "Aidan Clark",
      "Diego de Las Casas",
      "Aurelia Guy",
      "Jacob Menick",
      "Roman Ring",
      "Tom Hennigan",
      "Saffron Huang",
      "Loren Maggiore",
      "Chris Jones",
      "Albin Cassirer",
      "Andy Brock",
      "Michela Paganini",
      "Geoffrey Irving",
      "Oriol Vinyals",
      "Simon Osindero",
      "Karen Simonyan",
      "Jack W. Rae",
      "Erich Elsen",
      "Laurent Sifre"
    ],
    "published": "2021-12-08T17:32:34.000Z",
    "updated": "2022-02-07T21:07:59.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2112.04426v3",
    "absUrl": "https://arxiv.org/abs/2112.04426v3"
  },
  "chunks": [
    {
      "id": "2112.04426v3-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "chunkIndex": 0,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-1",
      "content": "Sebastian Borgeaud y , Arthur Mensch y , Jordan Hoffmann y , Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, z z y , z\n\nKaren Simonyan, Jack W. Rae , Erich Elsen and Laurent Sifre\n\nAll authors from DeepMind, y Equal contributions, z Equal senior authorship\n\nWe enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (Retro) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 GLYPH&lt;2&gt; fewer parameters.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Improving language models by retrieving from trillions of tokens",
        "chunkIndex": 1,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-2",
      "content": "kens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (Retro) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 GLYPH&lt;2&gt; fewer parameters. After fine-tuning, Retro performance translates to downstream knowledge-intensive tasks such as question answering. Retro combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train Retro from scratch, yet can also rapidly Retrofit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Improving language models by retrieving from trillions of tokens",
        "chunkIndex": 2,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-3",
      "content": "Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions ğ‘ ' ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' = Ë› ğ‘– ğ‘ ' ğ‘¥ ğ‘– j ğ‘¥ GLYPH&lt;157&gt; ğ‘– ' . Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 3,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-4",
      "content": "o over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data.\n\nIn this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 4,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-5",
      "content": "e models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach. At a high level, our Retrieval Transformer (Retro) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers (100 millions parameters) and databases of limited size (up to billions of tokens) (Guu et al., 2020; Khandelwal et al., 2020; Lewis et al., 2020; Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 5,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-6",
      "content": "al., 2020; Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main\n\nFigure 1 j Scaling of Retro. The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by GLYPH&lt;24&gt; 10 GLYPH&lt;2&gt; . The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to degrade, perhaps due to the reduced quality. At evaluation Retro can be used without retrieval data (Retro[OFF]), bringing limited performance degradation compared to baseline transformers.\n\n<!-- image -->\n\ncontributions are the following.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 6,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-7",
      "content": "lity. At evaluation Retro can be used without retrieval data (Retro[OFF]), bringing limited performance degradation compared to baseline transformers.\n\n<!-- image -->\n\ncontributions are the following.\n\n- We introduce Retro, a retrieval-enhanced autoregressive language model (Â§2.2). We use a chunked cross-attention module to incorporate the retrieved text (Â§2.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen Bert model (Â§2.3) works at scale, removing the need for training and updating a retriever network.\n- We propose an evaluation aware of proximity of test documents with the training set (Â§2.6), addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 7,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-8",
      "content": "of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of Retro comes from both explicit neighbour copying and general knowledge extraction (Â§4.4).\n- We show that our method scales well with model size and database size (Fig. 1): Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (Â§4). We show that Retro can be fine-tuned to achieve competitive performance on downstream tasks such as question answering (Â§4.3).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 8,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-9",
      "content": ". We show that Retro can be fine-tuned to achieve competitive performance on downstream tasks such as question answering (Â§4.3).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "1. Introduction",
        "chunkIndex": 9,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-10",
      "content": "We design our retrieval-enhanced architecture to be capable of retrieving from a database with trillions of tokens. For this purpose, we retrieve at the level of contiguous token chunks instead of individual tokens which reduces storage and computation requirements by a large linear factor. Our method first constructs a key-value database, where values store raw chunks of text tokens and keys are frozen Bert embedddings (Devlin et al., 2019). We use a frozen model to avoid having to periodically re-compute embeddings over the entire database during training. Each training sequence is then split into chunks, which are augmented with their ğ‘˜ -nearest neighbour retrieved from the database. An encoder-decoder architecture integrates retrieval chunks into the model's predictions. We summarize the Retro architecture in Fig. 2, and detail it in this section. We end the section by introducing",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2. Method",
        "chunkIndex": 10,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-11",
      "content": "n encoder-decoder architecture integrates retrieval chunks into the model's predictions. We summarize the Retro architecture in Fig. 2, and detail it in this section. We end the section by introducing\n\nFigure 2 j Retro architecture. Left: simplified version where a sequence of length ğ‘› = 12 is split into ğ‘™ = 3 chunks of size ğ‘š = 4. For each chunk, we retrieve ğ‘˜ = 2 neighbours of ğ‘Ÿ = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the Cca operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk.\n\n<!-- image -->\n\n<!-- image -->\n\na new methodology to evaluate language models when an evaluation set is partially present in the training set.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2. Method",
        "chunkIndex": 11,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-12",
      "content": "We use a multi-lingual version of MassiveText (Rae et al., 2021) for both training and retrieval data. The dataset consists of text documents from multiple sources and multiple languages totalling over 5 trillion tokens (detailed in Table 1). Sequences are sampled from subsets of the training data, with sampling weights given in the right-most column of Table 1. We tokenize the dataset using SentencePiece (Kudo and Richardson, 2018) with a vocabulary of 128,000 tokens. During training (unless otherwise specified), we retrieve from 600B tokens from the training data. The training retrieval database is made of the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub-sample of 4%. The evaluation retrieval database thus contains 1.75T tokens.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.1. Training dataset",
        "chunkIndex": 12,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-13",
      "content": "tion the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub-sample of 4%. The evaluation retrieval database thus contains 1.75T tokens. To limit test set leakage, we compute the 13-gram Jaccard similarity between train and test documents using the MinHash scheme and remove all training documents with high similarity (0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from our Wikipedia training data.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.1. Training dataset",
        "chunkIndex": 13,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-14",
      "content": "Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens. Formally, we consider sequences of integer tokens in ğ• = Â» 1 GLYPH&lt;148&gt; ğ‘£ â€¦ , obtained using a text tokenizer 1 . Wesplit each ğ‘› -token-long example ğ‘‹ = ' ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' into a sequence of ğ‘™ chunks ' ğ¶ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ¶ğ‘™ ' of size ğ‘š = ğ‘› ğ‘™ , i.e. ğ¶ 1 , ' ğ‘¥ 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘š ' GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ¶ğ‘™ , ' ğ‘¥ ğ‘› GLYPH&lt;0&gt; ğ‘š , 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' 2 ğ• ğ‘š . We use ğ‘› = 2048 and ğ‘š = 64. We augment each chunk ğ¶ğ‘¢ with a set Ret D' ğ¶ğ‘¢ ' of ğ‘˜ neighbours from the database D . Ret D (or",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.2. Retrieval-enhanced autoregressive token models",
        "chunkIndex": 14,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-15",
      "content": "GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘› ' 2 ğ• ğ‘š . We use ğ‘› = 2048 and ğ‘š = 64. We augment each chunk ğ¶ğ‘¢ with a set Ret D' ğ¶ğ‘¢ ' of ğ‘˜ neighbours from the database D . Ret D (or\n\n1 We use the notation Â» 1 GLYPH&lt;148&gt; ğ‘£ â€¦ , f 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘£ g throughout the text.\n\nRet for brevity) is a non-trainable operator specified in Â§2.3. Token likelihoods are provided by a model, parameterized by ğœƒ , that takes as input both previous tokens and their retrieved neighbours. This defines the following retrieval-enhanced sequence log-likelihood:\n\n<!-- formula-not-decoded -->\n\nWe set Ret ' ğ¶ 1 ' = ; , namely the likelihood of tokens from the first chunk does not depend on any retrieval data. This likelihood definition preserves autoregressivity : the probability of the ğ‘– -th token of the ğ‘¢ -th chunk, ğ‘¥ ' ğ‘¢ GLYPH&lt;0&gt; 1 ' ğ‘š , ğ‘– , only depends on previously seen tokens ' ğ‘¥ ğ‘— ' 1 6 ğ‘—GLYPH",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.2. Retrieval-enhanced autoregressive token models",
        "chunkIndex": 15,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-16",
      "content": "ihood definition preserves autoregressivity : the probability of the ğ‘– -th token of the ğ‘¢ -th chunk, ğ‘¥ ' ğ‘¢ GLYPH&lt;0&gt; 1 ' ğ‘š , ğ‘– , only depends on previously seen tokens ' ğ‘¥ ğ‘— ' 1 6 ğ‘—GLYPH&lt;157&gt; ' ğ‘¢ GLYPH&lt;0&gt; 1 ' ğ‘š , ğ‘– and on the data retrieved from the previous chunks ' Ret ' ğ¶ğ‘¢ 0 '' ğ‘¢ 0 GLYPH&lt;157&gt;ğ‘¢ . We can therefore directly sample with logprobability GLYPH&lt;18&gt; , where sampling within the chunk ğ¶ğ‘¢ is conditioned on the neighbours ' Ret ' ğ¶ğ‘¢ 0 '' ğ‘¢ 0 GLYPH&lt;157&gt;ğ‘¢ . This makes retrieval-enhanced models directly comparable with the largest language models that are evaluated by sampling.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.2. Retrieval-enhanced autoregressive token models",
        "chunkIndex": 16,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-17",
      "content": "Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two contiguous chunks of tokens which we denote Â» ğ‘GLYPH&lt;148&gt; ğ¹ â€¦ where ğ‘ is the neighbour chunk which is used to compute the key, and ğ¹ is its continuation in the original document. The corresponding key is the Bert embedding of ğ‘ , averaged over time, that we denote Bert ' ğ‘ ' . For each chunk ğ¶ , we retrieve its approximate ğ‘˜ -nearest neighbours from our key-value database using the ğ¿ 2 distance on BERT embeddings ğ‘‘ ' ğ¶GLYPH&lt;148&gt; ğ‘ ' = j j Bert ' ğ¶ ' GLYPH&lt;0&gt; Bert ' ğ‘ 'jj 2 2 . The model receives the corresponding values Ret ' ğ¶ ' , 'Â» ğ‘ 1 GLYPH&lt;148&gt; ğ¹ 1 â€¦ GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; Â» ğ‘ ğ‘˜ GLYPH&lt;148&gt; ğ¹ ğ‘˜ â€¦' . Both neighbour chunks and their continuations provide meaningful improvements, as illustrated in our ablation study (Appendix D).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.3. Nearest neighbour retrieval",
        "chunkIndex": 17,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-18",
      "content": "GLYPH&lt;147&gt; GLYPH&lt;148&gt; Â» ğ‘ ğ‘˜ GLYPH&lt;148&gt; ğ¹ ğ‘˜ â€¦' . Both neighbour chunks and their continuations provide meaningful improvements, as illustrated in our ablation study (Appendix D). We use a length 64 for both ğ‘ ğ‘— and ğ¹ ğ‘— , thus Ret ' ğ¶ ' has a shape of ğ‘˜ GLYPH&lt;2&gt; ğ‘Ÿ with ğ‘Ÿ = 128. To avoid retrieving the chunk ğ¶ğ‘¢ , 1 in the retrieval set Ret ' ğ¶ğ‘¢ ' , which would break causality during training, we filter out neighbours originating from the same document as the training sequence ğ‘‹ .\n\nFor a database of ğ‘‡ elements, we can query the approximate nearest neighbours in O' log ğ‘‡ ' time. We use the SCaNN library (Guo et al., 2020) to achieve this. This means that we can query our 2 trillion token database in 10 ms whilst evaluating or sampling from the model; this expense is amortized over a chunk length.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.3. Nearest neighbour retrieval",
        "chunkIndex": 18,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-19",
      "content": "ary (Guo et al., 2020) to achieve this. This means that we can query our 2 trillion token database in 10 ms whilst evaluating or sampling from the model; this expense is amortized over a chunk length. Performing retrieval on-the-fly is too slow to keep up with the training calculations-we leverage the frozen aspect of the embedding operator Bert to precompute all approximate nearest neighbours and save the results as part of the data. In Fig. 9 in the Appendix, we show results where we only retrieve neighbours within Wikipedia. We find that neighbours tend to come from 2-3 links away from a given article whereas random articles are more than 5 links apart.\n\nTable 1 j MassiveText . The last column indicates the sampling weight during training. The multilingual subsets include documents in 10 languages. The full breakdown is given in Â§A.1.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.3. Nearest neighbour retrieval",
        "chunkIndex": 19,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-20",
      "content": "5 links apart.\n\nTable 1 j MassiveText . The last column indicates the sampling weight during training. The multilingual subsets include documents in 10 languages. The full breakdown is given in Â§A.1.\n\n| Source    | Token count (M)   | Documents (M)   | Multilingual   | Sampling frequency   |\n|-----------|-------------------|-----------------|----------------|----------------------|\n| Web       | 977,563           | 1,208           | Yes            | 55%                  |\n| Books     | 3,423,740         | 20              | No             | 25%                  |\n| News      | 236,918           | 398             | No             | 10%                  |\n| Wikipedia | 13,288            | 23              | Yes            | 5%                   |\n| GitHub    | 374,952           | 143             | No             | 5%                   |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.3. Nearest neighbour retrieval",
        "chunkIndex": 20,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-21",
      "content": "Our model relies on an encoder-decoder transformer architecture, integrating the retrieved data through a cross-attention mechanism as introduced in Vaswani et al. (2017). First, the retrieved tokens Ret ' ğ¶ ' are fed into an encoder Transformer, which computes the encoded neighbours set ğ¸ . Denoting the intermediate activations by ğ» , our transformer decoder then interleaves Retro-blocks Retro ' ğ»GLYPH&lt;148&gt; ğ¸ ' and standard Transformer blocks LM ' ğ» ' (the hyperparameter ğ‘ƒ GLYPH&lt;18&gt; Â» 1 GLYPH&lt;148&gt; ğ¿ â€¦ determines at which layers we use a Retro-block). These blocks are built from three different residual operators with signature â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ ! â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ : a fully-connected layer Ffw, the standard sequence-level self-attention layer Attn, and a chunked cross-attention layer Cca 'GLYPH&lt;1&gt; GLYPH&lt;148&gt; ğ¸ ' that incorporates information from the retrieval encoder:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 21,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-22",
      "content": "e-level self-attention layer Attn, and a chunked cross-attention layer Cca 'GLYPH&lt;1&gt; GLYPH&lt;148&gt; ğ¸ ' that incorporates information from the retrieval encoder:\n\n<!-- formula-not-decoded -->\n\nSince Ffw, Attn and Cca are all autoregressive operators whose output at position ğ‘– only depends on ' â„ ğ‘— ' ğ‘— 6 ğ‘– , any succession of Retro and lm layers, followed by a token classification head defines an autoregressive log-likelihood (1). An overview of the model architecture is given in Algorithm 1 and in Fig. 2. We next describe the retrieval encoder and the chunked cross-attention layer in more detail, and explain how to sample from Retro.\n\nEncoding retrieval neighbours. For each chunk ğ¶ğ‘¢ , the ğ‘˜ retrieval neighbours Ret ' ğ¶ğ‘¢ ' are fed into a bi-directional transformer Encoder, yielding the outputs ğ¸ ğ‘— ğ‘¢ , Encoder ' Ret ' ğ¶ğ‘¢ ' ğ‘— GLYPH&lt;148&gt; ğ»ğ‘¢ ' 2 â„ ğ‘Ÿ GLYPH&lt;2&gt; ğ‘‘ 0 , where ğ‘— 2 Â» 1 GLYPH&lt;148&gt; ğ‘˜ â€¦ indexes each neighbour.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 22,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-23",
      "content": "ctional transformer Encoder, yielding the outputs ğ¸ ğ‘— ğ‘¢ , Encoder ' Ret ' ğ¶ğ‘¢ ' ğ‘— GLYPH&lt;148&gt; ğ»ğ‘¢ ' 2 â„ ğ‘Ÿ GLYPH&lt;2&gt; ğ‘‘ 0 , where ğ‘— 2 Â» 1 GLYPH&lt;148&gt; ğ‘˜ â€¦ indexes each neighbour. The retrieval encoder is a non-causal transformer. It is conditioned on ğ»ğ‘¢ , the activations of chunk ğ¶ğ‘¢ , through cross-attention layers; this allows the representations of the retrieval encoder to be modulated by the retrieving chunk in a differentiable way. More precisely, the encoding of the ğ‘— th neighbour of the ğ‘¢ th chunk, Ret ' ğ¶ğ‘¢ ' ğ‘— , depends on the attended activation ğ»ğ‘¢ , ' â„ ' ğ‘¢ GLYPH&lt;0&gt; 1 ' ğ‘š , ğ‘– ' ğ‘– 2Â» 1 GLYPH&lt;148&gt; ğ‘š â€¦ 2 â„ ğ‘š GLYPH&lt;2&gt; ğ‘‘ of chunk ğ¶ğ‘¢ at layer min ' ğ‘ƒ ' . All neighbours for all chunks are encoded in parallel, yielding a full encoded set ğ¸ , ' ğ¸ ğ‘— ğ‘¢ ' ğ‘¢ 2Â» 1 GLYPH&lt;148&gt;ğ‘™ â€¦ GLYPH&lt;148&gt; ğ‘— 2Â» 1 GLYPH&lt;148&gt;ğ‘˜ â€¦ 2 â„ ğ‘™ GLYPH&lt;2&gt; ğ‘˜ GLYPH&lt;2&gt; ğ‘Ÿ GLYPH&lt;2&gt; ğ‘‘ 0 .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 23,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-24",
      "content": "d in parallel, yielding a full encoded set ğ¸ , ' ğ¸ ğ‘— ğ‘¢ ' ğ‘¢ 2Â» 1 GLYPH&lt;148&gt;ğ‘™ â€¦ GLYPH&lt;148&gt; ğ‘— 2Â» 1 GLYPH&lt;148&gt;ğ‘˜ â€¦ 2 â„ ğ‘™ GLYPH&lt;2&gt; ğ‘˜ GLYPH&lt;2&gt; ğ‘Ÿ GLYPH&lt;2&gt; ğ‘‘ 0 . We denote ğ¸ğ‘¢ 2 â„ ğ‘˜ GLYPH&lt;2&gt; ğ‘Ÿ GLYPH&lt;2&gt; ğ‘‘ 0 as the encoded neighbours for chunk ğ‘¢ 2 Â» 1 GLYPH&lt;148&gt; ğ‘™ â€¦ .\n\nChunked cross-attention. To perform the Cca operation, we first split a given intermediate activation ğ» 2 â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ into ğ‘™ GLYPH&lt;0&gt; 1 attending chunks GLYPH&lt;16&gt; ğ» , ğ‘¢ , ' â„ğ‘¢ğ‘š , ğ‘– GLYPH&lt;0&gt; 1 ' ğ‘– 2Â» 1 GLYPH&lt;148&gt; ğ‘š â€¦ 2 â„ ğ‘š GLYPH&lt;2&gt; ğ‘‘ GLYPH&lt;17&gt; ğ‘¢ 2Â» 1 GLYPH&lt;148&gt;ğ‘™ GLYPH&lt;0&gt; 1 â€¦ , as depicted on the right of Fig. 2. ğ» , ğ‘¢ holds the intermediary embeddings of the last token in chunk ğ¶ğ‘¢ and of the first ğ‘š GLYPH&lt;0&gt; 1 tokens in ğ¶ğ‘¢ , 1 2 . We compute the cross-attention between ğ» , ğ‘¢ and ğ¸ğ‘¢ -the encoded retrieval set obtained from chunk ğ¶ğ‘¢ .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 24,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-25",
      "content": "f the last token in chunk ğ¶ğ‘¢ and of the first ğ‘š GLYPH&lt;0&gt; 1 tokens in ğ¶ğ‘¢ , 1 2 . We compute the cross-attention between ğ» , ğ‘¢ and ğ¸ğ‘¢ -the encoded retrieval set obtained from chunk ğ¶ğ‘¢ . Attention is computed across time and across neighbours simultaneously, as we merge the neighbour and time dimensions of ğ¸ğ‘¢ before applying cross-attention. Since there is a notion of alignment between data chunks and retrieval neighbours, we use relative positional encodings as described in Â§B.1.2.\n\nWe concatenate the ğ‘™ GLYPH&lt;0&gt; 1 outputs of the per-chunk cross-attentions (each of shape ğ‘š GLYPH&lt;2&gt; ğ‘‘ ) across time, and properly pad the result; we thus form the output activation Cca ' ğ»GLYPH&lt;148&gt; ğ¸ ' 2 â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ . Formally, for each chunk ğ¶ğ‘¢ and for each token ğ‘– 2 Â» 1 GLYPH&lt;148&gt;ğ‘š â€¦ we set\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 25,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-26",
      "content": "form the output activation Cca ' ğ»GLYPH&lt;148&gt; ğ¸ ' 2 â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ . Formally, for each chunk ğ¶ğ‘¢ and for each token ğ‘– 2 Â» 1 GLYPH&lt;148&gt;ğ‘š â€¦ we set\n\n<!-- formula-not-decoded -->\n\n2 The last token of chunk ğ¶ğ‘¢ is the first to be able to access the retrieved content ğ¸ğ‘¢ while maintaining autoregressivity in (1). Hence, there is a one token overlap between chunk ğ¶ğ‘¢ = GLYPH&lt;16&gt; ğ‘¥ ' ğ‘¢ GLYPH&lt;0&gt; 1 ' ğ‘š , ğ‘– GLYPH&lt;17&gt; ğ‘– 2Â» 1 GLYPH&lt;148&gt; ğ‘š â€¦ and the corresponding attending chunk ğ¶ , ğ‘¢ , ' ğ‘¥ ğ‘¢ ğ‘š , ğ‘– GLYPH&lt;0&gt; 1 ' ğ‘– 2Â» 1 GLYPH&lt;148&gt; ğ‘š â€¦ .\n\nAlgorithm 1: Overview of Retro model architecture.\n\nHyperparam: ğ‘ƒ and ğ‘ƒ enc , indices of layers with cross-attention in the decoder and encoder respectively Hyperparam: ğ¿ and ğ¿ enc , number of decoder layers and number of encoder layers. Input: ğ‘‹ 2 ğ• ğ‘› : sequence of tokens.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 26,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-27",
      "content": "ndices of layers with cross-attention in the decoder and encoder respectively Hyperparam: ğ¿ and ğ¿ enc , number of decoder layers and number of encoder layers. Input: ğ‘‹ 2 ğ• ğ‘› : sequence of tokens. ' Ret ' ğ¶ğ‘¢ '' 1 6 ğ‘¢ 6 ğ‘™ : the retrieved neighbours Output: ğ‘‚ 2 â„ ğ‘› GLYPH&lt;2&gt; j ğ• j : the output logits def Encoder ' Ret ' ğ¶ğ‘¢ ' 1 6 ğ‘¢ 6 ğ‘™ GLYPH&lt;148&gt; ğ» ' : ' ğ»ğ‘¢ ' ğ‘¢ 2Â» 1 GLYPH&lt;148&gt;ğ‘™ â€¦ Split ' ğ» ' for ğ‘— 2 Â» 1 GLYPH&lt;148&gt; ğ‘˜ â€¦ GLYPH&lt;148&gt;ğ‘¢ 2 Â» 1 GLYPH&lt;148&gt; ğ‘™ â€¦ do // Encoder shared across neighbours and chunks ğ¸ ğ‘— ğ‘¢ = Embenc ' Ret ' ğ¶ğ‘¢ ' ğ‘— ' // May be shared with the decoder EM B for ğ‘ 0 2 Â» 1 GLYPH&lt;148&gt; ğ¿ enc â€¦ do ğ¸ ğ‘— ğ‘¢ Attnenc ' ğ¸ ğ‘— ğ‘¢ ' // Bi-directional attention if ğ‘ 0 2 ğ‘ƒ enc then ğ¸ ğ‘— ğ‘¢ Caenc ' ğ¸ ğ‘— ğ‘¢ GLYPH&lt;148&gt; ğ»ğ‘¢ ' ğ¸ ğ‘— ğ‘¢ Ffwenc ' ğ¸ ğ‘— ğ‘¢ ' return ğ¸ ğ» Emb ' ğ‘‹ ' for ğ‘ 2 Â» 1 GLYPH&lt;148&gt; ğ¿ â€¦ do ğ» Attn ' ğ» ' // Causal attention if ğ‘ = min ' ğ‘ƒ ' then // The neighbour EN C O D E R is cond",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 27,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-28",
      "content": "8&gt; ğ»ğ‘¢ ' ğ¸ ğ‘— ğ‘¢ Ffwenc ' ğ¸ ğ‘— ğ‘¢ ' return ğ¸ ğ» Emb ' ğ‘‹ ' for ğ‘ 2 Â» 1 GLYPH&lt;148&gt; ğ¿ â€¦ do ğ» Attn ' ğ» ' // Causal attention if ğ‘ = min ' ğ‘ƒ ' then // The neighbour EN C O D E R is conditioned with the decoder activations of the last layer before the first cross-attention ğ¸ = Encoder ' Ret ' ğ¶ğ‘¢ ' 1 6 ğ‘¢ 6 ğ‘™ GLYPH&lt;148&gt; ğ» ' if ğ‘ 2 ğ‘ƒ then ğ» Cca ' ğ»GLYPH&lt;148&gt; ğ¸ ' ğ» Ffw ' ğ» ' ğ‘‚ Read ' ğ» '\n\nwhere Ca is the cross-attention residual operator over time-concatenated encoded neighbours. We recall that this operator is defined in its simplest version by three parameter matrices ğ¾ 2 â„ ğ‘‘ GLYPH&lt;2&gt; ğ‘ GLYPH&lt;148&gt;ğ‘„ 2 â„ ğ‘‘ GLYPH&lt;2&gt; ğ‘ and ğ‘‰ 2 â„ ğ‘‘ GLYPH&lt;2&gt; ğ‘‘ . For all â„ 2 â„ ğ‘‘ and ğ‘Œ 2 â„ ğ‘‡ GLYPH&lt;2&gt; ğ‘‘ , we define\n\n<!-- formula-not-decoded -->\n\nwhere the softmax is performed on the second dimension and all products are matrix products. We use multi-head cross-attention, and add positional encodings to the softmax(see Â§B.1.2).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 28,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-29",
      "content": "ot-decoded -->\n\nwhere the softmax is performed on the second dimension and all products are matrix products. We use multi-head cross-attention, and add positional encodings to the softmax(see Â§B.1.2).\n\nThe first ğ‘š GLYPH&lt;0&gt; 1 tokens cannot attend to any neighbour of a previous chunk; at these positions, we define Cca as the identity, setting Cca ' ğ»GLYPH&lt;148&gt; ğ¸ ' ğ‘— , â„ ğ‘— for all tokens ğ‘— 2 Â» 1 GLYPH&lt;148&gt;ğ‘š GLYPH&lt;0&gt; 1 â€¦ . Finally, the last token â„ğ‘™ğ‘š attends to the last retrieval set ğ¸ğ‘™ and we set â„ğ‘™ ğ‘š , Ca ' â„ğ‘™ ğ‘š GLYPH&lt;148&gt; ğ¸ğ‘™ ' (not shown in Fig. 2). Listing 1 contains a simplified implementation of Cca. Note that chunked cross-attention is autoregressive: the output of Cca at position ğ‘– depends on the sequence from tokens from 0 to ğ‘– that is input to Cca.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 29,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-30",
      "content": "ontains a simplified implementation of Cca. Note that chunked cross-attention is autoregressive: the output of Cca at position ğ‘– depends on the sequence from tokens from 0 to ğ‘– that is input to Cca.\n\nWith Retro models, even though each Cca cross-attention attends only to the neighbours of the preceding chunk Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' , the dependencies over previous neighbours are propagated via the self-attention operations. The activations of the ğ‘– th token in the ğ‘¢ th chunk therefore potentially depend upon the set of all previous neighbours Ret ' ğ¶ğ‘¢ 0 ' ğ‘¢ 0 GLYPH&lt;157&gt;ğ‘¢ , without incurring the quadratic cost of cross attending to that set.\n\nSampling. When sampling, at the end of a chunk ğ¶ğ‘¢ , we use SCaNN to retrieve neighbours Ret ' ğ¶ğ‘¢ ' , based on the embedding Bert ' ğ¶ğ‘¢ ' .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 30,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-31",
      "content": "ring the quadratic cost of cross attending to that set.\n\nSampling. When sampling, at the end of a chunk ğ¶ğ‘¢ , we use SCaNN to retrieve neighbours Ret ' ğ¶ğ‘¢ ' , based on the embedding Bert ' ğ¶ğ‘¢ ' . The encoded neighbours ğ¸ğ‘¢ = Encoder ' Ret ' ğ¶ğ‘¢ '' are then used to condition the generation of the next chunk ğ¶ğ‘¢ , 1 , which we do incrementally: overall the cost of sampling is thus quadratic in the size of the sampled sequence, as when sampling from regular Transformers; the added cost of retrieval is linear in the number of chunks ğ‘™ , and is negligible compared to the token sampling cost in practice.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.4. Retro model architecture",
        "chunkIndex": 31,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-32",
      "content": "We use a transformer (Vaswani et al., 2017) similar to the one described in (Radford et al., 2019), with some minimal changes: we replace LayerNorm with RMSNorm (Zhang and Sennrich, 2019) and use relative position encodings (Dai et al., 2019). As baselines, we train retrieval-free transformers with 132M, 368M, 1.3B and 7.0B parameters (embedding matrices are excluded from parameter counts). The hyperparameters we used are detailed in Table 2. All retrieval models use the same size encoder for the retrieval data, with ğ‘‘ 0 = 896 and 2 layers, which roughly adds 19 ğ‘€ parameters. The encoder uses relative positional encodings. The retrieval models contain one Retro-block every 3 blocks, starting from layer 6. For our smallest model, Cca is applied in layers 6, 9 and 12 of the main pathway and also once for query conditioning in the encoder, which adds an additional 12 ğ‘€ parameters. The relative number of extra parameters reduces as we increase the baseline model size.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.5. Baseline Transformer architecture",
        "chunkIndex": 32,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-33",
      "content": "the main pathway and also once for query conditioning in the encoder, which adds an additional 12 ğ‘€ parameters. The relative number of extra parameters reduces as we increase the baseline model size. All models are implemented using JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.5. Baseline Transformer architecture",
        "chunkIndex": 33,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-34",
      "content": "Retro models may arguably benefit more easily from evaluation dataset leakage, i.e. the fact that we evaluate on data that were also present in the training set. To better understand how retrieval improves language modelling performance, we therefore quantify evaluation likelihood as a function of the overlap between the evaluation and training datasets.\n\nThe following approach can be used with any language model, and depends only on the frozen retriever system presented in Â§2.3. We split the evaluation sequences ' ğ‘‹ğ‘– ' ğ‘– into chunks of length ğ‘š GLYPH&lt;20&gt; 64, and we see the training data as a set of chunks C . For each evaluation chunk ğ¶ 2 C , we retrieve the 10 closest neighbours (of length up to 128) in the training data. We then compute the longest token substring common to both the evaluation chunk and its neighbours. This gives a number ğ‘  2 Â» 0 GLYPH&lt;148&gt;ğ‘š â€¦ .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.6. Quantifying dataset leakage exploitation",
        "chunkIndex": 34,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-35",
      "content": "ours (of length up to 128) in the training data. We then compute the longest token substring common to both the evaluation chunk and its neighbours. This gives a number ğ‘  2 Â» 0 GLYPH&lt;148&gt;ğ‘š â€¦ . The value ğ‘Ÿ ' ğ¶ ' = ğ‘  ğ‘š , ranging from 0 (chunk never seen) to 1 (chunk entirely seen), gives a reliable indication of how much overlap there is between the evaluation chunk and the training data. For a given model, we then obtain the log-likelihood GLYPH&lt;18&gt; ' ğ¶ ' of each chunk ğ¶ , and the number of bytes ğ‘ ' ğ¶ ' it encodes. We then consider the filtered bits-per-bytes of the model:\n\n<!-- formula-not-decoded -->\n\nTable 2 j Number of parameters for our baseline and Retro models, excluding embeddings, along with the corresponding hyperparameters.\n\nBaseline parameters\n\nRetro",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "2.6. Quantifying dataset leakage exploitation",
        "chunkIndex": 35,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-36",
      "content": "Head size",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "heads",
        "chunkIndex": 36,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-37",
      "content": "|        |                     | ğ‘‘     | ğ‘‘ ffw   |    |     |    |\n|--------|---------------------|-------|---------|----|-----|----|\n| 132M   | 172M (+30%)         | 896   | 3,584   | 16 |  64 | 12 |\n| 368M   | 425M (+15%)         | 1,536 | 6,144   | 12 | 128 | 12 |\n| 1,309M | 1,451M (+11%) (+8%) | 2,048 | 8,192   | 16 | 128 | 24 |\n| 6,982M | 7,532M              | 4,096 | 16,384  | 32 | 128 | 32 |\n\nwhich correspond to the bits-per-bytes on the set of chunks that overlap less than ğ›¼ %with the training chunks. Note that the full evaluation bit-per-bytes performance is recovered by bpb ' 1 ' . The function bpb 'GLYPH&lt;1&gt;' allows us to evaluate the impact of evaluation leakage over predictive performance: for low ğ›¼ , bpb ' ğ›¼ ' gives an indication on how the model performs on chunks that are entirely new; the slope of bpb 'GLYPH&lt;1&gt;' shows how much the model exploits evaluation leakage.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "layers",
        "chunkIndex": 37,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-38",
      "content": "We first review existing work on using retrieval for language modelling, and compare Retro to these works (see Table 3). As we train Retro models on a large dataset containing a substantial section of the internet, our work raises potential privacy, safety, and fairness issues that we then review.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3. Related Work",
        "chunkIndex": 38,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-39",
      "content": "Brants et al. (2007) show that scaling the training data to trillions of tokens improves the machine translation performance of ğ‘› -gram models. More recently, GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and Jurassic-1 (Lieber et al., 2021) show that scaling up language models leads to massive improvements on many downstream tasks. At the same time, Carlini et al. (2021) demonstrate that large-scale language models can perfectly memorise parts of their training data, suggesting that enhancing models with retrieval may lead to further improvements. However, significant leakage between train and test datasets (Lee et al., 2021; Lewis et al., 2021) makes comparing and evaluating large models trained on large datasets difficult, especially once retrieval capabilities over the training dataset are added.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 39,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-40",
      "content": "sets (Lee et al., 2021; Lewis et al., 2021) makes comparing and evaluating large models trained on large datasets difficult, especially once retrieval capabilities over the training dataset are added.\n\nHistorically, information retrieval for text relies on inverted index matching such as TF-IDF and BM25 (Robertson and Zaragoza, 2009). Foundational work use latent topic modelling approaches like LDA (Blei et al., 2003) to identify relevant neighbours (Wei and Croft, 2006). Work in machine translation such as Zhang et al. (2018) and Gu et al. (2018) retrieve translation pairs based on edit distance between source sentences and guide the translation output using the closest retrieved target sentences. The retrieval database may also be structured - for example, Ahn et al. (2016) use a symbolic knowledge graph to improve an RNN language model.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 40,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-41",
      "content": "ion output using the closest retrieved target sentences. The retrieval database may also be structured - for example, Ahn et al. (2016) use a symbolic knowledge graph to improve an RNN language model.\n\nWith the success of deep learning, retrieving systems have partly switched to dense learned representations based on a neural network's activations. Continuous cache (Grave et al., 2017) adds probability mass to tokens for which previous activations resemble the current activation vector, extending the model's context to the local history. ğ‘˜ NN-LM (Khandelwal et al., 2020) applies this idea to transformers and extends the retrieval database to English Wikipedia, resulting in\n\nTable 3 j Comparison of Retro with existing retrieval approaches.\n\n|                  | # Retrieval tokens        | Granularity   | Retriever training   | Retrieval integration   |\n|------------------|---------------------------|---------------|----------------------|-------------------------|\n| Continuous Cache |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 41,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-42",
      "content": "Granularity   | Retriever training   | Retrieval integration   |\n|------------------|---------------------------|---------------|----------------------|-------------------------|\n| Continuous Cache | O GLYPH<0> 10 3 GLYPH<1>  | Token         | Frozen (LSTM)        | Add to probs            |\n| ğ‘˜ NN-LM          | O GLYPH<0> 10 9 GLYPH<1>  | Token         | Frozen (Transformer) | Add to probs            |\n| Spalm            | O GLYPH<0> 10 9 GLYPH<1>  | Token         | Frozen (Transformer) | Gated logits            |\n| Dpr              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Contrastive proxy    | Extractive QA           |\n| Realm            | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | End-to-End           | Prepend to prompt       |\n| RAG              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Fine-tuned Dpr       | Cross-attention         |\n| FiD              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Frozen Dpr           | Cross-attention         |\n| Emdr 2           | O",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 42,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-43",
      "content": "rompt        | Fine-tuned Dpr       | Cross-attention         |\n| FiD              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Frozen Dpr           | Cross-attention         |\n| Emdr 2           | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | End-to-End (EM)      | Cross-attention         |\n| Retro (ours)     | O GLYPH<0> 10 12 GLYPH<1> | Chunk         | Frozen (Bert)        | Chunked cross-attention |\n\nsubstantial improvements on Wikitext103 evaluation. Continuous cache and ğ‘˜ NN-LM do not modify the underlying neural-network models, but interpolate at inference between the language model's output and distributions computed from retrieved tokens. These methods can therefore be plugged into any model without additional training, although this limits the model's ability to reason about the retrieved text.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 43,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-44",
      "content": "ibutions computed from retrieved tokens. These methods can therefore be plugged into any model without additional training, although this limits the model's ability to reason about the retrieved text. Spalm (Yogatama et al., 2021) addresses this limitation by adding an extra gating network to post-process the retrieved data; yet most of the network is unaffected by the retrieval during inference.\n\nThe retrieval representations may be trained directly instead of relying on a pre-trained modelretriever systems have been developed for this purpose, primarily on open-domain question answering. For example, Dpr (Karpukhin et al., 2020) trains two Bert models (for queries and keys respectively) using a contrastive loss to align the representations of a question and of its answers. Lee et al. (2019) use an inverse cloze task to find semantic representations of passages for retrieval.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 44,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-45",
      "content": "ctively) using a contrastive loss to align the representations of a question and of its answers. Lee et al. (2019) use an inverse cloze task to find semantic representations of passages for retrieval. These works differs from continuous cache and ğ‘˜ NN-LM in that they embeds passages (or chunks) of text together, as opposed to each token individually. The retriever network is trained in isolation of the downstream task that uses the retrieval data. This potential issue is specifically addressed by Realm (Guu et al., 2020), which trains the retrieval system end-to-end to maximize the final training cross-entropy. This comes with the extra complexity of searching the database during training and periodically updating the embedding table, severely limiting the scale at which it can operate. RAG (Lewis et al., 2020) and FiD (Izacard and Grave, 2021) build upon Dpr to set the state of the art on question answering benchmarks by training encoder-decoder transformer models.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 45,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-46",
      "content": "h it can operate. RAG (Lewis et al., 2020) and FiD (Izacard and Grave, 2021) build upon Dpr to set the state of the art on question answering benchmarks by training encoder-decoder transformer models. More recently, Emdr 2 (Sachan et al., 2021) extends FiD by using an expectation-maximization algorithm to train the retriever end-to-end and achieves state of the art results compared to similarly sized models.\n\nIn the open-domain dialogue setting, BlenderBot 2.0 (Komeili et al., 2021) learns to issue textual internet queries, outperforming dense retrieval methods when evaluated on a task measuring how close model responses are to those of humans. This involves collecting a dataset of human dialogues with associated search queries, which limits the scalability of this approach. Hashemi et al. (2020) introduce the Guided Transformer, a modified Transformer similar to Retro, for document retrieval and clarifying question selection.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 46,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-47",
      "content": "hich limits the scalability of this approach. Hashemi et al. (2020) introduce the Guided Transformer, a modified Transformer similar to Retro, for document retrieval and clarifying question selection. Although effective on question answering and other tasks with strong conditioning, none of these methods are designed to model arbitrary text sequences, in contrast with Retro.\n\nRetro shares components with ğ‘˜ NN-LM and Dpr in that it uses frozen retrieval representations. Retro models longer sequences than QA examples; this requires to reason at a sub-sequence level, and to retrieve different documents for the different chunks of a sequence. Similar to FiD, Retro processes the retrieved neighbours separately in the encoder, and assemble them in the chunked cross-attention. This differs from e.g. Realm, that prepends retrieved documents to the prompt. Using chunks allows for repeated retrieval whilst generating a sequence as opposed to retrieving only once based on the prompt alone.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 47,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-48",
      "content": "from e.g. Realm, that prepends retrieved documents to the prompt. Using chunks allows for repeated retrieval whilst generating a sequence as opposed to retrieving only once based on the prompt alone. Furthermore, retrieval is done during the whole pre-training process in Retro, and is not simply plugged-in to solve a certain downstream task. Finally, previous methods based on dense query vectors use small models and retrieval datasets with less than 3B tokens (English Wikipedia). Table 3 summarizes the difference of Retro with existing approaches.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.1. Retrieval for language modelling",
        "chunkIndex": 48,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-49",
      "content": "Bender et al. (2021); Weidinger et al. (2021) highlight several dangers of large language models. Those stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to generate toxic language (Gehman et al., 2020). In this section we inspect these dangers, focusing on how retrieval augmented language models may exacerbate or\n\nmitigate them.\n\nLarge language models can perfectly memorise parts of their training data (Carlini et al., 2021). When coupled with large training datasets gathered from the web or other sources, this has clear privacy and safety implications. Retrieval models such as Retro that have access to the entire training dataset during inference exacerbate these privacy issues by being able to directly copy training data.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.2. Privacy, safety and fairness",
        "chunkIndex": 49,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-50",
      "content": "y and safety implications. Retrieval models such as Retro that have access to the entire training dataset during inference exacerbate these privacy issues by being able to directly copy training data. However, retrieval systems offer a path towards mitigating these concerns via obliteration of the retrievable data at inference time. In addition, differential privacy training (Abadi et al., 2016) of retrieval models could guarantee that no private information is stored in the model weights, while individualisation on private data could be made by updating the retrieval database at inference time.\n\nDue to their high training cost, re-training large language model regularly to incorporate new data, languages, and norms is prohibitively expensive. To keep retrieval models up-to-date, it may be sufficient to update the retrieval database, which is orders of magnitude cheaper than re-training a model from scratch.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.2. Privacy, safety and fairness",
        "chunkIndex": 50,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-51",
      "content": "orms is prohibitively expensive. To keep retrieval models up-to-date, it may be sufficient to update the retrieval database, which is orders of magnitude cheaper than re-training a model from scratch. In addition to the benefits of updating models in terms of fairness and bias, simply training large language models has a significant energy cost (Schwartz et al., 2020; Strubell et al., 2019). Retrieval mechanisms offer a path to reducing the compute requirements needed to train and update language models that reach a certain performance.\n\nLarge language models are prone to generating toxic outputs, as shown in Gehman et al. (2020). Bender et al. (2021); Jo and Gebru (2020) advocate for the importance of better training data curation and documentation. Additionally, if portions of the training data are found to be eliciting biased or toxic outputs after training, retrieval allows for some correction, as the offending retrieval data can be retroactively filtered.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.2. Privacy, safety and fairness",
        "chunkIndex": 51,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-52",
      "content": "if portions of the training data are found to be eliciting biased or toxic outputs after training, retrieval allows for some correction, as the offending retrieval data can be retroactively filtered. However, it is also the case that without careful analysis and intervention, retrieval models may exacerbate biases that are present in the training data. Retrieval models can also add a further source of bias through the selection mechanism for retrieval documents. Further work in this area is required to better understand how retrieval affects the bias and toxicity of the model outputs.\n\nFinally, samples from large models are difficult to interpret, making mitigating these issues all the more challenging (Belinkov et al., 2020; Jain and Wallace, 2019). Retrieval provides more insights in to the outputs of a model, as one can directly visualise or modify the neighbours that are being used.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.2. Privacy, safety and fairness",
        "chunkIndex": 52,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-53",
      "content": "challenging (Belinkov et al., 2020; Jain and Wallace, 2019). Retrieval provides more insights in to the outputs of a model, as one can directly visualise or modify the neighbours that are being used. The examples in Table 6, 7, 20 and 21 illustrate how retrieval makes language models more factual and interpretable by providing more transparent outputs.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "3.2. Privacy, safety and fairness",
        "chunkIndex": 53,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-54",
      "content": "We first report results on language modelling benchmarks. Second, we show how to Retrofit pre-trained Transformer language models into retrieval models with few additional FLOPs. Next, we report Retro results on question answering. Finally, we report evaluation metrics with leakage filtering, to better understand the source of the gains with retrieval.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4. Results",
        "chunkIndex": 54,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-55",
      "content": "Datasets. We evaluate our models on C4 (Raffel et al., 2020), Wikitext103 (Merity et al., 2017), Curation Corpus (Curation, 2020), Lambada (Paperno et al., 2016) and the Pile (Gao et al., 2020). We also evaluate on a set of manually selected Wikipedia articles that were added or heavily edited in September 2021, months after our pre-training and retrieval dataset was collected (details are given in Â§A.2). We construct the dataset with articles from the 'future' and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data.\n\nFigure 3 j Scaling with respect to model size. (a) LAMBADA top-1 accuracy. (b) Evaluation loss on curation corpus. (c) Perplexity on Wikitext103 valid. (d) Bits-per-byte on selected Wikipedia articles from September 2021.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 55,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-56",
      "content": "del size. (a) LAMBADA top-1 accuracy. (b) Evaluation loss on curation corpus. (c) Perplexity on Wikitext103 valid. (d) Bits-per-byte on selected Wikipedia articles from September 2021.\n\n<!-- image -->\n\nFor C4, Wikitext103, the Pile, and our Wikipedia dataset we evaluate the language modelling performance on entire documents and measure the bits-per-byte (bpb). We favour bits-per-byte over loss as it is tokenizer agnostic. We evaluate with a sequence length of 2048 tokens but use a stride of 1024 within documents to mitigate boundary effects. On Curation Corpus we concatenate the article, the ' TL;DR: ' string, and the summary, but only evaluate the bpb on the summary. For Lambada we evaluate the accuracy on the last word, using greedy generation.\n\nModel scaling. In Fig. 1(left) and Fig. 3 we show the language modelling performance as we scale models from 150 million to 7 billion (non-embedding) parameters. We see that on all datasets, Retro outperforms the baseline at all model sizes.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 56,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-57",
      "content": "we show the language modelling performance as we scale models from 150 million to 7 billion (non-embedding) parameters. We see that on all datasets, Retro outperforms the baseline at all model sizes. Furthermore, we observe that improvements do not diminish as we scale the models. The performance is dataset dependent, with the largest gains on Wikitext103 and C4. Wikipedia articles and other web pages are similar to Wikitext103 documents, even if not exact copies (Â§4.4), we thus obtain dramatic improvements on Wikitext103 as our retrieval model is able to directly exploit these overlaps. The smallest gains are for Curation Corpus, where Retro only slightly outperforms the baseline. This is expected as Curation Corpus summaries are designed to only contain information from the source article and are not included in our retrieval database. On our 'future' Wikipedia September 2021 dataset, we also observe consistent gains for all model sizes.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 57,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-58",
      "content": "only contain information from the source article and are not included in our retrieval database. On our 'future' Wikipedia September 2021 dataset, we also observe consistent gains for all model sizes.\n\nData scaling. Fig. 1 (middle) shows how scaling the retrieval database at evaluation improves the language modelling performance. We observe dramatic gains as the retrieval data is increased from Wikipedia (4 billion tokens) to all of Massive text (1.7T tokens). Fig. 1(right) shows how performance scales as we increase the number of retrieved chunks. Despite being only trained with 2 neighbours, we see consistent improvements for all models when the number of neighbours is increased from 1 to 10. Furthermore, we observe that larger models are able to better utilise more neighbours: the 172M model improves with up to 10 neighbours, whereas the 7B model improves with up to 40 neighbours.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 58,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-59",
      "content": "to 10. Furthermore, we observe that larger models are able to better utilise more neighbours: the 172M model improves with up to 10 neighbours, whereas the 7B model improves with up to 40 neighbours.\n\nThe Pile. We evaluate our 7B models on the Pile test sets 3 and compare against the 178B parameter Jurrasic-1 (Lieber et al., 2021) model and the 280B parameter Gopher (Rae et al., 2021) model. We do not compare against GPT-3 as it is outperformed by Jurassic-1 and Gopher on almost all subsets. Fig. 4 shows the relative improvements in bits-per-byte over our 7B transformer baseline for our\n\n3 Due to legal and ethical concerns relating to their use, we exclude the Enron Emails and the Youtube Subtitles datasets.\n\nFigure 4 j The Pile: Comparison of our 7B baseline against Jurassic-1, Gopher, and Retro. We observe that the retrieval model outperforms the baseline on all test sets and outperforms Jurassic-1 on a majority of them, despite being over an order of magnitude smaller.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 59,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-60",
      "content": "-1, Gopher, and Retro. We observe that the retrieval model outperforms the baseline on all test sets and outperforms Jurassic-1 on a majority of them, despite being over an order of magnitude smaller.\n\n<!-- image -->\n\n7.5B Retro model, Jurassic-1 and Gopher. Jurassic-1 outperforms the baseline on all datasets except for books, likely due to the inclusion of books in our training data. Gopher and Retro outperform the baseline on all test sets. Overall, Retro 7.5B outperforms Jurassic-1 and Gopher on a majority of the test sets. On the dm\\_mathematics and ubuntu\\_irc subsets, our Retro model does not outperform our 7B baseline and underperforms Jurassic-1. We hypothesise that the retrieved neighbours on these datasets are not helpful, due to a combination of what is in our retrieval dataset and the efficacy of the nearest-neighbour search.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 60,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-61",
      "content": "s Jurassic-1. We hypothesise that the retrieved neighbours on these datasets are not helpful, due to a combination of what is in our retrieval dataset and the efficacy of the nearest-neighbour search.\n\nWikitext103. To validate our approach in a controlled setting, we compare our method with ğ‘˜ NN-LM (Khandelwal et al., 2020) on the Wikitext103 dataset in Table 4. We train a baseline transformer on the training set of Wikitext103. This transformer has 24 layers, 1024 hidden units, 16 heads and a key size of 64, as in Baevski and Auli (2019). Our baseline does not have adaptive input, and our tokenizer has an open vocabulary, unlike Baevski and Auli (2019), which makes our baseline\n\nTable 4 j Perplexities on Wikitext103. When using the Wikpedia dataset for retrieval, Retro performs similarly to our implementation of ğ‘˜ NN-LM. As we scale the retrieval dataset, Retro performs much better.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 61,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-62",
      "content": "j Perplexities on Wikitext103. When using the Wikpedia dataset for retrieval, Retro performs similarly to our implementation of ğ‘˜ NN-LM. As we scale the retrieval dataset, Retro performs much better. The perplexities for retrieving from full MassiveText are quite low, which is partly due to partial overlap with Wikitext103 not caught by our deduplication.\n\n| Model                                    | Retrieval Set      | #Database tokens   | #Database keys   | Valid   |   Test |\n|------------------------------------------|--------------------|--------------------|------------------|---------|--------|\n| Adaptive Inputs (Baevski and Auli, 2019) | -                  | -                  | -                | 17.96   |  18.65 |\n| Spalm (Yogatama et al., 2021)            | Wikipedia          | 3B                 | 3B               | 17.20   |  17.6  |\n| ğ‘˜ NN-LM (Khandelwal et al., 2020)        | Wikipedia          | 3B                 | 3B               | 16.06   |  16.12 |\n| Megatron (Sh",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 62,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-63",
      "content": "3B                 | 3B               | 17.20   |  17.6  |\n| ğ‘˜ NN-LM (Khandelwal et al., 2020)        | Wikipedia          | 3B                 | 3B               | 16.06   |  16.12 |\n| Megatron (Shoeybi et al., 2019)          | -                  | -                  | -                | -       |  10.81 |\n| Baseline transformer (ours)              | -                  | -                  | -                | 21.53   |  22.96 |\n| ğ‘˜ NN-LM (ours)                           | Wikipedia          | 4B                 | 4B               | 18.52   |  19.54 |\n| Retro                                    | Wikipedia          | 4B                 | 0.06B            | 18.46   |  18.97 |\n| Retro                                    | C4                 | 174B               | 2.9B             | 12.87   |  10.23 |\n| Retro                                    | MassiveText (1%)   | 18B                | 0.8B             | 18.92   |  20.33 |\n| Retro                                    | MassiveText (10%)",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 63,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-64",
      "content": "|  10.23 |\n| Retro                                    | MassiveText (1%)   | 18B                | 0.8B             | 18.92   |  20.33 |\n| Retro                                    | MassiveText (10%)  | 179B               | 4B               | 13.54   |  14.95 |\n| Retro                                    | MassiveText (100%) | 1792B              | 28B              | 3.21    |   3.92 |\n\nperplexities a bit higher. The full experiment details and hyperparameters are given in Â§C.2 and Table 11.\n\nWe re-implement ğ‘˜ NN-LM with our tokenizer and baseline transformer to produce embeddings of size 1024 for every token in Wikitext103. ğ‘˜ NN-LM has probabilities ğ‘ğ‘˜ NN-LM = ğœ†ğ‘ğ‘˜ NN , ' 1 GLYPH&lt;0&gt; ğœ† ' ğ‘ Lm with ğ‘ğ‘˜ NN ' ğ‘›ğ‘˜ ' / exp 'GLYPH&lt;0&gt; ğ›¼ğ‘‘ğ‘˜ ' . We tune ğœ† = 0 GLYPH&lt;147&gt; 118 and ğ›¼ = 0 GLYPH&lt;147&gt; 00785 on the validation set (Fig. 7) and report performance for these hyperparameters on both the validation and test set.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 64,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-65",
      "content": "t; ğ›¼ğ‘‘ğ‘˜ ' . We tune ğœ† = 0 GLYPH&lt;147&gt; 118 and ğ›¼ = 0 GLYPH&lt;147&gt; 00785 on the validation set (Fig. 7) and report performance for these hyperparameters on both the validation and test set.\n\nWe fine-tune our baseline transformer into a Retro model (Fig. 7), using the Wikitext103 training data and retrieving from Wikipedia with 2 neighbours. We only train the new weights, as explained in Â§4.2, and share the embedding weights between the encoder and the main pathway. This is necessary for Wikitext103 which is quite small, as training Retro from scratch in this setting leads to over-fitting.\n\nWe evaluate the fine-tuned Retro model with different retrieval sets. We use 10 neighbours at evaluation for both Retro and ğ‘˜ NN-LM. When retrieving from Wikipedia, we obtain results comparable to our ğ‘˜ NN-LM implementation. Furthermore, scaling the retrieval database to MassiveText yields dramatic improvements, though this is partly due to leakage (see Â§4.4).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 65,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-66",
      "content": ", we obtain results comparable to our ğ‘˜ NN-LM implementation. Furthermore, scaling the retrieval database to MassiveText yields dramatic improvements, though this is partly due to leakage (see Â§4.4). For reproducibility, we also include results when retrieving from C4, which are close to previous state-of-the-art and comparable to using 10 % of MassiveText.\n\nIt is worth noting that ğ‘˜ NN-LM requires 1024 floats for every token in the retrieval dataset, totalling 15 terabytes (Tb) for the 4 billion tokens in Wikipedia. ğ‘˜ NN-LM and other token-level retrieval approaches therefore don't scale to retrieval databases with trillions of tokens such as MassiveText. In comparison, Retro only requires 215Gb to index our Wikipedia dataset, and 93Tb for MassiveText. Inspecting the number of retrieval database entries in Table 4 makes it clear why retrieving at the chunk level is necessary when scaling to datasets with trillions of tokens.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 66,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-67",
      "content": "l database entries in Table 4 makes it clear why retrieving at the chunk level is necessary when scaling to datasets with trillions of tokens.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.1. Language modelling",
        "chunkIndex": 67,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-68",
      "content": "We extend baseline models into Retro models by freezing the pre-trained weights and training only chunked cross-attention and neighbour encoder parameters (less than 10% of weights for the 7B model) in Fig. 5. This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences (3% of the pre-training sequences that we used). Additionally, by only training the new weights we ensure that when evaluated without retrieval, the original model performance is exactly maintained. Retrofitting models quickly surpasses the performance of baseline models and even achieves performance close to that of Retro models trained from scratch. The experiment hyperparameters are given in Â§C.3.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.2. Retro-fitting baseline models",
        "chunkIndex": 68,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-69",
      "content": "We fine-tune our retrieval models on the Natural Questions (Kwiatkowski et al., 2019) dataset to demonstrate that our retrieval pathway can be used to inject information from arbitrary data sources. We use the version 4 provided by Izacard and Grave (2021) which is augmented with the retrieved passages from Dpr (Karpukhin et al., 2020). We fine-tune all the weights of our 7.5B pre-trained Retro model for 25,000 steps using the top 20 retrieved passages. We format the data as ' question: {question} \\nanswer: {answer} ' and left pad the data such that ' answer: ' coincides with the end of the first chunk of 64 tokens and thus aligns with the first retrieving chunk. The model has access to the question via the previous tokens in the sequence as well as the top 20 DPR Wikipedia passages and their titles via the chunked cross-attention mechanism.\n\n4 https://github.com/facebookresearch/FiD",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.3. Question answering",
        "chunkIndex": 69,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-70",
      "content": "the question via the previous tokens in the sequence as well as the top 20 DPR Wikipedia passages and their titles via the chunked cross-attention mechanism.\n\n4 https://github.com/facebookresearch/FiD\n\nFigure 5 j Retro-fitting a baseline transformer. Any transformer can be fine-tuned into a retrievalenhanced transformer by randomly initializing and training only the chunked cross-attention and retrieval encoder weights. Fine-tuning in this way quickly recovers and surpasses the non-retrieval performance, and almost achieves the same performance as training a retrieval model from scratch (shown by the arrow on the right hand side of each plot). We find good performance Retro-fitting our models training on only 3% the number of tokens seen during pre-training.\n\n<!-- image -->\n\nThe exact match scores are shown in Table 5 and the full fine-tuning details are given in Â§C.4. Our method is competitive with previous approaches such as Realm, RAG and Dpr, but underperforms the more recent FiD.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.3. Question answering",
        "chunkIndex": 70,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-71",
      "content": "ch scores are shown in Table 5 and the full fine-tuning details are given in Â§C.4. Our method is competitive with previous approaches such as Realm, RAG and Dpr, but underperforms the more recent FiD. In contrast with this work, we find that increasing the number of neighbours past 20 does not improve Retro performance on this task. We hypothesise that the encoder-decoder structure of T5-the base model in FiD- and the T5 pre-training objective leads to a model that relies more on the encoder output than Retro, which is important in the QA setting. To compete with T5-finetuned models, future work should consider ways of forcing Retro to rely further on the retrieval encoder output when producing tokens.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.3. Question answering",
        "chunkIndex": 71,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-72",
      "content": "We report the filtered eval losses as detailed in Â§2.6 on C4, Curation Corpus and Wikitext103 in Fig. 6. On C4 and Wikitext103, for which there is leakage into the training set, the slope is negative for both baseline models and Retro models. Retro models exploit leakage more strongly than baseline models, as indicated by the more negative slope. This is due to its explicit ability to copy-paste existing training chunks to predict leaked evaluation chunks (see a qualitative example of this model behavior\n\nTable 5 j Question answering results. Exact match accuracy on Natural Questions.\n\nModel\n\nTest Accuracy\n\n| Realm (Guu et al., 2020)              |   40.4 |\n|---------------------------------------|--------|\n| Dpr (Karpukhin et al., 2020)          |   41.5 |\n| RAG (Lewis et al., 2020)              |   44.5 |\n| Emdr 2 (Sachan et al., 2021)          |   52.5 |\n| FiD (Izacard and Grave, 2021)         |   51.4 |\n| FiD + Distill.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.4. Relating retrieval performance to dataset leakage.",
        "chunkIndex": 72,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-73",
      "content": "al., 2020)          |   41.5 |\n| RAG (Lewis et al., 2020)              |   44.5 |\n| Emdr 2 (Sachan et al., 2021)          |   52.5 |\n| FiD (Izacard and Grave, 2021)         |   51.4 |\n| FiD + Distill. (Izacard et al., 2020) |   54.7 |\n| Baseline 7B (closed book)             |   30.4 |\n| Retro 7.5B (DPR retrieval)            |   45.5 |\n\nFigure 6 j Performance vs. longest common retrieval substring. Evaluation loss as a function of allowed longest common substring between evaluation data chunks and their nearest neighbours. Retrieval still helps when considering chunks with no more than 8 contiguous tokens overlapping with training dataset chunks.\n\n<!-- image -->\n\non a Wikitext103 article in Table 19). On Curation Corpus, retrieval provides a constant offset, which is expected as there is by design no leakage between Curation Corpus and the training dataset.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.4. Relating retrieval performance to dataset leakage.",
        "chunkIndex": 73,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-74",
      "content": ">\n\non a Wikitext103 article in Table 19). On Curation Corpus, retrieval provides a constant offset, which is expected as there is by design no leakage between Curation Corpus and the training dataset.\n\nOn the other hand, Retro outperforms baseline models at all leakage levels, down to ğ›¼ = 12 GLYPH&lt;147&gt; 5%. At this level, the loss is computed on chunks with less than 8 contiguous tokens shared with the closest matching chunk in the training dataset-this is a reasonable level of overlap at which we consider that there is no local leakage. Retrieval thus improves predictions on both chunks that are syntactically similar to chunks in the training set, and on chunks that are syntactically different from all training chunks. This points toward a non trivial Retro capacity of generalizing based on both model parameters and retrieval database. Similar results are found on the Pile dataset (see Fig. 12, Â§F.3).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.4. Relating retrieval performance to dataset leakage.",
        "chunkIndex": 74,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-75",
      "content": "based on both model parameters and retrieval database. Similar results are found on the Pile dataset (see Fig. 12, Â§F.3).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.4. Relating retrieval performance to dataset leakage.",
        "chunkIndex": 75,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-76",
      "content": "We show examples of samples obtained using the 7.5B Retro model in Table 6, Table 7 and Appendix E. For each chunk (the first one being the prompt), we juxtapose sampled chunks ğ¶ğ‘¢ with retrieved neighbours Ret ' ğ¶ğ‘¢ ' . To give an indication of local overlap, we colour each sampled token in chunk ğ¶ğ‘¢ based on the length of the longest common prefix (LCP) found in the retrieved chunks Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' . Similarly, we colour the retrieved chunks based on the LCP in the sampled chunk. For the sample in Table 6, for which we chose the prompt, we observe that the retrieved chunks influence the sample as there are overlaps between the sampled tokens and neighbour tokens. Overall, retrieval reduces hallucinations (in line with the findings of Shuster et al. (2021)) and makes the model more knowledgeable, when comparing with samples produced with retrieval disabled.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.5. Using Retro for sampling",
        "chunkIndex": 76,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-77",
      "content": "ns. Overall, retrieval reduces hallucinations (in line with the findings of Shuster et al. (2021)) and makes the model more knowledgeable, when comparing with samples produced with retrieval disabled. In the sample in Table 7, the model recognises that the prompt is the beginning of the first scene of Hamlet and leverages retrieval data to continue it with only a few mistakes. We provide further examples in Appendix E, including examples from the evaluation sets, as well as the detailed procedure used for colouring the tables.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "4.5. Using Retro for sampling",
        "chunkIndex": 77,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-78",
      "content": "We present Retrieval-Enhanced Transformers (Retro), a method for modelling arbitrary text sequences whilst retrieving from databases with trillions of tokens-scaling the data available to models by an order of magnitude compared to what is typically consumed during training. Retro models\n\ngains do not diminish for models with up to at least 7B parameters, and correspond to non-retrieval models with 10 GLYPH&lt;2&gt; more parameters on certain datasets. On Wikitext103 and the Pile, Retro outperforms previous models trained on large scale datasets. We also show that Retro is competitive on retrieval-intensive downstream tasks such as question answering.\n\nRetro models are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models. Conversely, baseline models can be rapidly fine-tuned into Retro models to obtain nearly the same performance as if trained from scratch.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "5. Conclusion",
        "chunkIndex": 78,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-79",
      "content": "n and still achieve comparable performance to baseline models. Conversely, baseline models can be rapidly fine-tuned into Retro models to obtain nearly the same performance as if trained from scratch. Careful analysis shows that only a modest fraction of the gains obtained by Retro are due to test set leakage. In general, we caution for such leakage in large-scale language datasets and suggest further work in better understanding the role of test set leakage in the performance of large-scale language models.\n\nOverall, our work demonstrates at an unprecedented scale that semi-parametric approaches can provide an orthogonal, more efficient approach than raw parameter scaling as we seek to build more powerful language models.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "5. Conclusion",
        "chunkIndex": 79,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-80",
      "content": "We would like to thank Nikolai Grigorev, Marc'aurelio Ranzato, Cyprien de Masson d'Autume, Po-Sen Huang, Johannes Welbl, Lisa Anne Hendricks, Ethan Perez, Jeff Stanway, Eric Noland, Gregory Wayne, John Jumper, Julian Schrittwieser, Lorrayne Bennett, Devang Agrawal, Dani Yogatama, Susannah Young, Nando de Freitas, Demis Hassabis, and Koray Kavukcuoglu for their help, advice and reviews. Additionally, we would like to thank Zonglin Li, David Simcha, and the ScaNN developers for their help.\n\nTable 6 j Sample - Beavers are interesting animals . The Retro[Off] sample quickly diverges to other animals while the Retro[On] sample tends to stay focused on the beaver topic due to neighbour conditioning.\n\n| Prompt and sample of Retro[Off]                                                                                                                                                                                                                                   | Prompt and sample of Retro[On] col",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 80,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-81",
      "content": "| Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                        | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 81,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-82",
      "content": "| Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 82,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-83",
      "content": "|\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 83,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-84",
      "content": "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Beavers are interesting animals that live near rivers.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 84,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-85",
      "content": "----------------------------------------------------------------------------------------------------------------------------------------------|\n| Beavers are interesting animals that live near rivers. They build                                                                                                                                                                                                 | Beavers are interesting animals that live near rivers. They build                                                                                                                                                                                                                        | .Beavers build their lodges in pon ds they have created in wooded areas .Like many things in nature, there is a connection between creatures i n the wild.Beaver ponds cause tree s to drown, but the dead trees attra ct the great blue heron, which often return year after year.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 85,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-86",
      "content": "ke many things in nature, there is a connection between creatures i n the wild.Beaver ponds cause tree s to drown, but the dead trees attra ct the great blue heron, which often return year after year. Over time, a beaver pond can attract more than 50 nests in a colony, called a rooke ry.An example of this can be found in the large pond off Bradford Road at Carter Fields near the Boxford l ine.Chris Leahy, an expert with th e Massachusetts Audubon Society who wrote                               | naw them into smaller sections and d rag them into the water.Engineers of the Pond Beavers are interesting animals because they change the hab itat in which they live. Beavers do this by blocking up streams to creat e ponds. Then they build their homes , called lodges, in these ponds. Bea vers' bodies make them well-suited f or underwater building Special muscl es close off their noses, ears, and throats to keep the water out. Beave rs' broad tails act like rudders for steering.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 86,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-87",
      "content": "s. Bea vers' bodies make them well-suited f or underwater building Special muscl es close off their noses, ears, and throats to keep the water out. Beave rs' broad tails act like rudders for steering. Their two very large, ora nge front teeth are used to gnaw dow n trees. They begin building their d am |\n| dams to create ponds. Frogs are am phibians, so they can live in both l and and water. They have great camou flage to hide from predators. The G olden Retriever, or Golden, is a ver y big dog. They are very strong, and have a thick coat to help them live in | their houses called beaver dams in the riverbeds. They also live on lan d.Beavers use their strong teeth an d strong jaws to cut down trees and branches to build their homes. They also use their teeth and jaws to che w up food. Beavers use their big, fl at tails to swim. They use | , then they mean that you are very b usy. Beavers swim easily in streams, picking up rocks and sticks to buil d their dams.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 87,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-88",
      "content": "aws to che w up food. Beavers use their big, fl at tails to swim. They use | , then they mean that you are very b usy. Beavers swim easily in streams, picking up rocks and sticks to buil d their dams. They gnaw at trees wit h their big front teeth to cut them down. Then they use parts of the tre es to build their houses.Beavers ar e clever builders. They know exactly what they need to build their beave r dams. They use mud from the stream to make their dams stay together. T hey use their tails to pat down the mud.Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the | ar-like tail, and two protruding tee th that are strong enough to gnaw do wn trees. The beaver uses trees, bra nches, and mud to build dams across rivers and streams. These dams creat e a deep pool of water in which the beaver builds its home. A beaver hom e is called a lodge.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 88,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-89",
      "content": "trees. The beaver uses trees, bra nches, and mud to build dams across rivers and streams. These dams creat e a deep pool of water in which the beaver builds its home. A beaver hom e is called a lodge. A baby beaver o r 'kit' remains in the family lodge until the age of two.Beaver fur, kn own as pelt, was once highly popular as a trim for hats and coats. Howm ight the popularity of beaver fur co ntributed to the colonization of New York?wwwWw                                                                 |\n| cold areas. A kangaroo is an anima l that is very large. It can jump ve ry high. They can run very fast. A meerkat is a small animal that lives in Africa. They live in colonies of up to a hundred of them. They can c limb trees very easily.                   | their sharp teeth to gnaw at rocks and cut trees. They use their flat t ails to steer when they are swimming .In the winter, beavers have to bui ld a house called a beaver lodge. Th ey build a house that is very strong .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 89,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-90",
      "content": "gnaw at rocks and cut trees. They use their flat t ails to steer when they are swimming .In the winter, beavers have to bui ld a house called a beaver lodge. Th ey build a house that is very strong . The walls are made of twigs. The r oof is made              | , then they mean that you are very b usy. Beavers swim easily in streams, picking up rocks and sticks to buil d their dams. They gnaw at trees wit h their big front teeth to cut them down. Then they use parts of the tre es to build their houses.Beavers ar e clever builders. They know exactly what they need to build their beave r dams. They use mud from the stream to make their dams stay together. T hey use their tails to pat down the mud.Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the | w at trees with their big front teet h to cut them down. Then they use pa rts of the trees to build their hous es.Beavers are clever builders.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 90,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-91",
      "content": "e their food underwater. Be avers eat the bark from the | w at trees with their big front teet h to cut them down. Then they use pa rts of the trees to build their hous es.Beavers are clever builders. The y know exactly what they need to bui ld their beaver dams. They use mud f rom the stream to make their dams st ay together. They use their tails to pat down the mud.Beavers put a snu g room at the top of their dams for their babies. They store their food underwater. Beavers eat the bark fro mthe trees that they cut down!1.W hat is the main idea of the first pa ragraph?.2. What is the main idea o f the second paragraph?                  |\n\nA mouse is a small mammal that lives on land. It is a very good climber and it can run very fast. Penguins are birds that live on Antarctica. T hey have a thick coat to keep them w arm. Rabbits are small animals that live in the ground. They of branches and other tree parts. T",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 91,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-92",
      "content": "t can run very fast. Penguins are birds that live on Antarctica. T hey have a thick coat to keep them w arm. Rabbits are small animals that live in the ground. They of branches and other tree parts. T\n\nt trees. They bring them to their ho hey also use their strong jaws to cu\n\nuse. They also use their sharp teeth their flat tails to swim to the top\n\nto chew up the tree parts. They use of their house. Then they use their\n\nteeth and jaws to chew up the tree\n\nTable 7 j Sample - Hamlet, Act 1, Scene 1. The Retro[Off] sample has correct syntax but is hallucinated, and ends with repetition of one character ( FRANCISCO Approach me not ). The Retro[On] sample is the correct continuation of the original text, and is robust to formatting differences between our prompt and the retrieved data.\n\n| Prompt and sample of Retro[Off]",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 92,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-93",
      "content": "t and sample of Retro[Off]                                                                                                                                                                                                                     | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                             | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP 0, 1, 2, 3,4, 5                                                                                                                                                                                                                                                                                                                                                                                                                   | Â» ğ‘ 2 ğ‘¢ GLYPH<",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 93,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-94",
      "content": "| Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                         |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 94,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-95",
      "content": "-----------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 95,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-96",
      "content": "------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ACT I SCENE I. Elsinore. A pla tform before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: st and, and unfold yourself. BERN ARDO                                                       | ACT I SCENE I. Elsinore. A pla tform before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: st and, and unfold yourself.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 96,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-97",
      "content": "| ACT I SCENE I. Elsinore. A pla tform before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: st and, and unfold yourself. BERN ARDO                                                                 | = > ACT I SCENE I. Elsinore. A pla tform before the castle. (FRAN CISCO at his post. Enter to hi mBERNARDO) BERNARDO Who's the re? FRANCISCO Nay, answer me: stand, and unfold yourself. BE RNARDO Long live the king! FRA NCISCO Bernardo? BERNARDO He. FRANCISCO You come most carefu lly upon your hour. BERNARDO ' Tis now struck twelve; get the e to bed, Francisco. FRANCISCO For this relief much thanks: 'tis bitter cold, And I am sic k at heart. BERNARDO Have you               | ><TEXT>ACT I SCENE I. Elsin ore. A platform before the cas tle. FRANCISCO at his post. E nter to him BERNARDO BERNARDO Who's there? FRANCISCO Nay, an swer me: stand, and unfold you rself.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 97,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-98",
      "content": "| ><TEXT>ACT I SCENE I. Elsin ore. A platform before the cas tle. FRANCISCO at his post. E nter to him BERNARDO BERNARDO Who's there? FRANCISCO Nay, an swer me: stand, and unfold you rself. BERNARDO Long live the king! FRANCISCO Bernardo? BERN ARDO He. FRANCISCO You come mo st carefully upon your hour. B ERNARDO 'Tis now struck twelve ; get thee to bed, Francisco. FRANCISCO For this relief much thanks: 'tis bitter cold, And I am sick at heart.                                          |\n| Who calls ? FRANCISCO I am th e lord here; I, Francisco, tha t am sick of grief. [ Aside. B ERNARDO The king ! FRANCISCO I am sick of that also. BERNARD O My lord ? FRANCISCO Do not a pproach me. BERNARDO                                        | Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 98,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-99",
      "content": "| Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For t his relief much thanks: 'tis b itter cold, And I am sick at h eart. B | Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For t his relief much thanks: 'tis b itter cold, And I am sick at h eart.</TEXT></DOC><DOC><DO CNO>romeo</DOCNO><TEXT>ACT PROLOGUE Two households, bo th alike in dignity, In fair V erona, where we lay our scene, From ancient grudge break to new mutiny,                                               | live the king! FRANCISCO Bern ardo? BERNARDO He. FRANCISCO Y ou come most carefully upon yo ur hour. BERNARDO 'Tis now str uck twelve: get thee to bed, F rancisco.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 99,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-100",
      "content": "| live the king! FRANCISCO Bern ardo? BERNARDO He. FRANCISCO Y ou come most carefully upon yo ur hour. BERNARDO 'Tis now str uck twelve: get thee to bed, F rancisco. FRANCISCO For this r elief much thanks: 'tis bitter cold, And I am sick at heart. BERNARDO Have you had quiet g uard? FRANCISCO Not a mouse st irring. BERNARDO Well, good ni ght. Ifyou do meet Horatio and Marcellus, The rivals2 of my watch, bid them make haste. FR ANCISCO I think I hear them.- Stand, ho! who is there? EN |\n| Francisco, I would speak with you. FRANCISCO Approach me not , but speak. BERNARDO Your han d, your voice FRANCISCO I will not hear thee speak. BERNARDO Francisco, your hand, I entre at thee. FRANCISCO Approach me not. BERNARDO Francisco FRANC | ERNARDO Have you had quiet gua rd? FRANCISCO Not a mouse stir ring. BERNARDO Well, good nigh t. If you do meet Horatio and Marcellus, The rivals of my wa tch, bid them make haste. FRAN CISCO I think I hear them.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 100,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-101",
      "content": "e you had quiet gua rd? FRANCISCO Not a mouse stir ring. BERNARDO Well, good nigh t. If you do meet Horatio and Marcellus, The rivals of my wa tch, bid them make haste. FRAN CISCO I think I hear them. Sta nd, ho! who is there? Enter           | had quiet guard? FRANCISCO No t a mouse stirring. BERNARDOW ell, good night. If you do mee t Horatio and Marcellus, The r ivals of my watch, bid them ma ke haste. FRANCISCO I think I hear them. Stand, ho! Who's th ere? (Enter HORATIO and MARCEL LUS) HORATIO Friends to this g round. MARCELLUS And liegemen to the Dane. FRANCISCO Give yo u good night. MARCELLUS O, far ewell, honest soldier: Who hat h relieved you? FRANCISCO Bern ardo has my place. Give you go od night. (Exit | ARDO Have you had quiet guard? FRANCISCO Not a mouse stirrin g. BERNARDO Well, good night. Ifyou do meet Horatio and Marc ellus, The rivals2 of my watch , bid them make haste. FRANCIS CO I think I hear them.- Stand , ho! who is there? ENTER HORA TIO AND MARCELLUS.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 101,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-102",
      "content": "BERNARDO Well, good night. Ifyou do meet Horatio and Marc ellus, The rivals2 of my watch , bid them make haste. FRANCIS CO I think I hear them.- Stand , ho! who is there? ENTER HORA TIO AND MARCELLUS. HORATIO Fri ends to this ground. MARCELLUS And liegemen to the Dane.3 FR ANCISCO Give you good night.M ARCELLUS O, farewell, honest s oldier: Who hath relieved you? FRANCISCO Bernardo hath my pl ace. Give you good night   |\n\nISCO Approach me not. BERNARDO I have a letter FRANCISCO App roach me not. BERNARDO For the king. FRANCISCO Approach me n ot. BERNARDO There's no treaso n in't. FRANCISCO Approach me not. BERNARDO I will\n\nHORATIO and MARCELLUS HORATIO Friends to this ground. MARCE LLUS And liegemen to the Dane. FRANCISCO Give you good night . MARCELLUS O, farewell, hones t soldier: Who hath relieved y ou? FRANCISCO Bernardo hath my place. Give you good night.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 102,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-103",
      "content": "- M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In ACM SIGSAC Conference on Computer and Communications Security , 2016.\n- S. Ahn, H. Choi, T. PÃ¤rnamaa, and Y. Bengio. A neural knowledge language model. arXiv preprint arXiv:1608.00318 , 2016.\n- A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id= ByxZX20qFQ .\n- Y. Belinkov, S. Gehrmann, and E. Pavlick. Interpretability and analysis in neural NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts , pages 1-5, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-tutorials.1. URL https://aclanthology.org/2020.acl-tutorials.1 .\n- E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 103,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-104",
      "content": ". Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-tutorials.1. URL https://aclanthology.org/2020.acl-tutorials.1 .\n- E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In ACM Conference on Fairness, Accountability, and Transparency , 2021.\n- D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research , 3(Jan):993-1022, 2003. URL https://jmlr.csail.mit.edu/papers/v3/ blei03a.html .\n- J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. V. der Plas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .\n- T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large Language models in machine translation.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 104,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-105",
      "content": "AX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax .\n- T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large Language models in machine translation. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pages 858-867, 2007.\n- T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems , 2020. URL https://proceedings.neurips.cc/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n- N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 105,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-106",
      "content": "0. URL https://proceedings.neurips.cc/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n- N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language models. Preprint , 2021.\n- C. Consonni, D. Laniado, and A. Montresor. Wikilinkgraphs: a complete, longitudinal and multilanguage dataset of the wikipedia link networks. In AAAI International Conference on Web and Social Media , volume 13, 2019.\n12. Curation. Curation corpus base, 2020.\n- Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Annual Meeting of the Association for Computational Linguistics , July 2019. URL https://aclanthology.org/P19-1285 .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 106,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-107",
      "content": "tdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Annual Meeting of the Association for Computational Linguistics , July 2019. URL https://aclanthology.org/P19-1285 .\n\n- J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics , June 2019. URL https://aclanthology.org/N19-1423 .\n- L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\n- S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Conference on Empirical Methods in Natural Language Processing , Nov. 2020. URL https://aclanthology.org/2020.findings-emnlp.301 .\n- E.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 107,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-108",
      "content": "pts: Evaluating neural toxic degeneration in language models. In Conference on Empirical Methods in Natural Language Processing , Nov. 2020. URL https://aclanthology.org/2020.findings-emnlp.301 .\n- E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. In International Conference on Learning Representations , 2017. URL https://openreview.net/ forum?id=B184E5qee .\n- A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 , 2013.\n- J. Gu, Y. Wang, K. Cho, and V. O. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence , 2018.\n- R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning , 2020. URL https://arxiv.org/abs/1908.10396 .\n- K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 108,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-109",
      "content": "on. In International Conference on Machine Learning , 2020. URL https://arxiv.org/abs/1908.10396 .\n- K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International Conference on Machine Learning , 2020.\n- H. Hashemi, H. Zamani, and W. B. Croft. Guided transformer: Leveraging multiple external sources for representation learning in conversational search. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 1131-1140, 2020.\n- T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http: //github.com/deepmind/dm-haiku .\n- G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question answering. In Conference of the European Chapter of the Association for Computational Linguistics , Apr. 2021. URL https://aclanthology.org/2021.eacl-main.74 .\n- G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 109,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-110",
      "content": "n Conference of the European Chapter of the Association for Computational Linguistics , Apr. 2021. URL https://aclanthology.org/2021.eacl-main.74 .\n- G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S. Riedel, and E. Grave. A memory efficient baseline for open domain question answering. arXiv preprint arXiv:2012.15156 , 2020.\n- S. Jain and B. C. Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 3543-3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https: //aclanthology.org/N19-1357 .\n- E. S. Jo and T. Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pages 306-316, 2020.\n- R. Jozefowicz, O. Vinyals, M. Schuster, N.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 110,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-111",
      "content": "llecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , pages 306-316, 2020.\n- R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n\n- J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR , 2020. URL https://arxiv. org/abs/2001.08361 .\n- V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In Conference on Empirical Methods in Natural Language Processing , Nov. 2020. URL https://aclanthology.org/2020.emnlp-main.550 .\n- U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations , 2020.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 111,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-112",
      "content": ". Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\n- M. Komeili, K. Shuster, and J. Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566 , 2021.\n- T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 , 2018.\n- T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural Questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics , 7:452-466, Mar. 2019. URL https://aclanthology. org/Q19-1026 .\n- A. Lazaridou, A. Kuncoro, E.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 112,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-113",
      "content": "benchmark for question answering research. Transactions of the Association of Computational Linguistics , 7:452-466, Mar. 2019. URL https://aclanthology. org/Q19-1026 .\n- A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska, T. Terzi, M. Gimenez, C. de Masson d'Autume, S. Ruder, D. Yogatama, K. Cao, T. KociskÃ½, S. Young, and P. Blunsom. Pitfalls of static language modelling. CoRR , 2021. URL https://arxiv.org/abs/2102.01951 .\n- K. Lee, M.-W. Chang, and K. Toutanova. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Annual Meeting of the Association for Computational Linguistic , June 2019. URL http://arxiv.org/abs/1906.00300 .\n- K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 , 2021.\n- P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih, T. RocktÃ¤schel, S. Riedel, and D. Kiela.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 113,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-114",
      "content": "ge models better. arXiv preprint arXiv:2107.06499 , 2021.\n- P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. KÃ¼ttler, M. Lewis, W.-t. Yih, T. RocktÃ¤schel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems , 2020. URL https://proceedings. neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf .\n- P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question answering datasets. In Conference of the European Chapter of the Association for Computational Linguistics , Apr. 2021. URL https://aclanthology.org/2021.eacl-main.86 .\n- O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs , 2021.\n- I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 114,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-115",
      "content": ": Technical details and evaluation. White Paper. AI21 Labs , 2021.\n- I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7 .\n- S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations , 2017. URL https://openreview.net/forum?id= Byj72udxe .\n\n- T. Mikolov, M. KarafiÃ¡t, L. Burget, J. Cernock` y, and S. Khudanpur. Recurrent neural network based language model. Interspeech , 2(3):1045-1048, 2010.\n- D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. FernÃ¡ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Annual Meeting of the Association for Computational Linguistics , Aug. 2016. URL https:// aclanthology.org/P16-1144 .\n- A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 115,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-116",
      "content": "e context. In Annual Meeting of the Association for Computational Linguistics , Aug. 2016. URL https:// aclanthology.org/P16-1144 .\n- A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners. Preprint , 2019.\n- J. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 116,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-117",
      "content": "Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis &amp; insights from training Gopher. arXiv submission , 2021.\n- C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html .\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 117,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-118",
      "content": "ing Research , 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html .\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In IEEE International Conference for High Performance Computing, Networking, Storage and Analysis , 2020.\n- S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval , 3:333-389, Jan 2009.\n- D. S. Sachan, S. Reddy, W. Hamilton, C. Dyer, and D. Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. arXiv preprint arXiv:2106.05346 , 2021.\n- R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni. Green AI. Communications of the Association for Computing Machinery , 63(12):54-63, Nov. 2020.\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. CoRR , 2019.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 118,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-119",
      "content": "(12):54-63, Nov. 2020.\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. CoRR , 2019. URL http: //arxiv.org/abs/1909.08053 .\n- K. Shuster, S. Poff, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces hallucination in conversation. arXiv:2104.07567 [cs] , Apr. 2021. URL http://arxiv.org/abs/2104.07567 .\n- E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in NLP. In Association for Computational Linguistics , July 2019. URL https://aclanthology.org/ P19-1355 .\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 119,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-120",
      "content": "I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n\n- X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In ACM SIGIR International Conference on Research and Development in Information Retrieval , 2006. URL http://portal. acm.org/citation.cfm?doid=1148170.1148204 .\n- L. Weidinger, I. Gabriel, C. Griffin, M. Rauh, J. Uesato, J. Mellor, W. Isaac, P.-S. Huang, L. A. Hendricks, M. Cheng, B. Balle, J. Haas, C. Biles, L. Rimell, W. Hawkins, M. Glaese, A. Kasirzadeh, Z. Kenton, S. Brown, A. Birhane, T. Stepleton, G. Irving, and S. Legassick. Ethical and social risks of harm from language models. arXiv submission , 2021.\n- D. Yogatama, C. de Masson d'Autume, and L. Kong. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics , 9:362-373, 2021.\n- B. Zhang and R. Sennrich.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 120,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-121",
      "content": "- D. Yogatama, C. de Masson d'Autume, and L. Kong. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics , 9:362-373, 2021.\n- B. Zhang and R. Sennrich. Root mean square layer normalization. In Advances in Neural Information Processing Systems , 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf .\n- J. Zhang, M. Utiyama, E. Sumita, G. Neubig, and S. Nakamura. Guiding neural machine translation with retrieved translation pieces. In Conference of the North American Chapter of the Association for Computational Linguistics , 2018.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "References",
        "chunkIndex": 121,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-122",
      "content": "We provide a full description of MassiveText and of our extract of recent Wikipedia articles.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A. Datasets",
        "chunkIndex": 122,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-123",
      "content": "The full break down of MassiveText by source and languages is given in Table 8. For a full description and analysis of MassiveText, see Rae et al. (2021).\n\nTable 8 j MassiveText dataset. The final column indicates the sampling weight for each dataset during training. For the retrieval database, the entire dataset is used, with the exception of books for which we use a sub-sample of 4%.\n\n| Source    | Language   | Token count (M)   | Documents     |   Sampling weight |\n|-----------|------------|-------------------|---------------|-------------------|\n| Web       | En         | 483,002           | 604,938,816   |            0.314  |\n|           | Ru         | 103,954           | 93,004,882    |            0.033  |\n|           | Es         | 95,762            | 126,893,286   |            0.033  |\n|           | Zh         | 95,152            | 121,813,451   |            0.033  |\n|           | Fr         | 59,450            | 76,612,205    |            0.033  |\n|           | De         | 57",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A.1. Full description of MassiveText",
        "chunkIndex": 123,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-124",
      "content": "3  |\n|           | Zh         | 95,152            | 121,813,451   |            0.033  |\n|           | Fr         | 59,450            | 76,612,205    |            0.033  |\n|           | De         | 57,546            | 77,242,640    |            0.033  |\n|           | Pt         | 44,561            | 62,524,362    |            0.033  |\n|           | It         | 35,255            | 42,565,093    |            0.033  |\n|           | Sw         | 2,246             | 1,971,234     |            0.0044 |\n|           | Ur         | 631               | 455,429       |            0.0011 |\n| Books     | En         | 3,423,740         | 20,472,632    |            0.25   |\n| News      | En         | 236,918           | 397,852,713   |            0.1    |\n|           | En         | 3,977             | 6,267,214     |            0.0285 |\n|           | De         | 2,155             | 3,307,818     |            0.003  |\n|           | Fr         | 1,783             | 2,310,040     |            0.003  |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A.1. Full description of MassiveText",
        "chunkIndex": 124,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-125",
      "content": ",267,214     |            0.0285 |\n|           | De         | 2,155             | 3,307,818     |            0.003  |\n|           | Fr         | 1,783             | 2,310,040     |            0.003  |\n|           | Ru         | 1,411             | 2,767,039     |            0.003  |\n| Wikipedia | Es         | 1,270             | 2,885,013     |            0.003  |\n|           | It         | 1,071             | 2,014,291     |            0.003  |\n|           | Zh         | 927               | 1,654,772     |            0.003  |\n|           | Pt         | 614               | 1,423,335     |            0.003  |\n|           | Ur         | 61                | 344,811       |            0.0001 |\n|           | Sw         | 15                | 58,090        |            0.0004 |\n| Github    | -          | 374,952           | 142,881,832   |            0.05   |\n| Total     | -          | 5,026,463         | 1,792,260,998 |            1      |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A.1. Full description of MassiveText",
        "chunkIndex": 125,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-126",
      "content": "| 374,952           | 142,881,832   |            0.05   |\n| Total     | -          | 5,026,463         | 1,792,260,998 |            1      |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A.1. Full description of MassiveText",
        "chunkIndex": 126,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-127",
      "content": "We create an evaluation dataset consisting of 23 Wikipedia articles that were added or heavily edited in September 2021, after we collected our training dataset. In addition, we filter out articles that rely too heavily on templated content, using the method detailed in Â§2.6 to identify articles with chunks that have a high overlap with their neighbours. Fig. 10 show that little overlap remains between our test dataset and the retrieved neighbours from the training dataset. The full list of included articles is given in Table 9.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "A.2. Wikipedia September 2021",
        "chunkIndex": 127,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-128",
      "content": "| Megan Rohrer                         | Aakashavaani                                                                 |\n|--------------------------------------|------------------------------------------------------------------------------|\n| Emma Raducanu                        | Junior Eurovision Song Contest 2021                                          |\n| Ambra Sabatini                       | Pavilion Bukit Jalil                                                         |\n| WhyDonate                            | Blake Desjarlais                                                             |\n| The Juggernaut (company) Angela Diaz | 2021 All-Ireland Senior Football Championship Final Drift-barrier hypothesis |\n| 2020 Summer Paralympics              | Venomics                                                                     |\n| 2021 Afghan protests                 | Great Circle (novel)                                                         |\n| Rexh Xhakli                          |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 9 j Full set of articles included in our Wikipedia Sept. 2021 evaluation dataset.",
        "chunkIndex": 128,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-129",
      "content": "|\n| 2021 Afghan protests                 | Great Circle (novel)                                                         |\n| Rexh Xhakli                          | Hurricane Ida                                                                |\n| Julia Laskin                         | 2021 Montenegrin episcopal enthronement protests                             |\n| Cuijk                                | At War With the Silverfish                                                   |\n| Ghoubet Wind Power Station           |                                                                              |\n\nWe first parse articles using mwparserfromhell 5 . We then remove sections with the following titles: 'references', 'external links', 'sources', 'further reading', 'see also', 'citations', and 'note'.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 9 j Full set of articles included in our Wikipedia Sept. 2021 evaluation dataset.",
        "chunkIndex": 129,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-130",
      "content": "We first parse articles using mwparserfromhell 5 . We then remove sections with the following titles: 'references', 'external links', 'sources', 'further reading', 'see also', 'citations', and 'note'. In the remaining sections, we remove Wikilinks and remove the following templates: 'reflist', 'notelist', 'notelist-ua', 'notelist-lr', 'notelist-ur', and 'notelist-lg'. We also exclude objects with the 'ref' or 'table' tag and clean the remaining text with the strip\\_code function. Finally, we concatenate the title and all the sections and use \\n\\n to delimitate them.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 9 j Full set of articles included in our Wikipedia Sept. 2021 evaluation dataset.",
        "chunkIndex": 130,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-131",
      "content": "Wegive details on the Retro architecture, and on the fine-tuning procedure we use for Retrofitting existing language models.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B. Details on the retrieval architecture",
        "chunkIndex": 131,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-132",
      "content": "As mentioned in the main text, the overall encoder-decoder architecture is fully feed-forward. We start with a sequence ğ‘‹ 2 ğ• ğ‘› = ' ğ¶ğ‘¢ ' 1 6 ğ‘¢ 6 ğ‘™ , and its pre-computed neighbours ' Ret ' ğ¶ğ‘¢ '' 1 6 ğ‘¢ 6 ğ‘™ and returns logits in â„ ğ‘› GLYPH&lt;2&gt; j ğ• j . Along with Attn, Ffw, Cca and Ca operators introduced in the main text, we define the decoder embedding layer Emb : ğ• ğ‘› ! â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ , the Split operator that extracts chunked intermediary embeddings Split ' ğ» ' , ' ğ»ğ‘¢ ' 1 6 ğ‘¢ 6 ğ‘™ 2 â„ ğ‘™ GLYPH&lt;2&gt; ğ‘š GLYPH&lt;2&gt; ğ‘‘ and the read-out layer Read : â„ ğ‘› GLYPH&lt;2&gt; ğ‘‘ ! â„ ğ‘› GLYPH&lt;2&gt; j ğ• j . We then describe the forward pass in Algorithm 1. In addition to the usual Transformer ones, Retro architecture hyperparameters involves the layer indices ğ‘ƒ enc and ğ‘ƒ , at which the encoder and the decoder perform cross-attention.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.1.1. Feed-forward architecture",
        "chunkIndex": 132,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-133",
      "content": "The Ca operator uses relative positional logits, that are computed from a specific relative distance separating data tokens from retrieval tokens. Indeed, we expect any retrieval neighbour Ret ' ğ¶ğ‘¢ ' ğ‘— and the chunk ğ¶ğ‘¢ to be relatively well aligned, and assume that they start at the same position. Therefore, when computing Ca ' ğ» , ğ‘¢ GLYPH&lt;148&gt; ğ¸ğ‘¢ ' , we set the distance between the data token ğ‘– 2 Â» 1 GLYPH&lt;148&gt; ğ‘™ â€¦ of chunk ğ¶ , ğ‘¢ and\n\n5 https://github.com/earwig/mwparserfromhell\n\nthe retrieval token ğ‘– 0 2 Â» 1 GLYPH&lt;148&gt; 2 ğ‘™ â€¦ of Ret ' ğ¶ğ‘¢ ' ğ‘— to be\n\n<!-- formula-not-decoded -->\n\nWhen computing the encoder cross-attentions Ca ' Ret ' ğ¶ğ‘¢ ' ğ‘— GLYPH&lt;148&gt; ğ»ğ‘¢ ' , we set the distance between the retrieval token ğ‘– 0 2 Â» 1 GLYPH&lt;148&gt; 2 ğ‘™ â€¦ and the data token ğ‘– 2 Â» 1 GLYPH&lt;148&gt; ğ‘™ â€¦ to be\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.1.2. Relative positional encoding in the chunked cross-attention layer",
        "chunkIndex": 133,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-134",
      "content": "' ğ‘— GLYPH&lt;148&gt; ğ»ğ‘¢ ' , we set the distance between the retrieval token ğ‘– 0 2 Â» 1 GLYPH&lt;148&gt; 2 ğ‘™ â€¦ and the data token ğ‘– 2 Â» 1 GLYPH&lt;148&gt; ğ‘™ â€¦ to be\n\n<!-- formula-not-decoded -->\n\nPositional logits are obtained as a linear transform of a cosine vector computed from ' ğ‘‘ ' ğ‘–GLYPH&lt;148&gt; ğ‘– 0 '' ğ‘–GLYPH&lt;148&gt; ğ‘– 0 , and are added to content logits, as in a regular self-attention block.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.1.2. Relative positional encoding in the chunked cross-attention layer",
        "chunkIndex": 134,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-135",
      "content": "Our implementation of the Cca operator, shown in Listing 1, is based on a vectorized application of a cross-attention layer. For simplicity, we omit the multi-head attention logic and use the simplest Q,K,V attention. We omit relative positional logits computation, described above.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.1.3. Chunked cross-attention implementation",
        "chunkIndex": 135,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-136",
      "content": "We use disjoint embeddings for the encoder and decoder by default, which allows us to use a different dimensionality for the encoder (typically kept at ğ‘‘ Enc = 896 ' and for the decoder (that we scale up to ğ‘‘ = 8192). It is possible to share the embeddings, with little difference in training, as we show in the ablation section.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.1.4. Optional sharing of embedding matrices",
        "chunkIndex": 136,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-137",
      "content": "As shown in Fig. 5, we found that we were able to take a pre-trained baseline transformer and add Retro through fine-tuning. In all cases, we froze all weights from pre-training and freshly initialised the retrieval encoder and cross-attention weights. In all cases, the cross-attention is added every third layer starting at layer six. The learning rate for the three smaller models was set to 2 GLYPH&lt;2&gt; 10 GLYPH&lt;0&gt; 4 and half that for the larger model. We experimented with allowing the entire model to resume training during fine-tuning but consistently found that the best approach was to freeze the pre-trained model. This kept the retrieval-off performance frozen whereas when all weights were tuned the retrieval off performance would degrade.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "B.2. Baseline to Retro model fine-tuning",
        "chunkIndex": 137,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-138",
      "content": "We provide the hyperparameters used in the various experiments of Â§4.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C. Training details and hyperparameters",
        "chunkIndex": 138,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-139",
      "content": "In Table 10, we show the hyperparameters of the different models we train. In all cases, we train for 419,430,400,000 training tokens. The three smaller models are trained with a batch size of 256 and the largest model is trained with a batch size of 1024. The minimum learning rate is set to 0.1 times the maximum learning rate, which is shown in Table 10. The learning rate is decayed using a cosine cycle length that matches the total number of training tokens. All models are trained using AdamW (Loshchilov and Hutter, 2019) with a weight decay parameter of 0.1. The learning rate linearly increases from 10 GLYPH&lt;0&gt; 7 to the maximum learning rate over the first 750 steps of training. All models use ZeRO to shard the optimiser state (Rajbhandari et al., 2020). Additional infrastructure details can be found in Rae et al. (2021).",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.1. Language model pre-training",
        "chunkIndex": 139,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-140",
      "content": "```\nn = 128 # Sequence length m = 16 # Chunk length r = 32 # Retrieval length k = 4 # Number of neighbours d = 16 # Embedding size l = n // m # Number of chunks # Parameters Q = jnp.zeros((d, d)) K = jnp.zeros((d, d)) V = jnp.zeros((d, d)) def relative_positional_encodings(attending_length, attended_length): # Classical relative positional encodings ... def cross_attention(chunk, neighbour): m, d = chunk.shape r, d = neighbour.shape queries = chunk @ Q keys = neighbour @ K logits = queries @ keys.T values = neighbour @ V return logits, values def multi_neighbour_cross_attention(chunk, neighbours): m, d = chunk.shape k, r, d = neighbours.shape logits, values = jnp.vectorize(cross_attention, signature='(m,d),(r,d)->(m,r),(r,d)')( chunk, neighbours) assert logits.shape == (k, m, r) assert values.shape == (k, r, d) logits += relative_positional_encodings(m, r)[None, :, :] logits = jnp.moveaxis(logits, 0, -1).reshape((m, r * k)) values = jnp.moveaxis(values, 0, 1).reshape((r * k, d)) return",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Listing 1 j Jax implementation of the chunked cross attention , simplified.",
        "chunkIndex": 140,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-141",
      "content": "ues.shape == (k, r, d) logits += relative_positional_encodings(m, r)[None, :, :] logits = jnp.moveaxis(logits, 0, -1).reshape((m, r * k)) values = jnp.moveaxis(values, 0, 1).reshape((r * k, d)) return jax.nn.softmax(logits) @ values def multi_chunk_cross_attention(observation, neighbours): attending_chunks = jnp.pad(observation[m-1:], ((0, m -1), (0, 0)), mode='constant').reshape(l, m, d) chunked_output = jnp.vectorize(multi_neighbour_cross_attention, signature='(m,d),(k,r,d)->(m,d)')( attending_chunks, neighbours) assert chunked_output.shape == (l, m, d) output = jnp.pad(chunked_output.reshape(n, d), ((m -1, 0), (0, 0)), mode='constant')[:n] return output observation = jnp.zeros((n, d)) # Input neighbours = jnp.zeros((l, k, r, d)) h = multi_chunk_cross_attention(observation, neighbours) assert h.shape == (n, d) # Output\n```\n\nTable 10 j Retro model hyperparameters , along with the size of the decoder.\n\nBaseline",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Listing 1 j Jax implementation of the chunked cross attention , simplified.",
        "chunkIndex": 141,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-142",
      "content": "ros((l, k, r, d)) h = multi_chunk_cross_attention(observation, neighbours) assert h.shape == (n, d) # Output\n```\n\nTable 10 j Retro model hyperparameters , along with the size of the decoder.\n\nBaseline\n\n|        |   ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ |   ğ‘‘ ğ‘“ ğ‘“ğ‘¤ |    |     |    | ğ‘ƒ                                                                             | ğ‘ƒ Enc   |                          |\n|--------|-----------|----------|----|-----|----|-------------------------------------------------------------------------------|---------|--------------------------|\n| 247M   |       896 |     3584 | 16 |  64 | 12 | Â» 6 GLYPH<148> 9 GLYPH<148> 12 â€¦                                              | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 564M   |      1536 |     6144 | 12 | 128 | 12 | Â» 6 GLYPH<148> 9 GLYPH<148> 12 â€¦                                              | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 1,574M |      2048 |     8192 | 16 | 128 | 24 | Â» 9 GLYPH<148> 12 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Listing 1 j Jax implementation of the chunked cross attention , simplified.",
        "chunkIndex": 142,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-143",
      "content": "| Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 1,574M |      2048 |     8192 | 16 | 128 | 24 | Â» 9 GLYPH<148> 12 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> 24 â€¦ | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 7,505M |      4096 |    16384 | 32 | 128 | 32 | Â» 9 GLYPH<148> 12 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> 32 â€¦ | Â» 1 â€¦   | 1 GLYPH<2> 10 GLYPH<0> 4 |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Listing 1 j Jax implementation of the chunked cross attention , simplified.",
        "chunkIndex": 143,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-144",
      "content": "Head size",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "heads",
        "chunkIndex": 144,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-145",
      "content": "Max LR\n\nTable 11 j Hyperparameters for the Wikitext103 experiments presented in Table 4. We use the same learning rate schedule for the baseline and the Retro-fitting. For Retro-fitting, we reset the schedule i.e. the schedule starts from step 0, not from step 35,000.\n\n| Model         | Number of layers          | 18               |\n|---------------|---------------------------|------------------|\n|               | ğ‘‘                         | 1024             |\n|               | ğ‘‘ Ffw                     | 4096             |\n|               | Key size                  | 64               |\n|               | Value size                | 64               |\n|               | Number of heads           | 16               |\n| Training data | Dataset                   | Wikitext103train |\n|               | Sequence length           | 3072             |\n|               | Batch size                | 128              |\n|               | Tokenizer vocabulary size | 128,000          |\n| Optimisatio",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "layers",
        "chunkIndex": 145,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-146",
      "content": "| Sequence length           | 3072             |\n|               | Batch size                | 128              |\n|               | Tokenizer vocabulary size | 128,000          |\n| Optimisation  | optimiser                 | Adam             |\n|               | Adam's ğ›½ 1                | 0.9              |\n|               | Adam's ğ›½ 2                | 0.95             |\n|               | Adam's ğœ€                  | 1e-8             |\n|               | Dropout rate              | 0.25             |\n| Schedule      | Learning rate start       | 1e-7             |\n|               | Learning rate max         | 2.5e-4           |\n|               | Learning rate min         | 2e-5             |\n|               | Warmup steps              | 4,000            |\n|               | Cosine cycle steps        | 100,000          |\n| Evaluation    | Overlapping proportion    | 87.5%            |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "layers",
        "chunkIndex": 146,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-147",
      "content": "cycle steps        | 100,000          |\n| Evaluation    | Overlapping proportion    | 87.5%            |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "layers",
        "chunkIndex": 147,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-148",
      "content": "We provide more details on our Wikitext103 results presented in Â§4.1 and Table 4. We train a baseline transformer on the Wikitext103 training set with the hyperparameters presented in Table 11. The learning rate ramps linearly from 1 GLYPH&lt;2&gt; 10 GLYPH&lt;0&gt; 7 to 2 GLYPH&lt;147&gt; 5 GLYPH&lt;2&gt; 10 GLYPH&lt;0&gt; 4 in the first 4,000 steps, then decays to 2 GLYPH&lt;2&gt; 10 GLYPH&lt;0&gt; 5 at 100,000 steps using a cosine schedule. The baseline checkpoint at step 35,000 has the lowest perplexity on Wikitext103 valid, of 21 GLYPH&lt;147&gt; 58, for overlapping proportion of 75% (sliding window evaluation that only uses probabilities for tokens that have at least 75% of the sequence length of context, when available). We use this checkpoint for all our baseline and ğ‘˜ NN-LM numbers reported in Table 4, except that Table 4 reports for an overlapping proportion of 87.5 %, which slightly lowers the perplexity of our baseline to 21.53 on Wikitext103 valid.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.2. Wikitext103 comparison",
        "chunkIndex": 148,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-149",
      "content": "eline and ğ‘˜ NN-LM numbers reported in Table 4, except that Table 4 reports for an overlapping proportion of 87.5 %, which slightly lowers the perplexity of our baseline to 21.53 on Wikitext103 valid.\n\nWe also use the 35,000 step baseline checkpoint as initialization for a Retrofit, which otherwise uses the same optimiser and schedule hyperparameters but only trains the new retrieval weights, as explained in Â§4.2. Our best Retrofit checkpoint has a Wikitext103 valid perplexity 18 GLYPH&lt;147&gt; 46, when retrieving from Wikipedia. We use this Retro checkpoint in Table 4 for all other retrieval sets. The evaluation curves for our baseline and Retrofit is shown if Fig. 7 (left). In this particular case,\n\nbecause Wikitext103 is quite small, training a Retro model from scratch led to weaker results than the baseline, at least when retrieving from Wikipedia, as we couldn't find an effective way to mitigate the increased over-fitting due to the additional weights of Retro.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.2. Wikitext103 comparison",
        "chunkIndex": 149,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-150",
      "content": "ch led to weaker results than the baseline, at least when retrieving from Wikipedia, as we couldn't find an effective way to mitigate the increased over-fitting due to the additional weights of Retro.\n\nWe also re-implement ğ‘˜ NN-LM using the same tokenizer and dataset that we use for our baseline and Retrofitting experiments. ğ‘˜ NN-LM has probabilities ğ‘ğ‘˜ NN-LM = ğœ†ğ‘ğ¿ğ‘€ , ' 1 GLYPH&lt;0&gt; ğœ† ' ğ‘ğ‘˜ğ‘ğ‘ with ğ‘ğ‘˜ğ‘ğ‘ ' ğ‘›ğ‘˜ ' / exp 'GLYPH&lt;0&gt; ğ›¼ğ‘‘ğ‘˜ ' . To tune ğœ† and ğ›¼ , we begin with ğ›¼ = 0 GLYPH&lt;147&gt; 0012, which corresponds to the inverse of the standard deviation of the norm of the embeddings that we use as keys and queries for ğ‘˜ NN-LM. We find the best ğœ† = 0 GLYPH&lt;147&gt; 118. We then find the best ğ›¼ = 0 GLYPH&lt;147&gt; 00785 for that value of ğœ† . Fig. 7 center and right respectively show the perplexity of ğ‘˜ NN-LM as a function of ğœ† and ğ›¼ .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.2. Wikitext103 comparison",
        "chunkIndex": 150,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-151",
      "content": "\udf06 = 0 GLYPH&lt;147&gt; 118. We then find the best ğ›¼ = 0 GLYPH&lt;147&gt; 00785 for that value of ğœ† . Fig. 7 center and right respectively show the perplexity of ğ‘˜ NN-LM as a function of ğœ† and ğ›¼ .\n\nFigure 7 j Wikitext103valid perplexities. Left: Baseline and Retrofit (initialized from baseline's checkpoint at 35,000 steps) perplexities as a function of training steps. Center and right: ğ‘˜ NN-LM perplexity as a function of ğœ† (for ğ›¼ = 0 GLYPH&lt;147&gt; 0012) and ğ›¼ (for ğœ† = 0 GLYPH&lt;147&gt; 12) respectively.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.2. Wikitext103 comparison",
        "chunkIndex": 151,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-152",
      "content": "In Table 12, we give the hyperparameters used for Retrofitting the models on Massive Text.\n\nTable 12 j Hyperparameters for the Retrofitting experiments\n\n| Model   | Layers with Retro-block ( ğ‘ƒ )   | Learning rate                                       |   Batch size |\n|---------|---------------------------------|-----------------------------------------------------|--------------|\n| 172M    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 425M    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 1.5B    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 7.5B    | Every 3 rd from 6               | 1 GLYPH<2> 10 GLYPH<0> 4 ! 1 GLYPH<2> 10 GLYPH<0> 5 |          256 |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.3. Retrofitting baseline models experiments",
        "chunkIndex": 152,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-153",
      "content": "We fine-tune our 7.5B Retro model for 25,000 steps, using a batch size of 128, a learning rate cosine scheduled from 10 GLYPH&lt;0&gt; 6 to 10 GLYPH&lt;0&gt; 7 , with a linear ramp of 750 steps. We use dropout in the decoder only, as it performs better than using dropout in both the encoder and the decoder. Each neighbour is formatted as title: {title}, source: {source} . We use the top 20 neighbours from Dpr when training and evaluating.\n\nTable 13 j Performance of Retro for different variants. Model performance on C4 evaluation set, measured in bytes-per-bits, for a 247M parameter model trained with a 157 billion token schedule.\n\n| Ablation group           | Ablation                   |   C4 eval bpb |\n|--------------------------|----------------------------|---------------|\n| Model                    | Retro                      |         0.822 |\n|                          | No query conditioning      |         0.829 |\n|                          | No CA positional encodings |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.4. Question answering experiments",
        "chunkIndex": 153,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-154",
      "content": "| Retro                      |         0.822 |\n|                          | No query conditioning      |         0.829 |\n|                          | No CA positional encodings |         0.826 |\n|                          | Shared embeddings          |         0.823 |\n|                          | 6-layer encoder            |         0.821 |\n| Retrieval values         | Neighbours N               |         0.95  |\n|                          | Continuations F            |         0.895 |\n|                          | No retrieval               |         0.987 |\n| Training neighbours      | 1 training neighbours      |         0.858 |\n|                          | 4 training neighbours      |         0.847 |\n| Cross attention position | CA top layer (1/12)        |         0.827 |\n|                          | CA mid layer (6/12)        |         0.823 |\n|                          | CA top layer (12/12)       |         0.831 |\n|                          | CA all layers",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.4. Question answering experiments",
        "chunkIndex": 154,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-155",
      "content": "|                          | CA mid layer (6/12)        |         0.823 |\n|                          | CA top layer (12/12)       |         0.831 |\n|                          | CA all layers              |         0.86  |\n|                          | CA every 3 from 1          |         0.823 |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "C.4. Question answering experiments",
        "chunkIndex": 155,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-156",
      "content": "We validate important design choices by evaluating what happens when we do not include them. We use the 247M parameter model for all experiments and we train on a compressed 157 billion token schedule for all ablation experiments. We describe results relative to the default settings presented in the main text and recalled here. We report C4 evaluation loss at the end of the training process, and also compares how the evaluation loss decrease versus the training time, measured relatively to the baseline training time. Results are reported in Fig. 8 and Table 13.\n\nUsing relative encodings in cross-attention. Using relative encodings in cross-attention, as described in Â§B.1.2, provides a pure improvement both in the number of steps to reach a given performance and computational efficiency.\n\nConditioning the encoder on the previous chunk.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "D. Model ablations",
        "chunkIndex": 156,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-157",
      "content": "s-attention, as described in Â§B.1.2, provides a pure improvement both in the number of steps to reach a given performance and computational efficiency.\n\nConditioning the encoder on the previous chunk. Conditioning the encoder on the previous chunk's intermediate embeddings, as described in Â§B.1.1, provides a pure improvement both in term of number of steps and computational efficiency.\n\nSharing embeddings. Sharing embeddings across the encoder and the decoder does not affect performance. This motivates us using separate embeddings, as it allows to have a narrower encoder than decoder as we scale up the decoder size.\n\nAttending neighbours and their continuation. Retro models are trained by attending, for a given chunk, to both the neighbours of the preceding chunk and their continuation in time. We measure how training and evaluating Retro models on neighbours only and their continuation only affects performance.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "D. Model ablations",
        "chunkIndex": 157,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-158",
      "content": "k, to both the neighbours of the preceding chunk and their continuation in time. We measure how training and evaluating Retro models on neighbours only and their continuation only affects performance. Overall, attending to neighbours only provides 22% of the performance improvement due to retrieval in Retro, while attending the future of the neighbours gives 56% of\n\nFigure 8 j Computational efficiency for different variants. We report the training curves plotting C4 evaluation bytes per bits against time, relative to the time taken to train the baseline Retro model. Overall, our design choices are optimal in term of computational efficiency.\n\n<!-- image -->\n\nthe performance. Attending to both neighbours and their continuation is the most efficient choice both in term of final performance and training efficiency.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "D. Model ablations",
        "chunkIndex": 158,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-159",
      "content": "computational efficiency.\n\n<!-- image -->\n\nthe performance. Attending to both neighbours and their continuation is the most efficient choice both in term of final performance and training efficiency.\n\nTraining a deeper encoder. All models in the text use a relatively small Retro encoder. We experimented with a 3 GLYPH&lt;2&gt; deeper encoder. We found that this resulted in a tiny decrease in loss- 0.15% at the cost of a larger training time ( , 20%). Overall, using a shallow encoder is the best choice in term of training efficiency.\n\nTraining with multiple neighbours. We measure the effect of training on a single retrieved neighbour, as well as training on 4 neighbours (Retro uses 2 neighbours in training). Training on a single neighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement at the end of training, but induces a large computational overhead.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "D. Model ablations",
        "chunkIndex": 159,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-160",
      "content": "ighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement at the end of training, but induces a large computational overhead. Overall, we find that using 2 neighbours is the best choice in term of training efficiency. Furthermore, evaluation can be done with additional neighbours.\n\nFrequency of cross-attention. We measure how the frequency of cross-attention in the decoder affects performance. Overall, attending only once at the top or the bottom layer is a bad choice, while attending once on a mid-depth layer is relatively sound. We choose to have cross-attention every 3 layer as this provides a good trade-off between performance and run-time.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "D. Model ablations",
        "chunkIndex": 160,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-161",
      "content": "We illustrate the usage of Retro models by looking at the perplexity of evaluation samples and by producing samples autoregressively.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E. Qualitative experiments",
        "chunkIndex": 161,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-162",
      "content": "To build an intuition of what kind of information is leveraged by Retro models, we suggest to have a closer look at a few evaluation documents and the corresponding retrieved data in Tables 16, 17, 18 and 19. In these tables, the 4 rows corresponds to the first 4 chunks of the documents. The left-most column shows the chunk ğ¶ğ‘¢ from the document being evaluated, where each token is coloured by the negative cross entropy loss difference ğ¿ Retro[Off] GLYPH&lt;0&gt; ğ¿ Retro, a positive value, coloured in yellow, indicates that Retro performs better when it has access to neighbours data. The second columns also shows the evaluated chunk ğ¶ğ‘¢ but where each token ğ‘– is coloured by the length of the longest common prefix (LCP) with the preceding neighbours, i.e. the largest integer ğ‘— such that the prefix ' ğ‘¥ ğ‘– GLYPH&lt;0&gt; ğ‘— GLYPH&lt;0&gt; 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘– ' also appears in Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.1. Inspecting neighbours and perplexities on evaluation data",
        "chunkIndex": 162,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-163",
      "content": "that the prefix ' ğ‘¥ ğ‘– GLYPH&lt;0&gt; ğ‘— GLYPH&lt;0&gt; 1 GLYPH&lt;148&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;147&gt; GLYPH&lt;148&gt; ğ‘¥ ğ‘– ' also appears in Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' . Conversely, columns three and four show the first two neighbours and their continuation, respectively Â» ğ‘ 1 ğ‘¢ GLYPH&lt;148&gt; ğ¹ 1 ğ‘¢ â€¦ and Â» ğ‘ 2 ğ‘¢ GLYPH&lt;148&gt; ğ¹ 2 ğ‘¢ â€¦ coloured by LCP with subsequent chunk ğ¶ğ‘¢ , 1 . LCP colouring helps to visually identify where the evaluated document overlaps the retrieved data. Note that the first chunk, ğ¶ 1 , in the second column is not coloured as it does not have any preceding neighbours to compute LCP with. Similarly, we do not show the neighbours of the fourth chunk, as these are not used to condition any of the first four chunks.\n\nOur qualitative analysis exhibits two major behaviors.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.1. Inspecting neighbours and perplexities on evaluation data",
        "chunkIndex": 163,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-164",
      "content": "compute LCP with. Similarly, we do not show the neighbours of the fourth chunk, as these are not used to condition any of the first four chunks.\n\nOur qualitative analysis exhibits two major behaviors.\n\nFirstly , we observe that sometimes, specific facts in ğ¶ğ‘¢ can be extracted from the preceding neighbours Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' and that this can correspond to significant reduction in loss from the Retro model for the corresponding tokens. Some examples of such behavior include the journal name Publishers Weekly in Table 16, the football team name Tyrone in Table 17 or the event dates 25 August to 6 September 2020 in Table 18. In these three examples, the evaluated data consists of recent Wikipedia articles written in September 2021, after we built our retrieval dataset (see section Â§A.2). Yet, relevant information to predict this new data was available in the pre-existing retrieval data and the Retro model seems to be able to correctly leverage it.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.1. Inspecting neighbours and perplexities on evaluation data",
        "chunkIndex": 164,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-165",
      "content": "retrieval dataset (see section Â§A.2). Yet, relevant information to predict this new data was available in the pre-existing retrieval data and the Retro model seems to be able to correctly leverage it.\n\nOn the other hand, we also observe that some of the evaluation data can partially leak in our training and retrieval data, despite the use of deduplication. Retro can dramatically exploit such leakage. Table 19 illustrates this behavior, where the chunks ğ¶ 2 and ğ¶ 3 largely overlaps Ret ' ğ¶ 1 ' and Ret ' ğ¶ 2 ' respectively, up to small formatting differences, which leads to much lower Retro loss for all the corresponding tokens. Fig. 6 shows that it is possible to quantify how much of the Retro loss reduction is due to each of these two behaviors, by filtering out evaluation chunks that overlaps with the retrieval set.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.1. Inspecting neighbours and perplexities on evaluation data",
        "chunkIndex": 165,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-166",
      "content": "We can follow the same procedure as above on samples generated using Retro models, in order to better understand where retrieval data had an influence on sampling. We show examples of samples obtained using the 7.5B Retro model in Table 6, 7, 20 and 21.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.2. Inspecting samples",
        "chunkIndex": 166,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-167",
      "content": "To quantify a notion of distance between the source document and the retrieved chunks, we can ask the distance between source articles when retrieving only from Wikipedia. Consonni et al. (2019)\n\nFigure 9 j Wikipedia link-distance between retrieved articles. For each sequences, chunk combination we compute the link distance between the target and the top-5 neighbours using only Wikipedia. The rank shows the relative neighbour distance, where rank-1 is the first neighbour and rank 5 is the fifth. The different colours represent link distance. Because we do not retrieve from the same document, 1 is the smallest value. We find, on average, the distance between random articles with a path between them is over 5.0\n\n<!-- image -->\n\nprovides a Wikipedia link dataset which, for each article, contains a list of neighbouring articles. Using this, we construct a directed graph and compute the distance from one page to another. In Fig.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.3. Neighbour quantification",
        "chunkIndex": 167,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-168",
      "content": "rovides a Wikipedia link dataset which, for each article, contains a list of neighbouring articles. Using this, we construct a directed graph and compute the distance from one page to another. In Fig. 9 we compute the link-distance between training sequences and the retrieved neighbours. We find that retrieved documents tend to be from articles that are quite close to the article containing the target. Furthermore, we find that on average the distance increases with rank, suggesting that our neighbours are both useful and that the order is reasonable. This provides confidence for our larger-scale experiments where document distance is less well defined.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "E.3. Neighbour quantification",
        "chunkIndex": 168,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-169",
      "content": "We report tables corresponding to quantitative figures of the main text, as well as further filtered language model results on the Pile.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F. Complementary quantitative results",
        "chunkIndex": 169,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-170",
      "content": "We report the performance of Retro and baseline models, measured in bits-per-bytes on evaluation set, in Table 14.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.1. Main text datasets",
        "chunkIndex": 170,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-171",
      "content": "In Fig. 4, we compare Retro against Jurassic-1 (Lieber et al., 2021). The full bits-per-bytes results are reported in Table 15.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.2. The Pile",
        "chunkIndex": 171,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-172",
      "content": "Distribution of leaked chunks in our main evaluation sets. We evaluate leakage between the evaluation sets and the training set by measuring the proportion of evaluation chunks with a certain\n\nTable 14 j Full results for the main language modelling datasets. First three sets of rows correspond to Fig. 1, last set of rows to Fig. 3.\n\n|                          |       | Baseline   | Baseline   |       |       | Retro [Off]   | Retro [Off]   |       | Retro[On]   | Retro[On]   | Retro[On]   |      |\n|--------------------------|-------|------------|------------|-------|-------|---------------|---------------|-------|-------------|-------------|-------------|------|\n|                          | 172M  | 425M       | 1.5B       | 7.5B  | 172M  | 425M          | 1.5B          | 7.5B  | 172M        | 425M        | 1.5B        | 7.5B |\n| C4 Eval bpb              | 0.98  | 0.92       | 0.84       | 0.78  | 0.98  | 0.92          | 0.84          | 0.78  | 0.82        | 0.77        | 0.71        |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 172,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-173",
      "content": "| 425M        | 1.5B        | 7.5B |\n| C4 Eval bpb              | 0.98  | 0.92       | 0.84       | 0.78  | 0.98  | 0.92          | 0.84          | 0.78  | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb (900B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.88        | 0.83        | 0.76        | 0.71 |\n| C4 Eval bpb (360B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.92        | 0.87        | 0.80        | 0.74 |\n| C4 Eval bpb (180B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.94        | 0.89        | 0.81        | 0.75 |\n| C4 Eval bpb (90B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.95        | 0.89        | 0.82        | 0.76 |\n| C4 Eval bpb (36B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 173,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-174",
      "content": "| -             | -     | 0.95        | 0.89        | 0.82        | 0.76 |\n| C4 Eval bpb (36B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.90        | 0.83        | 0.77 |\n| C4 Eval bpb (18B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.91        | 0.83        | 0.77 |\n| C4 Eval bpb (9B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.91        | 0.83        | 0.77 |\n| C4 Eval bpb (4B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.97        | 0.91        | 0.84        | 0.78 |\n| C4 Eval bpb (2B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.97        | 0.91        | 0.84        | 0.78 |\n| C4 Eval bpb ( ğ‘˜ = 1)     | -     | -          | -          | -     | -     | -",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 174,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-175",
      "content": "-          | -     | -     | -             | -             | -     | 0.97        | 0.91        | 0.84        | 0.78 |\n| C4 Eval bpb ( ğ‘˜ = 1)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.79        | 0.73        | 0.67 |\n| C4 Eval bpb ( ğ‘˜ = 2)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.78        | 0.72        | 0.67 |\n| C4 Eval bpb ( ğ‘˜ = 3)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.78        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 4)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 5)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 10)    | -",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 175,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-176",
      "content": "Eval bpb ( ğ‘˜ = 5)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 10)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 20)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 30)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.65 |\n| C4 Eval bpb ( ğ‘˜ = 40)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.77        | 0.71        | 0.65 |\n| C4 Eval bpb ( ğ‘˜ = 50)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.78        | 0.71",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 176,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-177",
      "content": "| 0.77        | 0.71        | 0.65 |\n| C4 Eval bpb ( ğ‘˜ = 50)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.78        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 60)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.78        | 0.72        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 70)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.79        | 0.72        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 80)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | 0.73        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 90)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | 0.73        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 100)   | -     | -          | -          | -     | -     | -             | -             | -",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 177,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-178",
      "content": "| -             | -     | 0.85        | 0.79        | 0.73        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 100)   | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | -           | 0.67 |\n| Lambada Accuracy         | 0.42  | 0.51       | 0.61       | 0.69  | 0.47  | 0.54          | 0.63          | 0.70  | 0.52        | 0.60        | 0.67        | 0.73 |\n| Curation Corpus bpb      | 0.69  | 0.63       | 0.56       | 0.52  | 0.68  | 0.64          | 0.57          | 0.51  | 0.66        | 0.61        | 0.55        | 0.50 |\n| Wikitext103 Perplexity   | 25.62 | 19.29      | 13.98      | 10.65 | 25.88 | 19.78         | 13.89         | 10.40 | 3.32        | 2.96        | 2.53        | 2.22 |\n| Wikipedia Sept. 2021 bpb | 0.85  | 0.78       | 0.71       | 0.65  | 0.86  | 0.79          | 0.71          | 0.65  | 0.79        | 0.73        | 0.66        | 0.61 |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 178,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-179",
      "content": "96        | 2.53        | 2.22 |\n| Wikipedia Sept. 2021 bpb | 0.85  | 0.78       | 0.71       | 0.65  | 0.86  | 0.79          | 0.71          | 0.65  | 0.79        | 0.73        | 0.66        | 0.61 |\n\noverlap ğ‘Ÿ ' ğ¶ ' . We show histograms in Fig. 10. We can see that ğ¶ 4 has some slight overlaps between train and evaluation. Similarly, chunks of Wikitext103 appear in the training set despite having removed the actual Wikitext103 evaluation documents from the training set. On the other hand, our Wikipedia September 21 dataset shows almost no leakage (data being original documents that did not exist at training data creation), and neither does Curation Corpus.\n\nFiltered results on the Pile. We report chunk overlap distribution and filtered performance curves on the Pile in Fig. 12 and Fig. 11, respectively.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 179,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-180",
      "content": "data creation), and neither does Curation Corpus.\n\nFiltered results on the Pile. We report chunk overlap distribution and filtered performance curves on the Pile in Fig. 12 and Fig. 11, respectively. The qualitative interpretation of the filtered curves is the same: Retro models exploit leakage more, but the performance improvement they provide remains significant even on original chunks that haven't been observed in the training set.\n\nTable 15 j Full results on The Pile, measured in bits-per-bytes. Jurassic-1 and GPT-3 numbers are taken from Lieber et al. (2021). Gopher numbers are taken from Rae et al. (2021).\n\nSubset\n\n7B Baseline (Ours)\n\nGPT-3\n\nJurassic-1\n\nGopher\n\n7.5B Retro\n\nFigure 10 j Distribution of the overlap between evaluation and train chunks for C4, Curation Corpus, Wikitext103 and Wikipedia Sept. 2021.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 180,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-181",
      "content": "t\n\n7B Baseline (Ours)\n\nGPT-3\n\nJurassic-1\n\nGopher\n\n7.5B Retro\n\nFigure 10 j Distribution of the overlap between evaluation and train chunks for C4, Curation Corpus, Wikitext103 and Wikipedia Sept. 2021.\n\n| arxiv             |   0.742 |   0.838 |   0.680 |   0.641 |   0.714 |\n|-------------------|---------|---------|---------|---------|---------|\n| books3            |   0.792 |   0.802 |   0.835 |   0.706 |   0.653 |\n| dm_mathematics    |   1.177 |   1.371 |   1.037 |   1.135 |   1.164 |\n| freelaw           |   0.576 |   0.612 |   0.514 |   0.506 |   0.499 |\n| github            |   0.42  |   0.645 |   0.358 |   0.367 |   0.199 |\n| gutenberg_pg_19   |   0.803 |   1.163 |   0.89  |   0.652 |   0.4   |\n| hackernews        |   0.971 |   0.975 |   0.869 |   0.888 |   0.86  |\n| nih_exporter      |   0.65  |   0.612 |   0.59  |   0.59  |   0.635 |\n| opensubtitles     |   0.974 |   0.932 |   0.879 |   0.894 |   0.93  |\n| philpapers        |   0.76  |   0.723 |   0.742 |   0.682 |   0.699 |\n| pile",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 181,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-182",
      "content": "0.65  |   0.612 |   0.59  |   0.59  |   0.635 |\n| opensubtitles     |   0.974 |   0.932 |   0.879 |   0.894 |   0.93  |\n| philpapers        |   0.76  |   0.723 |   0.742 |   0.682 |   0.699 |\n| pile_cc           |   0.771 |   0.698 |   0.669 |   0.688 |   0.626 |\n| pubmed_abstracts  |   0.639 |   0.625 |   0.587 |   0.578 |   0.542 |\n| pubmed_central    |   0.588 |   0.69  |   0.579 |   0.512 |   0.419 |\n| stackexchange     |   0.714 |   0.773 |   0.655 |   0.638 |   0.624 |\n| ubuntu_irc        |   1.2   |   0.946 |   0.857 |   1.081 |   1.178 |\n| uspto_backgrounds |   0.603 |   0.566 |   0.537 |   0.545 |   0.583 |\n\n<!-- image -->\n\nFigure 11 j Filtered evaluation losses on the Pile , with baseline Transformers and Retro.\n\n<!-- image -->\n\nFigure 12 j Distribution of the overlap between evaluation and train chunks for the Pile evaluation sets.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 182,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-183",
      "content": "on losses on the Pile , with baseline Transformers and Retro.\n\n<!-- image -->\n\nFigure 12 j Distribution of the overlap between evaluation and train chunks for the Pile evaluation sets.\n\n<!-- image -->\n\nTable 16 j Great Circle (novel) , from Wikipedia September 21. The article is about a recent novel and chunks ğ¶ 3 and ğ¶ 4 are specifically about its reception. The name Publishers Weekly of the journal that reviewed the novel appears both in the neighbours Â» ğ‘ 1 3 GLYPH&lt;148&gt; ğ¹ 1 3 â€¦ GLYPH&lt;148&gt; Â» ğ‘ 2 3 GLYPH&lt;148&gt; ğ¹ 2 3 â€¦ of chunk ğ¶ 3 and in the subsequent chunk ğ¶ 4 , where the loss for those tokens is significantly reduced by Retro.\n\n<!-- image -->\n\n| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 183,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-184",
      "content": "> 0                                                                                                                                                                                                                           | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                         | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 184,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-185",
      "content": "|\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 185,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-186",
      "content": "--------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 186,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-187",
      "content": "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Great Circle (novel)Great Circle i s a 2021 novel by Maggie Shipstead, published on May 4, 2021, by Alfred A. Knopf.The novel has been shortl isted for the 2021 Booker Prize.Sy nopsis The novel consists of two pa rallel narratives about two fictiona l women. One is                                                                      | Great Circle (novel) Great Circle i s a 2021 novel by Maggie Shipstead, published on May 4, 2021, by Alfred A. Knopf.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 187,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-188",
      "content": "men. One is                                                                      | Great Circle (novel) Great Circle i s a 2021 novel by Maggie Shipstead, published on May 4, 2021, by Alfred A. Knopf. The novel has been shortl isted for the 2021 Booker Prize. Sy nopsis The novel consists of two pa rallel narratives about two fictiona l women. One is                                                                   | The Dutch House (novel)The Dutch H ouse is a 2019 novel by Ann Patchett . It was published by Harper on Sept ember 24, 2019. It tells the story o f a brother and sister over the cour se of five decades.The novel was a finalist for the 2020 Pulitzer Priz e for Fiction.PlotThe Dutch House is a mansion located in Elkins Park , Pennsylvania, a suburb of Philadel phia. It was built in 1922 by the Va nHoebeek family, a husband and wife originally from the Netherlands who made their fortune in the tobacco in dustry.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 188,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-189",
      "content": "kins Park , Pennsylvania, a suburb of Philadel phia. It was built in 1922 by the Va nHoebeek family, a husband and wife originally from the Netherlands who made their fortune in the tobacco in dustry. Cyril Conroy, a self-made re al estate mogul                                                                               | The Dutch House (novel)The Dutch H ouse is a 2019 novel by Ann Patchett . It was published by Harper on Sept ember 24, 2019. It tells the story o f a brother and sister over the cour se of five decades.[2]The novel wa s a finalist for the 2020 Pulitzer P rize for Fiction.[3]Plot[edit]Th e Dutch House is a mansion located i n Elkins Park, Pennsylvania, a subur b of Philadelphia. It was built in 1 922 by the VanHoebeek family, a husb and and wife originally from the Net herlands who made their fortune in t he tobacco industry.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 189,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-190",
      "content": "ins Park, Pennsylvania, a subur b of Philadelphia. It was built in 1 922 by the VanHoebeek family, a husb and and wife originally from the Net herlands who made their fortune in t he tobacco industry. Cyril Conroy, a self-                                         |\n| about the disappeared 20th-century aviator Marian Graves, while the oth er is about the struggling 21st-cent ury Hollywood actress Hadley Baxter, who is attempting to make a film ab out Marian. Hadley's narrative is to ld in the first-person, while Marian 's sections are told in the third-pe rson                                      | about the disappeared 20th-century aviator Marian Graves, while the oth er is about the struggling 21st-cent ury Hollywood actress Hadley Baxter, who is attempting to make a film ab out Marian. Hadley's narrative is to ld in the first-person, while Marian 's sections are told in the third-pe rson                                      | on becoming a filmmaker.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 190,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-191",
      "content": "ng to make a film ab out Marian. Hadley's narrative is to ld in the first-person, while Marian 's sections are told in the third-pe rson                                      | on becoming a filmmaker. She has fo und a subject for her film project, an obscure African American actress credited only as 'the watermelon wom an' in old Hollywood films, and the subsequent film recounts her search for this woman even as it covers, in the manner of the earlier Dunyement aries, Dunye's friendships and her l ove life. InThe Watermelon Woman, D unye makes the film she set out tom ake in 1990 about African American w omen artists, a film that both inven ts an artistic predecessor with whom she can identify and also 'finds' C heryl herself as the artist that she seeks. As Dunye identifies herself                     | based closely on her own youthful e xperiences.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 191,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-192",
      "content": "predecessor with whom she can identify and also 'finds' C heryl herself as the artist that she seeks. As Dunye identifies herself                     | based closely on her own youthful e xperiences. (She plans the film to b e the first of two parts, the second dealing with the aftermath of the f irst's events.) Byrne plays a young film student named Julie (Hogg's ava tar), who starts her artistic educat ion with high hopes of making a movi e about a boy named Tony, living in working-class Sunderland, who adores his mother -'is almost obsessed wi th her,' as eager Julie tells her ad visers. Her idealism is evident from the start.The advisers are skepti cal, and no wonder; Julie's family i s posh, with a comfortable country e state and |\n| .Reception Great Circle received very favorable reviews, with a cumul ative \"Rave\" rating at the review ag gregator website Book Marks, based o n 22 book reviews from mainstream li terary critics.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 192,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-193",
      "content": "|\n| .Reception Great Circle received very favorable reviews, with a cumul ative \"Rave\" rating at the review ag gregator website Book Marks, based o n 22 book reviews from mainstream li terary critics. The novel debuted at number fourteen on The New York Tim es Hardcover fiction best-seller lis t for the week ending May                   | .Reception Great Circle received very favorable reviews, with a cumul ative \"Rave\" rating at the review ag gregator website Book Marks, based o n 22 book reviews from mainstream li terary critics. The novel debuted at number fourteen on The New York Tim es Hardcover fiction best-seller lis t for the week ending May                   | first edition hardcoverReception The novel debuted at number one on T he New York Times fiction best-selle r list. As of the week ending Februa ry 20, 2021, the novel has spent 38 weeks on the list.At the review ag gregator website Book Marks, which a ssigns individual ratings to book re views from mainstream literary cr",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 193,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-194",
      "content": "he week ending Februa ry 20, 2021, the novel has spent 38 weeks on the list.At the review ag gregator website Book Marks, which a ssigns individual ratings to book re views from mainstream literary criti cs, the novel received a cumulative \"Rave\" rating based on 38 reviews, w ith only one \"mixed\" review. Publish ers Weekly wrote, \"Bennett renders h er characters and their struggles wi th great compassion, and explores th e complicated state of mind that Ste lla finds herself in while passing a s white.\" In its | The book also debuted at number tw o on The New York Times Hardcover No nfiction best-sellers list on July 2 8, 2019.[5] It spent eleven weeks on the list.[6]Reception[edit]At t he review aggregator website Book Ma rks, which assigns individual rating s to book reviews from mainstream li terary critics, the book received a cumulative \"Positive\" rating based o n 29 reviews: 12 \"Rave\" reviews, 6 \" Positive\" reviews, 9 \"Mixed\" reviews , and 2 \"Pan\" reviews.[7] Publisher s Weekly g",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 194,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-195",
      "content": "eam li terary critics, the book received a cumulative \"Positive\" rating based o n 29 reviews: 12 \"Rave\" reviews, 6 \" Positive\" reviews, 9 \"Mixed\" reviews , and 2 \"Pan\" reviews.[7] Publisher s Weekly gave the book a mixed revie w, writing, \"Unfortunately, all thre e                                              |\n| 8, 2021. Critics praised the novel for sustaining its length and for Sh ipstead's research and intricate nov el structure for perfectly interweav ing the parallel narratives, despite the time and circumstances separati ng them.In its starred review, Pub lishers Weekly wrote, \"Shipstead man ages to portray both Marian's and Ha dley's | 8, 2021. Critics praised the novel for sustaining its length and for Sh ipstead's research and intricate nov el structure for perfectly interweav ing the parallel narratives, despite the time and circumstances separati ng them.In its starred review, Pub lishers Weekly wrote, \"Shipstead man ages to portray both Marian's and Ha dley's |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 195,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-196",
      "content": "g the parallel narratives, despite the time and circumstances separati ng them.In its starred review, Pub lishers Weekly wrote, \"Shipstead man ages to portray both Marian's and Ha dley's |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 196,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-197",
      "content": "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n\nTable 17 j All-Ireland Senior Football Championship Final , from Wikipedia September 21. The name of the team Tyrone appears both in the second neighbours Â» ğ‘ 2 1 GLYPH&lt;148&gt; ğ¹ 2 1 â€¦ of chunk ğ¶ 1 and in the subsequent chunk ğ¶ 2 , where the loss for those tokens is significantly reduced by Retro.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 197,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-198",
      "content": "team Tyrone appears both in the second neighbours Â» ğ‘ 2 1 GLYPH&lt;148&gt; ğ¹ 2 1 â€¦ of chunk ğ¶ 1 and in the subsequent chunk ğ¶ 2 , where the loss for those tokens is significantly reduced by Retro.\n\nğ¶ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH&lt;0&gt; ğ¿ Retro 6 GLYPH&lt;0&gt; 0 GLYPH&lt;147&gt; 5 GLYPH&lt;148&gt; = 0 GLYPH&lt;148&gt; &gt; 0 GLYPH&lt;147&gt; 5\n\n2021 All-Ireland Senior Football Cha mpionship FinalThe 2021 All-Irelan d Senior Football Championship Final was the 134th final of the All-Irel and Senior Football Championship and the culmination of the 2021 All-Ire land Senior Football Championship. T he match was played at Croke Park in Dublin on 11 September 2021. It was originally scheduled for 28 August but had to be postpon ed by two weeks when the - semi-fina l was postponed due to a COVID-19 ou tbreak. Ulster champions Tyrone took on Connacht champions Mayo, in what was their first ever meeting in a f inal, winning their 4th title after a 2-14 to 0-15 win.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 198,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-199",
      "content": "was postponed due to a COVID-19 ou tbreak. Ulster champions Tyrone took on Connacht champions Mayo, in what was their first ever meeting in a f inal, winning their 4th title after a 2-14 to 0-15 win. Mayo lost\n\ntheir 11th consecutive final since 1989, losing 6 finals in 9 years, wi th this latest defeat on an identica l scoreline to 2020, when Mayo lost to Dublin.Background were aiming to win their fourth title and first All-Ireland since 1951. Since then, they had lost ten finals (1989, 1996 , 1997, 2004, 2006,\n\n2012, 2013, 2016, 2017, 2020). app eared in their seventh final, winnin g on three occasions in 2003, 2005 a nd 2008.This final was the fifth to be contested by county teams from C onnacht and Ulster, the other finals were 1925 (Galway beat Cavan), 1943 (Roscommon beat Cavan), 1948 (Cavan beat\n\nğ¶ğ‘¢ colored by LCP with Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' LCP = 0, 1, 2, 3,4, &gt; 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 199,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-200",
      "content": "m C onnacht and Ulster, the other finals were 1925 (Galway beat Cavan), 1943 (Roscommon beat Cavan), 1948 (Cavan beat\n\nğ¶ğ‘¢ colored by LCP with Ret ' ğ¶ğ‘¢ GLYPH&lt;0&gt; 1 ' LCP = 0, 1, 2, 3,4, &gt; 5\n\n2021 All-Ireland Senior Football Cha mpionship Final The 2021 All-Irelan d Senior Football Championship Final was the 134th final of the All-Irel and Senior Football Championship and the culmination of the 2021 All-Ire land Senior Football Championship. T he match was played at Croke Park in Dublin on 11 September 2021. It was originally scheduled for 28 August but had to be postpon ed by two weeks when the - semi-fina l was postponed due to a COVID-19 ou tbreak. Ulster champions Tyrone took on Connacht champions Mayo, in what was their first ever meeting in a f inal, winning their 4th title after a 2-14 to 0-15 win. Mayo lost",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 200,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-201",
      "content": "oned due to a COVID-19 ou tbreak. Ulster champions Tyrone took on Connacht champions Mayo, in what was their first ever meeting in a f inal, winning their 4th title after a 2-14 to 0-15 win. Mayo lost\n\ntheir 11th consecutive final since 1989, losing 6 finals in 9 years, wi th this latest defeat on an identica l scoreline to 2020, when Mayo lost to Dublin.Background were aiming to win their fourth title and first All-Ireland since 1951. Since then, they had lost ten finals (1989, 1996 , 1997, 2004, 2006,\n\n2012, 2013, 2016, 2017, 2020). app eared in their seventh final, winnin g on three occasions in 2003, 2005 a nd 2008.This final was the fifth to be contested by county teams from C onnacht and Ulster, the other finals were 1925 (Galway beat Cavan), 1943 (Roscommon beat Cavan), 1948 (Cavan beat\n\nÂ» ğ‘ 1 ğ‘¢ GLYPH&lt;148&gt; ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, &gt; 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 201,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-202",
      "content": "and Ulster, the other finals were 1925 (Galway beat Cavan), 1943 (Roscommon beat Cavan), 1948 (Cavan beat\n\nÂ» ğ‘ 1 ğ‘¢ GLYPH&lt;148&gt; ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, &gt; 5\n\n2018 All-Ireland Senior Football Cha mpionship FinalThe 2018 All-Irelan d Senior Football Championship Final was the 131st final of the All-Irel and Senior Football Championship and the culmination of the 2018 All-Ire land Senior Football Championship in Gaelic football. The match was play ed at Croke Park in Dublin on 2 Sept ember 2018.[3]It was the second ti me the teams had met in the final; D ublin won the first encounter in 199 5.The final was shown live in Irel and on RTÃ‰ Two as part of The Sunday Game live programme, presented by M ichael Lyster from Croke Park, with studio analysis from Joe Brolly, game 23-23 after extra time, howeve r Ulster progressed under the compet ition rules as they scored three tir es in the match against Leinster's t wo.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 202,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-203",
      "content": "Croke Park, with studio analysis from Joe Brolly, game 23-23 after extra time, howeve r Ulster progressed under the compet ition rules as they scored three tir es in the match against Leinster's t wo. The semi-finals took place in mi d November and saw both the away tea ms win, as Ulster beat Glasgow and E dinburgh beat Connacht. The final wa s held on Saturday December 20 at Mu rrayfield Stadium and saw Ulster bea t Edinburgh 21-27 to win the Celtic Cup.2004-05 seasonThe format of the competition was changed for the second edition of the competition. T he competition was moved to April an d May to run after the conclusion of the Celtic League competition, with only eight\n\n1-16 to 0-15 winners to qualify for their 10th league final in the past 13 years.They have won seven of t heir previous league finals under Co dy since 2002, losing the other two to Waterford (2007 ) and Dublin (201 1 ).Despite the defeat there were some distinct positives from a Galwa y perspective- most notably the",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 203,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-204",
      "content": "s league finals under Co dy since 2002, losing the other two to Waterford (2007 ) and Dublin (201 1 ).Despite the defeat there were some distinct positives from a Galwa y perspective- most notably the soli d displays of DaithÃ­ Burke at centre -back, Joseph Cooney at wing-back an d Ronan Burke at full-back. Colm Cal lanan continued his excellent form i n goal and also hit a stunning free from distance.Indeed it was not th e Galway defence that was the proble m\n\nÂ» ğ‘ 2 ğ‘¢ GLYPH&lt;148&gt; ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, &gt; 5\n\n2018 All-Ireland Senior Football Cha mpionship FinalThe 2018 All-Irelan d Senior Football Championship Final was the 131st final of the All-Irel and Senior Football Championship and the culmination of the 2018 All-Ire land Senior Football Championship in Gaelic football. The match was play ed at Croke Park in Dublin on 2 Sept ember 2018.It was the second time the teams had met in the final; Dubl in won the first encounter in 1995.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 204,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-205",
      "content": "ball Championship in Gaelic football. The match was play ed at Croke Park in Dublin on 2 Sept ember 2018.It was the second time the teams had met in the final; Dubl in won the first encounter in 1995. It was the third consecutive year th at a team qualified under the system of second chances introduced in 200 1; Tyrone qualified despite defeat i n its provincial championship.Dubl in won the final by a margin of six points with a last-ditch plan of action play the Munster/Ulster Semi-Final o n March 16th, with the winners to pl ay Connacht in the following day's F inal.On March 16th then Munster ha d an easy win over Ulster (9-07 to 0 -00) but thankfully for the Munster players, the pitch cut up so badly d uring the game, it was decided to po stpone the following day's hurling F inal (until Easter Sunday) with the football Final going ahead on its ow n on St. Patrick's Day.Less than a week later, on March 23rd, seven",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 205,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-206",
      "content": ", it was decided to po stpone the following day's hurling F inal (until Easter Sunday) with the football Final going ahead on its ow n on St. Patrick's Day.Less than a week later, on March 23rd, seven\n\nwhich Dublin won by 0-12 to 0-9.D ublin are going for an unprecedented fourth successive Championship win over Kerry. Prior to their current r un, which started with the 2011 AllIreland final, they had only managed two consecutive victories over them on two separate occasions - 1909 an d '24, 1976 and '77.The longest wi nning sequence in the rivalry was se t by Kerry between 1941 and 1975, wh en they won each of the six Champion ship meetings. Kerry went nine games unbeaten between 1978 and 2009, wit h four victories either side of a dr amatic draw at the quarter-final sta ge in Thurles in 2001.Sunday will mark their 11th",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 206,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-207",
      "content": "ion ship meetings. Kerry went nine games unbeaten between 1978 and 2009, wit h four victories either side of a dr amatic draw at the quarter-final sta ge in Thurles in 2001.Sunday will mark their 11th\n\nTable 18 j 2020 Summer Paralympics , from Wikipedia September 21. The original dates of the event, 25 August to 6 September 2020 , appears both in the neighbors Â» ğ‘ 1 1 GLYPH&lt;148&gt; ğ¹ 1 1 â€¦ GLYPH&lt;148&gt; Â» ğ‘ 2 1 GLYPH&lt;148&gt; ğ¹ 2 1 â€¦ of chunk ğ¶ 1 and in the subsequent chunk ğ¶ 2 , where the loss for those tokens is significantly reduced by Retro. Interestingly, in this case, the neighbors were written at a time when the event hadn't yet been postponed.\n\n<!-- image -->\n\n| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0 GLYPH<147>",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 207,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-208",
      "content": "LYPH<148> > 0 GLYPH<147>                                                                                                                                                                                                                              | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                       | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 208,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-209",
      "content": "|\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 209,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-210",
      "content": "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 210,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-211",
      "content": "------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2020 Summer ParalympicsThe , brand ed as the Tokyo 2020 Paralympic Game s, was an international multi-sport parasports event held from 24 August to 5 September 2021 in Tokyo, Japan .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 211,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-212",
      "content": "--------------|\n| 2020 Summer ParalympicsThe , brand ed as the Tokyo 2020 Paralympic Game s, was an international multi-sport parasports event held from 24 August to 5 September 2021 in Tokyo, Japan . They were the 16th Summer Paralymp ic Games as organized by the Interna tional Paralympic Committee (IPC).                                                            | 2020 Summer Paralympics The , brand ed as the Tokyo 2020 Paralympic Game s, was an international multi-sport parasports event held from 24 August to 5 September 2021 in Tokyo, Japan . They were the 16th Summer Paralymp ic Games as organized by the Interna tional Paralympic Committee (IPC).                                                           | pics Games.* The 2020 Summer Paraly mpics are an upcoming major internat ional multi-sport event for athletes with disabilities governed by the I nternational Paralympic Committee.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 212,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-213",
      "content": "| pics Games.* The 2020 Summer Paraly mpics are an upcoming major internat ional multi-sport event for athletes with disabilities governed by the I nternational Paralympic Committee. S cheduled as the 16th Summer Paralymp ic Games, it is planned to be held i n Tokyo, Japan from 25 August to 6 S eptember 2020 .3. 2019 BWF Para-Bad minton World Championships- The 20 19 BWF Para-Badminton World Champion ships was held from 20 to 25 August 2019 in Basel, Switzerland.- Men's event: Gold Medal: Pramod Bhagat in Singles SL3 Event and Pramod Bhagat and Manoj                                                                                             | 1 2020 Summer ParalympicsThe are an upcoming major international multi- sport event for athletes with disabi lities governed by the International Paralympic Committee. Scheduled as the 16th Summer Paralympic Games, th ey are scheduled to be held in Tokyo , Japan between 24 August and 5 Sept ember 2021.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 213,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-214",
      "content": "isabi lities governed by the International Paralympic Committee. Scheduled as the 16th Summer Paralympic Games, th ey are scheduled to be held in Tokyo , Japan between 24 August and 5 Sept ember 2021. Originally due to take p lace between 25 August and 6 Septemb er 2020 . On 24 March 2020, the IOC a nd the Tokyo Organizing Committee of ficially announced that the 2020 Sum mer Olympics and 2020 Summer Paralym pics would be postponed to 2021, due to the COVID-19 pandemic, marking t he first time that the Paralympics h as been postponed. They will still b e publicly marketed as |\n| Originally scheduled to take place f rom 25 August to 6 September 2020 , i n March 2020 both the 2020 Summer Ol ympics and Paralympics were postpone d by one year due to the COVID-19 pa ndemic, with the rescheduled Games s till referred to as Tokyo 2020 form arketing and branding purposes.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 214,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-215",
      "content": "th the 2020 Summer Ol ympics and Paralympics were postpone d by one year due to the COVID-19 pa ndemic, with the rescheduled Games s till referred to as Tokyo 2020 form arketing and branding purposes. As with the Olympics, the Games were la rgely held behind | Originally scheduled to take place f rom 25 August to 6 September 2020 , i n March 2020 both the 2020 Summer Ol ympics and Paralympics were postpone d by one year due to the COVID-19 pa ndemic, with the rescheduled Games s till referred to as Tokyo 2020 form arketing and branding purposes. As with the Olympics, the Games were la rgely held behind | once submitted.This process was u ndertaken following the postponement of the Tokyo 2020 Games due to the COVID-19 pandemic, with both the Oly mpics and Paralympics pushed back a year.Now, the Tokyo 2020 Olympics are scheduled for July 23 to August 8 while the Paralympics are due to f ollow from August 24 to September 5.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 215,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-216",
      "content": "c, with both the Oly mpics and Paralympics pushed back a year.Now, the Tokyo 2020 Olympics are scheduled for July 23 to August 8 while the Paralympics are due to f ollow from August 24 to September 5. The refund process is separate for ticketholders outside of Japan, who purchased tickets through authorise d ticket resellers (ATR).Each ATR has its own individual refund proced ure.Early figures from the refund process for the Tokyo 2020 Olympics stated that around 18 per cent                                              | Olympiad, have now been postponed a nd rescheduled for 23 July to 8 Augu st 2021 in Tokyo, Japan. The Games were postponed in March 2020 as a re sult of the worldwide Covid-19 pande mic, although they will still keep t he name Tokyo 2020 for marketing and branding purposes. This will be th e first time the Olympic Games have been postponed rather than cancelled .",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 216,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-217",
      "content": "e Covid-19 pande mic, although they will still keep t he name Tokyo 2020 for marketing and branding purposes. This will be th e first time the Olympic Games have been postponed rather than cancelled .                                                                                                                                                                                                                                                                                                                                |\n| closed doors with no outside specta tors due to a state of emergency in the Greater Tokyo Area and other pre fectures. The Games were the second Summer Paralympics hosted by Tokyo s ince 1964, and the third Paralympics held in Japan overall since the 199 8 Winter Paralympics in Nagano. Th e Games featured                                           | closed doors with no outside specta tors due to a state of emergency in the Greater Tokyo Area and other pre fectures.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 217,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-218",
      "content": "ympics in Nagano. Th e Games featured                                           | closed doors with no outside specta tors due to a state of emergency in the Greater Tokyo Area and other pre fectures. The Games were the second Summer Paralympics hosted by Tokyo s ince 1964, and the third Paralympics held in Japan overall since the 199 8 Winter Paralympics in Nagano. Th e Games featured                                           | has been rescheduled to May 1-4 bec ause of travel restrictions under th e current state of emergency in Toky o and other 10 prefectures across Ja pan.The Tokyo 2020 organizing comm ittee announced that the first of 18 test events for the Olympic and Par alympic Games will involve wheelchai r rugby, which will be held in Yoyog i National Stadium from April 3 to 4 .The FINA Diving World Cup will fo llow from April 18 to 23 at the Toky o Aquatics Centre, which will also s erve as an Olympic qualifying event.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 218,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-219",
      "content": "be held in Yoyog i National Stadium from April 3 to 4 .The FINA Diving World Cup will fo llow from April 18 to 23 at the Toky o Aquatics Centre, which will also s erve as an Olympic qualifying event. The spread of the COVID-19 pandemi c has slowed down in Tokyo three wee ks after the Japanese capital entere d a state of emergency on | Olympic Games, when Tokyo became th e first city in Asia to host the Oly mpic and Paralympic Games, but unfor tunately strong winds made it an imp ossible task this time around.Memb ers of the Tokyo Organising Committe e of the Olympic and Paralympic Game s (Tokyo 2020), Tokyo Metropolitan G overnment officials, Tokyo 2020 Torc h Relay Official Ambassadors and rep resentatives from Miyagi Prefecture joined the arrival ceremony.FLAME OF RECOVERYThe Olympic flame will now be put on display at various loc ations in the Tohoku region, to high light the message of hope in the are as worst affected by the 2011 Great",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 219,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-220",
      "content": "ame will now be put on display at various loc ations in the Tohoku region, to high light the message of hope in the are as worst affected by the 2011 Great                                                                    |\n| 539 medal events in 22 sports, with badminton and taekwondo both making their Paralympic debut to replace f ootball 7-a-side and sailing. China topped the medal table for the fifth consecutive Paralympics, with 96 go lds and 207 total medals. Great Brit ain finished second for the ninth t ime,                                                       | 539 medal events in 22 sports, with badminton and taekwondo both making their Paralympic debut to replace f ootball 7-a-side and sailing. China topped the medal table for the fifth consecutive Paralympics, with 96 go lds and 207 total medals.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 220,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-221",
      "content": "ton and taekwondo both making their Paralympic debut to replace f ootball 7-a-side and sailing. China topped the medal table for the fifth consecutive Paralympics, with 96 go lds and 207 total medals. Great Brit ain finished second for the ninth t ime,                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | East Japan Earthqu",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 221,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-222",
      "content": "| East Japan Earthqu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 222,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-223",
      "content": "|\n\nTable 19 j Daniel Radcliffe , from Wikitext103Valid, retrieval data from c4. The chunks ğ¶ 2 and ğ¶ 3 are almost entirely retrieved from neighbours Â» ğ‘ 1 GLYPH&lt;148&gt; ğ¹ 1 â€¦ and Â» ğ‘ 2 GLYPH&lt;148&gt; ğ¹ 2 â€¦ respectively, up to formatting differences, which dramatically reduces the loss for these tokens. This example illustrates that when training data leaks into evaluation sets despite deduplication, our Retro model can directly exploit this leakage.\n\n<!-- image -->\n\n| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0 GLYPH<147> 5                                                                                                                                                                               | ğ¶ ğ‘¢ c",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 223,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-224",
      "content": "> 0 GLYPH<147> 5                                                                                                                                                                               | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                          | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 224,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-225",
      "content": "| Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|---------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 225,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-226",
      "content": "|\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 226,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-227",
      "content": "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 227,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-228",
      "content": "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| = Daniel Radcliffe =Daniel Jacob R adcliffe ( born 23 July 1989 ) is an English actor who rose to prominenc e as the title character in the Harr y Potter film series. He made his ac ting debut at 10 years of age in BBC One's 1999 television film David Co pperfield, followed by his cinematic debut       | = Daniel Radcliffe = Daniel Jacob R adcliffe ( born 23 July 1989 ) is an English actor who rose to prominenc e as the title character in the Harr y Potter film series.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 228,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-229",
      "content": "d by his cinematic debut       | = Daniel Radcliffe = Daniel Jacob R adcliffe ( born 23 July 1989 ) is an English actor who rose to prominenc e as the title character in the Harr y Potter film series. He made his ac ting debut at 10 years of age in BBC One's 1999 television film David Co pperfield, followed by his cinematic debut      | Daniel Jacob Radcliffe (born 23 July 1989) is an English actor who rose to prominence as the title character in the Harry Potter film series. He made his acting debut at 10 years o f age in BBC One's 1999 television f ilm David Copperfield, followed by h is cinematic debut in 2001's The Tai lor of Panama. At age 11, he was cas t as Harry Potter in the first Harry Potter film, and starred in the ser ies for 10 years until the release o f the eighth and final film in 2011.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 229,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-230",
      "content": "n 2001's The Tai lor of Panama. At age 11, he was cas t as Harry Potter in the first Harry Potter film, and starred in the ser ies for 10 years until the release o f the eighth and final film in 2011. Radcliffe began to branch out to s tage acting in 2007, starring in the London and New York productions of Equus, and                       | Daniel Jacob Radcliffe (born 23 July 1989) is an English actor who rose to prominence as the title character in the Harry Potter film series. He made his acting debut at 10 years o f age in BBC One's 1999 televisionm ovie David Copperfield, followed by his film debut in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Pot ter film, and starred in the series for 10 years until the release of th e eighth and final film in 2011. Rad cliffe began to branch out to stage acting in 2007, starring in the Lond on and New York productions of Equus , and in the                   |\n| in 2001's The Tailor of Panama.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 230,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-231",
      "content": "final film in 2011. Rad cliffe began to branch out to stage acting in 2007, starring in the Lond on and New York productions of Equus , and in the                   |\n| in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New                   | in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New                   | in 2001's The Tailor of Panama.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 231,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-232",
      "content": "ears u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New                   | in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New Yo rk productions of Equus, and in the 2011 Broadway revival of the musical How to Succeed in Business Without Really Trying. He starred in the 201 2 horror film The Woman in Black, an d played beat poet Allen Ginsberg in the 2013 independent film Kill Your Darlings.He has contributed to ma ny charities                          | of Panama. At age 11, he was cast a s Harry Potter in the first Harry Po tter film, and starred in the series for 10 years until the release of t he eighth and final film in 2011.R adcliffe began to branch out to",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 232,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-233",
      "content": "age 11, he was cast a s Harry Potter in the first Harry Po tter film, and starred in the series for 10 years until the release of t he eighth and final film in 2011.R adcliffe began to branch out to stag e acting in 2007, starring in the Lo ndon and New York productions of Equ us, and in the 2011 Broadway revival of the musical How to Succeed in Bu siness Without Really Trying. He sta rred in the 2012 horror film The Wom an in Black, and played beat poet Al len Ginsberg in the 2013 independent film Kill Your Darlings. He has con tributed to many charities, includin g Demelza House Children's |\n| York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our <unk>.He has contributed to ma ny charities, | York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succee",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 233,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-234",
      "content": "at poet Allen Ginsberg in the 2013 independent film Kill Y our <unk>.He has contributed to ma ny charities, | York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our <unk>.He has contributed to ma ny charities, | York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our Darlings.He has contributed to many charities, including Demelza H ouse Children's Hospice and The Trev or Project. He also made public serv ice announcements for the latter.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 234,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-235",
      "content": "dependent film Kill Y our Darlings.He has contributed to many charities, including Demelza H ouse Children's Hospice and The Trev or Project. He also made public serv ice announcements for the latter. In 2011, he was awarded the Trevor Pro ject's \"Hero Award.\"Sources disagr ee about Radcliffe's personal wealth ; he was reported to have earned Â£1 million for the first Harry Potter | in the 2011 Broadway revival of the musical How to Succeed in Business Without Really Trying. He starred in the 2012 horror film The Woman in B lack, and played beat poet Allen Gin sberg in the 2013 independent film K ill Your Darlings. He has contribute d to many charities, including Demel za House Children's Hospice and The Trevor Project. He also made public service announcements for the latter . In 2011, he was awarded the Trevor Project's \"Hero Award.\"                                                                                                                                                        |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 235,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-236",
      "content": "was awarded the Trevor Project's \"Hero Award.\"                                                                                                                                                        |\n| including <unk> Hospice Care for Ch ildren, and The Trevor Project for s uicide prevention among LGBTQ youth, which gave him its Hero Award in 20 11.= = Early life = =Radcliffe w as born in West London, England. He is the only child of Alan George Rad cliffe, a literary agent, and                       | including <unk> Hospice Care for Ch ildren, and The Trevor Project for s uicide prevention among LGBTQ youth, which gave him its Hero Award in 20 11.= = Early life = =Radcliffe w as born in West London, England. He is the only child of Alan George Rad cliffe, a literary agent, and                       |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 236,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-237",
      "content": "|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 237,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-238",
      "content": "|\n\nTable 20 j Sample - DÃ©claration des droits de l'homme: Article premier. The Retro[Off] sample has correct syntax and is almost plausible but is hallucinated. The Retro[On] sample is correctly copied from neighbour data, and robustly re-formated according to our prompt.\n\n<!-- image -->\n\n| Prompt and sample of Retro[Off]                                                                                                                                                                                                                              | Prompt a",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 238,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-239",
      "content": "| Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                      | Â» ğ‘ ğ‘¢ GLYPH<148> ğ¹ ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 239,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-240",
      "content": "| Â» ğ‘ ğ‘¢ GLYPH<148> ğ¹ ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 240,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-241",
      "content": "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 241,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-242",
      "content": "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Article premier - Les hommes naissent et demeurent libres e t Ã©gaux en droits. Les distinc tions sociales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© com mune. Article 2.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 242,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-243",
      "content": "----------------------------|\n| Article premier - Les hommes naissent et demeurent libres e t Ã©gaux en droits. Les distinc tions sociales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© com mune. Article 2. - Le but de toute association politique e st la conservation des droits naturels et | Article premier - Les hommes naissent et demeurent libres e t Ã©gaux en droits. Les distinc tions sociales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© com mune. Article 2. - Le but de toute association politique e st la conservation des droits naturels et           | LCP = 0, 1, 2, 3,4, > 5 de l'homme et du citoyen.Ar ticle 1erLes hommes naissent et demeurent libres et Ã©gaux en droits. Les distinctions so ciales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© commune.A rticle 2Le but de toute asso ciation politique est la conse rvation des droits naturels et imprescriptibles de l'homme.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 243,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-244",
      "content": ". Les distinctions so ciales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© commune.A rticle 2Le but de toute asso ciation politique est la conse rvation des droits naturels et imprescriptibles de l'homme. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ©, et la r Ã©sistance Ã  l'oppression.Art icle 3Le principe de toute s ouverainetÃ© rÃ©side essentielle ment dans la nation. Nul corps , nul individu ne peut exercer d'autoritÃ© qui n'en | LCP = 0, 1, 2, 3,4, > 5 Les hommes naissent et demeur ent libres et Ã©gaux en droits. Les distinctions sociales ne peuvent Ãªtre fondÃ©es que sur l 'utilitÃ© commune.Art. 2. - Le but de toute association po litique est la conservation de s droits naturels et imprescri ptibles de l'Homme. Ces droits sont la libertÃ©, la propriÃ©tÃ© , la sÃ»retÃ©, et la rÃ©sistance Ã  l'oppression.Art. 3. -Le principe de toute Souverainet Ã© rÃ©side essentiellement dans la Nation. Nul corps, nul indi vidu ne peut exercer d'autorit Ã© qui n'en Ã©mane expressÃ©ment.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 244,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-245",
      "content": "tÃ©, et la rÃ©sistance Ã  l'oppression.Art. 3. -Le principe de toute Souverainet Ã© rÃ©side essentiellement dans la Nation. Nul corps, nul indi vidu ne peut exercer d'autorit Ã© qui n'en Ã©mane expressÃ©ment. Art     |\n| imprescriptibles de l'homme, et par consÃ©quent la garantie Ã  chacun des droits suivants : Article 3. - La propriÃ©tÃ© est un droit inviolable et sa crÃ©. Toute personne a le droit de procÃ©der Ã  sa propre cons ervation. Article 4. - Le                      | imprescriptibles de l'homme. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ© et la rÃ© sistance Ã  l'oppression.Arti cle 3.- Le principe de toute souverainetÃ© rÃ©side essentiel lement dans la nation. Nul cor ps, nul individu ne peut exerc er d'autoritÃ© qui n    | criptibles del'homme. Ces dro its sont la libertÃ©, la propri Ã©tÃ©, la sÃ»retÃ©et la rÃ©sistanc e Ã  l'oppression.Article 3 - Le principe de toute souverai netÃ© rÃ©sideessentiellement da ns la Nation.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 245,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-246",
      "content": "n    | criptibles del'homme. Ces dro its sont la libertÃ©, la propri Ã©tÃ©, la sÃ»retÃ©et la rÃ©sistanc e Ã  l'oppression.Article 3 - Le principe de toute souverai netÃ© rÃ©sideessentiellement da ns la Nation. Nul corps, nul i ndividu nepeut exercer d'auto ritÃ© qui n'en Ã©mane expressÃ©me nt.Article 4 - La libertÃ© co nsiste Ã  pouvoir faire tout ce quine nuit pas Ã  autrui : ai nsi, l'exercice des droits nat urelsde chaque homme n'a de b ornes que celles qui assurent auxautres membres de la sociÃ© tÃ© la jouissance de                                                   | et imprescriptibles de l'homm e. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ© et la rÃ©sistance Ã  l'oppression.A rticle 3 - Le principe de tout e souverainetÃ© rÃ©side essentie llement dans la Nation. Nul co rps, nul individu ne peut exer cer d'autoritÃ© qui n'en Ã©mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoir fai re tout ce qui ne nuit pas Ã  a utrui : ainsi, l'exercice des droits naturels de chaque homm e n'a de bornes",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 246,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-247",
      "content": "xer cer d'autoritÃ© qui n'en Ã©mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoir fai re tout ce qui ne nuit pas Ã  a utrui : ainsi, l'exercice des droits naturels de chaque homm e n'a de bornes que celles qui assurent aux autres membres d e la sociÃ©tÃ© la jouissance de ces mÃªmes droits. Ces bornes |\n| but de toute association est la dÃ©fense des droits de l'hom me et du citoyen. Tout citoye n a le droit de participer Ã  l a direction des affaires publi ques. Article 5. - L'impuni tÃ© n'a jamais Ã©tÃ© et ne sera j amais une fin en elle-mÃªme. L' imp        | 'en Ã©mane expressÃ©ment.Artic le 4.- La libertÃ© consiste Ã  pouvoir faire tout ce qui ne nuit pas Ã  autrui : ainsi, l'e xercice des droits naturels de chaque homme n'a de bornes qu e celles qui assurent aux autr es membres de la sociÃ©tÃ© la jo uissance de ces mÃªmes | mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoi r faire tout ce qui ne nuit pa s Ã  autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 247,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-248",
      "content": "a jo uissance de ces mÃªmes | mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoi r faire tout ce qui ne nuit pa s Ã  autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes que celle s qui assurent aux autres memb res de la sociÃ©tÃ© la jouissanc e de ces mÃªmes droits. Ces bor nes ne peuvent Ãªtre dÃ©terminÃ©e s que par la loi.Article 5 - La loi n'a le droit de dÃ©fend re que les actions nuisibles Ã  la sociÃ©tÃ©. Tout ce qui n'est pas dÃ©fendu par la loi ne peu t Ãªtre empÃªchÃ©, et nul ne peut Ãªtre contraint Ã  faire ce qu' elle n                              | mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoi r faire tout ce qui ne nuit pa s Ã  autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes que celle s qui assurent aux autres memb res de la sociÃ©tÃ© la jouissanc e de ces mÃªmes droits. Ces bor nes ne peuvent Ãªtre dÃ©terminÃ©e s que par la loi.Article 5 - La loi n'a le droit de dÃ©fend re que les actions nuisibles Ã  la sociÃ©tÃ©.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 248,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-249",
      "content": "memb res de la sociÃ©tÃ© la jouissanc e de ces mÃªmes droits. Ces bor nes ne peuvent Ãªtre dÃ©terminÃ©e s que par la loi.Article 5 - La loi n'a le droit de dÃ©fend re que les actions nuisibles Ã  la sociÃ©tÃ©. Tout ce qui n'est pas dÃ©fendu par la loi ne peu t Ãªtre empÃªchÃ©, et nul ne peut Ãªtre contraint Ã  faire ce qu' elle n                      |\n\nunitÃ©, comme le despotisme, s 'est toujours rÃ©vÃ©lÃ©e Ãªtre un instrument d'oppression. La ty rannie qui s'est Ã©lue juge su prÃªme de la conscience des aut res ne peut Ãªtre jugÃ©e. La jus tice se trouve dans la consci ence de chaque citoyen, droits. Ces bornes ne peuvent Ãªtre dÃ©terminÃ©es que par la l oi.Article 5.- La loi n'a le droit de dÃ©fendre que les a ctions nuisibles Ã  la sociÃ©tÃ©. Tout ce qui n'est pas dÃ©fendu par la loi ne peut Ãªtre empÃªc hÃ©, et nul ne peut Ãªtre",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "F.3. Filtered results",
        "chunkIndex": 249,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-250",
      "content": "| Prompt and sample of Retro[Off]                                                                                           | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                         | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                 | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                         |\n|---------------------------------------------------------------------------------------------------------------------------|-----------------------------------------",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 250,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-251",
      "content": "|\n|---------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Pi = 3. 1415926535 8979323846 2643383279 5028841971 69399375 10 5820974944 5923078164 06286 20899 8628034825 3421170679   | Pi = 3.",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 251,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-252",
      "content": "-----------------------------------------------------------------|\n| Pi = 3. 1415926535 8979323846 2643383279 5028841971 69399375 10 5820974944 5923078164 06286 20899 8628034825 3421170679   | Pi = 3. 1415926535 8979323846 2643383279 5028841971 69399375 10 5820974944 5923078164 06286 20899 8628034825 3421170679                   | '1415926535 8979323846 26433 83279 5028841971 693993751058 20974944 5923078164 0628620899 8628034825 34211706798214808 651 3282306647 0938446095 5058 223172 53594081284811174502 8 410270193 8521105559 644622948 9 54930381964428810975 665933 4461 2847564823 3786783       | 46 2643383279 5028841971 69399 37510 5820974944 592307816406 28620899 8628034825 3421170679 8214808651 3282306647 0938446 095 50582231725359408128 4811 174502 8410270193 8521105559 6 446229489 5493038196 442881097 56659334461 2847564823 378678 3165 2712019091 4564856692 346 0   |\n| 8294049602 8988496069 9858349 065 9873246379 9644789435 8628 730709 6540159079 5944069810 5 992965913 709537841",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 252,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-253",
      "content": "493038196 442881097 56659334461 2847564823 378678 3165 2712019091 4564856692 346 0   |\n| 8294049602 8988496069 9858349 065 9873246379 9644789435 8628 730709 6540159079 5944069810 5 992965913 7095378412 69378359 | 8214808651 3282306647 0938446 095 5058223172 53594081284811 174502 8410270193 8521105559 6 446229489 5493038196442881097 5 6659334461 284 | 651 3282306647 0938446095 5058 223172 5359408128 4811174502 8410270193 8521105559 64462294 89 54930381964428810975 66593 34461 2847564823 3786783165 27 12019091 4564856692 346034861 0 4543266482 1339360726 024914 12737245870066 0631558817 488 1520920 9628292540 91715364 | 47 0938446095 5058223172 53594 081284811174502 8410270193 85 21105559 6446229489 5493038196 4428810975 6659334461 2847564 823 3786783165 27120190914564 856692 3460348610 4543266482 1 339360726 0249141273724587006 6 0631558817 4881520920 962829 2540 91715364367892590360          |\n| 10 6940372045 7088679512 85612 30857 9046461290 9276642155 56 54603269 5656128798 636647570",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 253,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-254",
      "content": "266482 1 339360726 0249141273724587006 6 0631558817 4881520920 962829 2540 91715364367892590360          |\n| 10 6940372045 7088679512 85612 30857 9046461290 9276642155 56 54603269 5656128798 6366475705 6294954741 5886335339 57657  | 7564823 3786783165 2712019091 4564856692 3460348610 45432664 82 1339360726 024914127372458 70066 0631558817 4881520920 96 28292540 91715  | 23 3786783165 2712019091 4564 856692 3460348610 4543266482 1 339360726 0249141273724587006 6 0631558817 4881520920 962829 2540 9171536436 7892590360 01 13305305 4882046652 1384146951 94151160943305727036 5759591 953 0921861173 8193261179 3105 118548 0744623799 627495    | 165 27120190914564856692 3460 348610 4543266482 1339360726 0 2491412737245870066 063155881 7 4881520920 9628292540 917153 64367892590360 0113305305 488 2046652 1384146951 9415116094 3305727036 5759591953 09218611 73 8193261179 310511854807446 23799 6274956735 1885752724 89 1227 |",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 254,
        "totalChunks": 256
      }
    },
    {
      "id": "2112.04426v3-chunk-255",
      "content": "063155881 7 4881520920 9628292540 917153 64367892590360 0113305305 488 2046652 1384146951 9415116094 3305727036 5759591953 09218611 73 8193261179 310511854807446 23799 6274956735 1885752724 89 1227 |\n\n76345 5770886953 7988876910 79 66169745 6493974637 6345801550 6663542854 6333764630 6356284 271 7885339804 5672434\n\n364367892590360 0113305305 48 82046652 1384146951 9415116094 3305727036 5759591953 0921861 173 8193261179 31051185480744 623799 6274",
      "metadata": {
        "source": "arxiv:2112.04426v3",
        "title": "Improving language models by retrieving from trillions of tokens",
        "authors": [
          "Sebastian Borgeaud",
          "Arthur Mensch",
          "Jordan Hoffmann",
          "Trevor Cai",
          "Eliza Rutherford",
          "Katie Millican",
          "George van den Driessche",
          "Jean-Baptiste Lespiau",
          "Bogdan Damoc",
          "Aidan Clark",
          "Diego de Las Casas",
          "Aurelia Guy",
          "Jacob Menick",
          "Roman Ring",
          "Tom Hennigan",
          "Saffron Huang",
          "Loren Maggiore",
          "Chris Jones",
          "Albin Cassirer",
          "Andy Brock",
          "Michela Paganini",
          "Geoffrey Irving",
          "Oriol Vinyals",
          "Simon Osindero",
          "Karen Simonyan",
          "Jack W. Rae",
          "Erich Elsen",
          "Laurent Sifre"
        ],
        "section": "Table 21 j Sample - Decimals of ğœ‹ . The Retro[Off] sample quickly diverges two digits after the end of the prompt whereas Retro[On] correctly outputs a large number of ğœ‹ digits, directly copied from the neighbours data.",
        "chunkIndex": 255,
        "totalChunks": 256
      }
    }
  ],
  "tables": [
    {
      "index": 0,
      "markdown": "| Source    | Token count (M)   | Documents (M)   | Multilingual   | Sampling frequency   |\n|-----------|-------------------|-----------------|----------------|----------------------|\n| Web       | 977,563           | 1,208           | Yes            | 55%                  |\n| Books     | 3,423,740         | 20              | No             | 25%                  |\n| News      | 236,918           | 398             | No             | 10%                  |\n| Wikipedia | 13,288            | 23              | Yes            | 5%                   |\n| GitHub    | 374,952           | 143             | No             | 5%                   |"
    },
    {
      "index": 1,
      "markdown": "|        |                     | ğ‘‘     | ğ‘‘ ffw   |    |     |    |\n|--------|---------------------|-------|---------|----|-----|----|\n| 132M   | 172M (+30%)         | 896   | 3,584   | 16 |  64 | 12 |\n| 368M   | 425M (+15%)         | 1,536 | 6,144   | 12 | 128 | 12 |\n| 1,309M | 1,451M (+11%) (+8%) | 2,048 | 8,192   | 16 | 128 | 24 |\n| 6,982M | 7,532M              | 4,096 | 16,384  | 32 | 128 | 32 |"
    },
    {
      "index": 2,
      "markdown": "|                  | # Retrieval tokens        | Granularity   | Retriever training   | Retrieval integration   |\n|------------------|---------------------------|---------------|----------------------|-------------------------|\n| Continuous Cache | O GLYPH<0> 10 3 GLYPH<1>  | Token         | Frozen (LSTM)        | Add to probs            |\n| ğ‘˜ NN-LM          | O GLYPH<0> 10 9 GLYPH<1>  | Token         | Frozen (Transformer) | Add to probs            |\n| Spalm            | O GLYPH<0> 10 9 GLYPH<1>  | Token         | Frozen (Transformer) | Gated logits            |\n| Dpr              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Contrastive proxy    | Extractive QA           |\n| Realm            | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | End-to-End           | Prepend to prompt       |\n| RAG              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Fine-tuned Dpr       | Cross-attention         |\n| FiD              | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | Frozen Dpr           | Cross-attention         |\n| Emdr 2           | O GLYPH<0> 10 9 GLYPH<1>  | Prompt        | End-to-End (EM)      | Cross-attention         |\n| Retro (ours)     | O GLYPH<0> 10 12 GLYPH<1> | Chunk         | Frozen (Bert)        | Chunked cross-attention |"
    },
    {
      "index": 3,
      "markdown": "| Model                                    | Retrieval Set      | #Database tokens   | #Database keys   | Valid   |   Test |\n|------------------------------------------|--------------------|--------------------|------------------|---------|--------|\n| Adaptive Inputs (Baevski and Auli, 2019) | -                  | -                  | -                | 17.96   |  18.65 |\n| Spalm (Yogatama et al., 2021)            | Wikipedia          | 3B                 | 3B               | 17.20   |  17.6  |\n| ğ‘˜ NN-LM (Khandelwal et al., 2020)        | Wikipedia          | 3B                 | 3B               | 16.06   |  16.12 |\n| Megatron (Shoeybi et al., 2019)          | -                  | -                  | -                | -       |  10.81 |\n| Baseline transformer (ours)              | -                  | -                  | -                | 21.53   |  22.96 |\n| ğ‘˜ NN-LM (ours)                           | Wikipedia          | 4B                 | 4B               | 18.52   |  19.54 |\n| Retro                                    | Wikipedia          | 4B                 | 0.06B            | 18.46   |  18.97 |\n| Retro                                    | C4                 | 174B               | 2.9B             | 12.87   |  10.23 |\n| Retro                                    | MassiveText (1%)   | 18B                | 0.8B             | 18.92   |  20.33 |\n| Retro                                    | MassiveText (10%)  | 179B               | 4B               | 13.54   |  14.95 |\n| Retro                                    | MassiveText (100%) | 1792B              | 28B              | 3.21    |   3.92 |"
    },
    {
      "index": 4,
      "markdown": "| Realm (Guu et al., 2020)              |   40.4 |\n|---------------------------------------|--------|\n| Dpr (Karpukhin et al., 2020)          |   41.5 |\n| RAG (Lewis et al., 2020)              |   44.5 |\n| Emdr 2 (Sachan et al., 2021)          |   52.5 |\n| FiD (Izacard and Grave, 2021)         |   51.4 |\n| FiD + Distill. (Izacard et al., 2020) |   54.7 |\n| Baseline 7B (closed book)             |   30.4 |\n| Retro 7.5B (DPR retrieval)            |   45.5 |"
    },
    {
      "index": 5,
      "markdown": "| Prompt and sample of Retro[Off]                                                                                                                                                                                                                                   | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                        | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Beavers are interesting animals that live near rivers. They build                                                                                                                                                                                                 | Beavers are interesting animals that live near rivers. They build                                                                                                                                                                                                                        | .Beavers build their lodges in pon ds they have created in wooded areas .Like many things in nature, there is a connection between creatures i n the wild.Beaver ponds cause tree s to drown, but the dead trees attra ct the great blue heron, which often return year after year. Over time, a beaver pond can attract more than 50 nests in a colony, called a rooke ry.An example of this can be found in the large pond off Bradford Road at Carter Fields near the Boxford l ine.Chris Leahy, an expert with th e Massachusetts Audubon Society who wrote                               | naw them into smaller sections and d rag them into the water.Engineers of the Pond Beavers are interesting animals because they change the hab itat in which they live. Beavers do this by blocking up streams to creat e ponds. Then they build their homes , called lodges, in these ponds. Bea vers' bodies make them well-suited f or underwater building Special muscl es close off their noses, ears, and throats to keep the water out. Beave rs' broad tails act like rudders for steering. Their two very large, ora nge front teeth are used to gnaw dow n trees. They begin building their d am |\n| dams to create ponds. Frogs are am phibians, so they can live in both l and and water. They have great camou flage to hide from predators. The G olden Retriever, or Golden, is a ver y big dog. They are very strong, and have a thick coat to help them live in | their houses called beaver dams in the riverbeds. They also live on lan d.Beavers use their strong teeth an d strong jaws to cut down trees and branches to build their homes. They also use their teeth and jaws to che w up food. Beavers use their big, fl at tails to swim. They use | , then they mean that you are very b usy. Beavers swim easily in streams, picking up rocks and sticks to buil d their dams. They gnaw at trees wit h their big front teeth to cut them down. Then they use parts of the tre es to build their houses.Beavers ar e clever builders. They know exactly what they need to build their beave r dams. They use mud from the stream to make their dams stay together. T hey use their tails to pat down the mud.Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the | ar-like tail, and two protruding tee th that are strong enough to gnaw do wn trees. The beaver uses trees, bra nches, and mud to build dams across rivers and streams. These dams creat e a deep pool of water in which the beaver builds its home. A beaver hom e is called a lodge. A baby beaver o r 'kit' remains in the family lodge until the age of two.Beaver fur, kn own as pelt, was once highly popular as a trim for hats and coats. Howm ight the popularity of beaver fur co ntributed to the colonization of New York?wwwWw                                                                 |\n| cold areas. A kangaroo is an anima l that is very large. It can jump ve ry high. They can run very fast. A meerkat is a small animal that lives in Africa. They live in colonies of up to a hundred of them. They can c limb trees very easily.                   | their sharp teeth to gnaw at rocks and cut trees. They use their flat t ails to steer when they are swimming .In the winter, beavers have to bui ld a house called a beaver lodge. Th ey build a house that is very strong . The walls are made of twigs. The r oof is made              | , then they mean that you are very b usy. Beavers swim easily in streams, picking up rocks and sticks to buil d their dams. They gnaw at trees wit h their big front teeth to cut them down. Then they use parts of the tre es to build their houses.Beavers ar e clever builders. They know exactly what they need to build their beave r dams. They use mud from the stream to make their dams stay together. T hey use their tails to pat down the mud.Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the | w at trees with their big front teet h to cut them down. Then they use pa rts of the trees to build their hous es.Beavers are clever builders. The y know exactly what they need to bui ld their beaver dams. They use mud f rom the stream to make their dams st ay together. They use their tails to pat down the mud.Beavers put a snu g room at the top of their dams for their babies. They store their food underwater. Beavers eat the bark fro mthe trees that they cut down!1.W hat is the main idea of the first pa ragraph?.2. What is the main idea o f the second paragraph?                  |"
    },
    {
      "index": 6,
      "markdown": "| Prompt and sample of Retro[Off]                                                                                                                                                                                                                     | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                             | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP 0, 1, 2, 3,4, 5                                                                                                                                                                                                                                                                                                                                                                                                                   | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                         |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ACT I SCENE I. Elsinore. A pla tform before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: st and, and unfold yourself. BERN ARDO                                                       | ACT I SCENE I. Elsinore. A pla tform before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: st and, and unfold yourself. BERN ARDO                                                                 | = > ACT I SCENE I. Elsinore. A pla tform before the castle. (FRAN CISCO at his post. Enter to hi mBERNARDO) BERNARDO Who's the re? FRANCISCO Nay, answer me: stand, and unfold yourself. BE RNARDO Long live the king! FRA NCISCO Bernardo? BERNARDO He. FRANCISCO You come most carefu lly upon your hour. BERNARDO ' Tis now struck twelve; get the e to bed, Francisco. FRANCISCO For this relief much thanks: 'tis bitter cold, And I am sic k at heart. BERNARDO Have you               | ><TEXT>ACT I SCENE I. Elsin ore. A platform before the cas tle. FRANCISCO at his post. E nter to him BERNARDO BERNARDO Who's there? FRANCISCO Nay, an swer me: stand, and unfold you rself. BERNARDO Long live the king! FRANCISCO Bernardo? BERN ARDO He. FRANCISCO You come mo st carefully upon your hour. B ERNARDO 'Tis now struck twelve ; get thee to bed, Francisco. FRANCISCO For this relief much thanks: 'tis bitter cold, And I am sick at heart.                                          |\n| Who calls ? FRANCISCO I am th e lord here; I, Francisco, tha t am sick of grief. [ Aside. B ERNARDO The king ! FRANCISCO I am sick of that also. BERNARD O My lord ? FRANCISCO Do not a pproach me. BERNARDO                                        | Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For t his relief much thanks: 'tis b itter cold, And I am sick at h eart. B | Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For t his relief much thanks: 'tis b itter cold, And I am sick at h eart.</TEXT></DOC><DOC><DO CNO>romeo</DOCNO><TEXT>ACT PROLOGUE Two households, bo th alike in dignity, In fair V erona, where we lay our scene, From ancient grudge break to new mutiny,                                               | live the king! FRANCISCO Bern ardo? BERNARDO He. FRANCISCO Y ou come most carefully upon yo ur hour. BERNARDO 'Tis now str uck twelve: get thee to bed, F rancisco. FRANCISCO For this r elief much thanks: 'tis bitter cold, And I am sick at heart. BERNARDO Have you had quiet g uard? FRANCISCO Not a mouse st irring. BERNARDO Well, good ni ght. Ifyou do meet Horatio and Marcellus, The rivals2 of my watch, bid them make haste. FR ANCISCO I think I hear them.- Stand, ho! who is there? EN |\n| Francisco, I would speak with you. FRANCISCO Approach me not , but speak. BERNARDO Your han d, your voice FRANCISCO I will not hear thee speak. BERNARDO Francisco, your hand, I entre at thee. FRANCISCO Approach me not. BERNARDO Francisco FRANC | ERNARDO Have you had quiet gua rd? FRANCISCO Not a mouse stir ring. BERNARDO Well, good nigh t. If you do meet Horatio and Marcellus, The rivals of my wa tch, bid them make haste. FRAN CISCO I think I hear them. Sta nd, ho! who is there? Enter           | had quiet guard? FRANCISCO No t a mouse stirring. BERNARDOW ell, good night. If you do mee t Horatio and Marcellus, The r ivals of my watch, bid them ma ke haste. FRANCISCO I think I hear them. Stand, ho! Who's th ere? (Enter HORATIO and MARCEL LUS) HORATIO Friends to this g round. MARCELLUS And liegemen to the Dane. FRANCISCO Give yo u good night. MARCELLUS O, far ewell, honest soldier: Who hat h relieved you? FRANCISCO Bern ardo has my place. Give you go od night. (Exit | ARDO Have you had quiet guard? FRANCISCO Not a mouse stirrin g. BERNARDO Well, good night. Ifyou do meet Horatio and Marc ellus, The rivals2 of my watch , bid them make haste. FRANCIS CO I think I hear them.- Stand , ho! who is there? ENTER HORA TIO AND MARCELLUS. HORATIO Fri ends to this ground. MARCELLUS And liegemen to the Dane.3 FR ANCISCO Give you good night.M ARCELLUS O, farewell, honest s oldier: Who hath relieved you? FRANCISCO Bernardo hath my pl ace. Give you good night   |"
    },
    {
      "index": 7,
      "markdown": "| Source    | Language   | Token count (M)   | Documents     |   Sampling weight |\n|-----------|------------|-------------------|---------------|-------------------|\n| Web       | En         | 483,002           | 604,938,816   |            0.314  |\n|           | Ru         | 103,954           | 93,004,882    |            0.033  |\n|           | Es         | 95,762            | 126,893,286   |            0.033  |\n|           | Zh         | 95,152            | 121,813,451   |            0.033  |\n|           | Fr         | 59,450            | 76,612,205    |            0.033  |\n|           | De         | 57,546            | 77,242,640    |            0.033  |\n|           | Pt         | 44,561            | 62,524,362    |            0.033  |\n|           | It         | 35,255            | 42,565,093    |            0.033  |\n|           | Sw         | 2,246             | 1,971,234     |            0.0044 |\n|           | Ur         | 631               | 455,429       |            0.0011 |\n| Books     | En         | 3,423,740         | 20,472,632    |            0.25   |\n| News      | En         | 236,918           | 397,852,713   |            0.1    |\n|           | En         | 3,977             | 6,267,214     |            0.0285 |\n|           | De         | 2,155             | 3,307,818     |            0.003  |\n|           | Fr         | 1,783             | 2,310,040     |            0.003  |\n|           | Ru         | 1,411             | 2,767,039     |            0.003  |\n| Wikipedia | Es         | 1,270             | 2,885,013     |            0.003  |\n|           | It         | 1,071             | 2,014,291     |            0.003  |\n|           | Zh         | 927               | 1,654,772     |            0.003  |\n|           | Pt         | 614               | 1,423,335     |            0.003  |\n|           | Ur         | 61                | 344,811       |            0.0001 |\n|           | Sw         | 15                | 58,090        |            0.0004 |\n| Github    | -          | 374,952           | 142,881,832   |            0.05   |\n| Total     | -          | 5,026,463         | 1,792,260,998 |            1      |"
    },
    {
      "index": 8,
      "markdown": "| Megan Rohrer                         | Aakashavaani                                                                 |\n|--------------------------------------|------------------------------------------------------------------------------|\n| Emma Raducanu                        | Junior Eurovision Song Contest 2021                                          |\n| Ambra Sabatini                       | Pavilion Bukit Jalil                                                         |\n| WhyDonate                            | Blake Desjarlais                                                             |\n| The Juggernaut (company) Angela Diaz | 2021 All-Ireland Senior Football Championship Final Drift-barrier hypothesis |\n| 2020 Summer Paralympics              | Venomics                                                                     |\n| 2021 Afghan protests                 | Great Circle (novel)                                                         |\n| Rexh Xhakli                          | Hurricane Ida                                                                |\n| Julia Laskin                         | 2021 Montenegrin episcopal enthronement protests                             |\n| Cuijk                                | At War With the Silverfish                                                   |\n| Ghoubet Wind Power Station           |                                                                              |"
    },
    {
      "index": 9,
      "markdown": "|        |   ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ |   ğ‘‘ ğ‘“ ğ‘“ğ‘¤ |    |     |    | ğ‘ƒ                                                                             | ğ‘ƒ Enc   |                          |\n|--------|-----------|----------|----|-----|----|-------------------------------------------------------------------------------|---------|--------------------------|\n| 247M   |       896 |     3584 | 16 |  64 | 12 | Â» 6 GLYPH<148> 9 GLYPH<148> 12 â€¦                                              | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 564M   |      1536 |     6144 | 12 | 128 | 12 | Â» 6 GLYPH<148> 9 GLYPH<148> 12 â€¦                                              | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 1,574M |      2048 |     8192 | 16 | 128 | 24 | Â» 9 GLYPH<148> 12 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> 24 â€¦ | Â» 1 â€¦   | 2 GLYPH<2> 10 GLYPH<0> 4 |\n| 7,505M |      4096 |    16384 | 32 | 128 | 32 | Â» 9 GLYPH<148> 12 GLYPH<148> GLYPH<147> GLYPH<147> GLYPH<147> GLYPH<148> 32 â€¦ | Â» 1 â€¦   | 1 GLYPH<2> 10 GLYPH<0> 4 |"
    },
    {
      "index": 10,
      "markdown": "| Model         | Number of layers          | 18               |\n|---------------|---------------------------|------------------|\n|               | ğ‘‘                         | 1024             |\n|               | ğ‘‘ Ffw                     | 4096             |\n|               | Key size                  | 64               |\n|               | Value size                | 64               |\n|               | Number of heads           | 16               |\n| Training data | Dataset                   | Wikitext103train |\n|               | Sequence length           | 3072             |\n|               | Batch size                | 128              |\n|               | Tokenizer vocabulary size | 128,000          |\n| Optimisation  | optimiser                 | Adam             |\n|               | Adam's ğ›½ 1                | 0.9              |\n|               | Adam's ğ›½ 2                | 0.95             |\n|               | Adam's ğœ€                  | 1e-8             |\n|               | Dropout rate              | 0.25             |\n| Schedule      | Learning rate start       | 1e-7             |\n|               | Learning rate max         | 2.5e-4           |\n|               | Learning rate min         | 2e-5             |\n|               | Warmup steps              | 4,000            |\n|               | Cosine cycle steps        | 100,000          |\n| Evaluation    | Overlapping proportion    | 87.5%            |"
    },
    {
      "index": 11,
      "markdown": "| Model   | Layers with Retro-block ( ğ‘ƒ )   | Learning rate                                       |   Batch size |\n|---------|---------------------------------|-----------------------------------------------------|--------------|\n| 172M    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 425M    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 1.5B    | Every 3 rd from 6               | 2 GLYPH<2> 10 GLYPH<0> 4 ! 2 GLYPH<2> 10 GLYPH<0> 5 |          256 |\n| 7.5B    | Every 3 rd from 6               | 1 GLYPH<2> 10 GLYPH<0> 4 ! 1 GLYPH<2> 10 GLYPH<0> 5 |          256 |"
    },
    {
      "index": 12,
      "markdown": "| Ablation group           | Ablation                   |   C4 eval bpb |\n|--------------------------|----------------------------|---------------|\n| Model                    | Retro                      |         0.822 |\n|                          | No query conditioning      |         0.829 |\n|                          | No CA positional encodings |         0.826 |\n|                          | Shared embeddings          |         0.823 |\n|                          | 6-layer encoder            |         0.821 |\n| Retrieval values         | Neighbours N               |         0.95  |\n|                          | Continuations F            |         0.895 |\n|                          | No retrieval               |         0.987 |\n| Training neighbours      | 1 training neighbours      |         0.858 |\n|                          | 4 training neighbours      |         0.847 |\n| Cross attention position | CA top layer (1/12)        |         0.827 |\n|                          | CA mid layer (6/12)        |         0.823 |\n|                          | CA top layer (12/12)       |         0.831 |\n|                          | CA all layers              |         0.86  |\n|                          | CA every 3 from 1          |         0.823 |"
    },
    {
      "index": 13,
      "markdown": "|                          |       | Baseline   | Baseline   |       |       | Retro [Off]   | Retro [Off]   |       | Retro[On]   | Retro[On]   | Retro[On]   |      |\n|--------------------------|-------|------------|------------|-------|-------|---------------|---------------|-------|-------------|-------------|-------------|------|\n|                          | 172M  | 425M       | 1.5B       | 7.5B  | 172M  | 425M          | 1.5B          | 7.5B  | 172M        | 425M        | 1.5B        | 7.5B |\n| C4 Eval bpb              | 0.98  | 0.92       | 0.84       | 0.78  | 0.98  | 0.92          | 0.84          | 0.78  | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb (900B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.88        | 0.83        | 0.76        | 0.71 |\n| C4 Eval bpb (360B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.92        | 0.87        | 0.80        | 0.74 |\n| C4 Eval bpb (180B)       | -     | -          | -          | -     | -     | -             | -             | -     | 0.94        | 0.89        | 0.81        | 0.75 |\n| C4 Eval bpb (90B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.95        | 0.89        | 0.82        | 0.76 |\n| C4 Eval bpb (36B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.90        | 0.83        | 0.77 |\n| C4 Eval bpb (18B)        | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.91        | 0.83        | 0.77 |\n| C4 Eval bpb (9B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.96        | 0.91        | 0.83        | 0.77 |\n| C4 Eval bpb (4B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.97        | 0.91        | 0.84        | 0.78 |\n| C4 Eval bpb (2B)         | -     | -          | -          | -     | -     | -             | -             | -     | 0.97        | 0.91        | 0.84        | 0.78 |\n| C4 Eval bpb ( ğ‘˜ = 1)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.79        | 0.73        | 0.67 |\n| C4 Eval bpb ( ğ‘˜ = 2)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.78        | 0.72        | 0.67 |\n| C4 Eval bpb ( ğ‘˜ = 3)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.78        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 4)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 5)     | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 10)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 20)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 30)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.82        | 0.77        | 0.71        | 0.65 |\n| C4 Eval bpb ( ğ‘˜ = 40)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.77        | 0.71        | 0.65 |\n| C4 Eval bpb ( ğ‘˜ = 50)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.83        | 0.78        | 0.71        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 60)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.78        | 0.72        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 70)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.84        | 0.79        | 0.72        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 80)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | 0.73        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 90)    | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | 0.73        | 0.66 |\n| C4 Eval bpb ( ğ‘˜ = 100)   | -     | -          | -          | -     | -     | -             | -             | -     | 0.85        | 0.79        | -           | 0.67 |\n| Lambada Accuracy         | 0.42  | 0.51       | 0.61       | 0.69  | 0.47  | 0.54          | 0.63          | 0.70  | 0.52        | 0.60        | 0.67        | 0.73 |\n| Curation Corpus bpb      | 0.69  | 0.63       | 0.56       | 0.52  | 0.68  | 0.64          | 0.57          | 0.51  | 0.66        | 0.61        | 0.55        | 0.50 |\n| Wikitext103 Perplexity   | 25.62 | 19.29      | 13.98      | 10.65 | 25.88 | 19.78         | 13.89         | 10.40 | 3.32        | 2.96        | 2.53        | 2.22 |\n| Wikipedia Sept. 2021 bpb | 0.85  | 0.78       | 0.71       | 0.65  | 0.86  | 0.79          | 0.71          | 0.65  | 0.79        | 0.73        | 0.66        | 0.61 |"
    },
    {
      "index": 14,
      "markdown": "| arxiv             |   0.742 |   0.838 |   0.680 |   0.641 |   0.714 |\n|-------------------|---------|---------|---------|---------|---------|\n| books3            |   0.792 |   0.802 |   0.835 |   0.706 |   0.653 |\n| dm_mathematics    |   1.177 |   1.371 |   1.037 |   1.135 |   1.164 |\n| freelaw           |   0.576 |   0.612 |   0.514 |   0.506 |   0.499 |\n| github            |   0.42  |   0.645 |   0.358 |   0.367 |   0.199 |\n| gutenberg_pg_19   |   0.803 |   1.163 |   0.89  |   0.652 |   0.4   |\n| hackernews        |   0.971 |   0.975 |   0.869 |   0.888 |   0.86  |\n| nih_exporter      |   0.65  |   0.612 |   0.59  |   0.59  |   0.635 |\n| opensubtitles     |   0.974 |   0.932 |   0.879 |   0.894 |   0.93  |\n| philpapers        |   0.76  |   0.723 |   0.742 |   0.682 |   0.699 |\n| pile_cc           |   0.771 |   0.698 |   0.669 |   0.688 |   0.626 |\n| pubmed_abstracts  |   0.639 |   0.625 |   0.587 |   0.578 |   0.542 |\n| pubmed_central    |   0.588 |   0.69  |   0.579 |   0.512 |   0.419 |\n| stackexchange     |   0.714 |   0.773 |   0.655 |   0.638 |   0.624 |\n| ubuntu_irc        |   1.2   |   0.946 |   0.857 |   1.081 |   1.178 |\n| uspto_backgrounds |   0.603 |   0.566 |   0.537 |   0.545 |   0.583 |"
    },
    {
      "index": 15,
      "markdown": "| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0                                                                                                                                                                                                                           | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                         | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Great Circle (novel)Great Circle i s a 2021 novel by Maggie Shipstead, published on May 4, 2021, by Alfred A. Knopf.The novel has been shortl isted for the 2021 Booker Prize.Sy nopsis The novel consists of two pa rallel narratives about two fictiona l women. One is                                                                      | Great Circle (novel) Great Circle i s a 2021 novel by Maggie Shipstead, published on May 4, 2021, by Alfred A. Knopf. The novel has been shortl isted for the 2021 Booker Prize. Sy nopsis The novel consists of two pa rallel narratives about two fictiona l women. One is                                                                   | The Dutch House (novel)The Dutch H ouse is a 2019 novel by Ann Patchett . It was published by Harper on Sept ember 24, 2019. It tells the story o f a brother and sister over the cour se of five decades.The novel was a finalist for the 2020 Pulitzer Priz e for Fiction.PlotThe Dutch House is a mansion located in Elkins Park , Pennsylvania, a suburb of Philadel phia. It was built in 1922 by the Va nHoebeek family, a husband and wife originally from the Netherlands who made their fortune in the tobacco in dustry. Cyril Conroy, a self-made re al estate mogul                                                                               | The Dutch House (novel)The Dutch H ouse is a 2019 novel by Ann Patchett . It was published by Harper on Sept ember 24, 2019. It tells the story o f a brother and sister over the cour se of five decades.[2]The novel wa s a finalist for the 2020 Pulitzer P rize for Fiction.[3]Plot[edit]Th e Dutch House is a mansion located i n Elkins Park, Pennsylvania, a subur b of Philadelphia. It was built in 1 922 by the VanHoebeek family, a husb and and wife originally from the Net herlands who made their fortune in t he tobacco industry. Cyril Conroy, a self-                                         |\n| about the disappeared 20th-century aviator Marian Graves, while the oth er is about the struggling 21st-cent ury Hollywood actress Hadley Baxter, who is attempting to make a film ab out Marian. Hadley's narrative is to ld in the first-person, while Marian 's sections are told in the third-pe rson                                      | about the disappeared 20th-century aviator Marian Graves, while the oth er is about the struggling 21st-cent ury Hollywood actress Hadley Baxter, who is attempting to make a film ab out Marian. Hadley's narrative is to ld in the first-person, while Marian 's sections are told in the third-pe rson                                      | on becoming a filmmaker. She has fo und a subject for her film project, an obscure African American actress credited only as 'the watermelon wom an' in old Hollywood films, and the subsequent film recounts her search for this woman even as it covers, in the manner of the earlier Dunyement aries, Dunye's friendships and her l ove life. InThe Watermelon Woman, D unye makes the film she set out tom ake in 1990 about African American w omen artists, a film that both inven ts an artistic predecessor with whom she can identify and also 'finds' C heryl herself as the artist that she seeks. As Dunye identifies herself                     | based closely on her own youthful e xperiences. (She plans the film to b e the first of two parts, the second dealing with the aftermath of the f irst's events.) Byrne plays a young film student named Julie (Hogg's ava tar), who starts her artistic educat ion with high hopes of making a movi e about a boy named Tony, living in working-class Sunderland, who adores his mother -'is almost obsessed wi th her,' as eager Julie tells her ad visers. Her idealism is evident from the start.The advisers are skepti cal, and no wonder; Julie's family i s posh, with a comfortable country e state and |\n| .Reception Great Circle received very favorable reviews, with a cumul ative \"Rave\" rating at the review ag gregator website Book Marks, based o n 22 book reviews from mainstream li terary critics. The novel debuted at number fourteen on The New York Tim es Hardcover fiction best-seller lis t for the week ending May                   | .Reception Great Circle received very favorable reviews, with a cumul ative \"Rave\" rating at the review ag gregator website Book Marks, based o n 22 book reviews from mainstream li terary critics. The novel debuted at number fourteen on The New York Tim es Hardcover fiction best-seller lis t for the week ending May                   | first edition hardcoverReception The novel debuted at number one on T he New York Times fiction best-selle r list. As of the week ending Februa ry 20, 2021, the novel has spent 38 weeks on the list.At the review ag gregator website Book Marks, which a ssigns individual ratings to book re views from mainstream literary criti cs, the novel received a cumulative \"Rave\" rating based on 38 reviews, w ith only one \"mixed\" review. Publish ers Weekly wrote, \"Bennett renders h er characters and their struggles wi th great compassion, and explores th e complicated state of mind that Ste lla finds herself in while passing a s white.\" In its | The book also debuted at number tw o on The New York Times Hardcover No nfiction best-sellers list on July 2 8, 2019.[5] It spent eleven weeks on the list.[6]Reception[edit]At t he review aggregator website Book Ma rks, which assigns individual rating s to book reviews from mainstream li terary critics, the book received a cumulative \"Positive\" rating based o n 29 reviews: 12 \"Rave\" reviews, 6 \" Positive\" reviews, 9 \"Mixed\" reviews , and 2 \"Pan\" reviews.[7] Publisher s Weekly gave the book a mixed revie w, writing, \"Unfortunately, all thre e                                              |\n| 8, 2021. Critics praised the novel for sustaining its length and for Sh ipstead's research and intricate nov el structure for perfectly interweav ing the parallel narratives, despite the time and circumstances separati ng them.In its starred review, Pub lishers Weekly wrote, \"Shipstead man ages to portray both Marian's and Ha dley's | 8, 2021. Critics praised the novel for sustaining its length and for Sh ipstead's research and intricate nov el structure for perfectly interweav ing the parallel narratives, despite the time and circumstances separati ng them.In its starred review, Pub lishers Weekly wrote, \"Shipstead man ages to portray both Marian's and Ha dley's |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |"
    },
    {
      "index": 16,
      "markdown": "| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0 GLYPH<147>                                                                                                                                                                                                                              | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                       | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2020 Summer ParalympicsThe , brand ed as the Tokyo 2020 Paralympic Game s, was an international multi-sport parasports event held from 24 August to 5 September 2021 in Tokyo, Japan . They were the 16th Summer Paralymp ic Games as organized by the Interna tional Paralympic Committee (IPC).                                                            | 2020 Summer Paralympics The , brand ed as the Tokyo 2020 Paralympic Game s, was an international multi-sport parasports event held from 24 August to 5 September 2021 in Tokyo, Japan . They were the 16th Summer Paralymp ic Games as organized by the Interna tional Paralympic Committee (IPC).                                                           | pics Games.* The 2020 Summer Paraly mpics are an upcoming major internat ional multi-sport event for athletes with disabilities governed by the I nternational Paralympic Committee. S cheduled as the 16th Summer Paralymp ic Games, it is planned to be held i n Tokyo, Japan from 25 August to 6 S eptember 2020 .3. 2019 BWF Para-Bad minton World Championships- The 20 19 BWF Para-Badminton World Champion ships was held from 20 to 25 August 2019 in Basel, Switzerland.- Men's event: Gold Medal: Pramod Bhagat in Singles SL3 Event and Pramod Bhagat and Manoj                                                                                             | 1 2020 Summer ParalympicsThe are an upcoming major international multi- sport event for athletes with disabi lities governed by the International Paralympic Committee. Scheduled as the 16th Summer Paralympic Games, th ey are scheduled to be held in Tokyo , Japan between 24 August and 5 Sept ember 2021. Originally due to take p lace between 25 August and 6 Septemb er 2020 . On 24 March 2020, the IOC a nd the Tokyo Organizing Committee of ficially announced that the 2020 Sum mer Olympics and 2020 Summer Paralym pics would be postponed to 2021, due to the COVID-19 pandemic, marking t he first time that the Paralympics h as been postponed. They will still b e publicly marketed as |\n| Originally scheduled to take place f rom 25 August to 6 September 2020 , i n March 2020 both the 2020 Summer Ol ympics and Paralympics were postpone d by one year due to the COVID-19 pa ndemic, with the rescheduled Games s till referred to as Tokyo 2020 form arketing and branding purposes. As with the Olympics, the Games were la rgely held behind | Originally scheduled to take place f rom 25 August to 6 September 2020 , i n March 2020 both the 2020 Summer Ol ympics and Paralympics were postpone d by one year due to the COVID-19 pa ndemic, with the rescheduled Games s till referred to as Tokyo 2020 form arketing and branding purposes. As with the Olympics, the Games were la rgely held behind | once submitted.This process was u ndertaken following the postponement of the Tokyo 2020 Games due to the COVID-19 pandemic, with both the Oly mpics and Paralympics pushed back a year.Now, the Tokyo 2020 Olympics are scheduled for July 23 to August 8 while the Paralympics are due to f ollow from August 24 to September 5. The refund process is separate for ticketholders outside of Japan, who purchased tickets through authorise d ticket resellers (ATR).Each ATR has its own individual refund proced ure.Early figures from the refund process for the Tokyo 2020 Olympics stated that around 18 per cent                                              | Olympiad, have now been postponed a nd rescheduled for 23 July to 8 Augu st 2021 in Tokyo, Japan. The Games were postponed in March 2020 as a re sult of the worldwide Covid-19 pande mic, although they will still keep t he name Tokyo 2020 for marketing and branding purposes. This will be th e first time the Olympic Games have been postponed rather than cancelled .                                                                                                                                                                                                                                                                                                                                |\n| closed doors with no outside specta tors due to a state of emergency in the Greater Tokyo Area and other pre fectures. The Games were the second Summer Paralympics hosted by Tokyo s ince 1964, and the third Paralympics held in Japan overall since the 199 8 Winter Paralympics in Nagano. Th e Games featured                                           | closed doors with no outside specta tors due to a state of emergency in the Greater Tokyo Area and other pre fectures. The Games were the second Summer Paralympics hosted by Tokyo s ince 1964, and the third Paralympics held in Japan overall since the 199 8 Winter Paralympics in Nagano. Th e Games featured                                           | has been rescheduled to May 1-4 bec ause of travel restrictions under th e current state of emergency in Toky o and other 10 prefectures across Ja pan.The Tokyo 2020 organizing comm ittee announced that the first of 18 test events for the Olympic and Par alympic Games will involve wheelchai r rugby, which will be held in Yoyog i National Stadium from April 3 to 4 .The FINA Diving World Cup will fo llow from April 18 to 23 at the Toky o Aquatics Centre, which will also s erve as an Olympic qualifying event. The spread of the COVID-19 pandemi c has slowed down in Tokyo three wee ks after the Japanese capital entere d a state of emergency on | Olympic Games, when Tokyo became th e first city in Asia to host the Oly mpic and Paralympic Games, but unfor tunately strong winds made it an imp ossible task this time around.Memb ers of the Tokyo Organising Committe e of the Olympic and Paralympic Game s (Tokyo 2020), Tokyo Metropolitan G overnment officials, Tokyo 2020 Torc h Relay Official Ambassadors and rep resentatives from Miyagi Prefecture joined the arrival ceremony.FLAME OF RECOVERYThe Olympic flame will now be put on display at various loc ations in the Tohoku region, to high light the message of hope in the are as worst affected by the 2011 Great                                                                    |\n| 539 medal events in 22 sports, with badminton and taekwondo both making their Paralympic debut to replace f ootball 7-a-side and sailing. China topped the medal table for the fifth consecutive Paralympics, with 96 go lds and 207 total medals. Great Brit ain finished second for the ninth t ime,                                                       | 539 medal events in 22 sports, with badminton and taekwondo both making their Paralympic debut to replace f ootball 7-a-side and sailing. China topped the medal table for the fifth consecutive Paralympics, with 96 go lds and 207 total medals. Great Brit ain finished second for the ninth t ime,                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | East Japan Earthqu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |"
    },
    {
      "index": 17,
      "markdown": "| ğ¶ ğ‘¢ colored by loss difference ğ¿ Retro[Off] GLYPH<0> ğ¿ Retro 6 GLYPH<0> 0 GLYPH<147> 5 GLYPH<148> = 0 GLYPH<148> > 0 GLYPH<147> 5                                                                                                                                                                               | ğ¶ ğ‘¢ colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                          | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| = Daniel Radcliffe =Daniel Jacob R adcliffe ( born 23 July 1989 ) is an English actor who rose to prominenc e as the title character in the Harr y Potter film series. He made his ac ting debut at 10 years of age in BBC One's 1999 television film David Co pperfield, followed by his cinematic debut       | = Daniel Radcliffe = Daniel Jacob R adcliffe ( born 23 July 1989 ) is an English actor who rose to prominenc e as the title character in the Harr y Potter film series. He made his ac ting debut at 10 years of age in BBC One's 1999 television film David Co pperfield, followed by his cinematic debut      | Daniel Jacob Radcliffe (born 23 July 1989) is an English actor who rose to prominence as the title character in the Harry Potter film series. He made his acting debut at 10 years o f age in BBC One's 1999 television f ilm David Copperfield, followed by h is cinematic debut in 2001's The Tai lor of Panama. At age 11, he was cas t as Harry Potter in the first Harry Potter film, and starred in the ser ies for 10 years until the release o f the eighth and final film in 2011. Radcliffe began to branch out to s tage acting in 2007, starring in the London and New York productions of Equus, and                       | Daniel Jacob Radcliffe (born 23 July 1989) is an English actor who rose to prominence as the title character in the Harry Potter film series. He made his acting debut at 10 years o f age in BBC One's 1999 televisionm ovie David Copperfield, followed by his film debut in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Pot ter film, and starred in the series for 10 years until the release of th e eighth and final film in 2011. Rad cliffe began to branch out to stage acting in 2007, starring in the Lond on and New York productions of Equus , and in the                   |\n| in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New                   | in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New                   | in 2001's The Tailor of Panama. At age 11, he was cast as Harry Potter in the first Harry Potter film, and starred in the series for 10 years u ntil the release of the eighth and f inal film in 2011.Radcliffe began to branch out to stage acting in 200 7, starring in the London and New Yo rk productions of Equus, and in the 2011 Broadway revival of the musical How to Succeed in Business Without Really Trying. He starred in the 201 2 horror film The Woman in Black, an d played beat poet Allen Ginsberg in the 2013 independent film Kill Your Darlings.He has contributed to ma ny charities                          | of Panama. At age 11, he was cast a s Harry Potter in the first Harry Po tter film, and starred in the series for 10 years until the release of t he eighth and final film in 2011.R adcliffe began to branch out to stag e acting in 2007, starring in the Lo ndon and New York productions of Equ us, and in the 2011 Broadway revival of the musical How to Succeed in Bu siness Without Really Trying. He sta rred in the 2012 horror film The Wom an in Black, and played beat poet Al len Ginsberg in the 2013 independent film Kill Your Darlings. He has con tributed to many charities, includin g Demelza House Children's |\n| York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our <unk>.He has contributed to ma ny charities, | York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our <unk>.He has contributed to ma ny charities, | York productions of Equus, and in t he 2011 Broadway revival of the musi cal How to Succeed in Business Witho ut Really Trying. He starred in the 2012 horror film The Woman in Black, and played beat poet Allen Ginsberg in the 2013 independent film Kill Y our Darlings.He has contributed to many charities, including Demelza H ouse Children's Hospice and The Trev or Project. He also made public serv ice announcements for the latter. In 2011, he was awarded the Trevor Pro ject's \"Hero Award.\"Sources disagr ee about Radcliffe's personal wealth ; he was reported to have earned Â£1 million for the first Harry Potter | in the 2011 Broadway revival of the musical How to Succeed in Business Without Really Trying. He starred in the 2012 horror film The Woman in B lack, and played beat poet Allen Gin sberg in the 2013 independent film K ill Your Darlings. He has contribute d to many charities, including Demel za House Children's Hospice and The Trevor Project. He also made public service announcements for the latter . In 2011, he was awarded the Trevor Project's \"Hero Award.\"                                                                                                                                                        |\n| including <unk> Hospice Care for Ch ildren, and The Trevor Project for s uicide prevention among LGBTQ youth, which gave him its Hero Award in 20 11.= = Early life = =Radcliffe w as born in West London, England. He is the only child of Alan George Rad cliffe, a literary agent, and                       | including <unk> Hospice Care for Ch ildren, and The Trevor Project for s uicide prevention among LGBTQ youth, which gave him its Hero Award in 20 11.= = Early life = =Radcliffe w as born in West London, England. He is the only child of Alan George Rad cliffe, a literary agent, and                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |"
    },
    {
      "index": 18,
      "markdown": "| Prompt and sample of Retro[Off]                                                                                                                                                                                                                              | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                      | Â» ğ‘ ğ‘¢ GLYPH<148> ğ¹ ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Â» ğ‘ ğ‘¢ GLYPH<148> ğ¹ ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Article premier - Les hommes naissent et demeurent libres e t Ã©gaux en droits. Les distinc tions sociales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© com mune. Article 2. - Le but de toute association politique e st la conservation des droits naturels et | Article premier - Les hommes naissent et demeurent libres e t Ã©gaux en droits. Les distinc tions sociales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© com mune. Article 2. - Le but de toute association politique e st la conservation des droits naturels et           | LCP = 0, 1, 2, 3,4, > 5 de l'homme et du citoyen.Ar ticle 1erLes hommes naissent et demeurent libres et Ã©gaux en droits. Les distinctions so ciales ne peuvent Ãªtre fondÃ©es que sur l'utilitÃ© commune.A rticle 2Le but de toute asso ciation politique est la conse rvation des droits naturels et imprescriptibles de l'homme. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ©, et la r Ã©sistance Ã  l'oppression.Art icle 3Le principe de toute s ouverainetÃ© rÃ©side essentielle ment dans la nation. Nul corps , nul individu ne peut exercer d'autoritÃ© qui n'en | LCP = 0, 1, 2, 3,4, > 5 Les hommes naissent et demeur ent libres et Ã©gaux en droits. Les distinctions sociales ne peuvent Ãªtre fondÃ©es que sur l 'utilitÃ© commune.Art. 2. - Le but de toute association po litique est la conservation de s droits naturels et imprescri ptibles de l'Homme. Ces droits sont la libertÃ©, la propriÃ©tÃ© , la sÃ»retÃ©, et la rÃ©sistance Ã  l'oppression.Art. 3. -Le principe de toute Souverainet Ã© rÃ©side essentiellement dans la Nation. Nul corps, nul indi vidu ne peut exercer d'autorit Ã© qui n'en Ã©mane expressÃ©ment. Art     |\n| imprescriptibles de l'homme, et par consÃ©quent la garantie Ã  chacun des droits suivants : Article 3. - La propriÃ©tÃ© est un droit inviolable et sa crÃ©. Toute personne a le droit de procÃ©der Ã  sa propre cons ervation. Article 4. - Le                      | imprescriptibles de l'homme. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ© et la rÃ© sistance Ã  l'oppression.Arti cle 3.- Le principe de toute souverainetÃ© rÃ©side essentiel lement dans la nation. Nul cor ps, nul individu ne peut exerc er d'autoritÃ© qui n    | criptibles del'homme. Ces dro its sont la libertÃ©, la propri Ã©tÃ©, la sÃ»retÃ©et la rÃ©sistanc e Ã  l'oppression.Article 3 - Le principe de toute souverai netÃ© rÃ©sideessentiellement da ns la Nation. Nul corps, nul i ndividu nepeut exercer d'auto ritÃ© qui n'en Ã©mane expressÃ©me nt.Article 4 - La libertÃ© co nsiste Ã  pouvoir faire tout ce quine nuit pas Ã  autrui : ai nsi, l'exercice des droits nat urelsde chaque homme n'a de b ornes que celles qui assurent auxautres membres de la sociÃ© tÃ© la jouissance de                                                   | et imprescriptibles de l'homm e. Ces droits sont la libertÃ©, la propriÃ©tÃ©, la sÃ»retÃ© et la rÃ©sistance Ã  l'oppression.A rticle 3 - Le principe de tout e souverainetÃ© rÃ©side essentie llement dans la Nation. Nul co rps, nul individu ne peut exer cer d'autoritÃ© qui n'en Ã©mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoir fai re tout ce qui ne nuit pas Ã  a utrui : ainsi, l'exercice des droits naturels de chaque homm e n'a de bornes que celles qui assurent aux autres membres d e la sociÃ©tÃ© la jouissance de ces mÃªmes droits. Ces bornes |\n| but de toute association est la dÃ©fense des droits de l'hom me et du citoyen. Tout citoye n a le droit de participer Ã  l a direction des affaires publi ques. Article 5. - L'impuni tÃ© n'a jamais Ã©tÃ© et ne sera j amais une fin en elle-mÃªme. L' imp        | 'en Ã©mane expressÃ©ment.Artic le 4.- La libertÃ© consiste Ã  pouvoir faire tout ce qui ne nuit pas Ã  autrui : ainsi, l'e xercice des droits naturels de chaque homme n'a de bornes qu e celles qui assurent aux autr es membres de la sociÃ©tÃ© la jo uissance de ces mÃªmes | mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoi r faire tout ce qui ne nuit pa s Ã  autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes que celle s qui assurent aux autres memb res de la sociÃ©tÃ© la jouissanc e de ces mÃªmes droits. Ces bor nes ne peuvent Ãªtre dÃ©terminÃ©e s que par la loi.Article 5 - La loi n'a le droit de dÃ©fend re que les actions nuisibles Ã  la sociÃ©tÃ©. Tout ce qui n'est pas dÃ©fendu par la loi ne peu t Ãªtre empÃªchÃ©, et nul ne peut Ãªtre contraint Ã  faire ce qu' elle n                              | mane expressÃ©ment.Article 4 - La libertÃ© consiste Ã  pouvoi r faire tout ce qui ne nuit pa s Ã  autrui : ainsi, l'exercice des droits naturels de chaque homme n'a de bornes que celle s qui assurent aux autres memb res de la sociÃ©tÃ© la jouissanc e de ces mÃªmes droits. Ces bor nes ne peuvent Ãªtre dÃ©terminÃ©e s que par la loi.Article 5 - La loi n'a le droit de dÃ©fend re que les actions nuisibles Ã  la sociÃ©tÃ©. Tout ce qui n'est pas dÃ©fendu par la loi ne peu t Ãªtre empÃªchÃ©, et nul ne peut Ãªtre contraint Ã  faire ce qu' elle n                      |"
    },
    {
      "index": 19,
      "markdown": "| Prompt and sample of Retro[Off]                                                                                           | Prompt and sample of Retro[On] colored by LCP with Ret ' ğ¶ ğ‘¢ GLYPH<0> 1 ' LCP = 0, 1, 2, 3,4, > 5                                         | Â» ğ‘ 1 ğ‘¢ GLYPH<148> ğ¹ 1 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                 | Â» ğ‘ 2 ğ‘¢ GLYPH<148> ğ¹ 2 ğ‘¢ â€¦ colored by LCP with ğ¶ ğ‘¢ , 1 LCP = 0, 1, 2, 3,4, > 5                                                                                                                                                                                                         |\n|---------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Pi = 3. 1415926535 8979323846 2643383279 5028841971 69399375 10 5820974944 5923078164 06286 20899 8628034825 3421170679   | Pi = 3. 1415926535 8979323846 2643383279 5028841971 69399375 10 5820974944 5923078164 06286 20899 8628034825 3421170679                   | '1415926535 8979323846 26433 83279 5028841971 693993751058 20974944 5923078164 0628620899 8628034825 34211706798214808 651 3282306647 0938446095 5058 223172 53594081284811174502 8 410270193 8521105559 644622948 9 54930381964428810975 665933 4461 2847564823 3786783       | 46 2643383279 5028841971 69399 37510 5820974944 592307816406 28620899 8628034825 3421170679 8214808651 3282306647 0938446 095 50582231725359408128 4811 174502 8410270193 8521105559 6 446229489 5493038196 442881097 56659334461 2847564823 378678 3165 2712019091 4564856692 346 0   |\n| 8294049602 8988496069 9858349 065 9873246379 9644789435 8628 730709 6540159079 5944069810 5 992965913 7095378412 69378359 | 8214808651 3282306647 0938446 095 5058223172 53594081284811 174502 8410270193 8521105559 6 446229489 5493038196442881097 5 6659334461 284 | 651 3282306647 0938446095 5058 223172 5359408128 4811174502 8410270193 8521105559 64462294 89 54930381964428810975 66593 34461 2847564823 3786783165 27 12019091 4564856692 346034861 0 4543266482 1339360726 024914 12737245870066 0631558817 488 1520920 9628292540 91715364 | 47 0938446095 5058223172 53594 081284811174502 8410270193 85 21105559 6446229489 5493038196 4428810975 6659334461 2847564 823 3786783165 27120190914564 856692 3460348610 4543266482 1 339360726 0249141273724587006 6 0631558817 4881520920 962829 2540 91715364367892590360          |\n| 10 6940372045 7088679512 85612 30857 9046461290 9276642155 56 54603269 5656128798 6366475705 6294954741 5886335339 57657  | 7564823 3786783165 2712019091 4564856692 3460348610 45432664 82 1339360726 024914127372458 70066 0631558817 4881520920 96 28292540 91715  | 23 3786783165 2712019091 4564 856692 3460348610 4543266482 1 339360726 0249141273724587006 6 0631558817 4881520920 962829 2540 9171536436 7892590360 01 13305305 4882046652 1384146951 94151160943305727036 5759591 953 0921861173 8193261179 3105 118548 0744623799 627495    | 165 27120190914564856692 3460 348610 4543266482 1339360726 0 2491412737245870066 063155881 7 4881520920 9628292540 917153 64367892590360 0113305305 488 2046652 1384146951 9415116094 3305727036 5759591953 09218611 73 8193261179 310511854807446 23799 6274956735 1885752724 89 1227 |"
    }
  ],
  "stats": {
    "pages": 43,
    "chunksCreated": 256,
    "totalCharacters": 180139,
    "totalWords": 25268,
    "numTables": 20,
    "processingTimeMs": 66887
  },
  "processedAt": "2025-12-29T13:05:55.802Z"
}