{
  "paper": {
    "id": "2310.19773v1",
    "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
    "abstract": "We present MM-VID, an integrated system that harnesses the capabilities of GPT-4V, combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.",
    "authors": [
      "Kevin Lin",
      "Faisal Ahmed",
      "Linjie Li",
      "Chung-Ching Lin",
      "Ehsan Azarnasab",
      "Zhengyuan Yang",
      "Jianfeng Wang",
      "Lin Liang",
      "Zicheng Liu",
      "Yumao Lu",
      "Ce Liu",
      "Lijuan Wang"
    ],
    "published": "2023-10-30T17:44:09.000Z",
    "updated": "2023-10-30T17:44:09.000Z",
    "primaryCategory": "cs.CV",
    "categories": [
      "cs.CV"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.19773v1",
    "absUrl": "https://arxiv.org/abs/2310.19773v1"
  },
  "chunks": [
    {
      "id": "2310.19773v1-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "chunkIndex": 0,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-1",
      "content": "∗ Core Contribution ♠ Project Lead https://multimodal-vid.github.io/\n\n<!-- image -->\n\nFigure 1. MM-VID allocates specialized vision, audio, speech experts with GPT-4V(ision) to address challenging video understanding tasks. For example, the system could associate information from multiple uploaded episodes and reason the storyline of the queried characters ('Multi-Video Episodic Analysis'). We highlight key information here and postpone full MM-VID responses to Figures 4-32. Demo videos are available at this link.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Kevin Lin ∗ , Faisal Ahmed ∗ , Linjie Li ∗ , Chung-Ching Lin ∗ , Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang ∗♠ Microsoft Azure AI",
        "chunkIndex": 1,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-2",
      "content": "We present MM-VID , an integrated system that harnesses the capabilities of GPT-4V 1 , combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-3",
      "content": "scription, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Abstract",
        "chunkIndex": 3,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-4",
      "content": "People around the world create numerous videos on a daily basis [14, 27, 48, 55], including user-generated live streams, video-game live streams, short clips, movies, sports broadcasts, advertising, and more. Videos serve as a versatile medium for conveying information and content through various modalities [12, 59, 60, 74, 77, 81, 82], such as text, visuals, and audio. Developing methods that can learn from diverse modalities will enable us to design cognitive machines with enhanced capabilities for analyzing uncurated real-world videos, extending beyond the confines of hand-curated datasets. However, this rich representation introduces many challenges for the study of video understanding, particularly when dealing with extended-duration videos [62,70].\n\nUnderstanding long videos, especially those spanning over an hour, is a complex task that demands advanced methods capable of analyzing sequences of images and audio across multiple episodes.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 4,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-5",
      "content": "62,70].\n\nUnderstanding long videos, especially those spanning over an hour, is a complex task that demands advanced methods capable of analyzing sequences of images and audio across multiple episodes. This challenge is compounded by the need to extract information from various sources, such as distinguishing speakers [17,49,61], identifying characters [29, 46, 50], and maintaining narrative coherence [26, 57]. Additionally, answering questions based on video evidence [31] requires a deep comprehension of the content, context, and subtitles. When it comes to live streaming and gaming videos [1, 11, 55], there are challenges in processing dynamic environments in real-time, re-\n\n1 In this work, we explore GPT-4V(ision) with the vision capability and refers to the model as 'GPT-4V,' following the OpenAI reports [51, 52]. We refer to the text-only version of the model as 'GPT-4' [51].\n\nquiring semantic understanding, and the ability of long-term strategy planning [11,54,67,75,83].",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 5,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-6",
      "content": "following the OpenAI reports [51, 52]. We refer to the text-only version of the model as 'GPT-4' [51].\n\nquiring semantic understanding, and the ability of long-term strategy planning [11,54,67,75,83].\n\nRecently, substantial advances have been made with large pre-trained video models [9, 13, 20, 21, 42, 69] and video-language models [10, 22, 23, 30, 36-40, 66], which have demonstrated their reasoning capabilities for video content. However, these models are usually trained on short clips ( e.g., 10-second videos in Kinetics [15] and VATEX [68]) or pre-defined action classes ( e.g., 174 classes in Something-Something v1 [24]). Consequently, these models may fall short in providing a detailed comprehension of intricate videos in real world [62, 70]. To achieve a more comprehensive understanding of the videos we encounter in daily life, we need methods capable of addressing complex challenges.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 6,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-7",
      "content": "prehension of intricate videos in real world [62, 70]. To achieve a more comprehensive understanding of the videos we encounter in daily life, we need methods capable of addressing complex challenges. It involves not only identifying who are in the scene and what they do, but also pinpointing when and how they act, while recognizing subtle nuances and visual cues across different scenes. The aim of this work is to address these challenges and explore methods that can be applied directly to real-world video understanding. Our approach involves breaking down extended video content into coherent narratives and subsequently employing these generated stories for video analysis.\n\nRecent advances in Large Multimodal Models (LMMs) [7,8,19,47,51,52,78], such as GPT-4V(ision) [52], have demonstrated significant breakthroughs in processing both input images and text for multimodal understanding. This has sparked interest in applying LMMs to the video domain.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 7,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-8",
      "content": "s GPT-4V(ision) [52], have demonstrated significant breakthroughs in processing both input images and text for multimodal understanding. This has sparked interest in applying LMMs to the video domain. In this work, we present MM-VID, a system that integrates specialized tools with GPT-4V for video understanding. Given an input video, MM-VID performs multimodal pre-processing, including scene detection and automatic speech recognition (ASR), to collect important information in the video. The input video is then split into multiple clips according to the scene detection algorithm. Then, we employ GPT-4V, which takes the clip-level video frames as input and generates a detailed description for each video clip. Finally, GPT-4 is adopted to generate a coherent script for the full video, conditioning on the clip-level video descriptions, ASR, and video metadata if available. As shown in Figure 1, the generated script allows MM-VID to perform a diverse set of video tasks.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 8,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-9",
      "content": "e full video, conditioning on the clip-level video descriptions, ASR, and video metadata if available. As shown in Figure 1, the generated script allows MM-VID to perform a diverse set of video tasks.\n\nExperimental results demonstrate the effectiveness of MM-VID in different challenging scenarios. MM-VID is able to comprehend hour-long videos through multiple modalities, and localize specific events with correct timestamps. MM-VID also demonstrates intriguing results in an interactive environment, such as predicting the possible next steps when playing a video game [4] or interacting with a graphical user interface (GUI) [78].\n\nTask List: Audio Description, Grounded QA, Summarization, Speaker Identification, Character Identification, Multimodal Reasoning, etc. Chat with Reference (Script) Please generate audio description for the input video Figure 2. Overview of MM-VID. Our system takes a video file as input, and outputs a long textual script describing the video contents.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 9,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-10",
      "content": "ce (Script) Please generate audio description for the input video Figure 2. Overview of MM-VID. Our system takes a video file as input, and outputs a long textual script describing the video contents. MM-VID consists of four modules: (i) Multimodal Pre-Processing , (ii) External Knowledge Collection , (iii) Clip-Level Video Description Generation , and (iv) Script Generation .\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "1. Introduction",
        "chunkIndex": 10,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-11",
      "content": "Conventional Video Understanding Methods. Early work in computer vision centered on building video foundation models [9,13,20,21,42,69]. These models, with different neural network architecture designs and training methods, have achieved great breakthrough at analyzing short video clips [14,15,28,63], typically lasting less than 30 seconds. However, these models are typically pre-trained with vision modality only, and thus may require specific adjustment or fine-tuning for multimodal downstream tasks.\n\nVideo-Language Models. Recent studies [10, 22, 23, 30, 36-40, 66] have made remarkable improvements in multimodal representation learning for video-and-language understanding. These advancements have been particularly evident in popular downstream tasks such as video question answering [31], text-video retrieval [32, 73] and video captioning [68].",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "2. Related Work",
        "chunkIndex": 11,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-12",
      "content": "nd-language understanding. These advancements have been particularly evident in popular downstream tasks such as video question answering [31], text-video retrieval [32, 73] and video captioning [68]. Building on this momentum, researchers typically embark on a pretrain-finetune paradigm: initially pre-training a video-language foundation model on largescale video-text pairs, followed by a fine-tuning process on specific downstream datasets. However, these methods are usually trained on short video clips, often restricted to durations of around 10 seconds, posing potential challenges in comprehending longer video sequences.\n\nVisual Instruction Tuning. Inspired by the breakthrough of Large Language Models (LLMs) [18, 19, 51, 64, 85], recent studies [35, 43, 44, 62, 84] suggest using a frozen LLM combined with an image encoder and a few learnable modules for video understanding tasks.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "2. Related Work",
        "chunkIndex": 12,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-13",
      "content": "ge Language Models (LLMs) [18, 19, 51, 64, 85], recent studies [35, 43, 44, 62, 84] suggest using a frozen LLM combined with an image encoder and a few learnable modules for video understanding tasks. Specifically, researchers propose the visual instruction tuning [35, 41, 44], which aims to fine-tune the learnable modules and thus enable LLMs to generate textual descriptions for the video content. While promising performance is presented, these models may fall short when it comes to handling videos with extended duration. Our work aims to fill this gap, exploring methods that can be directly applied to the understanding of long videos in real-world situations.\n\nPrompting LLMs for Video Understanding. Recently, researchers [6, 33, 65, 72] explore the LangChain system paradigm [16], which aims to integrate expert tools with existing LLMs to create new functionalities.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "2. Related Work",
        "chunkIndex": 13,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-14",
      "content": "g LLMs for Video Understanding. Recently, researchers [6, 33, 65, 72] explore the LangChain system paradigm [16], which aims to integrate expert tools with existing LLMs to create new functionalities. For example, VLog [6] uses BLIP2 [34] and GRIT [71] as dense image captioners, Whisper [56] as ASR translator, and ChatGPT as a reasoner. By transcribing a given video to textual descriptions ( e.g., document), it enables ChatGPT for video question-answering tasks. Inspired by the efficacy of these tool-using approaches [16, 65, 79], we explore integration with GPT-4V for video understanding.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "2. Related Work",
        "chunkIndex": 14,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-15",
      "content": "Recent studies [3, 51, 52, 78] show that GPT-4V can accept a range of inputs, such as textual descriptions, questions, or even visual cues like images or short video clips. GPT-4V's inherent ability to comprehend visual inputs and generate contextually relevant text opens the door for a wide range of applications. By introducing a sequence of frames as input, GPT-4V can grasp temporal relationships and interactions, aiding in the identification and interpretation of dynamic visual content.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "3. Preliminary Study with GPT-4V(ision)",
        "chunkIndex": 15,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-16",
      "content": "Figure 2 shows the overview of our system pipeline. MM-VID takes the video file as input, and outputs a script describing the video contents. The generated script enables LLMs to achieve various video understanding capabilities. MM-VID consists of four modules: (i) Multimodal PreProcessing , (ii) External Knowledge Collection , (iii) ClipLevel Video Description Generation , and (iv) Script Generation . We describe each module in detail below.\n\nMultimodal Pre-Processing. Starting with an input video file, our process begins by using the established ASR tool to extract transcriptions from the video. Following this, we divide the video into several short video clips. This process involves uniform sampling of video frames, with each clip consisting of 10 frames. To enhance the overall quality of frame sampling, we use established scene detection",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "4. MM-VID",
        "chunkIndex": 16,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-17",
      "content": "short video clips. This process involves uniform sampling of video frames, with each clip consisting of 10 frames. To enhance the overall quality of frame sampling, we use established scene detection\n\nFigure 3. MM-VID for streaming inputs. MM-VID can serve as an agent in an interactive environment, continually receiving and processing the streaming video frames.\n\n<!-- image -->\n\ntools like PySceneDetect [5] to help identify crucial scene boundaries.\n\nExternal Knowledge Collection. We incorporate external knowledge into our input prompts to GPT-4V. This involves gathering available information, such as metadata, title, abstract, and face photos of characters within the video. In our experiments, the metadata, title, and abstract are gathered from YouTube.\n\nClip-Level Video Description Generation. During our multimodal pre-processing, the input video is segmented into multiple clips. For each clip, which typically consists of 10 frames, we employ GPT-4V to generate video descriptions.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "4. MM-VID",
        "chunkIndex": 17,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-18",
      "content": "neration. During our multimodal pre-processing, the input video is segmented into multiple clips. For each clip, which typically consists of 10 frames, we employ GPT-4V to generate video descriptions. By feeding the video frames along with the associated text prompt into the model, GPT-4V utilizes the input to generate detailed descriptions that capture the visual elements, actions, and events depicted in those frames.\n\nIn addition, we explore the use of visual prompting, where the character's face photos are presented alongside the character's name in the input to GPT-4V. Our empirical results suggest that visual prompting is helpful to enhance the quality of video descriptions, particularly for more accurate character identification. These findings align with the insights from [78].\n\nScript Generation using LLM. After generating the descriptions for each video clip, we use GPT-4 to integrate these clip-level descriptions into a coherent script.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "4. MM-VID",
        "chunkIndex": 18,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-19",
      "content": "align with the insights from [78].\n\nScript Generation using LLM. After generating the descriptions for each video clip, we use GPT-4 to integrate these clip-level descriptions into a coherent script. This script serves as a comprehensive description of the entire video, and is used by GPT-4 for a diverse set of video understanding tasks.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "4. MM-VID",
        "chunkIndex": 19,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-20",
      "content": "Figure 3 shows the diagram of MM-VID when applied to the context of streaming inputs. Our system operates as an agent within a dynamic environment where streaming video frames serve as the primary input. In this context, the agent continually receives streaming video frames as states, rep- resenting the ongoing visual information unfolding in the environment. These states are then processed by GPT-4V to make informed decisions and generate responses.\n\nBy continually analyzing the streaming video frames, MM-VID plays a crucial role in transforming raw visual data into meaningful insights, making it valuable for applications such as video game play, the embodied agent, and GUI navigation.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "5. MM-VID for Streaming Inputs",
        "chunkIndex": 20,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-21",
      "content": "We implement MM-VID based on MM-REACT [79] codebase. We use the Automatic Speech Recognition (ASR) tool publicly available via the Azure Cognitive Services APIs [2], and utilize PySceneDetect [5] for scene detection.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.1. Experiment Setup",
        "chunkIndex": 21,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-22",
      "content": "Audio Description Generation,\n\nFigures 4-9 provide illustrative examples of MM-VID's complete execution flow. When a user uploads a video file, MM-VID initiates the process by first assessing the estimated video length. Subsequently, it performs multimodal pre-processing by invoking expert tools, including scene detection and ASR. Additionally, MM-VID collects external knowledge, encompassing video metadata such as title and abstract.\n\nFollowing this preliminary stage, MM-VID proceeds to generate clip-level video descriptions for each segment of the video. Finally, it invokes GPT-4, integrating these cliplevel descriptions into a coherent script. Once the script is generated, it empowers LLMs to provide a summarized understanding of the video content. That equips the system to address users' questions with grounded answers. We discuss MM-VID's distinct capabilities as below.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.2. MM-VID Capabilities",
        "chunkIndex": 22,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-23",
      "content": "empowers LLMs to provide a summarized understanding of the video content. That equips the system to address users' questions with grounded answers. We discuss MM-VID's distinct capabilities as below.\n\nGrounded Question-Answer (QA). The generation of a comprehensive script empowers our system with the capability of grounded QA. As shown in Figure 8, let us consider a scenario where a user poses the question, 'Show me the most exciting moment in this video.' In response, MMVID displays a highlight, specifically featuring a home run, and provides the corresponding timestamp. When a user asks 'Who are the best pitchers in this video?' MM-VID addresses the question by referring to relevant evidence in the generated script. This grounding capability owes its success to the extensive and detailed script generation process, which documents essential timestamps and significant events within the video, enabling accurate and contextually grounded responses to user inquiries.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.2. MM-VID Capabilities",
        "chunkIndex": 23,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-24",
      "content": "extensive and detailed script generation process, which documents essential timestamps and significant events within the video, enabling accurate and contextually grounded responses to user inquiries.\n\nMultimodal Reasoning. MM-VID considers multimodal inputs, including video frames, speech transcriptions, and external knowledge if available. In Figure 8, when a user inquires, 'How did you know the sound is different?'\n\nk heart\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.2. MM-VID Capabilities",
        "chunkIndex": 24,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-25",
      "content": "Video length is  9 minutes 19 seconds.\n\n<!-- image -->\n\nInvoke tools: Scene Detection, ASR, Metadata Collection",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Loading video …",
        "chunkIndex": 25,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-26",
      "content": "00:00:00, 00:00:02, 00:00:20, 00:00:24,\n\n..., 00:08:42, 00:08:53, 00:09:01, 00:09:07",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Scene Detection:",
        "chunkIndex": 26,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-27",
      "content": "0:05 gets ready for his second start of the 0:08 Season began this home stand got the win\n\n:\n\n:\n\n:\n\n:\n\n9:04 that's the ball game Dodgers sweep the\n\n9:07 two-game series and they go four and two 9:09 on the season opening home stand",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "ASR:",
        "chunkIndex": 27,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-28",
      "content": "Title: Rockies vs. Dodgers Game Highlights Abstract: Rockies vs. Dodgers Game Highlights (4/4/23) | MLB Highlights\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Metadata Collection:",
        "chunkIndex": 28,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-29",
      "content": "Timestamp: 00:00:00 - 00:00:02\n\nScene: Opening graphics introducing the game between the Colorado Rockies and the Los Angeles Dodgers.\n\nTimestamp: 00:00:20 - 00:00:24\n\nScene: Pitcher on the mound at Dodger Stadium.\n\n:\n\n:\n\n:\n\nTimestamp 00:09:01-00:09:07\n\nScene Description: In this scene, we see a baseball game between the Los Angeles Dodgers and the Colorado Rockies. The Dodgers are leading with a score of 5-2. A player from the Dodgers hits the ball, and it is fielded by a player from the Rockies.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 29,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-30",
      "content": "<!-- image -->\n\nYou are an expert at understanding audio descriptions of different scenes in a video. Can you leverage the information provided, including title, abstract, audio descriptions and generate full audio description of each scene with non overlapping time ranges. Keep as many scenes possible covering all time ranges. You may find character names in the title or abstract.  Use character names wherever possible in the audio descriptions. Keep the audio description for each time range within one short sentence.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Input Prompt:",
        "chunkIndex": 30,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-31",
      "content": "0:00:00 - 0:00:14: Video begins with a montage of the Colorado Rockies and the Los Angeles Dodgers players warming up for their game at Dodger Stadium over the backdrop of the Los Angeles skyline.\n\n0:00:15 - 0:00:30: Dodge.rs pitcher, Julio Urias, makes his second start of the season and deals a powerful pitch towards the Rocky's batter. As the batter swings and makes contact, the ball heads down the line under the glove of the Dodgers and is marked as a double for the Rockies.\n\n0:00:31 - 0:00:46: Urias successfully strikes out\n\nFigure 4. An example of MM-VID's execution flow. Given a baseball video, MM-VID provides an estimated video length, and then invokes scene detection and ASR tools, and collects external knowledge. Then, we generate clip-level video descriptions by using GPT-4V. GPT-4V takes the video frames and the text prompt as input, and outputs the video descriptions.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 31,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-32",
      "content": "tools, and collects external knowledge. Then, we generate clip-level video descriptions by using GPT-4V. GPT-4V takes the video frames and the text prompt as input, and outputs the video descriptions. Finally, given the clip-level descriptions, video metadata and ASR, we use GPT-4 to generate a coherent script for the input video. Figures 7-9 show continued outputs. The original video is available at https://www.youtube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\n<!-- image -->\n\nMM-VID explains that this information was derived from the commentator's remarks during the game. The examples illustrate MM-VID's multimodal reasoning capabilities, where it integrates both visual and auditory cues to provide contextually accurate responses to user queries.\n\nHour-Long Video Comprehension. Figures 10-13 demonstrate MM-VID's capabilities in processing lengthy videos. In this example, MM-VID effectively analyzes a documentary video spanning approximately 50 minutes in duration.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 32,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-33",
      "content": "rehension. Figures 10-13 demonstrate MM-VID's capabilities in processing lengthy videos. In this example, MM-VID effectively analyzes a documentary video spanning approximately 50 minutes in duration. For simplicity, the intermediate outputs are omitted in the figures, and only the final generated script is presented. We observe that MM-VID is able to generate a long script with the corresponding timestamps to represent the documentary video. By leveraging this generated script as contextual information, MM-VID is equipped to perform a range of tasks, including summarizing the lengthy video, addressing specific queries raised within the video, and indexing pivotal moments.\n\nMulti-Video Episodic Analysis. MM-VID's proficiency in handling extensive video content can be expanded to encompass multiple lengthy videos, as illustrated in Figures 14-16. In these examples, we upload multiple episodes to MM-VID, showcasing its ability to perform a variety of complex tasks.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 33,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-34",
      "content": "expanded to encompass multiple lengthy videos, as illustrated in Figures 14-16. In these examples, we upload multiple episodes to MM-VID, showcasing its ability to perform a variety of complex tasks. MM-VID exhibits the capability to summarize the video series, engage in cross-episode reasoning, provide detailed descriptions of character journeys across multiple episodes, and facilitate grounded QA interactions.\n\nCharacter Identification. We found that incorporating visual prompts enhances the quality of script generation, particularly with regards to character identification. In Figure 17, we illustrate this by providing MM-VID with additional inputs consisting of characters' face photos and their corresponding names. MM-VID effectively utilizes these visual prompts to identify the characters depicted in the video, based on the provided face photos.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 34,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-35",
      "content": "nsisting of characters' face photos and their corresponding names. MM-VID effectively utilizes these visual prompts to identify the characters depicted in the video, based on the provided face photos. As a result, the script generation process is notably improved, ensuring more accurate and contextually relevant descriptions of characters and their interactions within the video content.\n\nSpeaker Identification. Our exploration has revealed another valuable application of visual prompting in enhancing the quality of Automatic Speech Recognition (ASR). In Figures 18-19, we highlight a scenario where conventional ASRstruggles to accurately recognize the number of speakers and their identities in the video. Visual prompting plays a pivotal role in enhancing ASR performance by providing contextual cues to identify individuals and attribute speech to specific speakers.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 35,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-36",
      "content": "rs and their identities in the video. Visual prompting plays a pivotal role in enhancing ASR performance by providing contextual cues to identify individuals and attribute speech to specific speakers. This improvement ensures more precise transcriptions, enabling a more accurate representation of the dialogue and interactions within the video content.\n\nAudio Description Generation. Audio descriptions [26, 57] play a crucial role in making videos accessible to individuals who are blind, have low vision, or face difficulties in visually understanding the content. These descrip- tions provide contextual narration of meaningful visual elements, clarify speakers, and convey the essence of visual information within a video. In our experiments, we also explore MM-VID's performance in audio description generation. We experiment with videos where there is limited or no speech content. In Figure 20, we showcase an example featuring a short film of Mr.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 36,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-37",
      "content": "explore MM-VID's performance in audio description generation. We experiment with videos where there is limited or no speech content. In Figure 20, we showcase an example featuring a short film of Mr. Bean taking an exam, which primarily lacks speech. Without ASR inputs, MM-VID processes the video and generates a detailed script. This shows MM-VID's versatility in handling various types of video content and its potential in creating inclusive and accessible multimedia content.\n\nSelf-Refinement. While the generated script offers a comprehensive understanding of video content, our experiments have unveiled occasional inaccuracies, especially in cases involving blurry or low-resolution video frames, as demonstrated in Figure 21. In this example, MM-VID mistakenly identifies a bird as a rock due to the challenges posed by the video's visual quality. To address such inconsistencies and elevate the overall accuracy of the generated script, we employ a self-refinement approach [45, 58, 80].",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 37,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-38",
      "content": "due to the challenges posed by the video's visual quality. To address such inconsistencies and elevate the overall accuracy of the generated script, we employ a self-refinement approach [45, 58, 80]. This involves revising the script based on both the initially generated script and a concurrently generated video summary. Through this process, MM-VID is able to rectify errors and inaccuracies, resulting in a more refined output.\n\nFast-Changing Short Videos. In Figure 22, we present an example of our experimentation with fast-changing shortform videos, such as those found on platforms like TikTok. Short videos often feature non-standard frame sizes and significantly shorter durations compared to conventional videos. Remarkably, MM-VID excels at accurately describing the cooking recipes depicted in these short videos, despite the distinct characteristics of such content.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 38,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-39",
      "content": "durations compared to conventional videos. Remarkably, MM-VID excels at accurately describing the cooking recipes depicted in these short videos, despite the distinct characteristics of such content.\n\nThese examples demonstrate the versatility of MM-VID in processing a diverse array of video content. Whether dealing with lengthy documentaries, episodic series, or short-form clips, MM-VID adapts seamlessly to the unique attributes of each video type, consistently delivering meaningful and contextually relevant descriptions.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 39,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-40",
      "content": "In the following section, we evaluate MM-VID when applying to the context of streaming inputs. MM-VID serves as an agent in an interactive environment, continually receiving streaming video frames as the inputs.\n\nEmbodied Agent. Figure 23 illustrates an example where MM-VID is applied to an egocentric video captured by a head-mounted camera. This video, collected from Ego4D dataset [25], provides a brief glimpse into the wearer's daily life within their home environment. Remarkably, MM-VID showcases its capability in understanding such video content and assists the user in a few practical tasks. Specifically, MM-VID helps the user locate items like the pink\n\nTable 1. Questionnaire for the group with visual impairments. Participants listen to a video and subsequently assign scores (ranging from 0 to 10) for distinct auditory criteria.\n\nEffectiveness of Delivery: If the original audio and the embedded AD are effectively presented?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.3. Applications to Interactive Environments",
        "chunkIndex": 40,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-41",
      "content": "listen to a video and subsequently assign scores (ranging from 0 to 10) for distinct auditory criteria.\n\nEffectiveness of Delivery: If the original audio and the embedded AD are effectively presented?\n\nInformative: Is it easy to follow the storyline? Does the AD provide context and background information when necessary?\n\nAudio Quality: Is the overall audio production quality good?\n\nOverall Satisfaction: Are you satisfied with the overall AD experience?\n\njacket and the laptop within the home. Additionally, it generates a list of the user's activities within a specified time range, offering insights into the wearer's daily routine.\n\nPlaying Video Games. Figures 24-27 demonstrate the results of applying MM-VID to a Mario video game [4]. In these experiments, our agent consistently receives three video frames as states and calculates the next possible control action.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.3. Applications to Interactive Environments",
        "chunkIndex": 41,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-42",
      "content": "emonstrate the results of applying MM-VID to a Mario video game [4]. In these experiments, our agent consistently receives three video frames as states and calculates the next possible control action. Remarkably, our agent displays an understanding of the specific video game dynamics and generates reasonable action controls to play the game effectively. These examples highlight MM-VID's ability to comprehend and navigate in an interactive gaming environment. Interested readers may find the full gameplay demonstration on our project website.\n\nGUI Navigation. Figures 28-32 provide the demonstration of MM-VID's performance in the GUI navigation scenario. In this context, the agent continually receives iPhone screenshots and previous user actions as states. The agent effectively predicts the possible next steps in the user's journey, which may include clicking on the correct shopping apps, initiating searches for items of interest, and ultimately placing an order.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.3. Applications to Interactive Environments",
        "chunkIndex": 42,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-43",
      "content": "effectively predicts the possible next steps in the user's journey, which may include clicking on the correct shopping apps, initiating searches for items of interest, and ultimately placing an order. These results demonstrate MM-VID's remarkable ability to interact with graphical user interfaces, facilitating seamless and intelligent navigation through digital interfaces.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.3. Applications to Interactive Environments",
        "chunkIndex": 43,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-44",
      "content": "Weexplore the potential of MM-VID for people who are blind or have low vision. Audio description (AD) [26, 57] provides an auditory narration integrated into the video's soundtrack, offering important visual details that may not be discernible from the main video soundtrack. Such descriptions play a pivotal role in conveying essential visual content to those with visual impairments.\n\nTo assess the efficacy of MM-VID in generating audio descriptions (AD), we conduct a user study. We invited 9 participants for the evaluation. 4 participants were either blind or had low vision, while the remaining 5 had normal\n\nTable 2. Questionnaire for the group with normal vision. Participants view a video and subsequently assign scores (ranging from 0 to 10) for various auditory and visual criteria.\n\nClarity: Are the visual elements clearly and accuratetly described?\n\nConciseness: Does the AD convey essential visual information without overloading the user?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4. User Study",
        "chunkIndex": 44,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-45",
      "content": "r various auditory and visual criteria.\n\nClarity: Are the visual elements clearly and accuratetly described?\n\nConciseness: Does the AD convey essential visual information without overloading the user?\n\nTiming and Synchronization: Are the original audio and the embedded AD effectively presented? Does the AD properly synchronize with visual contents?\n\nInformative: Is it easy to follow the storyline? Does the AD provide context and background information when necessary?\n\nAudio Quality: Is the overall audio production quality good?\n\nOverall Satisfaction: Are you satisfied with the overall AD experience?\n\nvision. All the participants have normal hearing. For the purposes of the experiments, we segregated participants into two distinct groups: ( i ) Group with visual impairments, and ( ii ) Group with normal vision.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4. User Study",
        "chunkIndex": 45,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-46",
      "content": "Our experiments utilize a curated set of videos, which are mainly suggested by the American Council of the Blind 2 . We also collected accessibility videos from YouTube 3 . For every video used in our evaluation, participants are exposed to two versions: the first containing human-crafted AD and the second powered by MM-VID-generated AD. Both renditions are narrated using text-to-speech (TTS) technology.\n\nWehave designed two questionnaires for the two groups, referenced in Table 1 and Table 2, respectively. Participants with visual impairments are instructed to base their evaluation exclusively on auditory cues. In contrast, those with normal vision are instructed to consider both visual and auditory elements.\n\nThe assessment adopts the standardized Likert scale for ratings. For each posed question, participants are guided to assign a score ranging from 0 to 10, with higher values indicating more favorable feedback.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.1 Evaluation Procedure",
        "chunkIndex": 46,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-47",
      "content": "ssment adopts the standardized Likert scale for ratings. For each posed question, participants are guided to assign a score ranging from 0 to 10, with higher values indicating more favorable feedback. Furthermore, participants are urged to share feedback and remarks concerning their overall experience.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.1 Evaluation Procedure",
        "chunkIndex": 47,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-48",
      "content": "We utilized 3 different videos for our evaluation, with durations of 1 minute, 1 minute 42 seconds, and 2 minutes\n\n2 The Audio Description Project: https://adp.acb.org/\n\n3 Apple Accessibility: https://www.youtube.com/watch?v= SL7YSqlEd8k\n\n42 seconds, respectively. Each of the 4 participants with visual impairment was well versed with screen reader and other common accessibility tools. After listening to the audio descriptions for each video, they were asked to respond to the 4 questions outlined in Table 1.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.2 Results on the Group with Visual Impairments",
        "chunkIndex": 48,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-49",
      "content": "H1: The MM-VID-generated audio description and original video dialogues are effectively presented to the participants.\n\nResults: Using the Likert scale (0=Not Effective to 10=Most Effective) the participants rated the effectiveness of the delivery of human-crafted AD and MM-VIDgenerated AD. On average, participants gave 7 . 14 ± 1 . 39 for MM-VID-generated AD and 8 . 33 ± 0 . 90 for humancrafted AD, which shows a MM-VID-generated AD very close to human-crafted one in terms of effective delivery (Figure 5).\n\nH2: Participants are able to follow the main story line of the video based on MM-VID-generated audio description only.\n\nResults: Using the Likert scale (0=Not Informative to 10=Highly Informative) the participants rated the informativeness of human-crafted AD and MM-VID-generated AD. On average, participants gave 7 . 14 ± 1 . 16 for MMVID-generated AD and 9 . 29 ± 0 .",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Hypotheses and Results",
        "chunkIndex": 49,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-50",
      "content": "to 10=Highly Informative) the participants rated the informativeness of human-crafted AD and MM-VID-generated AD. On average, participants gave 7 . 14 ± 1 . 16 for MMVID-generated AD and 9 . 29 ± 0 . 91 for human-crafted AD, which shows little difference in informativeness between MM-VID-generated AD and human-crafted one (Figure 5).\n\nH3: MM-VID-generated AD and human-crafted AD are close in terms of voice and audio quality.\n\nResults: Using the Likert scale (0=Low Quality to 10=High Quality) the participants rated the voice and audio quality on average as 8 . 91 ± 1 . 23 for MM-VID-generated AD and 9 . 07 ± 0 . 65 for human-crafted AD. This minimal difference between the scores indicates the close-to-human voice and audio quality of MM-VID-generated AD (Figure 5).",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Hypotheses and Results",
        "chunkIndex": 50,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-51",
      "content": "The results show that the participants' overall satisfaction of MM-VID-generated ADs was on average around 2 points less than human-crafted ones in the Likert scale (0=Not Satisfied to 10=Highly satisfied) (Figure 5). Some of the difficulties indicated by participants while listening to MM-VID-generated ADs were 1) occasional overlaps between AD audio and original video dialogues 2) wrong descriptions due to hallucinations of GPT-4V(ision). Regardless of the difference in overall satisfaction, all the participants agreed that MM-VID-generated AD can pro-\n\nFigure 5. Results on the group with visual impairments. MMVID-generated AD is close to human-generated ones in terms of audio quality and effectiveness of delivery. However, MMVID's AD yields lower satisfaction levels compared to the humangenerated ones. This was primarily attributed to occasional overlaps between the audio descriptions and the video dialogues.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Discussion:",
        "chunkIndex": 51,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-52",
      "content": "'s AD yields lower satisfaction levels compared to the humangenerated ones. This was primarily attributed to occasional overlaps between the audio descriptions and the video dialogues.\n\n<!-- image -->\n\nvide a cost-effective and scalable solution. Thus, millions of videos that cannot afford to be professionally audio described, can be auto-processed by a tool like MM-VID to make them accessible to the visual-impaired community.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Discussion:",
        "chunkIndex": 52,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-53",
      "content": "For sighted individuals, we used the same set of videos as we used for individuals with visual impairments. All of our 5 participants answered to 6 questions listed in Table 2 after watching videos embedded with MM-VID-generated AD as subtitles and audio track.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.3 Results on the Group with Normal Vision",
        "chunkIndex": 53,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-54",
      "content": "H1: The MM-VID-generated AD is accurate and conveys essential information without overloading the listener.\n\nResults: The sighted individuals rated the clarify and accuracy of MM-VID-generated AD as 7 . 83 ± 1 . 24 and human-curated AD as 8 . 9 ± 0 . 74 on average, using the Likert scale (0=Not Accurate to 10=Most Accurate). In terms of conciseness, the participants on average gave 8 . 73 ± 0 . 49 for the MM-VID-generated AD and 9 . 16 ± 0 . 54 for human-curated AD based on the Likert scale (0=Not concise to 10=Most concise). These results indicate MM-VID-generated ADs are close to human-curated ones in terms of accuracy and conciseness (Figure 6).\n\nH2: The MM-VID-generated ADs are in sync with visual content and do not overlap with other dialogues ensuring listeners can follow the story line.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Hypotheses and Results",
        "chunkIndex": 54,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-55",
      "content": "nes in terms of accuracy and conciseness (Figure 6).\n\nH2: The MM-VID-generated ADs are in sync with visual content and do not overlap with other dialogues ensuring listeners can follow the story line.\n\nResults: Participants gave on average 8 . 90 ± 0 . 90 and 7 . 97 ± 1 . 54 to human-crafted AD and MM-VID-generated AD respectively using the Likert scale (0=Not Informative to 10=Highly Informative). Human-crafted AD and MM-VID-generated AD received 8 . 59 ± 0 . 95 and 8 . 53 ± 0 . 58 respectively on the aspect of timing and synchronization using the Likert scale (0=Not Effective to 10=Most Effective). These indicates while listening to MM-VID-generated ADs participants were able to follow main story line and found the audios are in sync with video content very close to that of human-crafted ADs (Figure 6).\n\nH3: The voice and audio quality of MM-VID-generated ADs are close to human-crafted ADs.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Hypotheses and Results",
        "chunkIndex": 55,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-56",
      "content": "ry line and found the audios are in sync with video content very close to that of human-crafted ADs (Figure 6).\n\nH3: The voice and audio quality of MM-VID-generated ADs are close to human-crafted ADs.\n\nResults: The results are very similar to results on group with visual impairments. Sighted participants rated the voice and audio quality on average as 8 . 30 ± 0 . 89 for MMVID-generated AD and as 8 . 93 ± 0 . 32 for human-crafted AD. Therefore the voice and audio experience did not degrade much while listening to MM-VID-generated ADs compare to human-crafted ADs (Figure 6).",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Hypotheses and Results",
        "chunkIndex": 56,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-57",
      "content": "The evaluations on sighted individuals helped to cross verify the hypotheses of individuals with visual impairments, that are based on audio cues only. Although the overall satisfaction points for sighted participants with MM-VID-generated ADs was on average &lt; 1 points lower than human-generated ADs (Figure 6), the overall satisfaction points for participants who were blind was worse. This is expected because sighted individuals had access to both audio and video modalities but individuals with visual impairments did not. We also believe the reason for lower overall satisfaction, may have been the lack of practice listening to auto generated ADs. Some of the users also mentioned they have preference between pitches of voice and number of concurrent audio channels. These may add to the reason of lower overall satisfaction.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Discussion:",
        "chunkIndex": 57,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-58",
      "content": "We present a collection of interview quotes from our participants who were visually impaired, in which they share their personal experiences and insights about the audio descriptions (AD) generated by MM-VID. The participants expressed a unanimous desire to continue utilizing this AD generation service in the future, highlighting its exceptional quality ('Nearly perfect'), intricate details ('favorite was the details'), extensive applicability ('allowed me to follow anything visual'), and the profound impact it has on them ('I did not depend on someone else'). Below, we provide additional quotes for further insight.\n\nP1: 'I understand what is going on very quickly\n\nFigure 6. Results on the group with normal vision. MM-VIDgenerated AD was on average &lt; 1 points lower than humangenerated ADs. The participants were able to follow the main story line and the audios are in sync with the video content.\n\n<!-- image -->\n\nand I did not depend on someone else. '",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.4 Participant Feedback",
        "chunkIndex": 58,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-59",
      "content": "ints lower than humangenerated ADs. The participants were able to follow the main story line and the audios are in sync with the video content.\n\n<!-- image -->\n\nand I did not depend on someone else. '\n\n- P2: 'If it's AI-generated, there are so many places it's not available, and we need it there. '\n\nP2: 'First time listening to auto-generated AD. As a user, if I am offered this AD, I would take it. ' P3: 'Nearly perfect. Most favorite was the de-\n\n- tails. '\n\nP3: 'More information helped me follow the storyline. '\n\nP3: 'It allowed me to follow anything visual. It felt natural the way AD describes how the actor interacts with the environment. '\n\nP3: 'I love animal kingdom, and I watch Wild Earth safari virtual tour. I would love to have audio descriptions of Wild Earth videos and daily safaris. '\n\nP4: 'I would like to have auto-generated audio description for live conferences in Microsoft Teams.'\n\nP4: 'It worked best as the original audio had not much value.'",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.4 Participant Feedback",
        "chunkIndex": 59,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-60",
      "content": "Earth videos and daily safaris. '\n\nP4: 'I would like to have auto-generated audio description for live conferences in Microsoft Teams.'\n\nP4: 'It worked best as the original audio had not much value.'\n\nDespite the positive feedback, not all responses were favorable:\n\nP4: 'I am skeptical when it becomes subjective. Sometimes I feel they make up stories which is not good.'\n\nP4: 'After listening to the human-generated AD, I figured I misunderstood parts of the original story. '\n\nP1: 'It keeps referring to the same person using their names instead of pronouns. '\n\nP4: 'I don't deal well with overlapped or two parallel audios. '\n\nInterestingly, even those participants who provided critical feedback still rated the MM-VID-generated AD closely to human-generated AD, during the questionnaire sessions. This indicates that, similar to human-curated AD, adapting to MM-VID-generated ADs might necessitate some practice and acclimatization over time.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.4 Participant Feedback",
        "chunkIndex": 60,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-61",
      "content": "ns. This indicates that, similar to human-curated AD, adapting to MM-VID-generated ADs might necessitate some practice and acclimatization over time.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "6.4.4 Participant Feedback",
        "chunkIndex": 61,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-62",
      "content": "We have presented MM-VID, a system that synergizes with GPT-4V for advancing video understanding. MMVID employs GPT-4V to transcribe video content into long and detailed scripts, thereby enriching LLMs with advanced video understanding capabilities. Experimental results demonstrate the effectiveness of MM-VID in addressing challenging tasks, including comprehension of hourlong videos, analysis across multiple episodes, identification of characters and speakers, and interaction with video games and graphical user interfaces.\n\nBeyond the development of the MM-VID system, we conducted an extensive user study, drawing feedback from a varied group of participants. The outcomes of this study indicated that the audio descriptions generated by MM-VID closely mirror the quality of those crafted by humans. In our future work, we plan to explore SoM [76] and object tracking techniques to enhance various tasks and functionalities.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "7. Conclusion",
        "chunkIndex": 62,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-63",
      "content": "We are deeply grateful to OpenAI for providing access to their exceptional tool [3, 51-53]. We are profoundly thankful to Misha Bilenko for his invaluable guidance and support. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Cenyu Zhang, Saqib Shaikh, Ailsa Leen, Jeremy Curry, Crystal Jones, Roberto Perez, Ryan Shugart, Anne Taylor for their constructive feedback.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Acknowledgment",
        "chunkIndex": 63,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-64",
      "content": "- [1] Dota 2. https://openai.com/research/dota-2 , 2017. 2\n- [2] Azure cognitive services apis. https://azure. microsoft . com / products / cognitive -services , 2023. 4\n- [3] Chatgpt can now see, hear, and speak. https://openai. com/blog/chatgpt-can-now-see-hear-andspeak , 2023. 3, 10\n- [4] Pygame library. https://www.pygame.org/ , 2023. 2, 7, 31, 32, 33, 34\n- [5] Pyscenedetect: Video scene cut detection and analysis tool. https://www.scenedetect.com/ , 2023. 4\n- [6] Vlog: Video as a long document. https://github. com/showlab/VLog , 2023. 3\n- [7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022. 2\n- [8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-65",
      "content": "98 , 2022. 2\n- [8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023. 2\n- [9] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇ ci´ c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision , pages 6836-6846, 2021. 2, 3\n- [10] Max Bain, Arsha Nagrani, G¨ ul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1728-1738, 2021. 2, 3\n- [11] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-66",
      "content": "ker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems , 35:24639-24654, 2022. 2\n- [12] Tadas Baltruˇ saitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence , 41(2):423-443, 2018. 2\n- [13] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML , volume 2, page 4, 2021. 2, 3\n- [14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987 , 2019. 2, 3\n- [15] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-67",
      "content": "e on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987 , 2019. 2, 3\n- [15] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6299-6308, 2017. 2, 3\n- [16] Harrison Chase. Langchain. https://langchain. readthedocs.io/ , 2023. 3\n- [17] Sanyuan Chen, Yu Wu, Zhuo Chen, Jian Wu, Jinyu Li, Takuya Yoshioka, Chengyi Wang, Shujie Liu, and Ming Zhou. Continuous speech separation with conformer. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5749-5753. IEEE, 2021. 2\n- [18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-68",
      "content": "ao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 3\n- [19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 2, 3\n\n- [20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 6824-6835, October 2021. 2, 3\n- [21] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision , pages 6202-6211, 2019.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-69",
      "content": "h Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision , pages 6202-6211, 2019. 2, 3\n- [22] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681 , 2021. 2, 3\n- [23] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2289822909, 2023. 2, 3\n- [24] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-70",
      "content": "2023. 2, 3\n- [24] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The' something something' video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision , pages 5842-5850, 2017. 2\n- [25] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18995-19012, 2022. 6, 30\n- [26] Tengda Han, Max Bain, Arsha Nagrani, G¨ ul Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1893018940, 2023.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-71",
      "content": "rani, G¨ ul Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1893018940, 2023. 2, 6, 7\n- [27] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 1725-1732, 2014. 2\n- [28] Hildegard Kuehne, Hueihan Jhuang, Est´ ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision , pages 2556-2563. IEEE, 2011. 3\n- [29] Anna Kukleva, Makarand Tapaswi, and Ivan Laptev. Learning interactions and relationships between movie characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9849-9858, 2020.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-72",
      "content": "apaswi, and Ivan Laptev. Learning interactions and relationships between movie characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9849-9858, 2020. 2\n- [30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7331-7341, June 2021. 2, 3\n- [31] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. In EMNLP , 2018. 2, 3\n- [32] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. In ECCV , 2020. 3\n- [33] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-73",
      "content": "al. In ECCV , 2020. 3\n- [33] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 , 2023. 3\n- [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 3\n- [35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 3\n- [36] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200 , 2020. 2, 3\n- [37] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-74",
      "content": "encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200 , 2020. 2, 3\n- [37] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying videolanguage understanding as masked language modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 23119-23129, 2023. 2, 3\n- [38] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: A multi-task benchmark for video-and-language understanding evaluation. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks , 2021. 2, 3\n- [39] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-75",
      "content": ", 2021. 2, 3\n- [39] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1794917958, 2022. 2, 3\n- [40] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. Advances in Neural Information Processing Systems , 35:7575-7586, 2022. 2, 3\n- [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3\n- [42] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 3202-3211, 2022.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-76",
      "content": "a Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 3202-3211, 2022. 2, 3\n- [43] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207 , 2023. 3\n- [44] MuhammadMaaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video\n\nunderstanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023. 3\n\n- [45] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023. 6\n- [46] EVERINGHAM Mark. Hello! my name is... buffy'automatic naming of characters in tv video.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-77",
      "content": "g, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023. 6\n- [46] EVERINGHAM Mark. Hello! my name is... buffy'automatic naming of characters in tv video. In Proceedings of British Machine Vision Conference, 2006 , 2006. 2\n- [47] Microsoft. Bingchat. https://www.microsoft.com/ en-us/edge/features/bing-chat , 2023. 2\n- [48] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV , 2019. 2\n- [49] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612 , 2017. 2\n- [50] Arsha Nagrani and Andrew Zisserman. From benedict cumberbatch to sherlock holmes: Character identification in tv series without a script. arXiv preprint arXiv:1801.10442 , 2018. 2\n- [51] OpenAI. Gpt-4 technical report. 2023. 2, 3, 10\n- [52] OpenAI.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-78",
      "content": "t cumberbatch to sherlock holmes: Character identification in tv series without a script. arXiv preprint arXiv:1801.10442 , 2018. 2\n- [51] OpenAI. Gpt-4 technical report. 2023. 2, 3, 10\n- [52] OpenAI. Gpt-4v(ision) system card. 2023. 2, 3, 10\n- [53] OpenAI. Gpt-4v(ision) technical work and authors. https: //cdn.openai.com/contributions/gpt-4v. pdf , 2023. 10\n- [54] Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442 , 2023. 2\n- [55] Karine Pires and Gwendal Simon. Youtube live and twitch: a tour of user-generated live streaming systems. In Proceedings of the 6th ACM multimedia systems conference , pages 225-230, 2015. 2\n- [56] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-79",
      "content": "stems conference , pages 225-230, 2015. 2\n- [56] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning , pages 2849228518. PMLR, 2023. 3\n- [57] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3202-3212, 2015. 2, 6, 7\n- [58] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. 6\n- [59] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unified model for image, video, audio and language tasks. arXiv preprint arXiv:2307.16184 , 2023. 2\n- [60] Cees GM Snoek and Marcel Worring. Multimodal video indexing: A review of the state-of-the-art.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-80",
      "content": "nified model for image, video, audio and language tasks. arXiv preprint arXiv:2307.16184 , 2023. 2\n- [60] Cees GM Snoek and Marcel Worring. Multimodal video indexing: A review of the state-of-the-art. Multimedia tools and applications , 25:5-35, 2005. 2\n- [61] David Snyder, Daniel Garcia-Romero, Gregory Sell, Alan McCree, Daniel Povey, and Sanjeev Khudanpur. Speaker recognition for multi-speaker conversations using x-vectors. In ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP) , pages 5796-5800. IEEE, 2019. 2\n- [62] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449 , 2023. 2, 3\n- [63] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 80,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-81",
      "content": "arXiv:2307.16449 , 2023. 2, 3\n- [63] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012. 3\n- [64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 3\n- [65] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407 , 2023. 3\n- [66] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et al. All in one: Exploring unified video-language pre-training.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 81,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-82",
      "content": "[66] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et al. All in one: Exploring unified video-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6598-6608, 2023. 2, 3\n- [67] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023. 2\n- [68] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, highquality multilingual dataset for video-and-language research. In The IEEE International Conference on Computer Vision (ICCV) , October 2019. 2, 3\n- [69] Yunbo Wang, Mingsheng Long, Jianmin Wang, and Philip S Yu. Spatiotemporal pyramid network for video action recognition.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 82,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-83",
      "content": "E International Conference on Computer Vision (ICCV) , October 2019. 2, 3\n- [69] Yunbo Wang, Mingsheng Long, Jianmin Wang, and Philip S Yu. Spatiotemporal pyramid network for video action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 1529-1538, 2017. 2, 3\n- [70] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1884-1894, 2021. 2\n- [71] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 , 2022. 3\n- [72] Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, and Michael Zeng. Visual clues: Bridging vision and language foundations for image paragraph captioning. Advances in Neural Information Processing Systems , 35:17287-17300, 2022. 3",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 83,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-84",
      "content": "guyen Bach, Ce Liu, and Michael Zeng. Visual clues: Bridging vision and language foundations for image paragraph captioning. Advances in Neural Information Processing Systems , 35:17287-17300, 2022. 3\n\n- [73] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR , 2016. 3\n- [74] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023. 2\n- [75] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658 , 2023. 2\n- [76] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 84,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-85",
      "content": "2\n- [76] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023. 10\n- [77] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao, Yi-Ling Chen, et al. i-code: An integrative and composable multimodal learning framework. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 10880-10890, 2023. 2\n- [78] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). arXiv preprint arXiv:2309.17421 , 2023. 2, 3, 4\n- [79] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 85,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-86",
      "content": ", Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 3, 4\n- [80] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541 , 2023. 6\n- [81] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 , 2021. 2\n- [82] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems , 34:23634-23651, 2021.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 86,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-87",
      "content": "Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems , 34:23634-23651, 2021. 2\n- [83] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485 , 2023. 2\n- [84] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 , 2023. 3\n- [85] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv. org/abs/2205.01068 . 3\n\nt\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 87,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-88",
      "content": "Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv. org/abs/2205.01068 . 3\n\nt\n\n<!-- image -->\n\nFigure 7. An example of MM-VID's execution flow (continued). We present the full script generated by MM-VID. The original video is available at https://www.youtube.com/watch?v=-pNe0p4H8ec\n\nt",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "References",
        "chunkIndex": 88,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-89",
      "content": "Dodgers' catcher Will Smith lifts a high ball into center field which brings Mookie Betts running home, netting the Dodgers another point.\n\n0:06:23 - 0:06:38: The Rockies' show some fight as Chris Brian leads the ninth innings with a double bringing the Rockies trailing the Dodgers by five.\n\n0:06:39 - 0:06:54: A series of unfortunate events for the Rockies as batters Montero and Diaz get directly hit by pitched balls consecutively. Trejo from the Rockies finally manages to hit a fly ball into left field, allowing Montero to make it to the home plate, closing the gap between the two teams to three.\n\n0:06:55 - 0:07:10: The game comes to a close as Charlie Blackman grounds to the first base, ending the game in favor of Dodgers, after a thrilling and eventful match.\n\n0:07:11 - 0:09:09: The scene concludes with the teams leaving the field.\n\n<!-- image -->\n\nCan you summarize the game?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 89,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-90",
      "content": "base, ending the game in favor of Dodgers, after a thrilling and eventful match.\n\n0:07:11 - 0:09:09: The scene concludes with the teams leaving the field.\n\n<!-- image -->\n\nCan you summarize the game?\n\nThe game was a thrilling and eventful match between the Colorado Rockies and the Los Angeles Dodgers . The Dodgers dominated the game with superior pitching and powerful hits , while the Rockies showed some fight towards the end but ultimately fell short.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Show me the most exciting moment in this\n\nvideo\n\nThe most exciting moment in this video is when Will Smith hits a home run in the first inning at timestamp 0:01:19 - 0:01:34 in this video\n\n<!-- image -->\n\n<!-- image -->\n\n- Who won the game, and what are their final\n\nscores?\n\nFigure 8. An example of MM-VID's execution flow (continued). Given the generated script as reference, MM-VID achieves various video understanding capabilities, including grounded QA and multimodal reasoning .",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 90,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-91",
      "content": ". An example of MM-VID's execution flow (continued). Given the generated script as reference, MM-VID achieves various video understanding capabilities, including grounded QA and multimodal reasoning . The original video is available at https://www.\n\nyoutube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\nt\n\n<!-- image -->\n\n<!-- image -->\n\n- Who are the best pitchers in this video?\n\nJulio Urias was the best pitcher in this video, striking out multiple batters and inducing a double play.\n\n<!-- image -->\n\nEvidence in the Generated Script: 0:02:07 - 0:02:22: Urias faces a setback as McMahon bounces back with a high-quality deep shot to the center field, netting himself a double. 0:03:59 - 0:04:14: Julio Urias , from the Dodgers, efficiently strikes out the Venezuelans' Luis Montero and Ryan McMahon in quick succession. 0:04:47 - 0:05:02: Despite a promising start for the Rockies at the top of the third, their momentum slows down as they lose a run due to a double play induced by Urias.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 91,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-92",
      "content": "n McMahon in quick succession. 0:04:47 - 0:05:02: Despite a promising start for the Rockies at the top of the third, their momentum slows down as they lose a run due to a double play induced by Urias.\n\n0:05:35 - 0:05:50: Urias's superior pitching continues as he strikes out Ryan McMahon and skillfully fields a sharp grounder ending the fourth inning , serving a huge blow to the Rockies.\n\n<!-- image -->\n\nWhen did Julio Urias have his first strikeout?\n\nJulio Urias had his first strikeout in the video at 0:00:31 , striking out Chris Bryant from the Rockies.\n\n<!-- image -->\n\nFigure 9. An example of MM-VID's execution flow (continued). Given the generated script as reference, MM-VID achieves various video understanding capabilities, including grounded QA and multimodal reasoning . The original video is available at https://www. youtube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\n<!-- image -->\n\n- When did Julio Urias induce a double play?\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 92,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-93",
      "content": "easoning . The original video is available at https://www. youtube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\n<!-- image -->\n\n- When did Julio Urias induce a double play?\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 10. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension . Figures 11-13 show continued outputs. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\n<!-- image -->\n\nScene 31: 00:06:46\n\nAn elephant and a gorilla roam the jungle.\n\nScene 32: 00:06:57\n\nThe gorilla relocates deeper into the jungle.\n\nScene 33: 00:07:05\n\nThe gorilla is seen wading in a small body of water.\n\nScene 34: 00:07:13\n\nTwo gorillas spotted playing in the grass.\n\nScene 35: 00:07:27\n\nAdmiring the grass, the gorilla takes a deserved break.\n\nScene 36: 00:07:32\n\nAerial view of gorillas and elephants in the marsh.\n\nScene 37: 00:07:37\n\nClose-up of the elephant in the water.\n\nScene 38: 00:07:43",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 93,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-94",
      "content": "the grass, the gorilla takes a deserved break.\n\nScene 36: 00:07:32\n\nAerial view of gorillas and elephants in the marsh.\n\nScene 37: 00:07:37\n\nClose-up of the elephant in the water.\n\nScene 38: 00:07:43\n\nTwo elephants are seen fighting and splashing in the water.\n\nScene 39: 00:08:00\n\nA lone elephant is caught grazing in the landscape.\n\nScene 40: 00:08:08\n\nScene transitions to a group of gorillas in their natural habitat.\n\nScene 41: 00:08:16\n\nA herd of elephants is spotted in shallow water.\n\nScene 42: 00:08:22\n\nAerial view of a vast landscape with a mountain peak and flying birds.\n\nScene 43: 00:08:28\n\nAerial view of a wide, green landscape with scattered water bodies and flying birds.\n\nScene 44: 00:08:34\n\nAerial view of a mountainous range with a valley and a river.\n\nScene 45: 00:08:41\n\nClose-up of gorilla hanging from a tree branch.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 94,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-95",
      "content": "pe with scattered water bodies and flying birds.\n\nScene 44: 00:08:34\n\nAerial view of a mountainous range with a valley and a river.\n\nScene 45: 00:08:41\n\nClose-up of gorilla hanging from a tree branch.\n\nFigure 11. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; We present the full script generated by MM-VID. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\nScene 46: 00:09:03\n\nClose-up of a bug on a leaf.\n\nScene 47: 00:09:43\n\n<!-- image -->\n\nLandscape view of mountains with clouds hovering overhead.\n\nScene 48: 00:10:32\n\nClose-up of a bird as it takes flight.\n\nScene 49: 00:10:50\n\nClose-up of a gorilla's face hidden behind the leaves.\n\nScene 50: 00:11:23\n\nA gorilla is seen searching the dense forest floor.\n\nScene 51: 00:12:15\n\nClose-up of a black bird with large eyes.\n\nScene 52: 00:13:03\n\nA black bird is seen walking in a jungle path.\n\nScene 53: 00:13:29",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 95,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-96",
      "content": "gorilla is seen searching the dense forest floor.\n\nScene 51: 00:12:15\n\nClose-up of a black bird with large eyes.\n\nScene 52: 00:13:03\n\nA black bird is seen walking in a jungle path.\n\nScene 53: 00:13:29\n\nBird of prey perched on tree branches in the jungle.\n\nScene 54: 00:15:05\n\nClose-up of a bird's eye in the forest.\n\nScene 55: 00:16:38\n\nA shot of a gorilla hiding behind a tree in a dense jungle.\n\nScene 56: 00:17:50 Two elephants engage in a playful fight near a water body.\n\nScene 57: 00:19:20\n\nThe leopard is seen slowly moving through the foliage.\n\nScene 58: 00:21:51 Bird seen entering a hole in a tree.\n\nScene 59: 00:22:22\n\nA red ant crawls on a fern.\n\nScene 60: 00:23:32\n\nA peculiar mushroom blooms in the darkness near the forest floor.\n\nScene 61: 00:25:07\n\nA deer seen foraging in the foliage.\n\nCan you summarize the video?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 96,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-97",
      "content": "red ant crawls on a fern.\n\nScene 60: 00:23:32\n\nA peculiar mushroom blooms in the darkness near the forest floor.\n\nScene 61: 00:25:07\n\nA deer seen foraging in the foliage.\n\nCan you summarize the video?\n\nWhat is Cordyceps fungus? Figure 12. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; MM-VID is able to summarize the video and address specific questions with regard to the video contents. Blue highlights the correct answers. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\nThe video is a documentary that explores the diverse and rich ecosystems of jungles and\n\nrainforests around the world. It showcases various animal species, their behaviors, and their roles in\n\nmaintaining the balance of these ecosystems. The documentary also highlights the threats faced by\n\nthese habitats, such as deforestation and\n\nCordyceps fungus is a parasitic fungus that infects and takes control of its host, usually an",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 97,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-98",
      "content": "osystems. The documentary also highlights the threats faced by\n\nthese habitats, such as deforestation and\n\nCordyceps fungus is a parasitic fungus that infects and takes control of its host, usually an\n\ninsect. The fungus manipulates the host's behavior, eventually leading to the host's death.\n\nThe fungus then erupts from the host's body and releases spores to infect more victims. Cordyceps\n\nfungus plays a role in maintaining the balance of\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 13. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; MM-VID is able to localize the specific moments or topics in the lengthy documentary video. Blue highlights the correct answers. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 98,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-99",
      "content": "deo. Blue highlights the correct answers. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPitcher plants can be found in the video around the timestamp 00:18:18.\n\n<!-- image -->\n\nt\n\nDescribe Frank's journey in the two videos. Frank's journey in these two episodes involves Figure 14. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character's journey across multiple videos. Figure 15 shows continued outputs. Blue highlights the correct answers. The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n\n<!-- image -->\n\nhim preparing for his new role as Vice\n\nPresident while trying to maintain control over various situations.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 99,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-100",
      "content": "The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n\n<!-- image -->\n\nhim preparing for his new role as Vice\n\nPresident while trying to maintain control over various situations.\n\nHe deals with the consequences of his past actions, such as\n\n<!-- image -->\n\nDescribe Frank's journey in the two videos.\n\nFrank's journey in these two episodes involves him preparing for his new role as Vice President while trying to maintain control over various situations. He deals with the consequences of his past actions, such as managing Doug Stamper's attempts to erase their tracks and handling the fallout from Zoe's death. Frank also navigates the political landscape, working with Jackie to secure her position as Whip and engaging in tense negotiations with China. Throughout these challenges, Frank continues to demonstrate his cunning and ruthlessness in achieving his goals.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 100,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-101",
      "content": "th Jackie to secure her position as Whip and engaging in tense negotiations with China. Throughout these challenges, Frank continues to demonstrate his cunning and ruthlessness in achieving his goals.\n\nFigure 15. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character's journey across multiple videos. Blue highlights the correct answers. The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n\n<!-- image -->\n\nt\n\n<!-- image -->\n\nDid Pikachu use 10000000 Volts move?\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Did Pikachu use Gigantamax?\n\n<!-- image -->\n\n<!-- image -->\n\nShow me the keyframe when Pikachu defeats Charizard\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 101,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-102",
      "content": "10000000 Volts move?\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Did Pikachu use Gigantamax?\n\n<!-- image -->\n\n<!-- image -->\n\nShow me the keyframe when Pikachu defeats Charizard\n\n<!-- image -->\n\nFigure 16. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, and localize specific moments across multiple videos. Blue highlights the correct answers. The two videos are from Pok´ emon Journeys: episode 131 and episode 132.\n\nt\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation Using GPT-4 (continued):",
        "chunkIndex": 102,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-103",
      "content": "Video length is  2 minutes 43 seconds.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Loading video …",
        "chunkIndex": 103,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-104",
      "content": "0:00:00, 0:00:01, 0:00:02, 0:00:03, 0:00:04, 0:00:05, 0:00:06, 0:00:07, 0:00:08, 0:00:09, ..., 0:02:39, 0:02:40, 0:02:41, 0:02:42",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Scene Detection:",
        "chunkIndex": 104,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-105",
      "content": "Title: Mummies\n\nAbstract: Thut is a young charioteer. Nefer is the daughter of Faraon. In this video, Thut saves himself from falling off a cliff. Thut and Nefer meets Faraon. Thut and Nefer gets married in front of the goddess statue and all people. Thut and Nefer celebrates by singing and dancing.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Metadata Collection:",
        "chunkIndex": 105,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-106",
      "content": "<!-- image -->\n\nFaraon\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Visual Prompting:",
        "chunkIndex": 106,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-107",
      "content": "0:00:00 - 0:00:06: Nefer , an Egyptian princess, is seen gazing at the desert landscape and examining a rocky cliff.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Script Generation:",
        "chunkIndex": 107,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-108",
      "content": "0:00:06 - 0:00:08: Nefer looks down the cliff's edge, revealing her concerned reaction.\n\n0:00:12 - 0:00:14: A shot of a rock with a golden ankh necklace sets an ominous tone.\n\n0:00:15 - 0:00:16: Transition to a worried Nefer under the open sky.\n\n0:00:17 - 0:00:18: Faraon overlooks Thut lying on the rocky ground.\n\n0:00:18 - 0:00:24: Various interactions between Thut , Nefer , and Faraon in the desert transpire. 0:00:24 - 0:00:30: Nefer is hugged by Faraon as Thut watches closely.\n\n0:00:30 - 0:00:36: Thut shares a moment with Nefer before seating himself beside a frog as Faraon joins Nefer .\n\n0:00:36 - 0:00:42: Nefer and Faraon approach a slightly confused Thut .\n\n0:00:42 - 0:00:48: Thut and Nefer stand alone exchanging a few words.\n\n0:00:48 - 0:00:54: Faraon joins Thut and Nefer , looking at a ring Thut holds.\n\n0:01:00 - 0:01:06: Thut presents a ring between his and Nefer 's fingers.\n\n0:01:06 - 0:01:12: Thut listens as Nefer talks excitingly about the ring.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 108,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-109",
      "content": "ins Thut and Nefer , looking at a ring Thut holds.\n\n0:01:00 - 0:01:06: Thut presents a ring between his and Nefer 's fingers.\n\n0:01:06 - 0:01:12: Thut listens as Nefer talks excitingly about the ring.\n\n0:01:12 - 0:01:18: Faraon oversees a close moment between Thut and Nefer .\n\n0:01:18 - 0:01:24: Nefer and Thut stand in front of an audience as a small girl with a snake appears.\n\n0:01:24 - 0:01:30: Thut and Nefer perform in front of a crowd.\n\n0:01:42 - 0:01:48: Thut and Nefer share a kiss as the crowd cheers on, transitioning to the girl with her crocodile.\n\n0:01:54 - 0:02:00: Thut and Nefer perform on stage with others in a palace courtyard. 0:02:12 - 0:02:18: Multiple performances ensue Nefer in the courtyard, ending with Thut and sharing a gaze.\n\n0:02:18 - 0:02:24: Thut and Nefer continue their romantic interaction under the watchful eyes of their audience.\n\n0:02:24 - 0:02:30: The dancing in the courtyard continues.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 109,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-110",
      "content": "h Thut and sharing a gaze.\n\n0:02:18 - 0:02:24: Thut and Nefer continue their romantic interaction under the watchful eyes of their audience.\n\n0:02:24 - 0:02:30: The dancing in the courtyard continues.\n\n0:02:30 - 0:02:36: Thut , Nefer , and Faraon perform on stage as their performers enjoy their display.\n\n0:02:36 - 0:02:43: As the sun sets, Thut and Nefer conclude their musical performance on a balcony.\n\nNefer\n\n<!-- image -->\n\nFigure 17. Case studies of MM-VID 's capabilities and application scenarios: character identification ; By providing the face photos of the characters and their names, MM-VID can generate the script with the identified characters. Blue highlights the correct character identification. The original video is available at https://www.tokyvideo.com/video/mummies-online-free-film , from 1:19:47 to 1:22:30.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nThut\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nLoading video …\n\nVideo length is  2 minutes 57 seconds.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 110,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-111",
      "content": "e-film , from 1:19:47 to 1:22:30.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nThut\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nLoading video …\n\nVideo length is  2 minutes 57 seconds.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 111,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-112",
      "content": "<!-- image -->\n\n<!-- image -->\n\n0:00:01 Guest\\_0:  Good trade flesh and we run and we hide.\n\n0:00:33 Guest\\_2:  That was weird.\n\n0:00:34 Guest\\_3:  Hey boss.\n\n0:00:39 Guest\\_3:  I love the secret layer.\n\n…\n\n0:02:53 Guest\\_10:  From you. No. Fine then from.\n\n0:02:55 Guest\\_10:  Who? The guy who cut off your arms.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "ASR:",
        "chunkIndex": 112,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-113",
      "content": "Title: Nimona\n\nAbstract: Ballister boldheart is a knight. Nimona is a mischievous teen who is a shapeshifting creature.\n\nThe video shows how Ballister met Nimona first time.Nimona thought boldheart was the villain but disappointed to know he is not.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Metadata Collection:",
        "chunkIndex": 113,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-114",
      "content": "<!-- image -->\n\nNimona\n\n<!-- image -->\n\nBallister Boldheart\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Visual Prompting:",
        "chunkIndex": 114,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-115",
      "content": "0:00:01 Ballister Boldheart : Good trade flesh and we run and we hide.\n\n0:00:33 Unknown: That was weird.\n\n0:00:34 Nimona : Hey boss.\n\n0:00:39 Nimona : I love the secret layer.\n\n0:00:42 Nimona : The garbage and the smell of sadness really pull the whole thing together.\n\n0:00:45 Ballister Boldheart : Well, wait, wait, wait. Who? Who are you?\n\n0:00:47 Nimona : The names. Pneumonia.\n\nWhoa. Yeah. Sick arm.\n\n0:00:48 Ballister Boldheart : And how did you?\n\n0:00:51 Unknown: Did it bleed a lot? Did they let you keep the old one?\n\n0:00:54 Ballister Boldheart : No, let's go. That's wrong with you.\n\n0:00:58 Nimona : Can I have it? What? Put that?\n\n0:00:59 Ballister Boldheart : Down that is.\n\n0:01:00 Ballister Boldheart : Not for little girls, little.\n\n0:01:02\n\n: Girls. OK, how old do you think I?\n\n0:01:05 Ballister Boldheart : AM 10.\n\n0:01:09 Unknown: Alright, home.\n\n0:01:09 Ballister Boldheart : We are more or less than 10.\n\n0:01:10 Nimona : Not a lot of kids in your life, huh?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 115,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-116",
      "content": "o you think I?\n\n0:01:05 Ballister Boldheart : AM 10.\n\n0:01:09 Unknown: Alright, home.\n\n0:01:09 Ballister Boldheart : We are more or less than 10.\n\n0:01:10 Nimona : Not a lot of kids in your life, huh?\n\n0:01:12 Ballister Boldheart : You know what?\n\n0:01:13 Ballister Boldheart : No. And then like if you stay that?\n\n0:01:14 Ballister Boldheart : Way you have to go job.\n\n0:01:15 Nimona : But I'm here about this.\n\n0:01:16 Unknown: What job?\n\n0:01:18 Nimona : Oh, it's all here.\n\n0:01:18 Nimona : It's all here in my application.\n\n0:01:22 Nimona : This is just a bunch of drawings.\n\n0:01:26 Unknown: Very disturbing drawings.\n\n0:01:28 Ballister Boldheart : Oh, look, it's me on a rhinoceros skewering.\n\n0:01:32 Ballister Boldheart : Guards like a human kebab.\n\n0:01:34 Nimona : Yeah. Do you like?\n\n0:01:35 Nimona : It I thought a visual aid.\n\n0:01:37 Unknown: Really embarrass me. Puff up here. So about the job. What job?\n\n0:01:45 Nimona : To be your.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 116,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-117",
      "content": "kebab.\n\n0:01:34 Nimona : Yeah. Do you like?\n\n0:01:35 Nimona : It I thought a visual aid.\n\n0:01:37 Unknown: Really embarrass me. Puff up here. So about the job. What job?\n\n0:01:45 Nimona : To be your.\n\nFigure 18. Case studies of MM-VID 's capabilities and application scenarios: speaker identification ; By leveraging visual prompting, MM-VID can enhance ASR predictions with the speakers' identity. Blue and Red highlight the correct and incorrect predictions, respectively. Figure 19 shows continued outputs. The original video is available at https://www.netflix.com/title/81444554 , from 9:52 to 12:52.\n\nNimona\n\n<!-- image -->\n\nc\n\nt\n\n<!-- image -->\n\n```\n(continued): 0:01:45 Nimona : Sidekick, you know. 0:01:46 Nimona : To help you do whatever it takes to. 0:01:47 Nimona : Get revenge on. 0:01:48 Nimona : The cold, cruel world that rejected you. Shall we pillage village? Lay low until they don't remember you, and then we rise like a phoenix. From the Ashes, overthrow the government.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 117,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-118",
      "content": "n. 0:01:48 Nimona : The cold, cruel world that rejected you. Shall we pillage village? Lay low until they don't remember you, and then we rise like a phoenix. From the Ashes, overthrow the government. 0:01:58 Unknown: Or we could. 0:01:59 Nimona : Just talk. 0:02:02 Unknown: The the the point. 0:02:03 Nimona : Is whatever your dark heart desires. Boss, your sidekick has arrived. 0:02:08 Ballister Boldheart : I don't need a. 0:02:09 Ballister Boldheart : Side wait, I am not a. 0:02:09 Nimona : Every villain needs a sidekick. 0:02:12 Unknown: Sweetener. Murdo. That's how you want to kill first. Oh. 0:02:14 Nimona : Are these your next targets? 0:02:15 Nimona : Who do? 0:02:16 Ballister Boldheart : I'm not killing anybody. 0:02:18 Nimona : This one, this guy looks extremely punchable. 0:02:19 Nimona : Right. He is actually extremely. 0:02:21 Ballister Boldheart : You're right, he is actually. 0:02:22 Ballister Boldheart : Punchable. Wait, that is not a murder wall. It's a.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 118,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-119",
      "content": "nchable. 0:02:19 Nimona : Right. He is actually extremely. 0:02:21 Ballister Boldheart : You're right, he is actually. 0:02:22 Ballister Boldheart : Punchable. Wait, that is not a murder wall. It's a. 0:02:26 Ballister Boldheart : The innocence wall, yeah. 0:02:27 Nimona : Citizens will. 0:02:30 Nimona : We are you saying that you're not a villain? Yes. 0:02:36 Nimona : Yes you are. 0:02:37 Ballister Boldheart : No, no, no. I'm not a villain, no. 0:02:42 Ballister Boldheart : But you. 0:02:43 Ballister Boldheart : Disappointed that I'm not a murderer. 0:02:46 Nimona : I mean. 0:02:46 Nimona : I mean a little. Why? 0:02:48 Ballister Boldheart : You know what we. 0:02:49 Ballister Boldheart : Don't know. 0:02:50 Ballister Boldheart : The real villain is still out there and I do need help not. 0:02:53 Nimona : From you. No. Fine then from. 0:02:55 Nimona : Who? The guy who cut off your arms.\n```",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 119,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-120",
      "content": "know. 0:02:50 Ballister Boldheart : The real villain is still out there and I do need help not. 0:02:53 Nimona : From you. No. Fine then from. 0:02:55 Nimona : Who? The guy who cut off your arms.\n```\n\nFigure 19. Case studies of MM-VID 's capabilities and application scenarios: speaker identification ; By leveraging visual prompting, MM-VID can enhance ASR predictions with the speakers' identity. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.netflix.com/title/81444554 , from 9:52 to 12:52.\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Regenerated ASR with Identified Speakers:",
        "chunkIndex": 120,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-121",
      "content": "Video length is  1 minutes 44 seconds.\n\n<!-- image -->\n\n0:00:00 - 0:00:08 : Mr. Bean enters a classroom and takes a seat next to another man. They both anxiously look at their exam papers as they begin their test.\n\n0:00:09 - 0:00:16 : Mr. Bean glances over at the other man's paper and tries to sneakily copy his answers. Both men continue to work on their tests, unaware of their surroundings.\n\n0:00:17 - 0:00:23 : Close-up shot of Mr. Bean's face showing his frustration as he struggles to figure out the answers to the exam.\n\n0:00:24 - 0:00:27 : Mr. Bean continues to take the exam while holding a pink flamingo pen, struggling to concentrate.\n\n0:00:27 - 0:00:35 : Mr. Bean becomes more desperate, sticking his nose into the exam paper as he continues to copy answers from the man sitting next to him.\n\n0:00:36 - 0:00:45 : Mr. Bean takes a break, rubbing his eyes as the scene cuts back and forth between him and his colleague.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Loading video …",
        "chunkIndex": 121,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-122",
      "content": "am paper as he continues to copy answers from the man sitting next to him.\n\n0:00:36 - 0:00:45 : Mr. Bean takes a break, rubbing his eyes as the scene cuts back and forth between him and his colleague.\n\n0:00:45 - 0:00:54 : The students continue to work on their task as the scene shows the peaceful and quiet environment of the exam room.\n\n0:00:54 - 0:01:00 : Mr. Bean is deep in thought as he taps his pen on the desk, looking more worried and nervous about his exam performance.\n\n0:01:01 - 0:01:02 : Close-up of Mr. Bean struggling with a problem and looking upward with a frustrated expression.\n\nFigure 20. Case studies of MM-VID 's capabilities and application scenarios: audio description generation ; Without ASR inputs, MMVID processes the video and generates a detailed script. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.youtube.com/watch?v=29ZmXkICT1g , from 2:50 to 4:34.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Loading video …",
        "chunkIndex": 122,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-123",
      "content": "ghlight the correct and incorrect predictions, respectively. The original video is available at https://www.youtube.com/watch?v=29ZmXkICT1g , from 2:50 to 4:34.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Loading video …",
        "chunkIndex": 123,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-124",
      "content": "0:01:03 - 0:01:09 : Mr. Bean tries to regain focus in this scene, tapping his fingers as he nervously attempts to recall information for the exam.\n\n<!-- image -->\n\n0:01:11 - 0:01:12 : Two men, including Mr. Bean, are discussing a portion of the exam in a library, pointing at a book while brainstorming answers.\n\n0:01:12 - 0:01:21 : Mr. Bean and the man next to him become more desperate, whispering to each other and trying to cheat as their frustration grows. The mood is tense, mirroring their anxiety.\n\n<!-- image -->\n\nWhy Mr. Bean cheat in the exam?\n\nMr. Bean cheats in the exam because he is unprepared, lacks confidence in his ability to answer the questions, and feels desperate to pass the test . His comedic nature and inability to cope with the pressure of the exam lead him to attempt cheating as a last resort.\n\n<!-- image -->\n\nWhy do people find the video funny?",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 124,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-125",
      "content": "desperate to pass the test . His comedic nature and inability to cope with the pressure of the exam lead him to attempt cheating as a last resort.\n\n<!-- image -->\n\nWhy do people find the video funny?\n\nPeople might find the video funny because of Mr. Bean's exaggerated expressions and actions, such as sticking his nose into the exam paper and using a pink flamingo pen, which add humor to the otherwise tense and stressful situation of taking an exam.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n…\n\n<!-- image -->\n\nrohammad examining a small rohammad examining a small\n\nFigure 21. Case studies of MM-VID 's capabilities and application scenarios: self-refinement ; In this example, MM-VID mistakenly identifies a bird as a rock due to low-resolution video frames. The self-refinement helps rectify the errors, resulting in a more refined output. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 125,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-126",
      "content": "elf-refinement helps rectify the errors, resulting in a more refined output. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www. youtube.com/watch?v=o4UKvgBkO2Y , from 1:06 to 4:17.\n\n……\n\n<!-- image -->\n\nbapl biotects the biotects the\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n…\n\n…\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 22. Case studies of MM-VID 's capabilities and application scenarios: fast-changing short videos. The original videos are available at https://www.instagram.com/mealtimewithmummy/reels/\n\n<!-- image -->\n\n…\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 23. Case studies of MM-VID 's capabilities and application scenarios: embodied agent . Blue highlights the correct prediction. The original video is collected from Ego4D dataset [25].\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 126,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-127",
      "content": "23. Case studies of MM-VID 's capabilities and application scenarios: embodied agent . Blue highlights the correct prediction. The original video is collected from Ego4D dataset [25].\n\n<!-- image -->\n\nFigure 24. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figures 25-27 show continued outputs. The video is generated by Pygame library [4].\n\n<!-- image -->\n\nFigure 25. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figures 26-27 show continued outputs. The video is generated by Pygame library [4].\n\n<!-- image -->\n\n<!-- image -->\n\nYou are expert in playing Super Mario Bros. What is the next controls to be pressed after the last frame. Output JSON.\n\nAvailable controls are: Jump, ArrowLeft, ArrowRight, Wait\n\n```\nOutput: { \"controls\": \"ArrowLeft+Jump\", \"reason\": \"Mario should\" }\n```",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 127,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-128",
      "content": "he next controls to be pressed after the last frame. Output JSON.\n\nAvailable controls are: Jump, ArrowLeft, ArrowRight, Wait\n\n```\nOutput: { \"controls\": \"ArrowLeft+Jump\", \"reason\": \"Mario should\" }\n```\n\nFor this sequence of frames of super mario, what is the next controls.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 128,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-129",
      "content": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nYou are expert in playing Super Mario Bros.\n\nWhat is the next controls to be pressed after the last frame. Output JSON.\n\nAvailable controls are: Jump, ArrowLeft, ArrowRight, Wait\n\n```\nOutput: { \"controls\": \"ArrowLeft+Jump\", \"reason\": \"Mario should\" }\n```\n\nFigure 26. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figure 27 shows continued outputs. The video is generated by Pygame library [4].\n\n{\n\n\"co\n\n\"re\n\n}\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 27. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. The video is generated by Pygame library [4].\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 129,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-130",
      "content": "ies and application scenarios: playing video game. Blue highlights the correct prediction. The video is generated by Pygame library [4].\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 28. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 29-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 130,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-131",
      "content": "ur finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \"My next move would be to move my finger to the Amazon icon, which is located in the third row and third column of the screen. I would then click on the icon to open the app and search for an ergonomic keyboard within my budget of $50. \" You have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? moving your finger to an icon, describe the\n\nIf the action involves location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy next move would be to move my finger to the search bar , which is located at the top of the screen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard .",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 131,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-132",
      "content": "e -->\n\nMy next move would be to move my finger to the search bar , which is located at the top of the screen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard . After typing in the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate the search.\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right corner of the screen, next to the \"Windows\" button.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 132,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-133",
      "content": "tween $50 and $100.\n\nPrevious action : \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open the filter options and narrow down my search results to only show ergonomic keyboards within my budget range of $50 to $100. \" You have now completed the previous action as described above. Below is what you see on the phone screen. Please predict your immediate next move. If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy immediate next move would be to move my finger to the \"Price &amp; Deals\" filter option , which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &amp; Deals\" filter option to open the price range\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 133,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-134",
      "content": "er option , which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &amp; Deals\" filter option to open the price range\n\n<!-- image -->\n\nFigure 29. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 30-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "Output:",
        "chunkIndex": 134,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-135",
      "content": "filter and set my minimum price to $50 and my maximum price to $100. This would further narrow down my search results to only show ergonomic keyboards within my budget range.\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \" My immediate next move would be to move my finger to the \"Price &amp; Deals\" filter option, which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &amp; Deals\" filter option to open the price range filter and set my minimum price to $50 and my maximum price to $100. This would further narrow down my search results to only show ergonomic keyboards within my budget range.",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 135,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-136",
      "content": "n to open the price range filter and set my minimum price to $50 and my maximum price to $100. This would further narrow down my search results to only show ergonomic keyboards within my budget range. \" You have now completed the previous action as described above. Below is what you see on the phone screen. What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy next move would be to move my finger to the \"Show 1,987 results\" button , which is located at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range.\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 136,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-137",
      "content": "s\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range.\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \"",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 137,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-138",
      "content": "creen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen. What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\nFigure 30. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 31-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 31. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figure 32 shows continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 138,
        "totalChunks": 140
      }
    },
    {
      "id": "2310.19773v1-chunk-139",
      "content": "nd application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figure 32 shows continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 32. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.19773v1",
        "title": "MM-VID: Advancing Video Understanding with GPT-4V(ision)",
        "authors": [
          "Kevin Lin",
          "Faisal Ahmed",
          "Linjie Li",
          "Chung-Ching Lin",
          "Ehsan Azarnasab",
          "Zhengyuan Yang",
          "Jianfeng Wang",
          "Lin Liang",
          "Zicheng Liu",
          "Yumao Lu",
          "Ce Liu",
          "Lijuan Wang"
        ],
        "section": "(continued)",
        "chunkIndex": 139,
        "totalChunks": 140
      }
    }
  ],
  "fullText": "<!-- image -->\n\n## MM-VID : Advancing Video Understanding with GPT-4V(ision)\n\n## Kevin Lin ∗ , Faisal Ahmed ∗ , Linjie Li ∗ , Chung-Ching Lin ∗ , Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang ∗♠ Microsoft Azure AI\n\n∗ Core Contribution ♠ Project Lead https://multimodal-vid.github.io/\n\n<!-- image -->\n\nFigure 1. MM-VID allocates specialized vision, audio, speech experts with GPT-4V(ision) to address challenging video understanding tasks. For example, the system could associate information from multiple uploaded episodes and reason the storyline of the queried characters ('Multi-Video Episodic Analysis'). We highlight key information here and postpone full MM-VID responses to Figures 4-32. Demo videos are available at this link.\n\n## Abstract\n\nWe present MM-VID , an integrated system that harnesses the capabilities of GPT-4V 1 , combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.\n\n## 1. Introduction\n\nPeople around the world create numerous videos on a daily basis [14, 27, 48, 55], including user-generated live streams, video-game live streams, short clips, movies, sports broadcasts, advertising, and more. Videos serve as a versatile medium for conveying information and content through various modalities [12, 59, 60, 74, 77, 81, 82], such as text, visuals, and audio. Developing methods that can learn from diverse modalities will enable us to design cognitive machines with enhanced capabilities for analyzing uncurated real-world videos, extending beyond the confines of hand-curated datasets. However, this rich representation introduces many challenges for the study of video understanding, particularly when dealing with extended-duration videos [62,70].\n\nUnderstanding long videos, especially those spanning over an hour, is a complex task that demands advanced methods capable of analyzing sequences of images and audio across multiple episodes. This challenge is compounded by the need to extract information from various sources, such as distinguishing speakers [17,49,61], identifying characters [29, 46, 50], and maintaining narrative coherence [26, 57]. Additionally, answering questions based on video evidence [31] requires a deep comprehension of the content, context, and subtitles. When it comes to live streaming and gaming videos [1, 11, 55], there are challenges in processing dynamic environments in real-time, re-\n\n1 In this work, we explore GPT-4V(ision) with the vision capability and refers to the model as 'GPT-4V,' following the OpenAI reports [51, 52]. We refer to the text-only version of the model as 'GPT-4' [51].\n\nquiring semantic understanding, and the ability of long-term strategy planning [11,54,67,75,83].\n\nRecently, substantial advances have been made with large pre-trained video models [9, 13, 20, 21, 42, 69] and video-language models [10, 22, 23, 30, 36-40, 66], which have demonstrated their reasoning capabilities for video content. However, these models are usually trained on short clips ( e.g., 10-second videos in Kinetics [15] and VATEX [68]) or pre-defined action classes ( e.g., 174 classes in Something-Something v1 [24]). Consequently, these models may fall short in providing a detailed comprehension of intricate videos in real world [62, 70]. To achieve a more comprehensive understanding of the videos we encounter in daily life, we need methods capable of addressing complex challenges. It involves not only identifying who are in the scene and what they do, but also pinpointing when and how they act, while recognizing subtle nuances and visual cues across different scenes. The aim of this work is to address these challenges and explore methods that can be applied directly to real-world video understanding. Our approach involves breaking down extended video content into coherent narratives and subsequently employing these generated stories for video analysis.\n\nRecent advances in Large Multimodal Models (LMMs) [7,8,19,47,51,52,78], such as GPT-4V(ision) [52], have demonstrated significant breakthroughs in processing both input images and text for multimodal understanding. This has sparked interest in applying LMMs to the video domain. In this work, we present MM-VID, a system that integrates specialized tools with GPT-4V for video understanding. Given an input video, MM-VID performs multimodal pre-processing, including scene detection and automatic speech recognition (ASR), to collect important information in the video. The input video is then split into multiple clips according to the scene detection algorithm. Then, we employ GPT-4V, which takes the clip-level video frames as input and generates a detailed description for each video clip. Finally, GPT-4 is adopted to generate a coherent script for the full video, conditioning on the clip-level video descriptions, ASR, and video metadata if available. As shown in Figure 1, the generated script allows MM-VID to perform a diverse set of video tasks.\n\nExperimental results demonstrate the effectiveness of MM-VID in different challenging scenarios. MM-VID is able to comprehend hour-long videos through multiple modalities, and localize specific events with correct timestamps. MM-VID also demonstrates intriguing results in an interactive environment, such as predicting the possible next steps when playing a video game [4] or interacting with a graphical user interface (GUI) [78].\n\nTask List: Audio Description, Grounded QA, Summarization, Speaker Identification, Character Identification, Multimodal Reasoning, etc. Chat with Reference (Script) Please generate audio description for the input video Figure 2. Overview of MM-VID. Our system takes a video file as input, and outputs a long textual script describing the video contents. MM-VID consists of four modules: (i) Multimodal Pre-Processing , (ii) External Knowledge Collection , (iii) Clip-Level Video Description Generation , and (iv) Script Generation .\n\n<!-- image -->\n\n## 2. Related Work\n\nConventional Video Understanding Methods. Early work in computer vision centered on building video foundation models [9,13,20,21,42,69]. These models, with different neural network architecture designs and training methods, have achieved great breakthrough at analyzing short video clips [14,15,28,63], typically lasting less than 30 seconds. However, these models are typically pre-trained with vision modality only, and thus may require specific adjustment or fine-tuning for multimodal downstream tasks.\n\nVideo-Language Models. Recent studies [10, 22, 23, 30, 36-40, 66] have made remarkable improvements in multimodal representation learning for video-and-language understanding. These advancements have been particularly evident in popular downstream tasks such as video question answering [31], text-video retrieval [32, 73] and video captioning [68]. Building on this momentum, researchers typically embark on a pretrain-finetune paradigm: initially pre-training a video-language foundation model on largescale video-text pairs, followed by a fine-tuning process on specific downstream datasets. However, these methods are usually trained on short video clips, often restricted to durations of around 10 seconds, posing potential challenges in comprehending longer video sequences.\n\nVisual Instruction Tuning. Inspired by the breakthrough of Large Language Models (LLMs) [18, 19, 51, 64, 85], recent studies [35, 43, 44, 62, 84] suggest using a frozen LLM combined with an image encoder and a few learnable modules for video understanding tasks. Specifically, researchers propose the visual instruction tuning [35, 41, 44], which aims to fine-tune the learnable modules and thus enable LLMs to generate textual descriptions for the video content. While promising performance is presented, these models may fall short when it comes to handling videos with extended duration. Our work aims to fill this gap, exploring methods that can be directly applied to the understanding of long videos in real-world situations.\n\nPrompting LLMs for Video Understanding. Recently, researchers [6, 33, 65, 72] explore the LangChain system paradigm [16], which aims to integrate expert tools with existing LLMs to create new functionalities. For example, VLog [6] uses BLIP2 [34] and GRIT [71] as dense image captioners, Whisper [56] as ASR translator, and ChatGPT as a reasoner. By transcribing a given video to textual descriptions ( e.g., document), it enables ChatGPT for video question-answering tasks. Inspired by the efficacy of these tool-using approaches [16, 65, 79], we explore integration with GPT-4V for video understanding.\n\n## 3. Preliminary Study with GPT-4V(ision)\n\nRecent studies [3, 51, 52, 78] show that GPT-4V can accept a range of inputs, such as textual descriptions, questions, or even visual cues like images or short video clips. GPT-4V's inherent ability to comprehend visual inputs and generate contextually relevant text opens the door for a wide range of applications. By introducing a sequence of frames as input, GPT-4V can grasp temporal relationships and interactions, aiding in the identification and interpretation of dynamic visual content.\n\n## 4. MM-VID\n\nFigure 2 shows the overview of our system pipeline. MM-VID takes the video file as input, and outputs a script describing the video contents. The generated script enables LLMs to achieve various video understanding capabilities. MM-VID consists of four modules: (i) Multimodal PreProcessing , (ii) External Knowledge Collection , (iii) ClipLevel Video Description Generation , and (iv) Script Generation . We describe each module in detail below.\n\nMultimodal Pre-Processing. Starting with an input video file, our process begins by using the established ASR tool to extract transcriptions from the video. Following this, we divide the video into several short video clips. This process involves uniform sampling of video frames, with each clip consisting of 10 frames. To enhance the overall quality of frame sampling, we use established scene detection\n\nFigure 3. MM-VID for streaming inputs. MM-VID can serve as an agent in an interactive environment, continually receiving and processing the streaming video frames.\n\n<!-- image -->\n\ntools like PySceneDetect [5] to help identify crucial scene boundaries.\n\nExternal Knowledge Collection. We incorporate external knowledge into our input prompts to GPT-4V. This involves gathering available information, such as metadata, title, abstract, and face photos of characters within the video. In our experiments, the metadata, title, and abstract are gathered from YouTube.\n\nClip-Level Video Description Generation. During our multimodal pre-processing, the input video is segmented into multiple clips. For each clip, which typically consists of 10 frames, we employ GPT-4V to generate video descriptions. By feeding the video frames along with the associated text prompt into the model, GPT-4V utilizes the input to generate detailed descriptions that capture the visual elements, actions, and events depicted in those frames.\n\nIn addition, we explore the use of visual prompting, where the character's face photos are presented alongside the character's name in the input to GPT-4V. Our empirical results suggest that visual prompting is helpful to enhance the quality of video descriptions, particularly for more accurate character identification. These findings align with the insights from [78].\n\nScript Generation using LLM. After generating the descriptions for each video clip, we use GPT-4 to integrate these clip-level descriptions into a coherent script. This script serves as a comprehensive description of the entire video, and is used by GPT-4 for a diverse set of video understanding tasks.\n\n## 5. MM-VID for Streaming Inputs\n\nFigure 3 shows the diagram of MM-VID when applied to the context of streaming inputs. Our system operates as an agent within a dynamic environment where streaming video frames serve as the primary input. In this context, the agent continually receives streaming video frames as states, rep- resenting the ongoing visual information unfolding in the environment. These states are then processed by GPT-4V to make informed decisions and generate responses.\n\nBy continually analyzing the streaming video frames, MM-VID plays a crucial role in transforming raw visual data into meaningful insights, making it valuable for applications such as video game play, the embodied agent, and GUI navigation.\n\n## 6. Experiments\n\n## 6.1. Experiment Setup\n\nWe implement MM-VID based on MM-REACT [79] codebase. We use the Automatic Speech Recognition (ASR) tool publicly available via the Azure Cognitive Services APIs [2], and utilize PySceneDetect [5] for scene detection.\n\n## 6.2. MM-VID Capabilities\n\nAudio Description Generation,\n\nFigures 4-9 provide illustrative examples of MM-VID's complete execution flow. When a user uploads a video file, MM-VID initiates the process by first assessing the estimated video length. Subsequently, it performs multimodal pre-processing by invoking expert tools, including scene detection and ASR. Additionally, MM-VID collects external knowledge, encompassing video metadata such as title and abstract.\n\nFollowing this preliminary stage, MM-VID proceeds to generate clip-level video descriptions for each segment of the video. Finally, it invokes GPT-4, integrating these cliplevel descriptions into a coherent script. Once the script is generated, it empowers LLMs to provide a summarized understanding of the video content. That equips the system to address users' questions with grounded answers. We discuss MM-VID's distinct capabilities as below.\n\nGrounded Question-Answer (QA). The generation of a comprehensive script empowers our system with the capability of grounded QA. As shown in Figure 8, let us consider a scenario where a user poses the question, 'Show me the most exciting moment in this video.' In response, MMVID displays a highlight, specifically featuring a home run, and provides the corresponding timestamp. When a user asks 'Who are the best pitchers in this video?' MM-VID addresses the question by referring to relevant evidence in the generated script. This grounding capability owes its success to the extensive and detailed script generation process, which documents essential timestamps and significant events within the video, enabling accurate and contextually grounded responses to user inquiries.\n\nMultimodal Reasoning. MM-VID considers multimodal inputs, including video frames, speech transcriptions, and external knowledge if available. In Figure 8, when a user inquires, 'How did you know the sound is different?'\n\nk heart\n\n<!-- image -->\n\n<!-- image -->\n\n## Loading video …\n\nVideo length is  9 minutes 19 seconds.\n\n<!-- image -->\n\nInvoke tools: Scene Detection, ASR, Metadata Collection\n\n## Scene Detection:\n\n00:00:00, 00:00:02, 00:00:20, 00:00:24,\n\n..., 00:08:42, 00:08:53, 00:09:01, 00:09:07\n\n## ASR:\n\n0:05 gets ready for his second start of the 0:08 Season began this home stand got the win\n\n:\n\n:\n\n:\n\n:\n\n9:04 that's the ball game Dodgers sweep the\n\n9:07 two-game series and they go four and two 9:09 on the season opening home stand\n\n## Metadata Collection:\n\nTitle: Rockies vs. Dodgers Game Highlights Abstract: Rockies vs. Dodgers Game Highlights (4/4/23) | MLB Highlights\n\n<!-- image -->\n\n## Clip-Level Video Description Generation Using GPT-4V\n\n## Output:\n\nTimestamp: 00:00:00 - 00:00:02\n\nScene: Opening graphics introducing the game between the Colorado Rockies and the Los Angeles Dodgers.\n\nTimestamp: 00:00:20 - 00:00:24\n\nScene: Pitcher on the mound at Dodger Stadium.\n\n:\n\n:\n\n:\n\nTimestamp 00:09:01-00:09:07\n\nScene Description: In this scene, we see a baseball game between the Los Angeles Dodgers and the Colorado Rockies. The Dodgers are leading with a score of 5-2. A player from the Dodgers hits the ball, and it is fielded by a player from the Rockies.\n\n## Script Generation Using GPT-4:\n\n## Input Prompt:\n\n<!-- image -->\n\nYou are an expert at understanding audio descriptions of different scenes in a video. Can you leverage the information provided, including title, abstract, audio descriptions and generate full audio description of each scene with non overlapping time ranges. Keep as many scenes possible covering all time ranges. You may find character names in the title or abstract.  Use character names wherever possible in the audio descriptions. Keep the audio description for each time range within one short sentence.\n\n## Output:\n\n0:00:00 - 0:00:14: Video begins with a montage of the Colorado Rockies and the Los Angeles Dodgers players warming up for their game at Dodger Stadium over the backdrop of the Los Angeles skyline.\n\n0:00:15 - 0:00:30: Dodge.rs pitcher, Julio Urias, makes his second start of the season and deals a powerful pitch towards the Rocky's batter. As the batter swings and makes contact, the ball heads down the line under the glove of the Dodgers and is marked as a double for the Rockies.\n\n0:00:31 - 0:00:46: Urias successfully strikes out\n\nFigure 4. An example of MM-VID's execution flow. Given a baseball video, MM-VID provides an estimated video length, and then invokes scene detection and ASR tools, and collects external knowledge. Then, we generate clip-level video descriptions by using GPT-4V. GPT-4V takes the video frames and the text prompt as input, and outputs the video descriptions. Finally, given the clip-level descriptions, video metadata and ASR, we use GPT-4 to generate a coherent script for the input video. Figures 7-9 show continued outputs. The original video is available at https://www.youtube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\n<!-- image -->\n\nMM-VID explains that this information was derived from the commentator's remarks during the game. The examples illustrate MM-VID's multimodal reasoning capabilities, where it integrates both visual and auditory cues to provide contextually accurate responses to user queries.\n\nHour-Long Video Comprehension. Figures 10-13 demonstrate MM-VID's capabilities in processing lengthy videos. In this example, MM-VID effectively analyzes a documentary video spanning approximately 50 minutes in duration. For simplicity, the intermediate outputs are omitted in the figures, and only the final generated script is presented. We observe that MM-VID is able to generate a long script with the corresponding timestamps to represent the documentary video. By leveraging this generated script as contextual information, MM-VID is equipped to perform a range of tasks, including summarizing the lengthy video, addressing specific queries raised within the video, and indexing pivotal moments.\n\nMulti-Video Episodic Analysis. MM-VID's proficiency in handling extensive video content can be expanded to encompass multiple lengthy videos, as illustrated in Figures 14-16. In these examples, we upload multiple episodes to MM-VID, showcasing its ability to perform a variety of complex tasks. MM-VID exhibits the capability to summarize the video series, engage in cross-episode reasoning, provide detailed descriptions of character journeys across multiple episodes, and facilitate grounded QA interactions.\n\nCharacter Identification. We found that incorporating visual prompts enhances the quality of script generation, particularly with regards to character identification. In Figure 17, we illustrate this by providing MM-VID with additional inputs consisting of characters' face photos and their corresponding names. MM-VID effectively utilizes these visual prompts to identify the characters depicted in the video, based on the provided face photos. As a result, the script generation process is notably improved, ensuring more accurate and contextually relevant descriptions of characters and their interactions within the video content.\n\nSpeaker Identification. Our exploration has revealed another valuable application of visual prompting in enhancing the quality of Automatic Speech Recognition (ASR). In Figures 18-19, we highlight a scenario where conventional ASRstruggles to accurately recognize the number of speakers and their identities in the video. Visual prompting plays a pivotal role in enhancing ASR performance by providing contextual cues to identify individuals and attribute speech to specific speakers. This improvement ensures more precise transcriptions, enabling a more accurate representation of the dialogue and interactions within the video content.\n\nAudio Description Generation. Audio descriptions [26, 57] play a crucial role in making videos accessible to individuals who are blind, have low vision, or face difficulties in visually understanding the content. These descrip- tions provide contextual narration of meaningful visual elements, clarify speakers, and convey the essence of visual information within a video. In our experiments, we also explore MM-VID's performance in audio description generation. We experiment with videos where there is limited or no speech content. In Figure 20, we showcase an example featuring a short film of Mr. Bean taking an exam, which primarily lacks speech. Without ASR inputs, MM-VID processes the video and generates a detailed script. This shows MM-VID's versatility in handling various types of video content and its potential in creating inclusive and accessible multimedia content.\n\nSelf-Refinement. While the generated script offers a comprehensive understanding of video content, our experiments have unveiled occasional inaccuracies, especially in cases involving blurry or low-resolution video frames, as demonstrated in Figure 21. In this example, MM-VID mistakenly identifies a bird as a rock due to the challenges posed by the video's visual quality. To address such inconsistencies and elevate the overall accuracy of the generated script, we employ a self-refinement approach [45, 58, 80]. This involves revising the script based on both the initially generated script and a concurrently generated video summary. Through this process, MM-VID is able to rectify errors and inaccuracies, resulting in a more refined output.\n\nFast-Changing Short Videos. In Figure 22, we present an example of our experimentation with fast-changing shortform videos, such as those found on platforms like TikTok. Short videos often feature non-standard frame sizes and significantly shorter durations compared to conventional videos. Remarkably, MM-VID excels at accurately describing the cooking recipes depicted in these short videos, despite the distinct characteristics of such content.\n\nThese examples demonstrate the versatility of MM-VID in processing a diverse array of video content. Whether dealing with lengthy documentaries, episodic series, or short-form clips, MM-VID adapts seamlessly to the unique attributes of each video type, consistently delivering meaningful and contextually relevant descriptions.\n\n## 6.3. Applications to Interactive Environments\n\nIn the following section, we evaluate MM-VID when applying to the context of streaming inputs. MM-VID serves as an agent in an interactive environment, continually receiving streaming video frames as the inputs.\n\nEmbodied Agent. Figure 23 illustrates an example where MM-VID is applied to an egocentric video captured by a head-mounted camera. This video, collected from Ego4D dataset [25], provides a brief glimpse into the wearer's daily life within their home environment. Remarkably, MM-VID showcases its capability in understanding such video content and assists the user in a few practical tasks. Specifically, MM-VID helps the user locate items like the pink\n\nTable 1. Questionnaire for the group with visual impairments. Participants listen to a video and subsequently assign scores (ranging from 0 to 10) for distinct auditory criteria.\n\nEffectiveness of Delivery: If the original audio and the embedded AD are effectively presented?\n\nInformative: Is it easy to follow the storyline? Does the AD provide context and background information when necessary?\n\nAudio Quality: Is the overall audio production quality good?\n\nOverall Satisfaction: Are you satisfied with the overall AD experience?\n\njacket and the laptop within the home. Additionally, it generates a list of the user's activities within a specified time range, offering insights into the wearer's daily routine.\n\nPlaying Video Games. Figures 24-27 demonstrate the results of applying MM-VID to a Mario video game [4]. In these experiments, our agent consistently receives three video frames as states and calculates the next possible control action. Remarkably, our agent displays an understanding of the specific video game dynamics and generates reasonable action controls to play the game effectively. These examples highlight MM-VID's ability to comprehend and navigate in an interactive gaming environment. Interested readers may find the full gameplay demonstration on our project website.\n\nGUI Navigation. Figures 28-32 provide the demonstration of MM-VID's performance in the GUI navigation scenario. In this context, the agent continually receives iPhone screenshots and previous user actions as states. The agent effectively predicts the possible next steps in the user's journey, which may include clicking on the correct shopping apps, initiating searches for items of interest, and ultimately placing an order. These results demonstrate MM-VID's remarkable ability to interact with graphical user interfaces, facilitating seamless and intelligent navigation through digital interfaces.\n\n## 6.4. User Study\n\nWeexplore the potential of MM-VID for people who are blind or have low vision. Audio description (AD) [26, 57] provides an auditory narration integrated into the video's soundtrack, offering important visual details that may not be discernible from the main video soundtrack. Such descriptions play a pivotal role in conveying essential visual content to those with visual impairments.\n\nTo assess the efficacy of MM-VID in generating audio descriptions (AD), we conduct a user study. We invited 9 participants for the evaluation. 4 participants were either blind or had low vision, while the remaining 5 had normal\n\nTable 2. Questionnaire for the group with normal vision. Participants view a video and subsequently assign scores (ranging from 0 to 10) for various auditory and visual criteria.\n\nClarity: Are the visual elements clearly and accuratetly described?\n\nConciseness: Does the AD convey essential visual information without overloading the user?\n\nTiming and Synchronization: Are the original audio and the embedded AD effectively presented? Does the AD properly synchronize with visual contents?\n\nInformative: Is it easy to follow the storyline? Does the AD provide context and background information when necessary?\n\nAudio Quality: Is the overall audio production quality good?\n\nOverall Satisfaction: Are you satisfied with the overall AD experience?\n\nvision. All the participants have normal hearing. For the purposes of the experiments, we segregated participants into two distinct groups: ( i ) Group with visual impairments, and ( ii ) Group with normal vision.\n\n## 6.4.1 Evaluation Procedure\n\nOur experiments utilize a curated set of videos, which are mainly suggested by the American Council of the Blind 2 . We also collected accessibility videos from YouTube 3 . For every video used in our evaluation, participants are exposed to two versions: the first containing human-crafted AD and the second powered by MM-VID-generated AD. Both renditions are narrated using text-to-speech (TTS) technology.\n\nWehave designed two questionnaires for the two groups, referenced in Table 1 and Table 2, respectively. Participants with visual impairments are instructed to base their evaluation exclusively on auditory cues. In contrast, those with normal vision are instructed to consider both visual and auditory elements.\n\nThe assessment adopts the standardized Likert scale for ratings. For each posed question, participants are guided to assign a score ranging from 0 to 10, with higher values indicating more favorable feedback. Furthermore, participants are urged to share feedback and remarks concerning their overall experience.\n\n## 6.4.2 Results on the Group with Visual Impairments\n\nWe utilized 3 different videos for our evaluation, with durations of 1 minute, 1 minute 42 seconds, and 2 minutes\n\n2 The Audio Description Project: https://adp.acb.org/\n\n3 Apple Accessibility: https://www.youtube.com/watch?v= SL7YSqlEd8k\n\n42 seconds, respectively. Each of the 4 participants with visual impairment was well versed with screen reader and other common accessibility tools. After listening to the audio descriptions for each video, they were asked to respond to the 4 questions outlined in Table 1.\n\n## Hypotheses and Results\n\nH1: The MM-VID-generated audio description and original video dialogues are effectively presented to the participants.\n\nResults: Using the Likert scale (0=Not Effective to 10=Most Effective) the participants rated the effectiveness of the delivery of human-crafted AD and MM-VIDgenerated AD. On average, participants gave 7 . 14 ± 1 . 39 for MM-VID-generated AD and 8 . 33 ± 0 . 90 for humancrafted AD, which shows a MM-VID-generated AD very close to human-crafted one in terms of effective delivery (Figure 5).\n\nH2: Participants are able to follow the main story line of the video based on MM-VID-generated audio description only.\n\nResults: Using the Likert scale (0=Not Informative to 10=Highly Informative) the participants rated the informativeness of human-crafted AD and MM-VID-generated AD. On average, participants gave 7 . 14 ± 1 . 16 for MMVID-generated AD and 9 . 29 ± 0 . 91 for human-crafted AD, which shows little difference in informativeness between MM-VID-generated AD and human-crafted one (Figure 5).\n\nH3: MM-VID-generated AD and human-crafted AD are close in terms of voice and audio quality.\n\nResults: Using the Likert scale (0=Low Quality to 10=High Quality) the participants rated the voice and audio quality on average as 8 . 91 ± 1 . 23 for MM-VID-generated AD and 9 . 07 ± 0 . 65 for human-crafted AD. This minimal difference between the scores indicates the close-to-human voice and audio quality of MM-VID-generated AD (Figure 5).\n\n## Discussion:\n\nThe results show that the participants' overall satisfaction of MM-VID-generated ADs was on average around 2 points less than human-crafted ones in the Likert scale (0=Not Satisfied to 10=Highly satisfied) (Figure 5). Some of the difficulties indicated by participants while listening to MM-VID-generated ADs were 1) occasional overlaps between AD audio and original video dialogues 2) wrong descriptions due to hallucinations of GPT-4V(ision). Regardless of the difference in overall satisfaction, all the participants agreed that MM-VID-generated AD can pro-\n\nFigure 5. Results on the group with visual impairments. MMVID-generated AD is close to human-generated ones in terms of audio quality and effectiveness of delivery. However, MMVID's AD yields lower satisfaction levels compared to the humangenerated ones. This was primarily attributed to occasional overlaps between the audio descriptions and the video dialogues.\n\n<!-- image -->\n\nvide a cost-effective and scalable solution. Thus, millions of videos that cannot afford to be professionally audio described, can be auto-processed by a tool like MM-VID to make them accessible to the visual-impaired community.\n\n## 6.4.3 Results on the Group with Normal Vision\n\nFor sighted individuals, we used the same set of videos as we used for individuals with visual impairments. All of our 5 participants answered to 6 questions listed in Table 2 after watching videos embedded with MM-VID-generated AD as subtitles and audio track.\n\n## Hypotheses and Results\n\nH1: The MM-VID-generated AD is accurate and conveys essential information without overloading the listener.\n\nResults: The sighted individuals rated the clarify and accuracy of MM-VID-generated AD as 7 . 83 ± 1 . 24 and human-curated AD as 8 . 9 ± 0 . 74 on average, using the Likert scale (0=Not Accurate to 10=Most Accurate). In terms of conciseness, the participants on average gave 8 . 73 ± 0 . 49 for the MM-VID-generated AD and 9 . 16 ± 0 . 54 for human-curated AD based on the Likert scale (0=Not concise to 10=Most concise). These results indicate MM-VID-generated ADs are close to human-curated ones in terms of accuracy and conciseness (Figure 6).\n\nH2: The MM-VID-generated ADs are in sync with visual content and do not overlap with other dialogues ensuring listeners can follow the story line.\n\nResults: Participants gave on average 8 . 90 ± 0 . 90 and 7 . 97 ± 1 . 54 to human-crafted AD and MM-VID-generated AD respectively using the Likert scale (0=Not Informative to 10=Highly Informative). Human-crafted AD and MM-VID-generated AD received 8 . 59 ± 0 . 95 and 8 . 53 ± 0 . 58 respectively on the aspect of timing and synchronization using the Likert scale (0=Not Effective to 10=Most Effective). These indicates while listening to MM-VID-generated ADs participants were able to follow main story line and found the audios are in sync with video content very close to that of human-crafted ADs (Figure 6).\n\nH3: The voice and audio quality of MM-VID-generated ADs are close to human-crafted ADs.\n\nResults: The results are very similar to results on group with visual impairments. Sighted participants rated the voice and audio quality on average as 8 . 30 ± 0 . 89 for MMVID-generated AD and as 8 . 93 ± 0 . 32 for human-crafted AD. Therefore the voice and audio experience did not degrade much while listening to MM-VID-generated ADs compare to human-crafted ADs (Figure 6).\n\n## Discussion:\n\nThe evaluations on sighted individuals helped to cross verify the hypotheses of individuals with visual impairments, that are based on audio cues only. Although the overall satisfaction points for sighted participants with MM-VID-generated ADs was on average &lt; 1 points lower than human-generated ADs (Figure 6), the overall satisfaction points for participants who were blind was worse. This is expected because sighted individuals had access to both audio and video modalities but individuals with visual impairments did not. We also believe the reason for lower overall satisfaction, may have been the lack of practice listening to auto generated ADs. Some of the users also mentioned they have preference between pitches of voice and number of concurrent audio channels. These may add to the reason of lower overall satisfaction.\n\n## 6.4.4 Participant Feedback\n\nWe present a collection of interview quotes from our participants who were visually impaired, in which they share their personal experiences and insights about the audio descriptions (AD) generated by MM-VID. The participants expressed a unanimous desire to continue utilizing this AD generation service in the future, highlighting its exceptional quality ('Nearly perfect'), intricate details ('favorite was the details'), extensive applicability ('allowed me to follow anything visual'), and the profound impact it has on them ('I did not depend on someone else'). Below, we provide additional quotes for further insight.\n\nP1: 'I understand what is going on very quickly\n\nFigure 6. Results on the group with normal vision. MM-VIDgenerated AD was on average &lt; 1 points lower than humangenerated ADs. The participants were able to follow the main story line and the audios are in sync with the video content.\n\n<!-- image -->\n\nand I did not depend on someone else. '\n\n- P2: 'If it's AI-generated, there are so many places it's not available, and we need it there. '\n\nP2: 'First time listening to auto-generated AD. As a user, if I am offered this AD, I would take it. ' P3: 'Nearly perfect. Most favorite was the de-\n\n- tails. '\n\nP3: 'More information helped me follow the storyline. '\n\nP3: 'It allowed me to follow anything visual. It felt natural the way AD describes how the actor interacts with the environment. '\n\nP3: 'I love animal kingdom, and I watch Wild Earth safari virtual tour. I would love to have audio descriptions of Wild Earth videos and daily safaris. '\n\nP4: 'I would like to have auto-generated audio description for live conferences in Microsoft Teams.'\n\nP4: 'It worked best as the original audio had not much value.'\n\nDespite the positive feedback, not all responses were favorable:\n\nP4: 'I am skeptical when it becomes subjective. Sometimes I feel they make up stories which is not good.'\n\nP4: 'After listening to the human-generated AD, I figured I misunderstood parts of the original story. '\n\nP1: 'It keeps referring to the same person using their names instead of pronouns. '\n\nP4: 'I don't deal well with overlapped or two parallel audios. '\n\nInterestingly, even those participants who provided critical feedback still rated the MM-VID-generated AD closely to human-generated AD, during the questionnaire sessions. This indicates that, similar to human-curated AD, adapting to MM-VID-generated ADs might necessitate some practice and acclimatization over time.\n\n## 7. Conclusion\n\nWe have presented MM-VID, a system that synergizes with GPT-4V for advancing video understanding. MMVID employs GPT-4V to transcribe video content into long and detailed scripts, thereby enriching LLMs with advanced video understanding capabilities. Experimental results demonstrate the effectiveness of MM-VID in addressing challenging tasks, including comprehension of hourlong videos, analysis across multiple episodes, identification of characters and speakers, and interaction with video games and graphical user interfaces.\n\nBeyond the development of the MM-VID system, we conducted an extensive user study, drawing feedback from a varied group of participants. The outcomes of this study indicated that the audio descriptions generated by MM-VID closely mirror the quality of those crafted by humans. In our future work, we plan to explore SoM [76] and object tracking techniques to enhance various tasks and functionalities.\n\n## Acknowledgment\n\nWe are deeply grateful to OpenAI for providing access to their exceptional tool [3, 51-53]. We are profoundly thankful to Misha Bilenko for his invaluable guidance and support. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Cenyu Zhang, Saqib Shaikh, Ailsa Leen, Jeremy Curry, Crystal Jones, Roberto Perez, Ryan Shugart, Anne Taylor for their constructive feedback.\n\n## References\n\n- [1] Dota 2. https://openai.com/research/dota-2 , 2017. 2\n- [2] Azure cognitive services apis. https://azure. microsoft . com / products / cognitive -services , 2023. 4\n- [3] Chatgpt can now see, hear, and speak. https://openai. com/blog/chatgpt-can-now-see-hear-andspeak , 2023. 3, 10\n- [4] Pygame library. https://www.pygame.org/ , 2023. 2, 7, 31, 32, 33, 34\n- [5] Pyscenedetect: Video scene cut detection and analysis tool. https://www.scenedetect.com/ , 2023. 4\n- [6] Vlog: Video as a long document. https://github. com/showlab/VLog , 2023. 3\n- [7] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022. 2\n- [8] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023. 2\n- [9] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇ ci´ c, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision , pages 6836-6846, 2021. 2, 3\n- [10] Max Bain, Arsha Nagrani, G¨ ul Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1728-1738, 2021. 2, 3\n- [11] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems , 35:24639-24654, 2022. 2\n- [12] Tadas Baltruˇ saitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence , 41(2):423-443, 2018. 2\n- [13] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML , volume 2, page 4, 2021. 2, 3\n- [14] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987 , 2019. 2, 3\n- [15] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6299-6308, 2017. 2, 3\n- [16] Harrison Chase. Langchain. https://langchain. readthedocs.io/ , 2023. 3\n- [17] Sanyuan Chen, Yu Wu, Zhuo Chen, Jian Wu, Jinyu Li, Takuya Yoshioka, Chengyi Wang, Shujie Liu, and Ming Zhou. Continuous speech separation with conformer. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 5749-5753. IEEE, 2021. 2\n- [18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 3\n- [19] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022. 2, 3\n\n- [20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 6824-6835, October 2021. 2, 3\n- [21] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision , pages 6202-6211, 2019. 2, 3\n- [22] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681 , 2021. 2, 3\n- [23] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2289822909, 2023. 2, 3\n- [24] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The' something something' video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision , pages 5842-5850, 2017. 2\n- [25] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18995-19012, 2022. 6, 30\n- [26] Tengda Han, Max Bain, Arsha Nagrani, G¨ ul Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1893018940, 2023. 2, 6, 7\n- [27] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 1725-1732, 2014. 2\n- [28] Hildegard Kuehne, Hueihan Jhuang, Est´ ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision , pages 2556-2563. IEEE, 2011. 3\n- [29] Anna Kukleva, Makarand Tapaswi, and Ivan Laptev. Learning interactions and relationships between movie characters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 9849-9858, 2020. 2\n- [30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7331-7341, June 2021. 2, 3\n- [31] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question answering. In EMNLP , 2018. 2, 3\n- [32] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. In ECCV , 2020. 3\n- [33] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020 , 2023. 3\n- [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023. 3\n- [35] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 , 2023. 3\n- [36] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. arXiv preprint arXiv:2005.00200 , 2020. 2, 3\n- [37] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying videolanguage understanding as masked language modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 23119-23129, 2023. 2, 3\n- [38] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: A multi-task benchmark for video-and-language understanding evaluation. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks , 2021. 2, 3\n- [39] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1794917958, 2022. 2, 3\n- [40] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric video-language pretraining. Advances in Neural Information Processing Systems , 35:7575-7586, 2022. 2, 3\n- [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3\n- [42] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 3202-3211, 2022. 2, 3\n- [43] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207 , 2023. 3\n- [44] MuhammadMaaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video\n\nunderstanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023. 3\n\n- [45] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023. 6\n- [46] EVERINGHAM Mark. Hello! my name is... buffy'automatic naming of characters in tv video. In Proceedings of British Machine Vision Conference, 2006 , 2006. 2\n- [47] Microsoft. Bingchat. https://www.microsoft.com/ en-us/edge/features/bing-chat , 2023. 2\n- [48] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV , 2019. 2\n- [49] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612 , 2017. 2\n- [50] Arsha Nagrani and Andrew Zisserman. From benedict cumberbatch to sherlock holmes: Character identification in tv series without a script. arXiv preprint arXiv:1801.10442 , 2018. 2\n- [51] OpenAI. Gpt-4 technical report. 2023. 2, 3, 10\n- [52] OpenAI. Gpt-4v(ision) system card. 2023. 2, 3, 10\n- [53] OpenAI. Gpt-4v(ision) technical work and authors. https: //cdn.openai.com/contributions/gpt-4v. pdf , 2023. 10\n- [54] Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442 , 2023. 2\n- [55] Karine Pires and Gwendal Simon. Youtube live and twitch: a tour of user-generated live streaming systems. In Proceedings of the 6th ACM multimedia systems conference , pages 225-230, 2015. 2\n- [56] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning , pages 2849228518. PMLR, 2023. 3\n- [57] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3202-3212, 2015. 2, 6, 7\n- [58] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. 6\n- [59] Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unified model for image, video, audio and language tasks. arXiv preprint arXiv:2307.16184 , 2023. 2\n- [60] Cees GM Snoek and Marcel Worring. Multimodal video indexing: A review of the state-of-the-art. Multimedia tools and applications , 25:5-35, 2005. 2\n- [61] David Snyder, Daniel Garcia-Romero, Gregory Sell, Alan McCree, Daniel Povey, and Sanjeev Khudanpur. Speaker recognition for multi-speaker conversations using x-vectors. In ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP) , pages 5796-5800. IEEE, 2019. 2\n- [62] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449 , 2023. 2, 3\n- [63] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012. 3\n- [64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 3\n- [65] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407 , 2023. 3\n- [66] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, et al. All in one: Exploring unified video-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 6598-6608, 2023. 2, 3\n- [67] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents, 2023. 2\n- [68] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, highquality multilingual dataset for video-and-language research. In The IEEE International Conference on Computer Vision (ICCV) , October 2019. 2, 3\n- [69] Yunbo Wang, Mingsheng Long, Jianmin Wang, and Philip S Yu. Spatiotemporal pyramid network for video action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 1529-1538, 2017. 2, 3\n- [70] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1884-1894, 2021. 2\n- [71] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280 , 2022. 3\n- [72] Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, and Michael Zeng. Visual clues: Bridging vision and language foundations for image paragraph captioning. Advances in Neural Information Processing Systems , 35:17287-17300, 2022. 3\n\n- [73] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR , 2016. 3\n- [74] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2023. 2\n- [75] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658 , 2023. 2\n- [76] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 , 2023. 10\n- [77] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao, Yi-Ling Chen, et al. i-code: An integrative and composable multimodal learning framework. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 10880-10890, 2023. 2\n- [78] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). arXiv preprint arXiv:2309.17421 , 2023. 2, 3, 4\n- [79] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 3, 4\n- [80] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v (ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541 , 2023. 6\n- [81] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 , 2021. 2\n- [82] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems , 34:23634-23651, 2021. 2\n- [83] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485 , 2023. 2\n- [84] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 , 2023. 3\n- [85] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models, 2022. URL https://arxiv. org/abs/2205.01068 . 3\n\nt\n\n<!-- image -->\n\nFigure 7. An example of MM-VID's execution flow (continued). We present the full script generated by MM-VID. The original video is available at https://www.youtube.com/watch?v=-pNe0p4H8ec\n\nt\n\n## Script Generation Using GPT-4 (continued):\n\nDodgers' catcher Will Smith lifts a high ball into center field which brings Mookie Betts running home, netting the Dodgers another point.\n\n0:06:23 - 0:06:38: The Rockies' show some fight as Chris Brian leads the ninth innings with a double bringing the Rockies trailing the Dodgers by five.\n\n0:06:39 - 0:06:54: A series of unfortunate events for the Rockies as batters Montero and Diaz get directly hit by pitched balls consecutively. Trejo from the Rockies finally manages to hit a fly ball into left field, allowing Montero to make it to the home plate, closing the gap between the two teams to three.\n\n0:06:55 - 0:07:10: The game comes to a close as Charlie Blackman grounds to the first base, ending the game in favor of Dodgers, after a thrilling and eventful match.\n\n0:07:11 - 0:09:09: The scene concludes with the teams leaving the field.\n\n<!-- image -->\n\nCan you summarize the game?\n\nThe game was a thrilling and eventful match between the Colorado Rockies and the Los Angeles Dodgers . The Dodgers dominated the game with superior pitching and powerful hits , while the Rockies showed some fight towards the end but ultimately fell short.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Show me the most exciting moment in this\n\nvideo\n\nThe most exciting moment in this video is when Will Smith hits a home run in the first inning at timestamp 0:01:19 - 0:01:34 in this video\n\n<!-- image -->\n\n<!-- image -->\n\n- Who won the game, and what are their final\n\nscores?\n\nFigure 8. An example of MM-VID's execution flow (continued). Given the generated script as reference, MM-VID achieves various video understanding capabilities, including grounded QA and multimodal reasoning . The original video is available at https://www.\n\nyoutube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\nt\n\n<!-- image -->\n\n<!-- image -->\n\n- Who are the best pitchers in this video?\n\nJulio Urias was the best pitcher in this video, striking out multiple batters and inducing a double play.\n\n<!-- image -->\n\nEvidence in the Generated Script: 0:02:07 - 0:02:22: Urias faces a setback as McMahon bounces back with a high-quality deep shot to the center field, netting himself a double. 0:03:59 - 0:04:14: Julio Urias , from the Dodgers, efficiently strikes out the Venezuelans' Luis Montero and Ryan McMahon in quick succession. 0:04:47 - 0:05:02: Despite a promising start for the Rockies at the top of the third, their momentum slows down as they lose a run due to a double play induced by Urias.\n\n0:05:35 - 0:05:50: Urias's superior pitching continues as he strikes out Ryan McMahon and skillfully fields a sharp grounder ending the fourth inning , serving a huge blow to the Rockies.\n\n<!-- image -->\n\nWhen did Julio Urias have his first strikeout?\n\nJulio Urias had his first strikeout in the video at 0:00:31 , striking out Chris Bryant from the Rockies.\n\n<!-- image -->\n\nFigure 9. An example of MM-VID's execution flow (continued). Given the generated script as reference, MM-VID achieves various video understanding capabilities, including grounded QA and multimodal reasoning . The original video is available at https://www. youtube.com/watch?v=-pNe0p4H8ec\n\n<!-- image -->\n\n<!-- image -->\n\n- When did Julio Urias induce a double play?\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 10. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension . Figures 11-13 show continued outputs. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\n<!-- image -->\n\nScene 31: 00:06:46\n\nAn elephant and a gorilla roam the jungle.\n\nScene 32: 00:06:57\n\nThe gorilla relocates deeper into the jungle.\n\nScene 33: 00:07:05\n\nThe gorilla is seen wading in a small body of water.\n\nScene 34: 00:07:13\n\nTwo gorillas spotted playing in the grass.\n\nScene 35: 00:07:27\n\nAdmiring the grass, the gorilla takes a deserved break.\n\nScene 36: 00:07:32\n\nAerial view of gorillas and elephants in the marsh.\n\nScene 37: 00:07:37\n\nClose-up of the elephant in the water.\n\nScene 38: 00:07:43\n\nTwo elephants are seen fighting and splashing in the water.\n\nScene 39: 00:08:00\n\nA lone elephant is caught grazing in the landscape.\n\nScene 40: 00:08:08\n\nScene transitions to a group of gorillas in their natural habitat.\n\nScene 41: 00:08:16\n\nA herd of elephants is spotted in shallow water.\n\nScene 42: 00:08:22\n\nAerial view of a vast landscape with a mountain peak and flying birds.\n\nScene 43: 00:08:28\n\nAerial view of a wide, green landscape with scattered water bodies and flying birds.\n\nScene 44: 00:08:34\n\nAerial view of a mountainous range with a valley and a river.\n\nScene 45: 00:08:41\n\nClose-up of gorilla hanging from a tree branch.\n\nFigure 11. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; We present the full script generated by MM-VID. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\nScene 46: 00:09:03\n\nClose-up of a bug on a leaf.\n\nScene 47: 00:09:43\n\n<!-- image -->\n\nLandscape view of mountains with clouds hovering overhead.\n\nScene 48: 00:10:32\n\nClose-up of a bird as it takes flight.\n\nScene 49: 00:10:50\n\nClose-up of a gorilla's face hidden behind the leaves.\n\nScene 50: 00:11:23\n\nA gorilla is seen searching the dense forest floor.\n\nScene 51: 00:12:15\n\nClose-up of a black bird with large eyes.\n\nScene 52: 00:13:03\n\nA black bird is seen walking in a jungle path.\n\nScene 53: 00:13:29\n\nBird of prey perched on tree branches in the jungle.\n\nScene 54: 00:15:05\n\nClose-up of a bird's eye in the forest.\n\nScene 55: 00:16:38\n\nA shot of a gorilla hiding behind a tree in a dense jungle.\n\nScene 56: 00:17:50 Two elephants engage in a playful fight near a water body.\n\nScene 57: 00:19:20\n\nThe leopard is seen slowly moving through the foliage.\n\nScene 58: 00:21:51 Bird seen entering a hole in a tree.\n\nScene 59: 00:22:22\n\nA red ant crawls on a fern.\n\nScene 60: 00:23:32\n\nA peculiar mushroom blooms in the darkness near the forest floor.\n\nScene 61: 00:25:07\n\nA deer seen foraging in the foliage.\n\nCan you summarize the video?\n\nWhat is Cordyceps fungus? Figure 12. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; MM-VID is able to summarize the video and address specific questions with regard to the video contents. Blue highlights the correct answers. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\nThe video is a documentary that explores the diverse and rich ecosystems of jungles and\n\nrainforests around the world. It showcases various animal species, their behaviors, and their roles in\n\nmaintaining the balance of these ecosystems. The documentary also highlights the threats faced by\n\nthese habitats, such as deforestation and\n\nCordyceps fungus is a parasitic fungus that infects and takes control of its host, usually an\n\ninsect. The fungus manipulates the host's behavior, eventually leading to the host's death.\n\nThe fungus then erupts from the host's body and releases spores to infect more victims. Cordyceps\n\nfungus plays a role in maintaining the balance of\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 13. Case studies of MM-VID 's capabilities and application scenarios: hour-long video comprehension ; MM-VID is able to localize the specific moments or topics in the lengthy documentary video. Blue highlights the correct answers. The original video is available at https://www.youtube.com/watch?v=um2Q9aUecy0\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPitcher plants can be found in the video around the timestamp 00:18:18.\n\n<!-- image -->\n\nt\n\nDescribe Frank's journey in the two videos. Frank's journey in these two episodes involves Figure 14. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character's journey across multiple videos. Figure 15 shows continued outputs. Blue highlights the correct answers. The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n\n<!-- image -->\n\nhim preparing for his new role as Vice\n\nPresident while trying to maintain control over various situations.\n\nHe deals with the consequences of his past actions, such as\n\n<!-- image -->\n\nDescribe Frank's journey in the two videos.\n\nFrank's journey in these two episodes involves him preparing for his new role as Vice President while trying to maintain control over various situations. He deals with the consequences of his past actions, such as managing Doug Stamper's attempts to erase their tracks and handling the fallout from Zoe's death. Frank also navigates the political landscape, working with Jackie to secure her position as Whip and engaging in tense negotiations with China. Throughout these challenges, Frank continues to demonstrate his cunning and ruthlessness in achieving his goals.\n\nFigure 15. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, perform cross-episodic reasoning, describe a specific character's journey across multiple videos. Blue highlights the correct answers. The two videos are from House of Cards, Season 2: episode 1 and episode 2.\n\n<!-- image -->\n\nt\n\n<!-- image -->\n\nDid Pikachu use 10000000 Volts move?\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Did Pikachu use Gigantamax?\n\n<!-- image -->\n\n<!-- image -->\n\nShow me the keyframe when Pikachu defeats Charizard\n\n<!-- image -->\n\nFigure 16. Case studies of MM-VID 's capabilities and application scenarios: multi-video episodic analysis ; Given multiple video files as inputs, MM-VID can summarize the video series, and localize specific moments across multiple videos. Blue highlights the correct answers. The two videos are from Pok´ emon Journeys: episode 131 and episode 132.\n\nt\n\n<!-- image -->\n\n<!-- image -->\n\n## Loading video …\n\nVideo length is  2 minutes 43 seconds.\n\n## Invoke tools: Scene Detection, Metadata Collection\n\n## Scene Detection:\n\n0:00:00, 0:00:01, 0:00:02, 0:00:03, 0:00:04, 0:00:05, 0:00:06, 0:00:07, 0:00:08, 0:00:09, ..., 0:02:39, 0:02:40, 0:02:41, 0:02:42\n\n## Metadata Collection:\n\nTitle: Mummies\n\nAbstract: Thut is a young charioteer. Nefer is the daughter of Faraon. In this video, Thut saves himself from falling off a cliff. Thut and Nefer meets Faraon. Thut and Nefer gets married in front of the goddess statue and all people. Thut and Nefer celebrates by singing and dancing.\n\n## Visual Prompting:\n\n<!-- image -->\n\nFaraon\n\n<!-- image -->\n\n<!-- image -->\n\n## Script Generation:\n\n0:00:00 - 0:00:06: Nefer , an Egyptian princess, is seen gazing at the desert landscape and examining a rocky cliff.\n\n## (continued)\n\n0:00:06 - 0:00:08: Nefer looks down the cliff's edge, revealing her concerned reaction.\n\n0:00:12 - 0:00:14: A shot of a rock with a golden ankh necklace sets an ominous tone.\n\n0:00:15 - 0:00:16: Transition to a worried Nefer under the open sky.\n\n0:00:17 - 0:00:18: Faraon overlooks Thut lying on the rocky ground.\n\n0:00:18 - 0:00:24: Various interactions between Thut , Nefer , and Faraon in the desert transpire. 0:00:24 - 0:00:30: Nefer is hugged by Faraon as Thut watches closely.\n\n0:00:30 - 0:00:36: Thut shares a moment with Nefer before seating himself beside a frog as Faraon joins Nefer .\n\n0:00:36 - 0:00:42: Nefer and Faraon approach a slightly confused Thut .\n\n0:00:42 - 0:00:48: Thut and Nefer stand alone exchanging a few words.\n\n0:00:48 - 0:00:54: Faraon joins Thut and Nefer , looking at a ring Thut holds.\n\n0:01:00 - 0:01:06: Thut presents a ring between his and Nefer 's fingers.\n\n0:01:06 - 0:01:12: Thut listens as Nefer talks excitingly about the ring.\n\n0:01:12 - 0:01:18: Faraon oversees a close moment between Thut and Nefer .\n\n0:01:18 - 0:01:24: Nefer and Thut stand in front of an audience as a small girl with a snake appears.\n\n0:01:24 - 0:01:30: Thut and Nefer perform in front of a crowd.\n\n0:01:42 - 0:01:48: Thut and Nefer share a kiss as the crowd cheers on, transitioning to the girl with her crocodile.\n\n0:01:54 - 0:02:00: Thut and Nefer perform on stage with others in a palace courtyard. 0:02:12 - 0:02:18: Multiple performances ensue Nefer in the courtyard, ending with Thut and sharing a gaze.\n\n0:02:18 - 0:02:24: Thut and Nefer continue their romantic interaction under the watchful eyes of their audience.\n\n0:02:24 - 0:02:30: The dancing in the courtyard continues.\n\n0:02:30 - 0:02:36: Thut , Nefer , and Faraon perform on stage as their performers enjoy their display.\n\n0:02:36 - 0:02:43: As the sun sets, Thut and Nefer conclude their musical performance on a balcony.\n\nNefer\n\n<!-- image -->\n\nFigure 17. Case studies of MM-VID 's capabilities and application scenarios: character identification ; By providing the face photos of the characters and their names, MM-VID can generate the script with the identified characters. Blue highlights the correct character identification. The original video is available at https://www.tokyvideo.com/video/mummies-online-free-film , from 1:19:47 to 1:22:30.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nThut\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nLoading video …\n\nVideo length is  2 minutes 57 seconds.\n\n## Invoke tools: ASR, Metadata Collection\n\n## ASR:\n\n<!-- image -->\n\n<!-- image -->\n\n0:00:01 Guest\\_0:  Good trade flesh and we run and we hide.\n\n0:00:33 Guest\\_2:  That was weird.\n\n0:00:34 Guest\\_3:  Hey boss.\n\n0:00:39 Guest\\_3:  I love the secret layer.\n\n…\n\n0:02:53 Guest\\_10:  From you. No. Fine then from.\n\n0:02:55 Guest\\_10:  Who? The guy who cut off your arms.\n\n## Metadata Collection:\n\nTitle: Nimona\n\nAbstract: Ballister boldheart is a knight. Nimona is a mischievous teen who is a shapeshifting creature.\n\nThe video shows how Ballister met Nimona first time.Nimona thought boldheart was the villain but disappointed to know he is not.\n\n## Visual Prompting:\n\n<!-- image -->\n\nNimona\n\n<!-- image -->\n\nBallister Boldheart\n\n<!-- image -->\n\n## Regenerated ASR with Identified Speakers:\n\n0:00:01 Ballister Boldheart : Good trade flesh and we run and we hide.\n\n0:00:33 Unknown: That was weird.\n\n0:00:34 Nimona : Hey boss.\n\n0:00:39 Nimona : I love the secret layer.\n\n0:00:42 Nimona : The garbage and the smell of sadness really pull the whole thing together.\n\n0:00:45 Ballister Boldheart : Well, wait, wait, wait. Who? Who are you?\n\n0:00:47 Nimona : The names. Pneumonia.\n\nWhoa. Yeah. Sick arm.\n\n0:00:48 Ballister Boldheart : And how did you?\n\n0:00:51 Unknown: Did it bleed a lot? Did they let you keep the old one?\n\n0:00:54 Ballister Boldheart : No, let's go. That's wrong with you.\n\n0:00:58 Nimona : Can I have it? What? Put that?\n\n0:00:59 Ballister Boldheart : Down that is.\n\n0:01:00 Ballister Boldheart : Not for little girls, little.\n\n0:01:02\n\n: Girls. OK, how old do you think I?\n\n0:01:05 Ballister Boldheart : AM 10.\n\n0:01:09 Unknown: Alright, home.\n\n0:01:09 Ballister Boldheart : We are more or less than 10.\n\n0:01:10 Nimona : Not a lot of kids in your life, huh?\n\n0:01:12 Ballister Boldheart : You know what?\n\n0:01:13 Ballister Boldheart : No. And then like if you stay that?\n\n0:01:14 Ballister Boldheart : Way you have to go job.\n\n0:01:15 Nimona : But I'm here about this.\n\n0:01:16 Unknown: What job?\n\n0:01:18 Nimona : Oh, it's all here.\n\n0:01:18 Nimona : It's all here in my application.\n\n0:01:22 Nimona : This is just a bunch of drawings.\n\n0:01:26 Unknown: Very disturbing drawings.\n\n0:01:28 Ballister Boldheart : Oh, look, it's me on a rhinoceros skewering.\n\n0:01:32 Ballister Boldheart : Guards like a human kebab.\n\n0:01:34 Nimona : Yeah. Do you like?\n\n0:01:35 Nimona : It I thought a visual aid.\n\n0:01:37 Unknown: Really embarrass me. Puff up here. So about the job. What job?\n\n0:01:45 Nimona : To be your.\n\nFigure 18. Case studies of MM-VID 's capabilities and application scenarios: speaker identification ; By leveraging visual prompting, MM-VID can enhance ASR predictions with the speakers' identity. Blue and Red highlight the correct and incorrect predictions, respectively. Figure 19 shows continued outputs. The original video is available at https://www.netflix.com/title/81444554 , from 9:52 to 12:52.\n\nNimona\n\n<!-- image -->\n\nc\n\nt\n\n<!-- image -->\n\n```\n(continued): 0:01:45 Nimona : Sidekick, you know. 0:01:46 Nimona : To help you do whatever it takes to. 0:01:47 Nimona : Get revenge on. 0:01:48 Nimona : The cold, cruel world that rejected you. Shall we pillage village? Lay low until they don't remember you, and then we rise like a phoenix. From the Ashes, overthrow the government. 0:01:58 Unknown: Or we could. 0:01:59 Nimona : Just talk. 0:02:02 Unknown: The the the point. 0:02:03 Nimona : Is whatever your dark heart desires. Boss, your sidekick has arrived. 0:02:08 Ballister Boldheart : I don't need a. 0:02:09 Ballister Boldheart : Side wait, I am not a. 0:02:09 Nimona : Every villain needs a sidekick. 0:02:12 Unknown: Sweetener. Murdo. That's how you want to kill first. Oh. 0:02:14 Nimona : Are these your next targets? 0:02:15 Nimona : Who do? 0:02:16 Ballister Boldheart : I'm not killing anybody. 0:02:18 Nimona : This one, this guy looks extremely punchable. 0:02:19 Nimona : Right. He is actually extremely. 0:02:21 Ballister Boldheart : You're right, he is actually. 0:02:22 Ballister Boldheart : Punchable. Wait, that is not a murder wall. It's a. 0:02:26 Ballister Boldheart : The innocence wall, yeah. 0:02:27 Nimona : Citizens will. 0:02:30 Nimona : We are you saying that you're not a villain? Yes. 0:02:36 Nimona : Yes you are. 0:02:37 Ballister Boldheart : No, no, no. I'm not a villain, no. 0:02:42 Ballister Boldheart : But you. 0:02:43 Ballister Boldheart : Disappointed that I'm not a murderer. 0:02:46 Nimona : I mean. 0:02:46 Nimona : I mean a little. Why? 0:02:48 Ballister Boldheart : You know what we. 0:02:49 Ballister Boldheart : Don't know. 0:02:50 Ballister Boldheart : The real villain is still out there and I do need help not. 0:02:53 Nimona : From you. No. Fine then from. 0:02:55 Nimona : Who? The guy who cut off your arms.\n```\n\nFigure 19. Case studies of MM-VID 's capabilities and application scenarios: speaker identification ; By leveraging visual prompting, MM-VID can enhance ASR predictions with the speakers' identity. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.netflix.com/title/81444554 , from 9:52 to 12:52.\n\n<!-- image -->\n\n<!-- image -->\n\n## Script Generation:\n\n## Loading video …\n\nVideo length is  1 minutes 44 seconds.\n\n<!-- image -->\n\n0:00:00 - 0:00:08 : Mr. Bean enters a classroom and takes a seat next to another man. They both anxiously look at their exam papers as they begin their test.\n\n0:00:09 - 0:00:16 : Mr. Bean glances over at the other man's paper and tries to sneakily copy his answers. Both men continue to work on their tests, unaware of their surroundings.\n\n0:00:17 - 0:00:23 : Close-up shot of Mr. Bean's face showing his frustration as he struggles to figure out the answers to the exam.\n\n0:00:24 - 0:00:27 : Mr. Bean continues to take the exam while holding a pink flamingo pen, struggling to concentrate.\n\n0:00:27 - 0:00:35 : Mr. Bean becomes more desperate, sticking his nose into the exam paper as he continues to copy answers from the man sitting next to him.\n\n0:00:36 - 0:00:45 : Mr. Bean takes a break, rubbing his eyes as the scene cuts back and forth between him and his colleague.\n\n0:00:45 - 0:00:54 : The students continue to work on their task as the scene shows the peaceful and quiet environment of the exam room.\n\n0:00:54 - 0:01:00 : Mr. Bean is deep in thought as he taps his pen on the desk, looking more worried and nervous about his exam performance.\n\n0:01:01 - 0:01:02 : Close-up of Mr. Bean struggling with a problem and looking upward with a frustrated expression.\n\nFigure 20. Case studies of MM-VID 's capabilities and application scenarios: audio description generation ; Without ASR inputs, MMVID processes the video and generates a detailed script. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www.youtube.com/watch?v=29ZmXkICT1g , from 2:50 to 4:34.\n\n## (continued)\n\n0:01:03 - 0:01:09 : Mr. Bean tries to regain focus in this scene, tapping his fingers as he nervously attempts to recall information for the exam.\n\n<!-- image -->\n\n0:01:11 - 0:01:12 : Two men, including Mr. Bean, are discussing a portion of the exam in a library, pointing at a book while brainstorming answers.\n\n0:01:12 - 0:01:21 : Mr. Bean and the man next to him become more desperate, whispering to each other and trying to cheat as their frustration grows. The mood is tense, mirroring their anxiety.\n\n<!-- image -->\n\nWhy Mr. Bean cheat in the exam?\n\nMr. Bean cheats in the exam because he is unprepared, lacks confidence in his ability to answer the questions, and feels desperate to pass the test . His comedic nature and inability to cope with the pressure of the exam lead him to attempt cheating as a last resort.\n\n<!-- image -->\n\nWhy do people find the video funny?\n\nPeople might find the video funny because of Mr. Bean's exaggerated expressions and actions, such as sticking his nose into the exam paper and using a pink flamingo pen, which add humor to the otherwise tense and stressful situation of taking an exam.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n…\n\n<!-- image -->\n\nrohammad examining a small rohammad examining a small\n\nFigure 21. Case studies of MM-VID 's capabilities and application scenarios: self-refinement ; In this example, MM-VID mistakenly identifies a bird as a rock due to low-resolution video frames. The self-refinement helps rectify the errors, resulting in a more refined output. Blue and Red highlight the correct and incorrect predictions, respectively. The original video is available at https://www. youtube.com/watch?v=o4UKvgBkO2Y , from 1:06 to 4:17.\n\n……\n\n<!-- image -->\n\nbapl biotects the biotects the\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n…\n\n…\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 22. Case studies of MM-VID 's capabilities and application scenarios: fast-changing short videos. The original videos are available at https://www.instagram.com/mealtimewithmummy/reels/\n\n<!-- image -->\n\n…\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 23. Case studies of MM-VID 's capabilities and application scenarios: embodied agent . Blue highlights the correct prediction. The original video is collected from Ego4D dataset [25].\n\n<!-- image -->\n\nFigure 24. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figures 25-27 show continued outputs. The video is generated by Pygame library [4].\n\n<!-- image -->\n\nFigure 25. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figures 26-27 show continued outputs. The video is generated by Pygame library [4].\n\n<!-- image -->\n\n<!-- image -->\n\nYou are expert in playing Super Mario Bros. What is the next controls to be pressed after the last frame. Output JSON.\n\nAvailable controls are: Jump, ArrowLeft, ArrowRight, Wait\n\n```\nOutput: { \"controls\": \"ArrowLeft+Jump\", \"reason\": \"Mario should\" }\n```\n\nFor this sequence of frames of super mario, what is the next controls.\n\n## Output:\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nYou are expert in playing Super Mario Bros.\n\nWhat is the next controls to be pressed after the last frame. Output JSON.\n\nAvailable controls are: Jump, ArrowLeft, ArrowRight, Wait\n\n```\nOutput: { \"controls\": \"ArrowLeft+Jump\", \"reason\": \"Mario should\" }\n```\n\nFigure 26. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. Figure 27 shows continued outputs. The video is generated by Pygame library [4].\n\n{\n\n\"co\n\n\"re\n\n}\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 27. Case studies of MM-VID 's capabilities and application scenarios: playing video game. Blue highlights the correct prediction. The video is generated by Pygame library [4].\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 28. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 29-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \"My next move would be to move my finger to the Amazon icon, which is located in the third row and third column of the screen. I would then click on the icon to open the app and search for an ergonomic keyboard within my budget of $50. \" You have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? moving your finger to an icon, describe the\n\nIf the action involves location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy next move would be to move my finger to the search bar , which is located at the top of the screen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard . After typing in the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate the search.\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at the top right corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open the filter options and narrow down my search results to only show ergonomic keyboards within my budget range of $50 to $100. \" You have now completed the previous action as described above. Below is what you see on the phone screen. Please predict your immediate next move. If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy immediate next move would be to move my finger to the \"Price &amp; Deals\" filter option , which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &amp; Deals\" filter option to open the price range\n\n<!-- image -->\n\nFigure 29. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 30-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n## (continued)\n\nfilter and set my minimum price to $50 and my maximum price to $100. This would further narrow down my search results to only show ergonomic keyboards within my budget range.\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action : \" My immediate next move would be to move my finger to the \"Price &amp; Deals\" filter option, which is located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price &amp; Deals\" filter option to open the price range filter and set my minimum price to $50 and my maximum price to $100. This would further narrow down my search results to only show ergonomic keyboards within my budget range. \" You have now completed the previous action as described above. Below is what you see on the phone screen. What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\n<!-- image -->\n\nMy next move would be to move my finger to the \"Show 1,987 results\" button , which is located at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range.\n\n<!-- image -->\n\n<!-- image -->\n\nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger to an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard.\n\nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100.\n\nPrevious action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located at the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \"\n\nYou have now completed the previous action as described above. Below is what you see on the phone screen. What will you do next? If the action involves moving your finger to an icon, describe the location of that icon as detailed as possible.\n\nFigure 30. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figures 31-32 show continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 31. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively. Figure 32 shows continued outputs.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 32. Case studies of MM-VID 's capabilities and application scenarios: GUI navigation. Blue and Red highlight the correct and incorrect predictions, respectively.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->",
  "tables": [],
  "stats": {
    "pages": 39,
    "chunksCreated": 140,
    "totalCharacters": 88274,
    "totalWords": 13658,
    "numTables": 0,
    "processingTimeMs": 46367
  }
}