{
  "paper": {
    "id": "2309.00071v2",
    "title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "abstract": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn",
    "authors": [
      "Bowen Peng",
      "Jeffrey Quesnelle",
      "Honglu Fan",
      "Enrico Shippole"
    ],
    "published": "2023-08-31T18:18:07.000Z",
    "updated": "2023-11-01T17:28:26.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2309.00071v2",
    "absUrl": "https://arxiv.org/abs/2309.00071v2"
  },
  "chunks": [
    {
      "id": "2309.00071v2-chunk-0",
      "content": "Bowen Peng 1 ∗\n\nJeffrey Quesnelle 1 †\n\nHonglu Fan 23\n\nEnrico Shippole ‡\n\n1 Nous Research\n\n2 EleutherAI\n\n3 University of Geneva",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "YaRN: Efficient Context Window Extension of Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-1",
      "content": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn .",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-2",
      "content": "te beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn .\n\nFigure 1: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n<!-- image -->\n\n∗ Reddit: /u/bloc97 GitHub: bloc97\n\n† Reddit: /u/emozilla X: @theemozilla GitHub: jquesnelle\n\n‡ X: @EnricoShippole GitHub: conceptofmind",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-3",
      "content": "Transformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for many natural language processing (NLP) tasks where long-range abilities such as in-context learning (ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the context window ) determined by its training processes has been one of the major limits of a pretrained LLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or without fine-tuning) has become more and more desirable. To this end, the position encodings of transformers are the center of the discussions.\n\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later improved to a learnable absolute position encoding [15]. Since then, relative positional encoding schemes [32] have further increased the performance of Transformers.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-4",
      "content": "osition encoding, which was later improved to a learnable absolute position encoding [15]. Since then, relative positional encoding schemes [32] have further increased the performance of Transformers. Currently, the most popular relative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].\n\nOne reoccurring limitation with positional encodings is the inability to generalize past the context window seen during training. While some methods such as ALiBi are able to do limited generalization, none are able to generalize to sequences significantly longer than their pre-trained length [22].\n\nSome works have been done to overcome such limitation. [9] and concurrently [21] proposed to extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning on a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by taking the loss of high frequency into account.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-5",
      "content": "difying RoPE via Position Interpolation (PI) and fine-tuning on a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by taking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\" interpolation have been proposed, with different emphasis:\n\n- the \"Dynamic NTK\" interpolation method [14] for pre-trained models without fine-tuning.\n- the \"NTK-by-parts\" interpolation method [7] which performs the best when fine-tuned on a small amount of longer-context data.\n\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their presence in the open-source models such as Code Llama [31] (using \"NTK-aware\" interpolation) and Qwen 7B [2] (using \"Dynamic NTK\").\n\nIn this paper, in addition to making a complete account of the previous unpublished works on the \"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet another RoPE extensioN method), an improved method to efficien",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-6",
      "content": "ccount of the previous unpublished works on the \"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet another RoPE extensioN method), an improved method to efficiently extend the context window of models trained with Rotary Position Embeddings (RoPE) including the LLaMA [38], the GPTNeoX [5], and the PaLM [10] families of models.\n\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less than ∼ 0.1% of the original pre-training data. In the meantime, by combining with the inference-time technique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window extension without any fine-tuning.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-7",
      "content": "The basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]. We work on a hidden layer where the set of hidden neurons are denoted by D . Given a sequence of vectors x 1 , · · · , x L ∈ R | D | , following the notation of [34], the attention layer first converts the vectors into the query vectors and the key vectors:\n\n<!-- formula-not-decoded -->\n\nNext, the attention weights are calculated as\n\n<!-- formula-not-decoded -->\n\nwhere q m , k n are considered as column vectors so that q T m k n is simply the Euclidean inner product. In RoPE, we first assume that | D | is even and identify the embedding space and the hidden states as\n\ncomplex vector spaces:\n\n<!-- formula-not-decoded -->\n\nwhere the inner product q T k becomes the real part of the standard Hermitian inner product Re ( q ∗ k ) . More specifically, the isomorphisms interleave the real part and the complex part\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.1 Rotary Position Embeddings",
        "chunkIndex": 7,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-8",
      "content": "art of the standard Hermitian inner product Re ( q ∗ k ) . More specifically, the isomorphisms interleave the real part and the complex part\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nTo convert embeddings x m , x n into query and key vectors, we are first given R -linear operators\n\n<!-- formula-not-decoded -->\n\nIn complex coordinates, the functions f q , f k are given by\n\n<!-- formula-not-decoded -->\n\nwhere θ = diag ( θ 1 , · · · , θ | D | / 2 ) is the diagonal matrix with θ d = b -2 d/ | D | and b = 10000 . This way, RoPE associates each (complex-valued) hidden neuron with a separate frequency θ d . The benefit of doing so is that the dot product between the query vector and the key vector only depends on the relative distance m -n as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn real coordinates, the RoPE can be written using the following function\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.1 Rotary Position Embeddings",
        "chunkIndex": 8,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-9",
      "content": "d -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn real coordinates, the RoPE can be written using the following function\n\n<!-- formula-not-decoded -->\n\nso that",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.1 Rotary Position Embeddings",
        "chunkIndex": 9,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-10",
      "content": "As language models are usually pre-trained with a fixed context length, it is natural to ask how to extend the context length by fine-tuning on relatively less amount of data. For language models using RoPE as the position embedding, Chen et al. [9], and concurrently kaiokendev [21] proposed the Position Interpolation (PI) to extend the context length beyond the pre-trained limit. While a direct extrapolation does not perform well on sequences w 1 , · · · , w L with L larger than the pre-trained limit, they discovered that interpolating the position indicies within the pre-trained limit works well with the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with RoPE, they modify the RoPE by\n\n<!-- formula-not-decoded -->\n\nwhere L ′ &gt; L is a new context window beyond the pre-trained limit.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.2 Position Interpolation",
        "chunkIndex": 10,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-11",
      "content": "f fine-tuning. Specifically, given a pre-trained language model with RoPE, they modify the RoPE by\n\n<!-- formula-not-decoded -->\n\nwhere L ′ &gt; L is a new context window beyond the pre-trained limit. With the original pre-trained model plus the modified RoPE formula, they fine-tuned the language model further on several orders of magnitude fewer tokens (a few billion in Chen et al. [9]) and successfully acheived context window extension.\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.2 Position Interpolation",
        "chunkIndex": 11,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-12",
      "content": "The ratio between the extended context length and the original context length has been of special importance, and we introduce the notation s defined by\n\n<!-- formula-not-decoded -->\n\nand we call s the scale factor .\n\nWe also rewrite and simplify Eq. 10 into the following general form:\n\n<!-- formula-not-decoded -->\n\nwhere g ( m ) , h ( θ d ) are method-dependent functions. For PI, we have g ( m ) = m/s,h ( θ d ) = θ d . In the subsequent sections, when we introduce a new interpolation method, we sometimes only specify the functions g ( m ) and h ( θ d ) .\n\nAdditionally, we define λ d as the wavelength of the RoPE embedding at d -th hidden dimension:\n\n<!-- formula-not-decoded -->\n\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension d to perform a full rotation ( 2 π ).",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.3 Additional Notation",
        "chunkIndex": 12,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-13",
      "content": "edding at d -th hidden dimension:\n\n<!-- formula-not-decoded -->\n\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension d to perform a full rotation ( 2 π ).\n\nGiven that some interpolation methods (eg. PI) do not care about the wavelength of the dimensions, we will refer to those methods as \"blind\" interpolation methods, while others do (eg. YaRN), which we will classify as \"targeted\" interpolation methods.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.3 Additional Notation",
        "chunkIndex": 13,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-14",
      "content": "ReRoPE [33] also aims to extend the context size of existing models pre-trained with RoPE, and claims \"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically decreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves context extension by modifying the attention mechanism and thus is not purely an embedding interpolation method. Since it is currently not compatible with Flash Attention 2 [13] and requires two attention passes during inference, we do not consider it for comparison.\n\nConcurrently with our work, LM-Infinite [16] proposes similar ideas to YaRN, but focuses on \"on-thefly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism of the models, it is not an embedding interpolation method and is not immediately compatible with Flash Attention 2.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "2.4 Related work",
        "chunkIndex": 14,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-15",
      "content": "Whereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound described by PI [9] is insufficient at predicting the complex dynamics between RoPE and the LLM's internal embeddings. In the following subsections, we describe the main issues with PI we have individually identified and solved, so as to give the readers the context, origin and justifications of each method which we use in concert to obtain the full YaRN method.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3 Methodology",
        "chunkIndex": 15,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-16",
      "content": "If we look at RoPE only from an information encoding perspective, it was shown in [36], using Neural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency information if the input dimension is low and the corresponding embeddings lack high frequency components. Here we can see the similarities: a token's positional information is one-dimensional, and RoPE expands it to an n-dimensional complex vector embedding.\n\nRoPE closely resembles Fourier Features [36] in many aspects, as it is possible to define RoPE as a special 1D case of a Fourier Feature. Stretching the RoPE embeddings indiscriminately results in the loss of important high frequency details which the network needs in order to resolve tokens that are both very similar and very close together (the rotation describing the smallest distance needs to not be too small for the network to be able to detect it).",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.1 Loss of High Frequency information - \"NTK-aware\" interpolation",
        "chunkIndex": 16,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-17",
      "content": "k needs in order to resolve tokens that are both very similar and very close together (the rotation describing the smallest distance needs to not be too small for the network to be able to detect it).\n\nWe hypothesise that the slight increase of perplexity for short context sizes after fine-tuning on larger context sizes seen in PI [9] might be related to this problem. Under ideal circumstances, there is no reason that fine-tuning on larger context sizes should degrade the performance of smaller context sizes.\n\nIn order to resolve the problem of losing high frequency information when interpolating the RoPE embeddings, the \"NTK-aware\" interpolation was developed in [6]. Instead of scaling every dimension of RoPE equally by a factor s , we spread out the interpolation pressure across multiple dimensions by scaling high frequencies less and low frequencies more. One can obtain such a transformation in many ways, but the simplest would be to perform a base change on the value of θ .",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.1 Loss of High Frequency information - \"NTK-aware\" interpolation",
        "chunkIndex": 17,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-18",
      "content": "multiple dimensions by scaling high frequencies less and low frequencies more. One can obtain such a transformation in many ways, but the simplest would be to perform a base change on the value of θ .\n\nMore precisely, following the notations set out in Section 2.3, we define the \"NTK-aware\" interpolation scheme as follows (see the Appendix A.1 for the details of the deduction).\n\nDefinition 1 The \"NTK-aware\" interpolation is a modification of RoPE by using Eq. 12 with the following functions.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nGiven the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9].",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.1 Loss of High Frequency information - \"NTK-aware\" interpolation",
        "chunkIndex": 18,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-19",
      "content": "given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.\n\nWe note that shortly before the release of this article, Code Llama [31] was released and uses \"NTK-aware\" scaling by manually scaling the base b to 1M.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.1 Loss of High Frequency information - \"NTK-aware\" interpolation",
        "chunkIndex": 19,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-20",
      "content": "In the case of blind interpolation methods like PI and \"NTK-aware\" interpolation, we treat all the RoPE hidden dimensions equally (as in they have the same effect on the network). However, there are strong clues that point us towards the need for targeted interpolation methods.\n\nIn this section, we think heavily in terms of the wavelengths λ d defined in Eq. 13 in the formula of RoPE. For simplicity, we omit the subscript d in λ d and the reader is encouraged to think about λ as the wavelength of an arbitrary periodic function.\n\nOne interesting observation of RoPE embeddings is that given a context size L , there are some dimensions d where the wavelength is longer than the maximum context length seen during pretraining ( λ &gt; L ), this suggests that some dimensions' embeddings might not be distributed evenly in the rotational domain. In such cases, we presume having all unique position pairs implies that the absolute positional information remains intact.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation",
        "chunkIndex": 20,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-21",
      "content": "nsions' embeddings might not be distributed evenly in the rotational domain. In such cases, we presume having all unique position pairs implies that the absolute positional information remains intact. On the contrary, when the wavelength is short, only relative positional information is accessible to the network.\n\nMoreover, when we stretch all the RoPE dimensions either by a scale s or using a base change b ′ , all tokens become closer to each other, as the dot product of two vectors rotated by a lesser amount is bigger. This scaling severely impairs a LLM's ability to understand small and local relationships between its internal embeddings. We hypothesize that such compression leads to the model being confused on the positional order of close-by tokens, and consequently harming the model's abilities.\n\nwhere",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation",
        "chunkIndex": 21,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-22",
      "content": "tween its internal embeddings. We hypothesize that such compression leads to the model being confused on the positional order of close-by tokens, and consequently harming the model's abilities.\n\nwhere\n\nIn order to remedy this issue, given the two previous observations that we have found, we choose not to interpolate the higher frequency dimensions at all while always interpolating the lower frequency dimensions. In particular,\n\n- if the wavelength λ is much smaller than the context size L , we do not interpolate;\n- if the wavelength λ is equal to or bigger than the context size L , we want to only interpolate and avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n- dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\n\nAs a result, it is more convenient to introduce the ratio r = L λ between the original context size L and the wavelength λ . In the d -th hidden state, the ratio r depends on d in the following way:",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation",
        "chunkIndex": 22,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-23",
      "content": "As a result, it is more convenient to introduce the ratio r = L λ between the original context size L and the wavelength λ . In the d -th hidden state, the ratio r depends on d in the following way:\n\n<!-- formula-not-decoded -->\n\nIn order to define the boundary of the different interpolation strategies as above, we introduce two extra parameters α, β . All hidden dimensions d where r ( d ) &lt; α are those where we linearly interpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r ( d ) &gt; β are those where we do not interpolate at all. Define the ramp function γ to be\n\n<!-- formula-not-decoded -->\n\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\n\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the following functions 4 .\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation",
        "chunkIndex": 23,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-24",
      "content": "cribed as follows.\n\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the following functions 4 .\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe values of α and β should be tuned on a case-by-case basis. For example, we have found experimentally that for the Llama family of models, good values for α and β are α = 1 and β = 32 .\n\nUsing the techniques described in this section, a variant of the resulting method was released under the name \"NTK-by-parts\" interpolation [7]. This improved method performs better than the previous PI [9] and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with fine-tuned models, as shown in [7].",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation",
        "chunkIndex": 24,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-25",
      "content": "In a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to the maximal context size. A typical example is the autoregressive generation where the sequence lengths increment by 1 after each step. There are two ways of applying an interpolation method that uses a scale factor s (including PI, \"NTK-aware\" and \"NTK-by-parts\"):\n\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale factor s = L ′ /L where L ′ is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max (1 , l ′ /L ) where l ′ is the sequence length of the current sequence.\n\nThe problem of (1) is that the model may experience a performance discount at a length less than L and an abrupt degradation when the sequence length is longer than L ′ . But by doing Dynamic",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.3 Dynamic Scaling - \"Dynamic NTK\" interpolation",
        "chunkIndex": 25,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-26",
      "content": "quence.\n\nThe problem of (1) is that the model may experience a performance discount at a length less than L and an abrupt degradation when the sequence length is longer than L ′ . But by doing Dynamic\n\n4 The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over θ d /s and θ d converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of implementation, but both would work.\n\nScaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting the trained context limit L ′ . We call this inference-time method the Dynamic Scaling method. When it is combined with \"NTK-awared\" interpolation, we call it \"Dynamic NTK\" interpolation. It first appeared in public as a reddit post in [14].\n\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pretrained on L without any finetuning ( L ′ = L ). This is supported by the experiment in Appendix B.3.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.3 Dynamic Scaling - \"Dynamic NTK\" interpolation",
        "chunkIndex": 26,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-27",
      "content": "].\n\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pretrained on L without any finetuning ( L ′ = L ). This is supported by the experiment in Appendix B.3.\n\nOften in the repeated forward-passes, the kv-caching [8] is applied so that we can reuse the previous key-value vectors and improve the overall efficiency. We point out that in some implementations when the RoPE embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling with kv-caching. The correct implementation should cache the kv-embeddings before applying RoPE, as the RoPE embedding of every token changes when s changes.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.3 Dynamic Scaling - \"Dynamic NTK\" interpolation",
        "chunkIndex": 27,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-28",
      "content": "In addition to the previous interpolation techniques, we also observe that introducing a temperature t on the logits before the attention softmax has a uniform impact on perplexity regardless of the data sample and the token position over the extended context window (See Appendix A.2). More precisely, instead of Eq. 2, we modify the computation of attention weights into\n\n<!-- formula-not-decoded -->\n\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of this attention scaling: we can instead use a \"length scaling\" trick which scales both q m and k n by a constant factor √ 1 /t by simply scaling the complex RoPE embeddings by the same amount. With this, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it has zero overhead during both inference and training, as RoPE embeddings are generated in advance and are reused for all forward passes.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.4 YaRN",
        "chunkIndex": 28,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-29",
      "content": "attention mechanism without modifying its code. Furthermore, it has zero overhead during both inference and training, as RoPE embeddings are generated in advance and are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have the YaRN method.\n\nDefinition 3 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 21 and the \"NTK-by-parts\" interpolation introduced in Section 3.2.\n\nFor LLaMA and Llama 2 models, we recommend the following values:\n\n<!-- formula-not-decoded -->\n\nThe equation above is found by fitting √ 1 /t at the lowest perplexity against the scale extension by various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and 65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama 2 models (7b, 13b and 70b).",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.4 YaRN",
        "chunkIndex": 29,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-30",
      "content": "s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and 65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama 2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature constant t may have certain degree of \"universality\" and may be generalizable across some models and training data.\n\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned and non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with libraries that modify the attention mechanism such as Flash Attention 2 [13].",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "3.4 YaRN",
        "chunkIndex": 30,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-31",
      "content": "We show that YaRN successfully achieves context window extension of language models using RoPE as its position embedding. Moreover, this result is achieved with only 400 training steps, representing approximately 0.1% of the model's original pre-training corpus, a 10x reduction from Rozière et al. [31] and 2.5x reduction in training steps from Chen et al. [9], making it highly compute-efficient for training with no additional inference costs. We calculate the perplexity of long documents and score\n\non established benchmarks to evaluate the resulting models, finding that they surpass all other context window extension methods.\n\nWe broadly followed the training and evaluation procedures as outlined in [9].",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4 Experiments",
        "chunkIndex": 31,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-32",
      "content": "For training, we extended the Llama 2 [39] 7B and 13B parameter models. No changes were made to the LLaMA model architecture other than the calculation of the embedding frequencies as described in 3.4 with s = 16 and s = 32 .\n\nWe used a learning rate of 2 × 10 -5 with no weight decay and a linear warmup of 20 steps along with AdamW [24] β 1 = 0 . 9 and β 2 = 0 . 95 . For s = 16 we fine-tuned for 400 steps with global batch size 64 using PyTorch [26] Fully Sharded Data Parallelism [42] and Flash Attention 2 [13] on the PG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token. For s = 32 we followed the same procedure, but started from the finished s = 16 checkpoint and trained for an additional 200 steps.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.1 Training",
        "chunkIndex": 32,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-33",
      "content": "In Code Llama [31], a dataset with 16k context was used with a scale factor set to s ≈ 88 . 6 , which corresponds to a context size of 355k. They show that the network extrapolates up to 100k context without ever seeing those context sizes during training. Similar to 3.1 and Rozière et al. [31], YaRN also supports training with a higher scale factor s than the length of the dataset. Due to compute constraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same dataset with 64k context.\n\nWe show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k context during training. Unlike previous \"blind\" interpolation methods, YaRN is much more efficient at transfer learning when increasing the scale s . This demonstrates successful transfer learning from s = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32 model is equivalent to the s = 16 model across the entire context siz",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.2 Extrapolation and Transfer Learning",
        "chunkIndex": 33,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-34",
      "content": "ccessful transfer learning from s = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32 model is equivalent to the s = 16 model across the entire context size, despite only being trained on s = 32 for 200 steps.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.2 Extrapolation and Transfer Learning",
        "chunkIndex": 34,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-35",
      "content": "The evaluations focus on three aspects:\n\n1. the perplexity scores of fine-tuned models with extended context window,\n2. the passkey retrieval task on fine-tuned models,\n3. the common LLM benchmark results of fine-tuned models,",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3 Evaluation",
        "chunkIndex": 35,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-36",
      "content": "To evaluate the long sequence language modeling performances, we use the GovReport [18] and Proof-pile [4] datasets both of which contain many long sequence samples. For all evaluations, the test splits of both datasets were used exclusively. All perplexity evaluations were calculated using the sliding window method from Press et al. [27] with S = 256 .\n\nFirstly, we evaluated how the model performed as the context window increased. We selected 10 random samples from Proof-pile with at least 128k tokens each and evaluated the perplexity of each of these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens.\n\nTable 1 shows a side-by-side comparison of Llama-2 model extended from 4096 to 8192 context length via PI (LLongMA-2 7b 5 ), \"NTK-aware\" and YaRN. Note that PI and \"NTK-aware\" models were trained using the methodology in Chen et al. [9], while YaRN used the same methodology but 2.5x less training steps and data, as described in 4.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 36,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-37",
      "content": "are\" and YaRN. Note that PI and \"NTK-aware\" models were trained using the methodology in Chen et al. [9], while YaRN used the same methodology but 2.5x less training steps and data, as described in 4.\n\n5 LLongMA-2 7b [28] is fine-tuned from Llama-2 7b, trained at 8k context length with PI using the RedPajama dataset [12].\n\nTable 1: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents over Llama-2 extended via PI, NTK and YaRN\n\n| Extension       | Trained   | Context   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |\n|-----------------|-----------|-----------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Method          | Tokens    | Window    |                          2048    |                          4096    |",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 37,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-38",
      "content": "---------------|----------------------------------|----------------------------------|\n| Method          | Tokens    | Window    |                          2048    |                          4096    |                          6144    |                          8192    |                         10240    |\n| PI ( s = 2 )    | 1B        | 8k        |                             3.92 |                             3.51 |                             3.51 |                             3.34 |                             8.07 |\n| NTK ( θ = 20 k) | 1B        | 8k        |                             4.2  |                             3.75 |                             3.74 |                             3.59 |                             6.24 |\n| YaRN ( s = 2 )  | 400M      | 8k        |                             3.91 |                             3.5  |                             3.51 |                             3.35 |                             6.04 |",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 38,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-39",
      "content": "| 400M      | 8k        |                             3.91 |                             3.5  |                             3.51 |                             3.35 |                             6.04 |\n\nWe further evaluated YaRN at the scale factor s = 16 , 32 and compared them against a few opensource models fine-tuned from Llama-2 and extended to more than 32k context window such as Together.ai [37] and \"NTK-aware\" Code Llama [31]. The results are summarized in Table 2 (with a more detailed plot in Figure 1).\n\nTable 2: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|--------------|-----------------|------------------|--------------------|----------------------------------",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 39,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-40",
      "content": "ntext Window Size   | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|--------------|-----------------|------------------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n|              |                 |                  |                    |                          8192    |                         32768    | 65536                            | 98304                            | 131072                           |\n| 7B           | Together        | 32k              | PI                 |                             3.5  |                             2.64 | > 10 2                           | > 10 3                           | > 10 4                           |\n| 7B           | Code Llama      | 100k             | NTK                |                             3.71 |                             2.74 | 2.55",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 40,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-41",
      "content": "| > 10 4                           |\n| 7B           | Code Llama      | 100k             | NTK                |                             3.71 |                             2.74 | 2.55                             | 2.54                             | 2.71                             |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.51 |                             2.65 | 2.42                             | > 10 1                           | > 10 1                           |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.56 |                             2.7  | 2.45                             | 2.36                             | 2.37                             |\n| 13B          | Code Llama      | 100k             | NTK                |                             3.54 |                             2.63 | 2.41                             | 2.37",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 41,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-42",
      "content": "| Code Llama      | 100k             | NTK                |                             3.54 |                             2.63 | 2.41                             | 2.37                             | 2.54                             |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.25 |                             2.5  | 2.29                             | > 10 1                           | > 10 1                           |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.29 |                             2.53 | 2.31                             | 2.23                             | 2.24                             |\n\nWe observe that the model exhibits strong performance across the entire targeted context size, with YaRN interpolation being the first method to successfully extend the effective context size of Llama 2 to 128k.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 42,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-43",
      "content": "that the model exhibits strong performance across the entire targeted context size, with YaRN interpolation being the first method to successfully extend the effective context size of Llama 2 to 128k. Of particular note are the YaRN ( s = 32 ) models, which show continued declining perplexity through 128k, despite the fine-tuning data being limited to 64k tokens in length, demonstrating that the model is able to generalize to unseen context lengths.\n\nFurthermore, in Appendix B.1, we show the results of the average perplexity on 50 untruncated GovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal context window without Dynamic Scaling in Table 4. Similar to the Proof-pile results, the GovReport results show that fine-tuning with YaRN achieves good performance on long sequences.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.1 Long Sequence Language Modeling",
        "chunkIndex": 43,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-44",
      "content": "The passkey retrieval task as defined in [25] measures a model's ability to retrieve a simple passkey (i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our evaluation of the models, we performed 10 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window on different context window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at 128k context size passes the passkey retrieval task with very high accuracy ( &gt; 99% ) within the entire context window size. We show detailed results in Appendix B.2.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.2 Passkey Retrieval",
        "chunkIndex": 44,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-45",
      "content": "The Hugging Face Open LLM Leaderboard [19] compares a multitude of LLMs across a standardized set of four public benchmarks. Specifically, we use 25-shot ARC-Challenge [11], 10-shot HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23].\n\nTo test the degradation of model performance under context extension, we evaluated our models using this suite and compared it to established scores for the Llama 2 baselines as well as publicly available PI and \"NTK-aware\" models. The results are summarized in Table 3.\n\nTable 3: Performance of context window extensions methods on the Hugging Face Open LLM benchmark suite compared with original Llama 2 baselines\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   ARC-c |   Hellaswag |   MMLU |   TruthfulQA |\n|--------------|-----------------|------------------|--------------------|---------|-------------|--------|--------------|\n| 7B           | Llama 2         | 4k               | None               |    53.1 |",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.3 Standardized Benchmarks",
        "chunkIndex": 45,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-46",
      "content": "|-----------------|------------------|--------------------|---------|-------------|--------|--------------|\n| 7B           | Llama 2         | 4k               | None               |    53.1 |        77.8 |   43.8 |         39   |\n| 7B           | Together        | 32k              | PI                 |    47.6 |        76.1 |   43.3 |         39.2 |\n| 7B           | Code Llama      | 100k             | NTK                |    39.9 |        60.8 |   31.1 |         37.8 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |    52.3 |        78.8 |   42.5 |         38.2 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |    52.1 |        78.4 |   41.7 |         37.3 |\n| 13B          | Llama 2         | 4k               | None               |    59.4 |        82.1 |   55.8 |         37.4 |\n| 13B          | Code Llama      | 100k             | NTK                |    40.9 |        63.4 |   32.8 |         43.8 |\n| 13B          | YaRN ( s = 16",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.3 Standardized Benchmarks",
        "chunkIndex": 46,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-47",
      "content": "59.4 |        82.1 |   55.8 |         37.4 |\n| 13B          | Code Llama      | 100k             | NTK                |    40.9 |        63.4 |   32.8 |         43.8 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |    58.1 |        82.3 |   52.8 |         37.8 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |    58   |        82.2 |   51.9 |         37.3 |\n\nWe observe that there is minimal performance degradation between the YaRN models and their respective Llama 2 baselines. We also observe that there was on average a 0.49% drop in scores between the YaRN s = 16 and s = 32 models. From this we conclude that the the iterative extension from 64k to 128k results in negligible performance loss.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "4.3.3 Standardized Benchmarks",
        "chunkIndex": 47,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-48",
      "content": "In conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods and can act as a drop-in replacement to PI, with no downsides and minimal implementation effort. The fine-tuned models preserve their original abilities on multiple benchmarks while being able to attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with finetuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of which are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of extrapolation with YaRN where it is able to \"train short, and test long\".",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 48,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-49",
      "content": "To aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to train the YaRN models in Table 2, as well as the evaluation code that produced Figure 1 and Tables 1, 2, 3, 4, and 5. The code also contains implementations of various extension methods referenced throughout the paper. For training YaRN, we used the publicly available PG19 dataset [29] tokenized to 64k tokens.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "6 Reproducibility",
        "chunkIndex": 49,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-50",
      "content": "- [1] Mistrallite. URL https://huggingface.co/amazon/MistralLite .\n- [2] Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts). URL https://github.com/QwenLM/Qwen-7B/blob/main/tech\\_memo.md .\n- [3] Long-data collections. URL https://huggingface.co/datasets/togethercomputer/ Long-Data-Collections .\n- [4] Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/ zhangir-azerbayev/proof-pile .\n- [5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022. arXiv: 2204.06745.\n- [6] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 50,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-51",
      "content": "language model, 2022. arXiv: 2204.06745.\n- [6] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware\\_ scaled\\_rope\\_allows\\_llama\\_models\\_to\\_have/ .\n- [7] bloc97. Add NTK-Aware interpolation \"by parts\" correction, 2023. URL https://github. com/jquesnelle/scaled-rope/pull/1 .\n- [8] C. Chen. Transformer Inference Arithmetic, 2022. URL https://kipp.ly/blog/ transformer-inference-arithmetic/ .\n- [9] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation, 2023. arXiv: 2306.15595.\n- [10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 51,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-52",
      "content": "rham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022. arXiv: 2204.02311.\n- [11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv: 1803.05457.\n- [12] T. Computer.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 52,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-53",
      "content": "I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv: 1803.05457.\n- [12] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- [13] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. arXiv: 2307.08691.\n\n- [14] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/ 14mrgpr/dynamically\\_scaled\\_rope\\_further\\_increases/ .\n- [15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning, 2017. arXiv: 1705.03122.\n- [16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length generalization for large language models, 2023.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-54",
      "content": "ence to sequence learning, 2017. arXiv: 1705.03122.\n- [16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length generalization for large language models, 2023. arXiv: 2308.16137.\n- [17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n- [18] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1419-1436. Association for Computational Linguistics, June 2021.\n- [19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/ HuggingFaceH4/open\\_llm\\_leaderboard .\n- [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-55",
      "content": "ce. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/ HuggingFaceH4/open\\_llm\\_leaderboard .\n- [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n- [21] kaiokendev. Things I'm learning while training superhot., 2023. URL https://kaiokendev. github.io/til#extending-context-to-8k .\n- [22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional encoding on length generalization in transformers, 2023. arXiv: 2305.19466.\n- [23] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214-3252, May 2022.\n- [24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-56",
      "content": "the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214-3252, May 2022.\n- [24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019.\n- [25] A. Mohtashami and M. Jaggi. Landmark attention: Random-access infinite context length for transformers, 2023. arXiv: 2305.16300.\n- [26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS , pages 8024-8035, 2019.\n- [27] O. Press, N. Smith, and M. Lewis. Train Short, Test Long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations , 2022.\n- [28] J. Quesnelle, E. Shippole, and \"Kaiokendev\".",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-57",
      "content": "n Short, Test Long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations , 2022.\n- [28] J. Quesnelle, E. Shippole, and \"Kaiokendev\". Llongma: Scaling rotary embeddings through linear positional interpolation. https://huggingface.co/conceptofmind/LLongMA-2-7b/ , 2023.\n- [29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations , 2020.\n- [30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y. Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. Technical report, Google, 2019.\n\n- [31] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-58",
      "content": "Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code Llama: Open foundation models for code, 2023. arXiv: 2308.12950.\n- [32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n- [33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope , 2023.\n- [34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with rotary position embedding, 2022. arXiv: 2104.09864.\n- [35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-59",
      "content": "34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with rotary position embedding, 2022. arXiv: 2104.09864.\n- [35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer, 2022. arXiv: 2212.10554.\n- [36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n- [37] Together.ai. LLaMA-2-7B-32K, 2023. URL https://huggingface.co/ togethercomputer/LLaMA-2-7B-32K .\n- [38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-60",
      "content": "computer/LLaMA-2-7B-32K .\n- [38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and efficient foundation language models, 2023. arXiv: 2302.13971.\n- [39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-61",
      "content": "eizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n- [40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n- [41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.\n- [42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-62",
      "content": "al Linguistics , 2019.\n- [42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch FSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-63",
      "content": "In Section 3.1, we introduce a change of basis from b to b ′ in the definition of \"NTK-aware\" interpolation method. Here is a short note on its mathematical deduction.\n\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a base-change instead of scaling the frequencies by a fixed factor s . The property we want to guarantee is that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest frequency to stay constant.\n\nWe introduce a new base b ′ such that the last dimension matches the wavelength of linear interpolation with a scale factor s . Since the original RoPE method skips odd dimensions in order to concatenate both cos( 2 πx λ ) and sin( 2 πx λ ) components into a single embedding, the last dimension d ∈ D is | D | -2 .\n\nThe new base b ′ can be chosen so that\n\nSolving for b ′ yields\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "A.1 Short notes on the deduction of \"NTK-aware\" interpolation",
        "chunkIndex": 63,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-64",
      "content": "In Section 3.4, we mention the impact of the factor t inside the softmax computation of attention weights. Here we fix 896 16 k-token documents from RedPajama [12] 6 , and calculate their perplexity scores with different scaling 1 / √ t . The result is in Figure 2. For comparison, recall that our recommended factor in this case ( s = 8 ) is given by the following.\n\n<!-- formula-not-decoded -->\n\n6 We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far as we are aware of.\n\nTo show the impact of the factor 1 / √ t on different token positions, we cut each 16 k-token document into chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in percentages\n\n<!-- formula-not-decoded -->\n\nof each chunk. The plot is shown in Figure 3.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "A.2 The impact of pre-softmax scaling of YaRN on perplexity",
        "chunkIndex": 64,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-65",
      "content": "k-token document into chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in percentages\n\n<!-- formula-not-decoded -->\n\nof each chunk. The plot is shown in Figure 3.\n\nTo further demonstrate the best values of t across all samples over different token positions, we plot the sample counts with minimal perplexity at a given 1 / √ t for each of the 8 position segments over the 16 k-token range in Figure 4.\n\nWe observe that:\n\n- for a suitable t , a sample may obtain better perplexity scores across the extended context window;\n- the best value of t is mostly consistent across different samples and different positions.\n\nWe remark that this finding is consistent for different values of s and the best value of t follows our recommended formula (Eq. 22) closely.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "A.2 The impact of pre-softmax scaling of YaRN on perplexity",
        "chunkIndex": 65,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-66",
      "content": "In Section 4.3.1, we mention the evaluation on GovReport documents. The evaluation results are detailed in Table 4 below.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.1 GovReport evaluations",
        "chunkIndex": 66,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-67",
      "content": "Here we can observe that the lowest perplexity point alone does not provide a comprehensive depiction on the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits increasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at a context length of 128k. This suggest that while the output of Code Llama might start to degrade in quality above 100k context size, it is still able to maintain strong retrieval capabilities.\n\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while having a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a great indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine long context performance. This also suggests that the YaRN models with s = 16 might be relatively undertrained for the passkey retrieval task.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.2 Passkey Retrieval",
        "chunkIndex": 67,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-68",
      "content": "We first recall from Section 3.3 that the Dynamic Scaling technique is an inference-time technique that dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN. We choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding window of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN. Since the original maximal context length of Llama 2 is 4096 , we observe that Dynamic Scaling effectively extend the inference length and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in Figure 5.\n\nWe see that\n\n- Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained context window;\n- Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained Llama-2 without any finetuning.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 68,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-69",
      "content": "the blow-up of perplexity score beyond pretrained context window;\n- Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained Llama-2 without any finetuning.\n\n<!-- image -->\n\nFigure 2: Fix s = 8 , compare the LLaMA 7b perplexity on 896 16 k-token documents over different scaling 1 / √ t . The shaded area represents 1 standard deviation ( 68% ).\n\nFigure 3: Fix s = 8 , compare the mean of perplexity change percentages ppl ( t ) -ppl ( t = 1) ppl ( t = 1) at different segments of token positions on 896 16 k-token documents over different scaling 1 / √ t .\n\n<!-- image -->\n\nTable 4: Sliding window perplexity ( S = 256 ) of 50 long GovReport documents with a fixed context window size of 32k\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   Perplexity |\n|--------------|-----------------|------------------|--------------------|--------------|\n| 7B           | Together        | 32k              | PI                 |         3.67 |\n| 7B",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 69,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-70",
      "content": "erplexity |\n|--------------|-----------------|------------------|--------------------|--------------|\n| 7B           | Together        | 32k              | PI                 |         3.67 |\n| 7B           | Code Llama      | 100k             | NTK                |         4.44 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |         3.59 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |         3.64 |\n| 13B          | Code Llama      | 100k             | NTK                |         4.22 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |         3.35 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |         3.39 |\n\n<!-- image -->\n\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1 / √ t for a given segment of token positions over the 16 k-token range.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 70,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-71",
      "content": "|         3.39 |\n\n<!-- image -->\n\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1 / √ t for a given segment of token positions over the 16 k-token range.\n\nTable 5: Passkey retrieval performance of various models. The passkey context denotes the maximum tested context window size where the accuracy of passkey retrieval was &gt; = 80% , and the passkey accuracy is the average accuracy of passkey retrieval on all context sizes tested that were smaller or equal than the passkey context size.\n\n| Model Size   | Model Name   |   Scaling Factor ( s ) | Context Window   | Training Data Context   | Extension Method   | Passkey Context   | Passkey Accuracy   |\n|--------------|--------------|------------------------|------------------|-------------------------|--------------------|-------------------|--------------------|\n| 7B           | Together     |                    4   | 32k              | 32k                     | PI                 | 32k",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 71,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-72",
      "content": "-----|--------------------|-------------------|--------------------|\n| 7B           | Together     |                    4   | 32k              | 32k                     | PI                 | 32k               | 100%               |\n| 7B           | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 112k              | 94.3%              |\n| 7B           | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 96.3%              |\n| 7B           | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |\n| 13B          | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 128k              | 99.4%              |\n| 13B          | YaRN         |                   16   | 64k              | 64k                     | YaRN",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 72,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-73",
      "content": "16k                     | NTK                | 128k              | 99.4%              |\n| 13B          | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 97.5%              |\n| 13B          | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |\n\nFigure 5: The comparison between RoPE, Dynamic-PI and Dynamic-YaRN using Llama 2 on a long GovReport sample. This model has not been finetuned for long context.\n\n<!-- image -->\n\nFigure 6: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.3 Dynamic scaling on models without any fine-tuning",
        "chunkIndex": 73,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-74",
      "content": "We additionally extended the Mistral 7B v0.1 model [20], which broadly follows the Llama architecture. For Mistral we trained a 64k context window model ( s = 8 ) for 1000 steps using 16k sequence lengths with a constant learning rate of 1 × 10 -6 . The model's sliding window attention size was set to the context window size, effectively disabling sliding window attention. We then trained for an additional 500 steps at s = 16 to arrive at a 128k context window model. The training data was a mix of the pre-train and fine-tune splits of Together Computer's Long-Data Collections [3].\n\nWe evaluated the models following the same procedure as described in 4.3.1, comparing against the base v0.1 model and MistralLite [1], an NTK-aware ( θ = 1 M) version of v0.1. The results (Figure 6 and Table 6) were consistent with those of the Llama family of models.\n\nTable 6: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.4 Mistral",
        "chunkIndex": 74,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-75",
      "content": "re 6 and Table 6) were consistent with those of the Llama family of models.\n\nTable 6: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n| Model   | Model Name      | Context   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|---------|-----------------|-----------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Size    |                 | Window    |                    |                          4096    |                          8192    |                         16384    | 65536                            | 131072                           |\n| 7B      | Mistral v0.1    | 8k        | -                  |",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.4 Mistral",
        "chunkIndex": 75,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-76",
      "content": "8192    |                         16384    | 65536                            | 131072                           |\n| 7B      | Mistral v0.1    | 8k        | -                  |                             3.09 |                             2.96 |                            36.8  | > 10 3                           | > 10 3                           |\n| 7B      | MistralLite     | 16k       | NTK                |                             3.26 |                             3.13 |                            47.3  | > 10 3                           | > 10 3                           |\n| 7B      | YaRN ( s = 8 )  | 64k       | YaRN               |                             3.18 |                             3.04 |                             2.65 | 2.20                             | 57.4                             |\n| 7B      | YaRN ( s = 16 ) | 128k      | YaRN               |                             3.21 |                             3.08 |                             2.68 | 2.",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.4 Mistral",
        "chunkIndex": 76,
        "totalChunks": 78
      }
    },
    {
      "id": "2309.00071v2-chunk-77",
      "content": "|\n| 7B      | YaRN ( s = 16 ) | 128k      | YaRN               |                             3.21 |                             3.08 |                             2.68 | 2.24                             | 2.19                             |",
      "metadata": {
        "source": "arxiv:2309.00071v2",
        "title": "YaRN: Efficient Context Window Extension of Large Language Models",
        "authors": [
          "Bowen Peng",
          "Jeffrey Quesnelle",
          "Honglu Fan",
          "Enrico Shippole"
        ],
        "section": "B.4 Mistral",
        "chunkIndex": 77,
        "totalChunks": 78
      }
    }
  ],
  "fullText": "## YaRN: Efficient Context Window Extension of Large Language Models\n\nBowen Peng 1 ∗\n\nJeffrey Quesnelle 1 †\n\nHonglu Fan 23\n\nEnrico Shippole ‡\n\n1 Nous Research\n\n2 EleutherAI\n\n3 University of Geneva\n\n## Abstract\n\nRotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn .\n\nFigure 1: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n<!-- image -->\n\n∗ Reddit: /u/bloc97 GitHub: bloc97\n\n† Reddit: /u/emozilla X: @theemozilla GitHub: jquesnelle\n\n‡ X: @EnricoShippole GitHub: conceptofmind\n\n## 1 Introduction\n\nTransformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for many natural language processing (NLP) tasks where long-range abilities such as in-context learning (ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the context window ) determined by its training processes has been one of the major limits of a pretrained LLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or without fine-tuning) has become more and more desirable. To this end, the position encodings of transformers are the center of the discussions.\n\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later improved to a learnable absolute position encoding [15]. Since then, relative positional encoding schemes [32] have further increased the performance of Transformers. Currently, the most popular relative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].\n\nOne reoccurring limitation with positional encodings is the inability to generalize past the context window seen during training. While some methods such as ALiBi are able to do limited generalization, none are able to generalize to sequences significantly longer than their pre-trained length [22].\n\nSome works have been done to overcome such limitation. [9] and concurrently [21] proposed to extend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning on a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by taking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\" interpolation have been proposed, with different emphasis:\n\n- the \"Dynamic NTK\" interpolation method [14] for pre-trained models without fine-tuning.\n- the \"NTK-by-parts\" interpolation method [7] which performs the best when fine-tuned on a small amount of longer-context data.\n\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their presence in the open-source models such as Code Llama [31] (using \"NTK-aware\" interpolation) and Qwen 7B [2] (using \"Dynamic NTK\").\n\nIn this paper, in addition to making a complete account of the previous unpublished works on the \"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet another RoPE extensioN method), an improved method to efficiently extend the context window of models trained with Rotary Position Embeddings (RoPE) including the LLaMA [38], the GPTNeoX [5], and the PaLM [10] families of models.\n\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less than ∼ 0.1% of the original pre-training data. In the meantime, by combining with the inference-time technique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window extension without any fine-tuning.\n\n## 2 Background and Related Work\n\n## 2.1 Rotary Position Embeddings\n\nThe basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]. We work on a hidden layer where the set of hidden neurons are denoted by D . Given a sequence of vectors x 1 , · · · , x L ∈ R | D | , following the notation of [34], the attention layer first converts the vectors into the query vectors and the key vectors:\n\n<!-- formula-not-decoded -->\n\nNext, the attention weights are calculated as\n\n<!-- formula-not-decoded -->\n\nwhere q m , k n are considered as column vectors so that q T m k n is simply the Euclidean inner product. In RoPE, we first assume that | D | is even and identify the embedding space and the hidden states as\n\ncomplex vector spaces:\n\n<!-- formula-not-decoded -->\n\nwhere the inner product q T k becomes the real part of the standard Hermitian inner product Re ( q ∗ k ) . More specifically, the isomorphisms interleave the real part and the complex part\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nTo convert embeddings x m , x n into query and key vectors, we are first given R -linear operators\n\n<!-- formula-not-decoded -->\n\nIn complex coordinates, the functions f q , f k are given by\n\n<!-- formula-not-decoded -->\n\nwhere θ = diag ( θ 1 , · · · , θ | D | / 2 ) is the diagonal matrix with θ d = b -2 d/ | D | and b = 10000 . This way, RoPE associates each (complex-valued) hidden neuron with a separate frequency θ d . The benefit of doing so is that the dot product between the query vector and the key vector only depends on the relative distance m -n as follows\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn real coordinates, the RoPE can be written using the following function\n\n<!-- formula-not-decoded -->\n\nso that\n\n## 2.2 Position Interpolation\n\nAs language models are usually pre-trained with a fixed context length, it is natural to ask how to extend the context length by fine-tuning on relatively less amount of data. For language models using RoPE as the position embedding, Chen et al. [9], and concurrently kaiokendev [21] proposed the Position Interpolation (PI) to extend the context length beyond the pre-trained limit. While a direct extrapolation does not perform well on sequences w 1 , · · · , w L with L larger than the pre-trained limit, they discovered that interpolating the position indicies within the pre-trained limit works well with the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with RoPE, they modify the RoPE by\n\n<!-- formula-not-decoded -->\n\nwhere L ′ &gt; L is a new context window beyond the pre-trained limit. With the original pre-trained model plus the modified RoPE formula, they fine-tuned the language model further on several orders of magnitude fewer tokens (a few billion in Chen et al. [9]) and successfully acheived context window extension.\n\n<!-- formula-not-decoded -->\n\n## 2.3 Additional Notation\n\nThe ratio between the extended context length and the original context length has been of special importance, and we introduce the notation s defined by\n\n<!-- formula-not-decoded -->\n\nand we call s the scale factor .\n\nWe also rewrite and simplify Eq. 10 into the following general form:\n\n<!-- formula-not-decoded -->\n\nwhere g ( m ) , h ( θ d ) are method-dependent functions. For PI, we have g ( m ) = m/s,h ( θ d ) = θ d . In the subsequent sections, when we introduce a new interpolation method, we sometimes only specify the functions g ( m ) and h ( θ d ) .\n\nAdditionally, we define λ d as the wavelength of the RoPE embedding at d -th hidden dimension:\n\n<!-- formula-not-decoded -->\n\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension d to perform a full rotation ( 2 π ).\n\nGiven that some interpolation methods (eg. PI) do not care about the wavelength of the dimensions, we will refer to those methods as \"blind\" interpolation methods, while others do (eg. YaRN), which we will classify as \"targeted\" interpolation methods.\n\n## 2.4 Related work\n\nReRoPE [33] also aims to extend the context size of existing models pre-trained with RoPE, and claims \"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically decreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves context extension by modifying the attention mechanism and thus is not purely an embedding interpolation method. Since it is currently not compatible with Flash Attention 2 [13] and requires two attention passes during inference, we do not consider it for comparison.\n\nConcurrently with our work, LM-Infinite [16] proposes similar ideas to YaRN, but focuses on \"on-thefly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism of the models, it is not an embedding interpolation method and is not immediately compatible with Flash Attention 2.\n\n## 3 Methodology\n\nWhereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound described by PI [9] is insufficient at predicting the complex dynamics between RoPE and the LLM's internal embeddings. In the following subsections, we describe the main issues with PI we have individually identified and solved, so as to give the readers the context, origin and justifications of each method which we use in concert to obtain the full YaRN method.\n\n## 3.1 Loss of High Frequency information - \"NTK-aware\" interpolation\n\nIf we look at RoPE only from an information encoding perspective, it was shown in [36], using Neural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency information if the input dimension is low and the corresponding embeddings lack high frequency components. Here we can see the similarities: a token's positional information is one-dimensional, and RoPE expands it to an n-dimensional complex vector embedding.\n\nRoPE closely resembles Fourier Features [36] in many aspects, as it is possible to define RoPE as a special 1D case of a Fourier Feature. Stretching the RoPE embeddings indiscriminately results in the loss of important high frequency details which the network needs in order to resolve tokens that are both very similar and very close together (the rotation describing the smallest distance needs to not be too small for the network to be able to detect it).\n\nWe hypothesise that the slight increase of perplexity for short context sizes after fine-tuning on larger context sizes seen in PI [9] might be related to this problem. Under ideal circumstances, there is no reason that fine-tuning on larger context sizes should degrade the performance of smaller context sizes.\n\nIn order to resolve the problem of losing high frequency information when interpolating the RoPE embeddings, the \"NTK-aware\" interpolation was developed in [6]. Instead of scaling every dimension of RoPE equally by a factor s , we spread out the interpolation pressure across multiple dimensions by scaling high frequencies less and low frequencies more. One can obtain such a transformation in many ways, but the simplest would be to perform a base change on the value of θ .\n\nMore precisely, following the notations set out in Section 2.3, we define the \"NTK-aware\" interpolation scheme as follows (see the Appendix A.1 for the details of the deduction).\n\nDefinition 1 The \"NTK-aware\" interpolation is a modification of RoPE by using Eq. 12 with the following functions.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nGiven the results from [6], this method performs much better at extending the context size of non-finetuned models compared to PI [9]. However, one major disadvantage of this method is that given it is not just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values, thus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore, due to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true context extension scale. In practice, the scale value s has to be set higher than the expected scale for a given context length extension.\n\nWe note that shortly before the release of this article, Code Llama [31] was released and uses \"NTK-aware\" scaling by manually scaling the base b to 1M.\n\n## 3.2 Loss of Relative Local Distances - \"NTK-by-parts\" interpolation\n\nIn the case of blind interpolation methods like PI and \"NTK-aware\" interpolation, we treat all the RoPE hidden dimensions equally (as in they have the same effect on the network). However, there are strong clues that point us towards the need for targeted interpolation methods.\n\nIn this section, we think heavily in terms of the wavelengths λ d defined in Eq. 13 in the formula of RoPE. For simplicity, we omit the subscript d in λ d and the reader is encouraged to think about λ as the wavelength of an arbitrary periodic function.\n\nOne interesting observation of RoPE embeddings is that given a context size L , there are some dimensions d where the wavelength is longer than the maximum context length seen during pretraining ( λ &gt; L ), this suggests that some dimensions' embeddings might not be distributed evenly in the rotational domain. In such cases, we presume having all unique position pairs implies that the absolute positional information remains intact. On the contrary, when the wavelength is short, only relative positional information is accessible to the network.\n\nMoreover, when we stretch all the RoPE dimensions either by a scale s or using a base change b ′ , all tokens become closer to each other, as the dot product of two vectors rotated by a lesser amount is bigger. This scaling severely impairs a LLM's ability to understand small and local relationships between its internal embeddings. We hypothesize that such compression leads to the model being confused on the positional order of close-by tokens, and consequently harming the model's abilities.\n\nwhere\n\nIn order to remedy this issue, given the two previous observations that we have found, we choose not to interpolate the higher frequency dimensions at all while always interpolating the lower frequency dimensions. In particular,\n\n- if the wavelength λ is much smaller than the context size L , we do not interpolate;\n- if the wavelength λ is equal to or bigger than the context size L , we want to only interpolate and avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n- dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\n\nAs a result, it is more convenient to introduce the ratio r = L λ between the original context size L and the wavelength λ . In the d -th hidden state, the ratio r depends on d in the following way:\n\n<!-- formula-not-decoded -->\n\nIn order to define the boundary of the different interpolation strategies as above, we introduce two extra parameters α, β . All hidden dimensions d where r ( d ) &lt; α are those where we linearly interpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r ( d ) &gt; β are those where we do not interpolate at all. Define the ramp function γ to be\n\n<!-- formula-not-decoded -->\n\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\n\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the following functions 4 .\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe values of α and β should be tuned on a case-by-case basis. For example, we have found experimentally that for the Llama family of models, good values for α and β are α = 1 and β = 32 .\n\nUsing the techniques described in this section, a variant of the resulting method was released under the name \"NTK-by-parts\" interpolation [7]. This improved method performs better than the previous PI [9] and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with fine-tuned models, as shown in [7].\n\n## 3.3 Dynamic Scaling - \"Dynamic NTK\" interpolation\n\nIn a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to the maximal context size. A typical example is the autoregressive generation where the sequence lengths increment by 1 after each step. There are two ways of applying an interpolation method that uses a scale factor s (including PI, \"NTK-aware\" and \"NTK-by-parts\"):\n\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale factor s = L ′ /L where L ′ is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max (1 , l ′ /L ) where l ′ is the sequence length of the current sequence.\n\nThe problem of (1) is that the model may experience a performance discount at a length less than L and an abrupt degradation when the sequence length is longer than L ′ . But by doing Dynamic\n\n4 The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over θ d /s and θ d converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of implementation, but both would work.\n\nScaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting the trained context limit L ′ . We call this inference-time method the Dynamic Scaling method. When it is combined with \"NTK-awared\" interpolation, we call it \"Dynamic NTK\" interpolation. It first appeared in public as a reddit post in [14].\n\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pretrained on L without any finetuning ( L ′ = L ). This is supported by the experiment in Appendix B.3.\n\nOften in the repeated forward-passes, the kv-caching [8] is applied so that we can reuse the previous key-value vectors and improve the overall efficiency. We point out that in some implementations when the RoPE embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling with kv-caching. The correct implementation should cache the kv-embeddings before applying RoPE, as the RoPE embedding of every token changes when s changes.\n\n## 3.4 YaRN\n\nIn addition to the previous interpolation techniques, we also observe that introducing a temperature t on the logits before the attention softmax has a uniform impact on perplexity regardless of the data sample and the token position over the extended context window (See Appendix A.2). More precisely, instead of Eq. 2, we modify the computation of attention weights into\n\n<!-- formula-not-decoded -->\n\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of this attention scaling: we can instead use a \"length scaling\" trick which scales both q m and k n by a constant factor √ 1 /t by simply scaling the complex RoPE embeddings by the same amount. With this, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it has zero overhead during both inference and training, as RoPE embeddings are generated in advance and are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have the YaRN method.\n\nDefinition 3 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 21 and the \"NTK-by-parts\" interpolation introduced in Section 3.2.\n\nFor LLaMA and Llama 2 models, we recommend the following values:\n\n<!-- formula-not-decoded -->\n\nThe equation above is found by fitting √ 1 /t at the lowest perplexity against the scale extension by various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and 65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama 2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature constant t may have certain degree of \"universality\" and may be generalizable across some models and training data.\n\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned and non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with libraries that modify the attention mechanism such as Flash Attention 2 [13].\n\n## 4 Experiments\n\nWe show that YaRN successfully achieves context window extension of language models using RoPE as its position embedding. Moreover, this result is achieved with only 400 training steps, representing approximately 0.1% of the model's original pre-training corpus, a 10x reduction from Rozière et al. [31] and 2.5x reduction in training steps from Chen et al. [9], making it highly compute-efficient for training with no additional inference costs. We calculate the perplexity of long documents and score\n\non established benchmarks to evaluate the resulting models, finding that they surpass all other context window extension methods.\n\nWe broadly followed the training and evaluation procedures as outlined in [9].\n\n## 4.1 Training\n\nFor training, we extended the Llama 2 [39] 7B and 13B parameter models. No changes were made to the LLaMA model architecture other than the calculation of the embedding frequencies as described in 3.4 with s = 16 and s = 32 .\n\nWe used a learning rate of 2 × 10 -5 with no weight decay and a linear warmup of 20 steps along with AdamW [24] β 1 = 0 . 9 and β 2 = 0 . 95 . For s = 16 we fine-tuned for 400 steps with global batch size 64 using PyTorch [26] Fully Sharded Data Parallelism [42] and Flash Attention 2 [13] on the PG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token. For s = 32 we followed the same procedure, but started from the finished s = 16 checkpoint and trained for an additional 200 steps.\n\n## 4.2 Extrapolation and Transfer Learning\n\nIn Code Llama [31], a dataset with 16k context was used with a scale factor set to s ≈ 88 . 6 , which corresponds to a context size of 355k. They show that the network extrapolates up to 100k context without ever seeing those context sizes during training. Similar to 3.1 and Rozière et al. [31], YaRN also supports training with a higher scale factor s than the length of the dataset. Due to compute constraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same dataset with 64k context.\n\nWe show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k context during training. Unlike previous \"blind\" interpolation methods, YaRN is much more efficient at transfer learning when increasing the scale s . This demonstrates successful transfer learning from s = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32 model is equivalent to the s = 16 model across the entire context size, despite only being trained on s = 32 for 200 steps.\n\n## 4.3 Evaluation\n\nThe evaluations focus on three aspects:\n\n1. the perplexity scores of fine-tuned models with extended context window,\n2. the passkey retrieval task on fine-tuned models,\n3. the common LLM benchmark results of fine-tuned models,\n\n## 4.3.1 Long Sequence Language Modeling\n\nTo evaluate the long sequence language modeling performances, we use the GovReport [18] and Proof-pile [4] datasets both of which contain many long sequence samples. For all evaluations, the test splits of both datasets were used exclusively. All perplexity evaluations were calculated using the sliding window method from Press et al. [27] with S = 256 .\n\nFirstly, we evaluated how the model performed as the context window increased. We selected 10 random samples from Proof-pile with at least 128k tokens each and evaluated the perplexity of each of these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens.\n\nTable 1 shows a side-by-side comparison of Llama-2 model extended from 4096 to 8192 context length via PI (LLongMA-2 7b 5 ), \"NTK-aware\" and YaRN. Note that PI and \"NTK-aware\" models were trained using the methodology in Chen et al. [9], while YaRN used the same methodology but 2.5x less training steps and data, as described in 4.\n\n5 LLongMA-2 7b [28] is fine-tuned from Llama-2 7b, trained at 8k context length with PI using the RedPajama dataset [12].\n\nTable 1: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents over Llama-2 extended via PI, NTK and YaRN\n\n| Extension       | Trained   | Context   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |\n|-----------------|-----------|-----------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Method          | Tokens    | Window    |                          2048    |                          4096    |                          6144    |                          8192    |                         10240    |\n| PI ( s = 2 )    | 1B        | 8k        |                             3.92 |                             3.51 |                             3.51 |                             3.34 |                             8.07 |\n| NTK ( θ = 20 k) | 1B        | 8k        |                             4.2  |                             3.75 |                             3.74 |                             3.59 |                             6.24 |\n| YaRN ( s = 2 )  | 400M      | 8k        |                             3.91 |                             3.5  |                             3.51 |                             3.35 |                             6.04 |\n\nWe further evaluated YaRN at the scale factor s = 16 , 32 and compared them against a few opensource models fine-tuned from Llama-2 and extended to more than 32k context window such as Together.ai [37] and \"NTK-aware\" Code Llama [31]. The results are summarized in Table 2 (with a more detailed plot in Figure 1).\n\nTable 2: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|--------------|-----------------|------------------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n|              |                 |                  |                    |                          8192    |                         32768    | 65536                            | 98304                            | 131072                           |\n| 7B           | Together        | 32k              | PI                 |                             3.5  |                             2.64 | > 10 2                           | > 10 3                           | > 10 4                           |\n| 7B           | Code Llama      | 100k             | NTK                |                             3.71 |                             2.74 | 2.55                             | 2.54                             | 2.71                             |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.51 |                             2.65 | 2.42                             | > 10 1                           | > 10 1                           |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.56 |                             2.7  | 2.45                             | 2.36                             | 2.37                             |\n| 13B          | Code Llama      | 100k             | NTK                |                             3.54 |                             2.63 | 2.41                             | 2.37                             | 2.54                             |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.25 |                             2.5  | 2.29                             | > 10 1                           | > 10 1                           |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.29 |                             2.53 | 2.31                             | 2.23                             | 2.24                             |\n\nWe observe that the model exhibits strong performance across the entire targeted context size, with YaRN interpolation being the first method to successfully extend the effective context size of Llama 2 to 128k. Of particular note are the YaRN ( s = 32 ) models, which show continued declining perplexity through 128k, despite the fine-tuning data being limited to 64k tokens in length, demonstrating that the model is able to generalize to unseen context lengths.\n\nFurthermore, in Appendix B.1, we show the results of the average perplexity on 50 untruncated GovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal context window without Dynamic Scaling in Table 4. Similar to the Proof-pile results, the GovReport results show that fine-tuning with YaRN achieves good performance on long sequences.\n\n## 4.3.2 Passkey Retrieval\n\nThe passkey retrieval task as defined in [25] measures a model's ability to retrieve a simple passkey (i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our evaluation of the models, we performed 10 iterations of the passkey retrieval task with the passkey placed at a random location uniformly distributed across the evaluation context window on different context window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at 128k context size passes the passkey retrieval task with very high accuracy ( &gt; 99% ) within the entire context window size. We show detailed results in Appendix B.2.\n\n## 4.3.3 Standardized Benchmarks\n\nThe Hugging Face Open LLM Leaderboard [19] compares a multitude of LLMs across a standardized set of four public benchmarks. Specifically, we use 25-shot ARC-Challenge [11], 10-shot HellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23].\n\nTo test the degradation of model performance under context extension, we evaluated our models using this suite and compared it to established scores for the Llama 2 baselines as well as publicly available PI and \"NTK-aware\" models. The results are summarized in Table 3.\n\nTable 3: Performance of context window extensions methods on the Hugging Face Open LLM benchmark suite compared with original Llama 2 baselines\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   ARC-c |   Hellaswag |   MMLU |   TruthfulQA |\n|--------------|-----------------|------------------|--------------------|---------|-------------|--------|--------------|\n| 7B           | Llama 2         | 4k               | None               |    53.1 |        77.8 |   43.8 |         39   |\n| 7B           | Together        | 32k              | PI                 |    47.6 |        76.1 |   43.3 |         39.2 |\n| 7B           | Code Llama      | 100k             | NTK                |    39.9 |        60.8 |   31.1 |         37.8 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |    52.3 |        78.8 |   42.5 |         38.2 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |    52.1 |        78.4 |   41.7 |         37.3 |\n| 13B          | Llama 2         | 4k               | None               |    59.4 |        82.1 |   55.8 |         37.4 |\n| 13B          | Code Llama      | 100k             | NTK                |    40.9 |        63.4 |   32.8 |         43.8 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |    58.1 |        82.3 |   52.8 |         37.8 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |    58   |        82.2 |   51.9 |         37.3 |\n\nWe observe that there is minimal performance degradation between the YaRN models and their respective Llama 2 baselines. We also observe that there was on average a 0.49% drop in scores between the YaRN s = 16 and s = 32 models. From this we conclude that the the iterative extension from 64k to 128k results in negligible performance loss.\n\n## 5 Conclusion\n\nIn conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods and can act as a drop-in replacement to PI, with no downsides and minimal implementation effort. The fine-tuned models preserve their original abilities on multiple benchmarks while being able to attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with finetuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of which are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of extrapolation with YaRN where it is able to \"train short, and test long\".\n\n## 6 Reproducibility\n\nTo aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to train the YaRN models in Table 2, as well as the evaluation code that produced Figure 1 and Tables 1, 2, 3, 4, and 5. The code also contains implementations of various extension methods referenced throughout the paper. For training YaRN, we used the publicly available PG19 dataset [29] tokenized to 64k tokens.\n\n## References\n\n- [1] Mistrallite. URL https://huggingface.co/amazon/MistralLite .\n- [2] Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts). URL https://github.com/QwenLM/Qwen-7B/blob/main/tech\\_memo.md .\n- [3] Long-data collections. URL https://huggingface.co/datasets/togethercomputer/ Long-Data-Collections .\n- [4] Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/ zhangir-azerbayev/proof-pile .\n- [5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022. arXiv: 2204.06745.\n- [6] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware\\_ scaled\\_rope\\_allows\\_llama\\_models\\_to\\_have/ .\n- [7] bloc97. Add NTK-Aware interpolation \"by parts\" correction, 2023. URL https://github. com/jquesnelle/scaled-rope/pull/1 .\n- [8] C. Chen. Transformer Inference Arithmetic, 2022. URL https://kipp.ly/blog/ transformer-inference-arithmetic/ .\n- [9] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation, 2023. arXiv: 2306.15595.\n- [10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022. arXiv: 2204.02311.\n- [11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv: 1803.05457.\n- [12] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data .\n- [13] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. arXiv: 2307.08691.\n\n- [14] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/ 14mrgpr/dynamically\\_scaled\\_rope\\_further\\_increases/ .\n- [15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning, 2017. arXiv: 1705.03122.\n- [16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length generalization for large language models, 2023. arXiv: 2308.16137.\n- [17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n- [18] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 1419-1436. Association for Computational Linguistics, June 2021.\n- [19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/ HuggingFaceH4/open\\_llm\\_leaderboard .\n- [20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n- [21] kaiokendev. Things I'm learning while training superhot., 2023. URL https://kaiokendev. github.io/til#extending-context-to-8k .\n- [22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional encoding on length generalization in transformers, 2023. arXiv: 2305.19466.\n- [23] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214-3252, May 2022.\n- [24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations , 2019.\n- [25] A. Mohtashami and M. Jaggi. Landmark attention: Random-access infinite context length for transformers, 2023. arXiv: 2305.16300.\n- [26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS , pages 8024-8035, 2019.\n- [27] O. Press, N. Smith, and M. Lewis. Train Short, Test Long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations , 2022.\n- [28] J. Quesnelle, E. Shippole, and \"Kaiokendev\". Llongma: Scaling rotary embeddings through linear positional interpolation. https://huggingface.co/conceptofmind/LLongMA-2-7b/ , 2023.\n- [29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations , 2020.\n- [30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y. Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. Technical report, Google, 2019.\n\n- [31] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code Llama: Open foundation models for code, 2023. arXiv: 2308.12950.\n- [32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pages 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n- [33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope , 2023.\n- [34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with rotary position embedding, 2022. arXiv: 2104.09864.\n- [35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A length-extrapolatable transformer, 2022. arXiv: 2212.10554.\n- [36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n- [37] Together.ai. LLaMA-2-7B-32K, 2023. URL https://huggingface.co/ togethercomputer/LLaMA-2-7B-32K .\n- [38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and efficient foundation language models, 2023. arXiv: 2302.13971.\n- [39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n- [40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n- [41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.\n- [42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott, S. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch FSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277.\n\n## A Additional details on interpolation methods\n\n## A.1 Short notes on the deduction of \"NTK-aware\" interpolation\n\nIn Section 3.1, we introduce a change of basis from b to b ′ in the definition of \"NTK-aware\" interpolation method. Here is a short note on its mathematical deduction.\n\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a base-change instead of scaling the frequencies by a fixed factor s . The property we want to guarantee is that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest frequency to stay constant.\n\nWe introduce a new base b ′ such that the last dimension matches the wavelength of linear interpolation with a scale factor s . Since the original RoPE method skips odd dimensions in order to concatenate both cos( 2 πx λ ) and sin( 2 πx λ ) components into a single embedding, the last dimension d ∈ D is | D | -2 .\n\nThe new base b ′ can be chosen so that\n\nSolving for b ′ yields\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n## A.2 The impact of pre-softmax scaling of YaRN on perplexity\n\nIn Section 3.4, we mention the impact of the factor t inside the softmax computation of attention weights. Here we fix 896 16 k-token documents from RedPajama [12] 6 , and calculate their perplexity scores with different scaling 1 / √ t . The result is in Figure 2. For comparison, recall that our recommended factor in this case ( s = 8 ) is given by the following.\n\n<!-- formula-not-decoded -->\n\n6 We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far as we are aware of.\n\nTo show the impact of the factor 1 / √ t on different token positions, we cut each 16 k-token document into chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in percentages\n\n<!-- formula-not-decoded -->\n\nof each chunk. The plot is shown in Figure 3.\n\nTo further demonstrate the best values of t across all samples over different token positions, we plot the sample counts with minimal perplexity at a given 1 / √ t for each of the 8 position segments over the 16 k-token range in Figure 4.\n\nWe observe that:\n\n- for a suitable t , a sample may obtain better perplexity scores across the extended context window;\n- the best value of t is mostly consistent across different samples and different positions.\n\nWe remark that this finding is consistent for different values of s and the best value of t follows our recommended formula (Eq. 22) closely.\n\n## B Additional tables and charts\n\n## B.1 GovReport evaluations\n\nIn Section 4.3.1, we mention the evaluation on GovReport documents. The evaluation results are detailed in Table 4 below.\n\n## B.2 Passkey Retrieval\n\nHere we can observe that the lowest perplexity point alone does not provide a comprehensive depiction on the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits increasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at a context length of 128k. This suggest that while the output of Code Llama might start to degrade in quality above 100k context size, it is still able to maintain strong retrieval capabilities.\n\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while having a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a great indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine long context performance. This also suggests that the YaRN models with s = 16 might be relatively undertrained for the passkey retrieval task.\n\n## B.3 Dynamic scaling on models without any fine-tuning\n\nWe first recall from Section 3.3 that the Dynamic Scaling technique is an inference-time technique that dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN. We choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding window of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN. Since the original maximal context length of Llama 2 is 4096 , we observe that Dynamic Scaling effectively extend the inference length and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in Figure 5.\n\nWe see that\n\n- Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained context window;\n- Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained Llama-2 without any finetuning.\n\n<!-- image -->\n\nFigure 2: Fix s = 8 , compare the LLaMA 7b perplexity on 896 16 k-token documents over different scaling 1 / √ t . The shaded area represents 1 standard deviation ( 68% ).\n\nFigure 3: Fix s = 8 , compare the mean of perplexity change percentages ppl ( t ) -ppl ( t = 1) ppl ( t = 1) at different segments of token positions on 896 16 k-token documents over different scaling 1 / √ t .\n\n<!-- image -->\n\nTable 4: Sliding window perplexity ( S = 256 ) of 50 long GovReport documents with a fixed context window size of 32k\n\n| Model Size   | Model Name      | Context Window   | Extension Method   |   Perplexity |\n|--------------|-----------------|------------------|--------------------|--------------|\n| 7B           | Together        | 32k              | PI                 |         3.67 |\n| 7B           | Code Llama      | 100k             | NTK                |         4.44 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |         3.59 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |         3.64 |\n| 13B          | Code Llama      | 100k             | NTK                |         4.22 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |         3.35 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |         3.39 |\n\n<!-- image -->\n\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1 / √ t for a given segment of token positions over the 16 k-token range.\n\nTable 5: Passkey retrieval performance of various models. The passkey context denotes the maximum tested context window size where the accuracy of passkey retrieval was &gt; = 80% , and the passkey accuracy is the average accuracy of passkey retrieval on all context sizes tested that were smaller or equal than the passkey context size.\n\n| Model Size   | Model Name   |   Scaling Factor ( s ) | Context Window   | Training Data Context   | Extension Method   | Passkey Context   | Passkey Accuracy   |\n|--------------|--------------|------------------------|------------------|-------------------------|--------------------|-------------------|--------------------|\n| 7B           | Together     |                    4   | 32k              | 32k                     | PI                 | 32k               | 100%               |\n| 7B           | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 112k              | 94.3%              |\n| 7B           | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 96.3%              |\n| 7B           | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |\n| 13B          | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 128k              | 99.4%              |\n| 13B          | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 97.5%              |\n| 13B          | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |\n\nFigure 5: The comparison between RoPE, Dynamic-PI and Dynamic-YaRN using Llama 2 on a long GovReport sample. This model has not been finetuned for long context.\n\n<!-- image -->\n\nFigure 6: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n<!-- image -->\n\n## B.4 Mistral\n\nWe additionally extended the Mistral 7B v0.1 model [20], which broadly follows the Llama architecture. For Mistral we trained a 64k context window model ( s = 8 ) for 1000 steps using 16k sequence lengths with a constant learning rate of 1 × 10 -6 . The model's sliding window attention size was set to the context window size, effectively disabling sliding window attention. We then trained for an additional 500 steps at s = 16 to arrive at a 128k context window model. The training data was a mix of the pre-train and fine-tune splits of Together Computer's Long-Data Collections [3].\n\nWe evaluated the models following the same procedure as described in 4.3.1, comparing against the base v0.1 model and MistralLite [1], an NTK-aware ( θ = 1 M) version of v0.1. The results (Figure 6 and Table 6) were consistent with those of the Llama family of models.\n\nTable 6: Sliding window perplexity ( S = 256 ) of ten 128k Proof-pile documents truncated to evaluation context window size\n\n| Model   | Model Name      | Context   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|---------|-----------------|-----------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Size    |                 | Window    |                    |                          4096    |                          8192    |                         16384    | 65536                            | 131072                           |\n| 7B      | Mistral v0.1    | 8k        | -                  |                             3.09 |                             2.96 |                            36.8  | > 10 3                           | > 10 3                           |\n| 7B      | MistralLite     | 16k       | NTK                |                             3.26 |                             3.13 |                            47.3  | > 10 3                           | > 10 3                           |\n| 7B      | YaRN ( s = 8 )  | 64k       | YaRN               |                             3.18 |                             3.04 |                             2.65 | 2.20                             | 57.4                             |\n| 7B      | YaRN ( s = 16 ) | 128k      | YaRN               |                             3.21 |                             3.08 |                             2.68 | 2.24                             | 2.19                             |",
  "tables": [
    {
      "index": 0,
      "markdown": "| Extension       | Trained   | Context   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size |\n|-----------------|-----------|-----------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Method          | Tokens    | Window    |                          2048    |                          4096    |                          6144    |                          8192    |                         10240    |\n| PI ( s = 2 )    | 1B        | 8k        |                             3.92 |                             3.51 |                             3.51 |                             3.34 |                             8.07 |\n| NTK ( θ = 20 k) | 1B        | 8k        |                             4.2  |                             3.75 |                             3.74 |                             3.59 |                             6.24 |\n| YaRN ( s = 2 )  | 400M      | 8k        |                             3.91 |                             3.5  |                             3.51 |                             3.35 |                             6.04 |"
    },
    {
      "index": 1,
      "markdown": "| Model Size   | Model Name      | Context Window   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|--------------|-----------------|------------------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n|              |                 |                  |                    |                          8192    |                         32768    | 65536                            | 98304                            | 131072                           |\n| 7B           | Together        | 32k              | PI                 |                             3.5  |                             2.64 | > 10 2                           | > 10 3                           | > 10 4                           |\n| 7B           | Code Llama      | 100k             | NTK                |                             3.71 |                             2.74 | 2.55                             | 2.54                             | 2.71                             |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.51 |                             2.65 | 2.42                             | > 10 1                           | > 10 1                           |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.56 |                             2.7  | 2.45                             | 2.36                             | 2.37                             |\n| 13B          | Code Llama      | 100k             | NTK                |                             3.54 |                             2.63 | 2.41                             | 2.37                             | 2.54                             |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |                             3.25 |                             2.5  | 2.29                             | > 10 1                           | > 10 1                           |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |                             3.29 |                             2.53 | 2.31                             | 2.23                             | 2.24                             |"
    },
    {
      "index": 2,
      "markdown": "| Model Size   | Model Name      | Context Window   | Extension Method   |   ARC-c |   Hellaswag |   MMLU |   TruthfulQA |\n|--------------|-----------------|------------------|--------------------|---------|-------------|--------|--------------|\n| 7B           | Llama 2         | 4k               | None               |    53.1 |        77.8 |   43.8 |         39   |\n| 7B           | Together        | 32k              | PI                 |    47.6 |        76.1 |   43.3 |         39.2 |\n| 7B           | Code Llama      | 100k             | NTK                |    39.9 |        60.8 |   31.1 |         37.8 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |    52.3 |        78.8 |   42.5 |         38.2 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |    52.1 |        78.4 |   41.7 |         37.3 |\n| 13B          | Llama 2         | 4k               | None               |    59.4 |        82.1 |   55.8 |         37.4 |\n| 13B          | Code Llama      | 100k             | NTK                |    40.9 |        63.4 |   32.8 |         43.8 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |    58.1 |        82.3 |   52.8 |         37.8 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |    58   |        82.2 |   51.9 |         37.3 |"
    },
    {
      "index": 3,
      "markdown": "| Model Size   | Model Name      | Context Window   | Extension Method   |   Perplexity |\n|--------------|-----------------|------------------|--------------------|--------------|\n| 7B           | Together        | 32k              | PI                 |         3.67 |\n| 7B           | Code Llama      | 100k             | NTK                |         4.44 |\n| 7B           | YaRN ( s = 16 ) | 64k              | YaRN               |         3.59 |\n| 7B           | YaRN ( s = 32 ) | 128k             | YaRN               |         3.64 |\n| 13B          | Code Llama      | 100k             | NTK                |         4.22 |\n| 13B          | YaRN ( s = 16 ) | 64k              | YaRN               |         3.35 |\n| 13B          | YaRN ( s = 32 ) | 128k             | YaRN               |         3.39 |"
    },
    {
      "index": 4,
      "markdown": "| Model Size   | Model Name   |   Scaling Factor ( s ) | Context Window   | Training Data Context   | Extension Method   | Passkey Context   | Passkey Accuracy   |\n|--------------|--------------|------------------------|------------------|-------------------------|--------------------|-------------------|--------------------|\n| 7B           | Together     |                    4   | 32k              | 32k                     | PI                 | 32k               | 100%               |\n| 7B           | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 112k              | 94.3%              |\n| 7B           | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 96.3%              |\n| 7B           | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |\n| 13B          | Code Llama   |                   88.6 | 100k             | 16k                     | NTK                | 128k              | 99.4%              |\n| 13B          | YaRN         |                   16   | 64k              | 64k                     | YaRN               | 64k               | 97.5%              |\n| 13B          | YaRN         |                   32   | 128k             | 64k                     | YaRN               | 128k              | 99.4%              |"
    },
    {
      "index": 5,
      "markdown": "| Model   | Model Name      | Context   | Extension Method   |   Evaluation Context Window Size |   Evaluation Context Window Size |   Evaluation Context Window Size | Evaluation Context Window Size   | Evaluation Context Window Size   |\n|---------|-----------------|-----------|--------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|----------------------------------|\n| Size    |                 | Window    |                    |                          4096    |                          8192    |                         16384    | 65536                            | 131072                           |\n| 7B      | Mistral v0.1    | 8k        | -                  |                             3.09 |                             2.96 |                            36.8  | > 10 3                           | > 10 3                           |\n| 7B      | MistralLite     | 16k       | NTK                |                             3.26 |                             3.13 |                            47.3  | > 10 3                           | > 10 3                           |\n| 7B      | YaRN ( s = 8 )  | 64k       | YaRN               |                             3.18 |                             3.04 |                             2.65 | 2.20                             | 57.4                             |\n| 7B      | YaRN ( s = 16 ) | 128k      | YaRN               |                             3.21 |                             3.08 |                             2.68 | 2.24                             | 2.19                             |"
    }
  ],
  "stats": {
    "pages": 18,
    "chunksCreated": 78,
    "totalCharacters": 54717,
    "totalWords": 8374,
    "numTables": 6,
    "processingTimeMs": 23075
  }
}