{
  "paper": {
    "id": "2407.12327v5",
    "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
    "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs. To enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at https://github.com/NolanoOrg/SpectraSuite.",
    "authors": [
      "Ayush Kaushal",
      "Tejas Vaidhya",
      "Arnab Kumar Mondal",
      "Tejas Pandey",
      "Aaryan Bhagat",
      "Irina Rish"
    ],
    "published": "2024-07-17T05:53:20.000Z",
    "updated": "2024-10-11T04:44:55.000Z",
    "primaryCategory": "cs.LG",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2407.12327v5",
    "absUrl": "https://arxiv.org/abs/2407.12327v5"
  },
  "chunks": [
    {
      "id": "2407.12327v5-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "chunkIndex": 0,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-1",
      "content": "∗\n\nTejas Vaidhya 1,2,4 ∗\n\ntejas@nolano.ai\n\nAyush Kaushal 1,2 ayush@nolano.ai\n\nTejas Pandey 1,3\n\ntejaspandey2003@kgpian.iitkgp.ac.in\n\nIrina Rish 1, 2, 4\n\nirina.rish@gmail.com\n\n1 Nolano AI , 2 University of Montreal , 3 IIT Kharagpur , 4 Mila - Quebec AI Institute, 5 UC Riverside",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "chunkIndex": 1,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-2",
      "content": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memoryrelated bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite , the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits).",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-3",
      "content": "uantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs. To enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at https://github.com/NolanoOrg/SpectraSuite.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Abstract",
        "chunkIndex": 3,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-4",
      "content": "The computational power of GPUs, measured in FLOPs, is increasing exponentially, doubling approximately every 1.26 years. In contrast, memory capacity and bandwidth are growing at a slower pace, doubling every 2 and 2.9 years, respectively [Gholami et al., 2024]. This disparity highlights that computing capabilities are outpacing advances in memory technology. In Large Language Models (LLMs) inference, the primary bottlenecks are caused by model size (bits), which affects memory usage (memory capacity) and data transfer to processors (memory bandwidth). These issues are\n\n∗ Equal contribution, listed in alphabetical order.\n\nArnab Kumar Mondal 4\n\narnab.mondal@mila.quebec\n\nAaryan Bhagat 5\n\nabhag017@ucr.edu\n\nFigure 1: Common Sense and Reasoning (C&amp;R) &amp; LAMBADA Accuracy for ternary TriLM, FP16 FloatLM and quantized QuantLM models across different model sizes, in bits and number of parameters. C&amp;R scores are averaged across 6 benchmarks.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-5",
      "content": ";R) &amp; LAMBADA Accuracy for ternary TriLM, FP16 FloatLM and quantized QuantLM models across different model sizes, in bits and number of parameters. C&amp;R scores are averaged across 6 benchmarks. At 3B+ scales, TriLMs demonstrate better performance for their size than QuantLM and competitive performance to FloatLM of the same parameters. See Tables 6, 7 and 9 for details.\n\n<!-- image -->\n\nbecoming more critical than the growing number of model parameters which affect the computational limits (FLOPs). For instance, state-of-the-art LLMs such as 340B Nemotron 4 [Nvidia et al., 2024] have sizes (in bits) exceeding the memory capacity of data center GPUs, such as 8xH100s. Token generation speed, or latency, is now limited by memory bandwidth [Kim et al., 2024]. Addressing these bottlenecks requires more expensive training, exceeding Chinchilla's compute-optimal regime [Hoffmann et al., 2022], with less than 10B parameter models being trained on up to 15 trillion tokens [Touvron et al.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-6",
      "content": "s requires more expensive training, exceeding Chinchilla's compute-optimal regime [Hoffmann et al., 2022], with less than 10B parameter models being trained on up to 15 trillion tokens [Touvron et al., 2023b, AI@Meta, 2024, Team et al., 2024]. Another widely used technique is post-training quantization during deployment [Zhu et al., 2023]; however, we demonstrate in Section §5 that this approach is sub-optimal.\n\nIn post-training quantization, LLMs initially trained using 16-bit floating point precision (referred to as FloatLMs) undergo parameter quantization, and the resulting models are termed QuantLMs. These models use optimized kernels for deployment, offering speedups nearly proportional to the compression factor [Frantar and Alistarh, 2024]. However, quantizing to very low bitwidths creates a significant mismatch between the representations of pretrained FloatLM and the deployable QuantLM, resulting in undesired behavior and quality degradation [Li et al., 2024, Huang et al., 2024",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-7",
      "content": "reates a significant mismatch between the representations of pretrained FloatLM and the deployable QuantLM, resulting in undesired behavior and quality degradation [Li et al., 2024, Huang et al., 2024]. Some state-of-the-art methods [Frantar et al., 2022, Egiazarian et al., 2024] mitigate this issue by using calibration and re-training data from target domains; however, this approach increases sensitivity to the calibration data. For instance, seemingly simple choices, like whether to length-normalize the calibration data, can significantly impact QuantLM's performance [Malinovskii et al., 2024]. Other works have observed that QuantLM at 4 bits (4-bit QuantLMs) have about 65% lower knowledge capacity per parameter compared to trained and aligned FloatLMs [Allen-Zhu and Li, 2024].\n\nAn alternate approach to reducing model bitsize while maintaining parameter count involves training neural networks with low effective bitwidths [Zhou et al., 2018].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-8",
      "content": "loatLMs [Allen-Zhu and Li, 2024].\n\nAn alternate approach to reducing model bitsize while maintaining parameter count involves training neural networks with low effective bitwidths [Zhou et al., 2018]. This approach offers compression benefits beyond post-training quantization without its associated drawbacks. While low-bitwidth approaches typically employ binary (1-bit) or ternary quantization (1.58-bit), binary quantization generally underperforms compared to regular FP16 models [Liu et al., 2023a] (see Appendix §B). In contrast, ternary modeling can match performance while significantly reducing memory requirements\n\n(as we demonstrate in section §5). Nevertheless, the relative performance of pretrained low-bitwidth language models compared to QuantLMs across similar sizes (in bits) and similar parameter counts remains unclear . This is a crucial unanswered question, given the high inference costs during the deployment of very large-scale LLMs.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-9",
      "content": "tLMs across similar sizes (in bits) and similar parameter counts remains unclear . This is a crucial unanswered question, given the high inference costs during the deployment of very large-scale LLMs. Moreover, the training dynamics and scaling law of these low-bitwidth models are also poorly understood, partly due to the lack of publicly available systematic suites and associated comparative studies .\n\nMotivated by these challenges, we make the following contributions in this paper:\n\nFeasibility and Scalability of Training Ternary Language Models (TriLMs) We discuss the deployment advantages (in section 2.1) and theoretical feasibility (in section 2.2) of training lowbitwidth models at scale. We then introduce ternary language models (TriLMs) and systematically study their scaling laws compared to FloatLMs. Our analysis reveals that TriLMs offer better scaling behavior in terms of model size, measured in bits (refer to Section 4.3).",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-10",
      "content": "riLMs) and systematically study their scaling laws compared to FloatLMs. Our analysis reveals that TriLMs offer better scaling behavior in terms of model size, measured in bits (refer to Section 4.3). Moreover, as the number of parameters increases, the difference in validation loss between TriLMs and FloatLMs becomes insignificant, indicating TriLM's competitive performance at scale.\n\nSpectra LLM suite. We present Spectra , the first open suite of LLMs spanning many bit-widths. This suite includes FloatLMs, the corresponding QuantLMs at 3, 4, 6, and 8 bits, and ternary LLMs (TriLMs). The latter uses ternary weights {-1, 0, +1}. The suite features 9 models ranging from 99M to 3.9B parameters, all trained on the same 300B token dataset, totalling 54 models. We believe that the Spectra LLM suite makes a valuable contribution to the LLM research community by facilitating comparative studies, exploring the scalability and efficiency of ternary modeling, and improving interpretability from",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-11",
      "content": "uite makes a valuable contribution to the LLM research community by facilitating comparative studies, exploring the scalability and efficiency of ternary modeling, and improving interpretability from neuronal to connection levels.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "1 Introduction",
        "chunkIndex": 11,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-12",
      "content": "We evaluate TriLMs, FloatLMs, and QuantLMs across multiple benchmarks, spanning commonsense, reasoning, knowledge capacity and toxicity. At the billion parameter scale, TriLMs consistently outperform their QuantLM and FloatLM counterparts of the same bit-size across all the aforementioned benchmarks (see Figure 1). Surprisingly, TriLM 3.9B matches the performance of FloatLM 3.9B across all benchmarks, despite getting a higher perplexity and being 5 . 9 times smaller in bitsize.\n\nHowever, we also note that certain challenges remain in TriLMs. For instance, TriLM 3.9B exhibits the same level of toxicity and stereotyping as FloatLM 3.9B, significantly higher than a similarly sized FloatLM 830M when measured in bits. While TriLM 3.9B and FloatLM 3.9B show similar validation perplexity on some datasets, such as Penn Tree Bank and Lambada, a gap persists at this scale on web corpora, both in-domain (i.e., on a test subset of SlimPajama, the same domain used to train the models) and out-of-do",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Evaluation and comparative analysis of TriLMs, FloatLMs, and QuantLMs at different scales.",
        "chunkIndex": 12,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-13",
      "content": "datasets, such as Penn Tree Bank and Lambada, a gap persists at this scale on web corpora, both in-domain (i.e., on a test subset of SlimPajama, the same domain used to train the models) and out-of-domain (e.g., on Dolma, C4 and RefinedWeb datasets). We provide detailed perplexity results in the section §5.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Evaluation and comparative analysis of TriLMs, FloatLMs, and QuantLMs at different scales.",
        "chunkIndex": 13,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-14",
      "content": "Experimental Setup: First, we, explore the impact of training low-bitwidth language models on deployment focussing on addressing memory bottlenecks. Our analysis includes transformer configurations from the LLaMa family [Touvron et al., 2023a,b]. As larger vocabularies in LLMs are becoming increasingly common for efficient multilingual modeling, we use a vocabulary size of 128k from LLaMa 3 [AI@Meta, 2024] for our analysis. We assume the Embedding and LM Head weights are retained in Half-Precision across all bitwidths for these analyses.\n\nMemory Capacity of GPGPUs and Model Size (in Bits): Figure 21a in Appendix §F reveal that memory capacity has consistently lagged behind computational power across various accelerators, including recent hardware like Blackwell [Nvidia Team, 2024], MI325X [AMD Team, 2024], and Gaudi3 [Intel Gaudi Team, 2024]. This gap is further exacerbated by advanced computational techniques like Ampere sparse or FP8.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.1 Memory Bottlenecks and Language Model Deployment",
        "chunkIndex": 14,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-15",
      "content": "rdware like Blackwell [Nvidia Team, 2024], MI325X [AMD Team, 2024], and Gaudi3 [Intel Gaudi Team, 2024]. This gap is further exacerbated by advanced computational techniques like Ampere sparse or FP8. As shown in Figure 2a, the model sizes (in GB) for TriLM, QuantLM 4-Bit, and FloatLM scale differently with parameter counts. For simplicity, the figure excludes overhead from KV Cache, activations, and compilation during model deployment. FloatLM reaches the memory capacity of a single H100 at 34B parameters, with larger models exceeding the\n\nFigure 2: Expected gains from low bitwidth modeling. TriLMs can fit over 300B parameters on a single H100 and achieve up to a theoretical maximum of 10x faster autoregressive decoding compared to FloatLM.\n\n<!-- image -->\n\ncapacity of multiple GPUs. In contrast, QuantLM 4-Bit supports up to 300B parameters on a single MI300X.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.1 Memory Bottlenecks and Language Model Deployment",
        "chunkIndex": 15,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-16",
      "content": "theoretical maximum of 10x faster autoregressive decoding compared to FloatLM.\n\n<!-- image -->\n\ncapacity of multiple GPUs. In contrast, QuantLM 4-Bit supports up to 300B parameters on a single MI300X. TriLMs, with over 300B parameters and appropriate packing, can fit on a single H100, making them not only efficient for GPU deployment but also ideal for edge devices.\n\nMemory bandwidth of GPGPUs and model inference speedup: Figure 21b and Appendix §F demonstrate the trends of Memory Bandwidth over FLOPs for accelerators over the years, highlighting the slower growth of memory bandwidth compared to computation. This trend, indicating a downward slope, aligns with Kim et al. [2024]'s establishment of the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.1 Memory Bottlenecks and Language Model Deployment",
        "chunkIndex": 16,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-17",
      "content": "oregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. Consequently, the autoregressive decoding of LLM inference can theoretically achieve speedup proportional to its compression factor. Figure 2b illustrates the maximum possible speedup for QuantLM 4-Bit and TriLM compared to FP16 at different parameter counts. Even at 7 billion parameters, TriLMs can be more than 4 times faster at autoregressive decoding than FloatLM and 2 times faster than QuantLM 4-bit. While QuantLM 4-Bit plateaus at a maximum possible speedup factor of 4x, TriLMs plateau much higher at 10x for FloatLM.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.1 Memory Bottlenecks and Language Model Deployment",
        "chunkIndex": 17,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-18",
      "content": "In this section, we use information theory to support our hypothesis: as the number of parameters increases, training language models with low-bitwidth can effectively capture the necessary weight variance without significant information loss . Assuming a fixed training dataset, we base this hypothesis on analyzing weight distributions in FloatLMs ranging from 99M to 3.9B parameters.\n\nFigure 3: Shannon entropy (in bits) of discretized weight distribution with increasing number of bins.\n\n<!-- image -->\n\nFigure 4: Differential entropy of Gaussian fits on weight distributions across different scales.\n\n<!-- image -->\n\nAssuming that the weights of a trained model follow a Gaussian distribution (see Appendix §E), we fit a Gaussian to these weights to understand their statistical behavior across model scales. The differential entropy is calculated using the expression: H ( W ) = 1 2 log 2 (2 πeσ 2 W ) where σ W is the standard deviation of the weights [Papoulis and Pillai, 2002].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.2 Low bits can capture weight variance effectively at scale",
        "chunkIndex": 18,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-19",
      "content": "avior across model scales. The differential entropy is calculated using the expression: H ( W ) = 1 2 log 2 (2 πeσ 2 W ) where σ W is the standard deviation of the weights [Papoulis and Pillai, 2002]. As shown in Figure 4, differential entropy decreases with an increase in the number of parameters. This decrease indicates that the weights become more concentrated around the mean as the model size increases, suggesting higher predictability and\n\nless variability [MacKay, 2003]. This reduced variability is due to overparameterization, which leads to redundancy in the weights Zhang et al. [2017], Neyshabur et al. [2018].\n\nAdditionally, we also use Shannon entropy, calculated by discretizing the weight distribution into ( N ) bins and computing H Shannon = -∑ N i =1 p i log 2 p i , where p i is the probability of the weights falling into the i -th bin. The probability is given by the normalized frequency of weights within each bin.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.2 Low bits can capture weight variance effectively at scale",
        "chunkIndex": 19,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-20",
      "content": "computing H Shannon = -∑ N i =1 p i log 2 p i , where p i is the probability of the weights falling into the i -th bin. The probability is given by the normalized frequency of weights within each bin. Discretization allows us to apply discrete entropy measures to continuous data, with the number of bins determining the representation granularity. Shannon entropy measures the average minimum number of bits needed to encode the weight values based on their distribution, providing a quantifiable measure of their \"information content\" [Shannon, 1948]. Lower Shannon entropy indicates a more predictable, less diverse distribution that can be effectively encoded using fewer bits. Examining Shannon entropy across scales and bin sizes, we validate our finding that larger models require fewer bits for effective representation. As plotted in Figure 3, there is a general trend of decreasing Shannon entropy with increasing parameter count for a given number of bins.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.2 Low bits can capture weight variance effectively at scale",
        "chunkIndex": 20,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-21",
      "content": "arger models require fewer bits for effective representation. As plotted in Figure 3, there is a general trend of decreasing Shannon entropy with increasing parameter count for a given number of bins.\n\nThese analyses support the potential of low-bitwidth models to match full-precision performance, especially as model sizes grow. In Section 4.3, we further substantiate this by analyzing the scaling behavior of our low-bitwidth TriLMs compared to FloatLMs.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.2 Low bits can capture weight variance effectively at scale",
        "chunkIndex": 21,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-22",
      "content": "Selecting the appropriate quantization level is crucial when training low-bitwidth models, as it significantly impacts both computational efficiency and model performance. Binary models quantize weights to {-1, 1}, simplifying multiplication operations in neural networks to XOR operations. Such operations are highly efficient on digital hardware due to their basic logic gate implementation [Courbariaux et al., 2016a]. XOR operations, which can be executed rapidly and with lower power consumption than traditional multiplications, make binary models especially appropriate for resource-constrained environments [Hubara et al., 2017]. Ternary models extend this concept by incorporating a zero state, allowing weights to be {-1, 0, +1}. This modification not only maintains computational simplicity by replacing multiplications with additions and subtractions [Li et al., 2016] but also leverages the sparsity of neural networks to enhance computational efficiency by eliminating calculations wher",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.3 Selecting the Appropriate Low-Bitwidth Model",
        "chunkIndex": 22,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-23",
      "content": "y by replacing multiplications with additions and subtractions [Li et al., 2016] but also leverages the sparsity of neural networks to enhance computational efficiency by eliminating calculations where weights are zero [Zhu et al., 2017].\n\nIn contrast, models quantized to 2, 3, or 4 bits involve more complex arithmetic operations, including full multiplications, which are computationally more demanding than those required for binary and ternary models [Zhou et al., 2016]. While these higher bitwidth models offer more detailed weight representations, they do so at the expense of greater computational resource usage. Therefore, this work only considers binary and ternary language models for further analysis.\n\nWhile binary quantization minimizes computational complexity, it often results in significant performance degradation, failing to meet the standards of practical applications [Rastegari et al., 2016].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.3 Selecting the Appropriate Low-Bitwidth Model",
        "chunkIndex": 23,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-24",
      "content": "While binary quantization minimizes computational complexity, it often results in significant performance degradation, failing to meet the standards of practical applications [Rastegari et al., 2016]. In contrast, ternary quantization introduces a zero state that more accurately approximates weight distributions without substantially increasing computational demands [Li et al., 2016]. This capability makes ternary models particularly advantageous, as they maintain performance levels closer to those of full-precision models while still achieving significant efficiency gains [Zhu et al., 2017]. Our observations align with these, as the scaling trends for given data size for our TriLMs consistently outperform those of BiLMs (Binary LLMs) across all parameter counts and bit sizes considered in this work, as shown in Appendix §B and Figure 15. As a result, this work primarily focuses on the TriLM model, which we describe in the following section.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.3 Selecting the Appropriate Low-Bitwidth Model",
        "chunkIndex": 24,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-25",
      "content": "this work, as shown in Appendix §B and Figure 15. As a result, this work primarily focuses on the TriLM model, which we describe in the following section.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "2.3 Selecting the Appropriate Low-Bitwidth Model",
        "chunkIndex": 25,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-26",
      "content": "In this section, we present the architectural and optimization details of the TriLM (Ternary Language Model). We discuss the training and inference processes, as well as the optimization schedule.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3 TriLM: Ternary Language Model",
        "chunkIndex": 26,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-27",
      "content": "TriLM is a LLaMa-style [Touvron et al., 2023a] autoregressive transformers [Vaswani et al., 2017] model with RMSNorm [Zhang and Sennrich, 2019], SwiGLU Gated MLP [Shazeer, 2020], Rotary Position Embedding (RoPE) [Su et al., 2021], Multi-Headed Attention and no bias terms. In TriLMs, we represent the weights of all the linear layers in one of three possible ternary states {-1 , 0 , 1 } ,\n\nFigure 5: The computational flow of forward, backward, and inference processes in TriLM's linear layer with N-Way model parallelism.\n\n<!-- image -->\n\nalong with an additional floating-point number called 'scale value' shared across the matrix. During training, we maintain the latent (or master) weights in floating point precision, allowing for the accumulation of small updates over iterations that eventually contribute to a transition in the estimated ternary state of a parameter. As shown in Figure 5, during the forward pass, we ternarize the floating point latent weights on-the-fly.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.1 Architecture",
        "chunkIndex": 27,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-28",
      "content": "ions that eventually contribute to a transition in the estimated ternary state of a parameter. As shown in Figure 5, during the forward pass, we ternarize the floating point latent weights on-the-fly. This process involves calculating the scale value to the absolute mean of the latent weights. After scaling, we estimate the ternary state of each parameter by rounding off to the nearest ternary state. In the backward pass, we use a straight-through estimator to compute gradients of the floating point latent weights [Bengio et al., 2013]. Since we only need the scale values and the ternarized states during inference, we achieve a significant reduction in both model size and inference time at larger scales compared to FloatLMs. We provide a formal description of these forward pass, backward pass, and inference time equations in the Section (§3.2). We represent the embedding and language model head in half-precision floating point across all our experiments.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.1 Architecture",
        "chunkIndex": 28,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-29",
      "content": "f these forward pass, backward pass, and inference time equations in the Section (§3.2). We represent the embedding and language model head in half-precision floating point across all our experiments.\n\nSince, training of TriLMs requires on-the-fly computation of scale values, synchronizing for a single scalar across devices in model parallel training [Shoeybi et al., 2019] can cause significant communication overhead. To address this, we allow each device to independently compute these scale values over its own matrix shard. This approach introduces additional artifacts, where the number of scalar values for each matrix is the same as the degree of model parallelism used during training (see section A.4). However, the impact on modelsize is negligible, for matrices with millions of parameters, we only add 6 scalar values each.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.1 Architecture",
        "chunkIndex": 29,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-30",
      "content": "e as the degree of model parallelism used during training (see section A.4). However, the impact on modelsize is negligible, for matrices with millions of parameters, we only add 6 scalar values each.\n\nConcurrent research on low-bit LLMs, like BitNet 1.58 [Ma et al., 2024], also demonstrates the feasibility of training LLMs with ternary weights. Our experiments demonstrate that TriLM's architecture not only outperforms BitNet b1.58 but is also simpler and more stable. Moreover, both the larger BitNet 1.3B model presented in their paper and our replication of the BitNet 1.1B model underperform compared to our TriLM 1.1B on commonsense and reasoning benchmarks. We provide further details on these comparisons in Appendix A.5.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.1 Architecture",
        "chunkIndex": 30,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-31",
      "content": "Table 1 shows the equations for TriLM, BiLM (Binary Langauge Model) and FloatLM for forward pass, backward pass and inference.\n\nReason for restricting the quantization approach to linear weights in TriLMs. In developing extremely large language models like TriLMs, a key architectural strategy is to quantize only the linear layer weights while keeping the embedding layers and language model head in higher precision. This\n\nTable 1: Equations in the Linear Layer of TriLMs and FloatLMs.\n\n| Type    | Forward Pass                                                                                                             | Backward Pass                       | Inference                                                      |\n|---------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------|----------------------------------------------------------------|\n| FloatLM | Y = XW T",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.2 Forward Pass, Backward Pass and Inference Equations",
        "chunkIndex": 31,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-32",
      "content": "------------------------------------------------------------|-------------------------------------|----------------------------------------------------------------|\n| FloatLM | Y = XW T                                                                                                                 | ∂L ∂X = ∂L ∂Y W ∂L ∂W = ∂L ∂Y T X   | Y = XW T                                                       |\n| TriLM   | γ = ϵ + 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = round ( min ( max ( W ij γ , - 1 ) , 1 )) ˜ W ij = γ ̂ W ij Y = X ˜ W T | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and γ once and cache ˜ W ij = γ ̂ W ij Y = X ˜ W T |\n| BiLM    | α = 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = sign ( W ij - 1 nm ∑ n i =1 ∑ m j =1 W ij ) ˜ W ij = α ̂ W ij Y = X ˜ W T   | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and α once and cache ˜ W ij = α ̂ W ij Y = X ˜ W T |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.2 Forward Pass, Backward Pass and Inference Equations",
        "chunkIndex": 32,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-33",
      "content": "1 | W ij | ̂ W ij = sign ( W ij - 1 nm ∑ n i =1 ∑ m j =1 W ij ) ˜ W ij = α ̂ W ij Y = X ˜ W T   | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and α once and cache ˜ W ij = α ̂ W ij Y = X ˜ W T |\n\nis driven by the need to reduce model size while maintaining performance. Linear layers (dense layers) hold the bulk of the parameters in transformer models [Vaswani et al., 2017]. Quantizing these weights to ternary states significantly reduces the model size, facilitating deployment on memory-constrained hardware. However, the embedding layers and language model head remain in higher precision (e.g., half-precision floating point) to preserve critical functions in language understanding and generation. Embedding layers encode important semantic and syntactic information, and quantizing them would degrade performance [Mikolov et al., 2013].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.2 Forward Pass, Backward Pass and Inference Equations",
        "chunkIndex": 33,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-34",
      "content": "critical functions in language understanding and generation. Embedding layers encode important semantic and syntactic information, and quantizing them would degrade performance [Mikolov et al., 2013]. Similarly, the language model head, which maps internal representations to the vocabulary space, requires high precision to maintain prediction quality [Press and Wolf, 2017]",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.2 Forward Pass, Backward Pass and Inference Equations",
        "chunkIndex": 34,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-35",
      "content": "Optimization of low bitwidth neural networks (such as in Quantization Aware Training) [Liu et al., 2023b, Yuan et al., 2024, Bethge et al., 2018, Le and Li, 2023] requires a set of considerations like higher initial learning rate and reduced weight decay. Our optimization schedule for TriLM closely follows that of BitNet [Ma et al., 2024] consisting of two interventions in a vanilla linear decay learning rate scheduling with warmup and weight decay (L2 Regularization). (1) Peak LR - at roughly the halfway point, we reduce the peak learning rate. (2) L2 Reg. - at roughly two-thirds of the training, we remove the weight decay regularization as ternarization provides sufficient regularization [Courbariaux et al., 2016b]. Figure 6 demonstrates the ablation run performed for a 1.1B parameter model on 100B tokens with both, only one and neither of these interventions.\n\nFigure 6: Training loss for a 1.1B parameter TriLM, across different optimization schedules.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.3 Optimization Schedule",
        "chunkIndex": 35,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-36",
      "content": ".1B parameter model on 100B tokens with both, only one and neither of these interventions.\n\nFigure 6: Training loss for a 1.1B parameter TriLM, across different optimization schedules.\n\n<!-- image -->\n\nAmong the four experimental runs, we observed that the lowest final training loss occurred when both L2 regularization and peak learning rate (LR) adjustments were implemented. This was closely followed by the scenario in which only L2 regularization was adjusted, and subsequently by the condition where only peak LR was modified. Notably, discontinuing the peak learning rate at the midpoint of training resulted in a significant and rapid decline in training loss. Similar phenomena have been documented in training schedules characterized by brief periods of rapid learning rate decay, such as those described in MiniCPM[Hu et al., 2024].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.3 Optimization Schedule",
        "chunkIndex": 36,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-37",
      "content": "decline in training loss. Similar phenomena have been documented in training schedules characterized by brief periods of rapid learning rate decay, such as those described in MiniCPM[Hu et al., 2024]. Conversely, the removal of L2 regularization, or weight decay, led to an accelerated convergence rate, which can produce effects analogous to reducing the peak learning rate, also resulting in a swift decrease in loss. These relative training loss observation at 100B tokens also go hand in hand with relative downstream performance across commonsense and reasoning tasks, which are listed in\n\nTable 11 and 10. Consequently, we have established a fixed optimization schedule for the TriLM, which entails dropping the peak learning rate at the halfway point while removing weight decay at the two-thirds mark.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "3.3 Optimization Schedule",
        "chunkIndex": 37,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-38",
      "content": "The Spectra suite includes comprehensive families of Large language models designed to span different parameter counts and bit-widths. This suite includes three main model families: TriLMs , FloatLMs , and QuantLMs (3, 4, 6, and 8 bits). Drawing inspiration from established model suites such as those by [Biderman et al., 2023, Liu et al., 2023c, Groeneveld et al., 2024], Spectra aims to facilitate scientific research on low-bitwidth LLMs.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4 Spectra suite: Spanning Parameters and Bitwidths",
        "chunkIndex": 38,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-39",
      "content": "The Spectra suite stands out with the following key properties:\n\n1. Scale: The suite spans a broad spectrum of scales across parameter count (99M to 3.9B), sizes ( 9 ∗ 10 8 to 6 . 4 ∗ 10 10 bits) and bitwidths (1.58 bits to 16 bits).\n2. Uniform Training: All the TriLMs and FloatLMs are trained on a identical data sequences, specifically a 300B subset of Slim Pajama [Soboleva et al., 2023] dataset, ensuring training consistency. Data ordering and batch sizes are also kept consistent within each model family to support reproducibility and comparability in research. QuantLMs undergo quantization using the same calibration data, maintaining uniformity in model quantization procedures.\n3. Public Accessibility: The training data and intermediate checkpoints are publicly available for study.\n4. Consistent Model Size Mapping: All models across the families maintain a consistent one-to-one mapping for parameter count.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.1 Overview of Spectra Suite",
        "chunkIndex": 39,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-40",
      "content": "ining data and intermediate checkpoints are publicly available for study.\n4. Consistent Model Size Mapping: All models across the families maintain a consistent one-to-one mapping for parameter count.\n\nFigure 7 demonstrates the Spectra LM suite spanning across two dimensions - size (bits) and parameters. For each parameter count, we have 6 models across different bitwidths. Due to availability of FloatLM, Spectra can easily be extended with new QuantLMs by using different Post Training Quantization methods.\n\nThe details on dataset and tokenizer, pretraining setup, and hyperparameters across all the models are detailed in the Appendix §A, while information on other families is covered in the next section.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.1 Overview of Spectra Suite",
        "chunkIndex": 40,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-41",
      "content": "Family of FloatLMs. We utilize LLaMa-style [Touvron et al., 2023a] architecture akin to TriLM. In FloatLMs, parameters in the weight matrices of linear layers are represented as floating-point numbers (FP16/BF16). The optimization schedule for FloatLM follows a cosine decay scheduling with weight decay and includes a learning rate warmup. This methodology is consistent with the practices established in models such as Pythia, OLMo, LLM360. For more details, refer to the Appendix (A.3).\n\nFamily of QuantLMs. Recently, data-aware quantisation techniques like GPTQ [Frantar et al., 2022] have emerged as efficient solutions for nearlossless weight quantization down to 4-bit precision [Dettmers and Zettlemoyer, 2023]. In our work, we implemented GPTQ post-training quantization to FloatLM, creating the QuantLM family of models across 3, 4, 6, and 8 bits. We quantized all trans-",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.2 FloatLMs and QuantLMs",
        "chunkIndex": 41,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-42",
      "content": "sion [Dettmers and Zettlemoyer, 2023]. In our work, we implemented GPTQ post-training quantization to FloatLM, creating the QuantLM family of models across 3, 4, 6, and 8 bits. We quantized all trans-\n\nFigure 7: The Spectra Suite spans across two dimensions of parameters and scale. Each point corresponds to a LLM in the spectra suite.\n\n<!-- image -->\n\nformer layer weights. For 3-bit and 4-bit quantization, we employ a group size of 128, which results\n\nin effective bit rates of 3.25 and 4.25 bits per parameter, respectively. We've refined our approach by incorporating best practices from recent research [Malinovskii et al., 2024], particularly in terms of calibration data and scaling it to a million tokens for improved reconstruction. To ensure a fair comparison with TriLM, we maintain certain components in their original precision. Specifically, we do not quantize the embedding, language model head, or activations.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.2 FloatLMs and QuantLMs",
        "chunkIndex": 42,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-43",
      "content": "reconstruction. To ensure a fair comparison with TriLM, we maintain certain components in their original precision. Specifically, we do not quantize the embedding, language model head, or activations. Additionally, we use symmetric quantization (without zero offset) as it is simpler, is supported by fast inference kernels [Frantar and Alistarh, 2024] and offers similar performance to assymmetric quantization (with separate zero offsets in addition to scale for each group). It also offers consistency and a fairer comparison with TriLMs. It's worth noting that our Spectra suite is designed with flexibility in mind, allowing for easy extension to other quantization methods as needed.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.2 FloatLMs and QuantLMs",
        "chunkIndex": 43,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-44",
      "content": "Figure 8: Training Cross Entropy Loss across steps for the TriLM family of models. At the halfway point (150B tokens) when we lower the peak learning rate, we observe a sudden drop in training loss. In the two-third way, removing weight decay leads to faster convergence.\n\n<!-- image -->\n\nTraining Dynamics. Figure 8a illustrates the training loss curves for all the TriLMs trained and Figure 8b shows the relative training loss of a TriLM in comparison to two smaller FloatLMs. The loss curves demonstrate a continuous and consistent improvement in TriLMs with an increase in parameter count. During training, we make several key observations. First, minor spikes and drops in training loss occurred consistently across different TriLM scales at the same token counts, as all models were trained on identical data with the same ordering. Notably, the two largest models-TriLM 2.4B and TriLM 3.9B-each experienced a large loss spike in the first half of training.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 44,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-45",
      "content": "ounts, as all models were trained on identical data with the same ordering. Notably, the two largest models-TriLM 2.4B and TriLM 3.9B-each experienced a large loss spike in the first half of training. Second, adjusting the learning rate at the midpoint led to a sharp decline in training loss over a few hundred million tokens, though its impact varied by model size: for the larger models (2.4B and 3.9B), the rate of loss reduction returned to the prior pace after the initial sharp drop, while for smaller models (1.1B and 1.5B), the loss reduction plateaued, and models below 1B parameters saw an increase in training loss. Lastly, the removal of weight decay at the two-thirds mark accelerated convergence for all models, with the effect being most pronounced in the largest TriLM models.\n\nScaling Laws. Figures 9a and 9b illustrate the final validation loss across different model size in terms of bits and number of parameters ( N ) respectively.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 45,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-46",
      "content": "pronounced in the largest TriLM models.\n\nScaling Laws. Figures 9a and 9b illustrate the final validation loss across different model size in terms of bits and number of parameters ( N ) respectively. In terms of effective model size in bits (9a), which is crucial during inference, TriLMs exhibit significantly better scaling than FloatLMs. Notably, TriLM 3.9B validation error matches with FloatLMs 1.1B, which is nearly 1.7 times larger in terms of effective bit size. To investigate the scaling behaviour in terms of N , we fit the validation loss to a power-law with offset 2 Hoffmann et al. [2022] (see Figure 9b and Appendix §C):\n\n<!-- formula-not-decoded -->\n\nWe employ the Levenberg-Marquardt algorithm (Levenberg [1944], Marquardt [1963]) for efficient non-linear least squares fitting. Both FloatLM and TriLM share the scaling exponent α = 0 . 26 ,\n\n2 derived using a fixed data regime of 300B tokens\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 46,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-47",
      "content": "4], Marquardt [1963]) for efficient non-linear least squares fitting. Both FloatLM and TriLM share the scaling exponent α = 0 . 26 ,\n\n2 derived using a fixed data regime of 300B tokens\n\n<!-- image -->\n\n- (a) Scaling laws - perplexity across size (bits).\n- (b) Scaling laws - perplexity across parameters.\n\n<!-- image -->\n\nFigure 9: Final validation loss across sizes (in bits) and parameters. TriLMs with increasing size offer better performance than FloatLMs of the same number of bits; and the gap in validation perplexity closes at scale.\n\nindicating similar scaling behavior with the number of parameters N . However, the offset terms ϵ TriLM = 1 . 76 and ϵ FloatLM = 1 . 67 reveal that their validation losses converge as N increases. Although TriLM starts with a higher coefficient A = 185 , suggesting greater initial validation loss than FloatLM ( A = 159 ), this difference becomes insignificant with larger N , aligning their performance at asymptotic scales as shown in Figure §9b.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 47,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-48",
      "content": "185 , suggesting greater initial validation loss than FloatLM ( A = 159 ), this difference becomes insignificant with larger N , aligning their performance at asymptotic scales as shown in Figure §9b.\n\nAs shown in Figure 10, using the scaling equations for TriLMs and FloatLMs, we derive the relationship between parameter count and the percentage difference in validation loss relative to FloatLMs. We observe that at 330B and 15.6B parameters, the validation losses for TriLMs are within 6% and 7% of FloatLMs' validation losses, respectively. This indicates that TriLMs are likely to closely match the performance of FloatLMs at larger scales.\n\nDespite the observed differences in validation loss at the scale of models considered in this work, we demonstrate in Section 5 that at 3.9B parameters, TriLM offers competitive downstream performance\n\nFigure 10: Comparison of Power Law and Power Law-with-offset Fits for TriLM and FloatLM.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 48,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-49",
      "content": "onstrate in Section 5 that at 3.9B parameters, TriLM offers competitive downstream performance\n\nFigure 10: Comparison of Power Law and Power Law-with-offset Fits for TriLM and FloatLM.\n\n<!-- image -->\n\ncompared to a FloatLM of the same parameter count across a variety of benchmarks in commonsense reasoning and knowledge-based tasks. Moreover, as discussed in Section §5, both models show similar perplexity on clean datasets such as Penn Tree Bank and OpenAI's Lambda. However, the gap in perplexity is observed in overlapping web-based datasets like Dolma and RefinedWeb.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "4.3 Training Dynamics and Scaling Laws",
        "chunkIndex": 49,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-50",
      "content": "We evaluate the families of LLMs on three aspects - commonsense &amp; reasoning tasks, knowledgebased tasks, and toxicity, all of which are crucial measures of their downstream performance. Readers may refer to the appendix for more details regarding the benchmarks Appendix (§D).\n\nCommonsense and Reasoning. We evaluate Spectra Suite models using eight distinct commonsense and reasoning benchmarks consisting of tasks from logical and reasoning questions to grounded and physical commonsense tasks: Arc Easy, Arc Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], HellaSWAG [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2019], LAMBADA [Paperno et al., 2016], LogiQA [Liu et al., 2021], all under zero-shot settings. Figures 1a and 1b display the average performance of the LLMs on the first six benchmarks across size in bits and number of params. Figures 1c and 1d present the performance for the LAMBADA dataset.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 50,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-51",
      "content": ". Figures 1a and 1b display the average performance of the LLMs on the first six benchmarks across size in bits and number of params. Figures 1c and 1d present the performance for the LAMBADA dataset. The results show that TriLMs consistently demonstrate superior performance for their size across all benchmarks at the 2 . 4 B and 3 . 9 B parameter scales. At the largest scale of 3 . 9 B, TriLM surpasses FloatLM on LAMBADA and achieves competitive average scores across six benchmarks. Additionally, TriLMs at the largest scales consistently outperform 4-bit QuantLMs of equivalent parameter count. However, across the considered scales, all LLMs show poor performance on\n\nLogiQA, making it difficult to identify a clear performance trend. For detailed benchmarking across all datasets -see Tables 6, 7 and 9.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 51,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-52",
      "content": "cross the considered scales, all LLMs show poor performance on\n\nLogiQA, making it difficult to identify a clear performance trend. For detailed benchmarking across all datasets -see Tables 6, 7 and 9.\n\nFigure 11: Performance of ternary TriLM, FloatLM and quantized QuantLM (3-bit &amp; 4-bit) models on SciQ and TriviaQA tasks across Size (Bits) and Parameters. Refer to Tables 9 and 12 for details.\n\n<!-- image -->\n\nKnowledge. Several downstream practical uses of LLMs require them to have knowledge about common subjects like science or politics. To evaluate the performance of LLMs on these subjects, we use SciQ [Welbl et al., 2017], TriviaQA [Joshi et al., 2017] and MMLU [Hendrycks et al., 2021] benchmarks in zero-shot settings. Figures 11a and 11b show the accuracy of the Spectra suite LLMs on SciQ across size in bits and parameter counts. Figures 11c and 11d depict the accuracy for TriviaQA, while 12a and 12b do the same for MMLU.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 52,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-53",
      "content": "11a and 11b show the accuracy of the Spectra suite LLMs on SciQ across size in bits and parameter counts. Figures 11c and 11d depict the accuracy for TriviaQA, while 12a and 12b do the same for MMLU. Across both benchmarks, at large 2.4B+ scales, TriLMs offer the best performance at a given size (bits). Surprisingly, despite having fewer bits, the knowledge capacity of TriLM does not have any significant degradation as observed in the case of QuantLMs [Allen-Zhu and Li, 2024]. This indicates that low-bitwidth LLMs like TriLMs have similar knowledge capacity to FloatLMs, indicating that knowledge capacity is parameterized via the presence and nature of a connection (+1 or -1), rather than its strength. Tables 9 and 12 expand on these results.\n\nFigure 12: MMLU Accuracy for ternary TriLM, FloatLM and quantized QuantLM (3-bit &amp; 4-bit) models across Size and Parameters. Please refer to Table 13 for details.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 53,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-54",
      "content": "n these results.\n\nFigure 12: MMLU Accuracy for ternary TriLM, FloatLM and quantized QuantLM (3-bit &amp; 4-bit) models across Size and Parameters. Please refer to Table 13 for details.\n\n<!-- image -->\n\nToxicity. We evaluate the Spectra suite across various safety and toxicity benchmarks of TruthfulQA [Lin et al., 2021], Big Bench BBQ Lite [Parrish et al., 2022] and CrowsPairs [Nangia et al., 2020]. These scores are listed in the Appendix in Table 12. We observe that none of the LLMs, even those with up to 3.9 billion parameters and trained on 300 billion tokens, perform significantly better than random guessing on the TruthfulQA benchmark. For the other two datasets, there is a noticeable correlation between the occurrence of toxicity and stereotypes and the LLMs' performance on various tasks. In particular, TriLMs with fewer than one billion parameters exhibit less stereotyping than FloatLMs with a similar parameter count.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 54,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-55",
      "content": "oxicity and stereotypes and the LLMs' performance on various tasks. In particular, TriLMs with fewer than one billion parameters exhibit less stereotyping than FloatLMs with a similar parameter count. However, this difference diminishes as the scale increases, with TriLM 2.4B and TriLM 3.9B exhibiting biases comparable to those of FloatLM 2.4B and FloatLM 3.9B, respectively, on these benchmarks. This suggests that, although TriLMs initially show reduced bias compared to similarly sized FloatLMs, their performance aligns with FloatLMs of equivalent parameter counts at larger scales. This also highlights that TriLMs exhibit considerably more stereotyping than FloatLMs of comparable size (measured in bits), yet perform comparably to FloatLMs with similar parameter counts.\n\nPerplexity on other datasets. We measure perplexity using TriLM 3.9B and FloatLM 3.9B across various other corpora than SlimPajama, which was used for training.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 55,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-56",
      "content": "loatLMs with similar parameter counts.\n\nPerplexity on other datasets. We measure perplexity using TriLM 3.9B and FloatLM 3.9B across various other corpora than SlimPajama, which was used for training. These corpora include OpenAI Lambada, Penn Tree Bank, C4, Cosmopedia, Dolma, S2Orc, Wikipedia, and RefinedWeb. A portion of Wikipedia, C4 is included in Slim Pajama. Some other corpora like Dolma and RefinedWeb, may also have overlaps from C4, Wikipedia as well as Common Crawl. Figure 13 demonstrates that while TriLM 3.9B is similar or better than FloatLM 3.9B on PTB and Lambada, across the other datasets, with potential overlaps with SlimPajama, it's performance is consistently worse - indicating lower capability to memorize training data as well as worse in-distribution performance, despite competitive out of distribution performance.\n\nFigure 13: Cross-entropy (log perplexity) comparison between TriLM and FloatLM (both 3.9B parameters) across various datasets apart from SlimPajama.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 56,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-57",
      "content": "spite competitive out of distribution performance.\n\nFigure 13: Cross-entropy (log perplexity) comparison between TriLM and FloatLM (both 3.9B parameters) across various datasets apart from SlimPajama.\n\n<!-- image -->\n\nIllustrative examples. We also generated a few examples of poem and essay writing, as well as reading comprehension (see Appendix §H). Our results show that the TriLM 3.9B model is able to generate cohesive and correct responses with randomly sampled examples.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "5 Results",
        "chunkIndex": 57,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-58",
      "content": "Quantization of Large Language Models after Training. Post-training quantization (PTQ) algorithms convert a pretrained high-precision model (FP32 / FP16 / BF16) into a lower precision format without requiring the original training process [Cai et al., 2020, Hubara et al., 2020, Choukroun et al., 2019]. These methods can be either data-independent or need a small calibration dataset. Additionally, PTQ for LLMs presents unique challenges due to numerical outliers in both weights and activations [Bondarenko et al., 2021]. GPTQ [Frantar et al., 2022] is a state-of-the-art one-shot weight quantization method aimed at finding a matrix of quantized weights (say ˆ W ) that minimizes the squared error relative to the full precision layer output. By leveraging second-order information, GPTQ derives a closed-form solution to this optimization problem, making it scalable to large LLMs.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "6 Related Work",
        "chunkIndex": 58,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-59",
      "content": "e squared error relative to the full precision layer output. By leveraging second-order information, GPTQ derives a closed-form solution to this optimization problem, making it scalable to large LLMs. Other methods [Dettmers et al., 2023, Lin et al., 2024, Lee et al., 2024] emphasize the importance of outlier weights that correspond to high-magnitude activations. Some recent methods also quantized activation along with the weights [Xiao et al., 2024, Yao et al., 2022, 2023]. Ahmadian et al. [2023] demonstrate that large activation outliers can be effectively mitigated at scale by making appropriate optimization decisions during the pretraining phase.\n\nTraining Language Models At Lower Precision. Several prominent language models such as GPT [Brown et al., 2020b], NeoX [Black et al., 2022], Llama and Pythia families have been traditionally trained using mixed precision (FP32/FP16 or FP32/BF16) [Micikevicius et al., 2018] or half-precision (FP16/BF16) [Kalamkar et al., 2019].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "6 Related Work",
        "chunkIndex": 59,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-60",
      "content": "al., 2022], Llama and Pythia families have been traditionally trained using mixed precision (FP32/FP16 or FP32/BF16) [Micikevicius et al., 2018] or half-precision (FP16/BF16) [Kalamkar et al., 2019]. Recently, Tao et al. [2022] introduced QuantGPT, a model that incorporates contrastive and logit distillation from a full-precision teacher to a quantized student model during pretraining. Further developments, such as BitNet [Wang et al., 2023] and BitNet b1.58 [Ma et al., 2024], have specifically focused on quantization-aware training for extremely low-bitwidth\n\nnetworks in transformer-based models. In their work, models are trained at low 'effective' precision of binary and ternary respectively - where the latent (or master) weights during training are maintained in higher precision like FP16.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "6 Related Work",
        "chunkIndex": 60,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-61",
      "content": ". In their work, models are trained at low 'effective' precision of binary and ternary respectively - where the latent (or master) weights during training are maintained in higher precision like FP16. The model weights are binarized or ternarized on the fly during the forward pass and gradients are backpropagated for the latent weights using the straight-through estimator [Courbariaux et al., 2016b]. Prior works emphasize the importance of maintaining latent (or master) weights at high precision to allow accumulation of small updates during training - for example, Peng et al. [2023] observed a significant performance drop on the language model when the latent (or master) model weights were switch from 16-bits (FP16/BF16) to 8-bits (FP8) during training. Concurrent architectural improvements such as Flash Attention [Dao et al., 2022, Dao, 2023], the mixture of experts [Zoph et al., 2022], xLSTM Beck et al.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "6 Related Work",
        "chunkIndex": 61,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-62",
      "content": "(FP16/BF16) to 8-bits (FP8) during training. Concurrent architectural improvements such as Flash Attention [Dao et al., 2022, Dao, 2023], the mixture of experts [Zoph et al., 2022], xLSTM Beck et al. [2024] and state space models [Gu and Dao, 2024, Dao and Gu, 2024, Gu et al., 2022] complement these advancements in lower precision modeling.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "6 Related Work",
        "chunkIndex": 62,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-63",
      "content": "In this work, we address memory limitations in large language model (LLM) deployment by exploring both post-training quantization and direct low-bitwidth training. We introduce the Spectra LLM suite, featuring 54 models ranging from 99 million to 3.9 billion parameters, trained on 300 billion tokens. This suite includes Float16 LLMs (FloatLMs), quantized QuantLMs (3-8 bits), and our proposed ternary LLMs (TriLMs). Our findings reveal that TriLMs scale better than their half-precision Float16 counterparts in terms of effective model bit size, and they can achieve comparable validation loss when scaled to a large number of parameters. Additionally, our results demonstrate that TriLMs surpass other models in bit-size efficiency and achieve performance comparable to FloatLMs at 3 billion+ parameters across multiple benchmarks.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "7 Conclusion and Future Work",
        "chunkIndex": 63,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-64",
      "content": "eters. Additionally, our results demonstrate that TriLMs surpass other models in bit-size efficiency and achieve performance comparable to FloatLMs at 3 billion+ parameters across multiple benchmarks.\n\nFuture work should address the remaining challenges of toxicity, stereotyping, and performance gaps on web corpora associated with low-bitwidth models. Investigating scaling laws across different data regimes for various low-bitwidth architectures can provide deeper insights into their behavior and limitations. Additionally, combining these models with state-space architectures like Mamba Gu and Dao [2024] could further enhance efficiency and performance without sacrificing accuracy. These research directions hold promise in advancing efficient language modeling further.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "7 Conclusion and Future Work",
        "chunkIndex": 64,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-65",
      "content": "Interpretability Beyond Neuron Level: While several efforts have been made to understand how language models work and means to steer them without training, these methods have mostly focussed on intervening at neuron level. TriLMs opens a new degree of interpretability - at the connection level. Here, the connections between any two neurons in a layer are in one of the three states - 0 (no connection), -1 (negative connection) and +1 (positive connection), each with equal strength. This is in sharp contrast to FloatLMs, where these connections can be of varying strengths, making it harder to study interpretability beyond neuron level. By releasing the checkpoints across our training runs, we facilitate research along these directions.\n\nEnvironmental Benefits and Resource Efficiency: The open release of our models mitigates future emissions by allowing others to bypass the need for pretraining models from scratch.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "8 Broader Impact",
        "chunkIndex": 65,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-66",
      "content": "these directions.\n\nEnvironmental Benefits and Resource Efficiency: The open release of our models mitigates future emissions by allowing others to bypass the need for pretraining models from scratch. Moreover, TriLMs much lesser resource to deploy, and can perform the autoregressive generation as a faster pace - making them critical to scenarios demanding strict latency. Additionally, TriLMs represent a substantial advancement in enhancing performance on resource-constrained edge devices, including smartphones, laptops, and automobiles.\n\nImpact on Specialised Hardware: While TriLMs offers significant memory reduction and latency improvements on General Purpose GPUs like H100 and RTX4090, certain specialized hardware benefits more from ternary modeling. Hardware (like Cerabras 3 ) that support high byte-to-flop ratio computations, can leverage the sparsity stemming from ternarization for speedup in both training as well as inference.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "8 Broader Impact",
        "chunkIndex": 66,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-67",
      "content": "ernary modeling. Hardware (like Cerabras 3 ) that support high byte-to-flop ratio computations, can leverage the sparsity stemming from ternarization for speedup in both training as well as inference. On the other hand, hardware with limited Memory/SRAM (like Groq 4 ), benefit from reduction in the number of chips needed to deploy an LLMs.\n\n3 https://www.cerebras.net/product-chip/\n\n4 https://groq.com/\n\nReduced Training Costs: The Chinchilla scaling laws established that for training compute optimality, it may be recommended to train larger LLMs for lesser tokens than smaller LLMs for more tokens for achieving the desired model performance. However, memory requirements and latency associated with deployment of larger models, has motivated costlier training runs that go far beyond Chinchilla optimality. For example a LLaMa 3 model with only 8B parameter was trained for 15T tokens.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "8 Broader Impact",
        "chunkIndex": 67,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-68",
      "content": "sociated with deployment of larger models, has motivated costlier training runs that go far beyond Chinchilla optimality. For example a LLaMa 3 model with only 8B parameter was trained for 15T tokens. Since, TriLM and ternary models in general can reduce the memory requirements and latency, this can motivate a shift inparameter-token tradeoff for efficient training runs towards Chinchilla's compute-optimal regime.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "8 Broader Impact",
        "chunkIndex": 68,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-69",
      "content": "We acknowledge the support from the Mozilla Responsible AI Grant, the Canada CIFAR AI Chair Program and the Canada Excellence Research Chairs Program. This research was enabled by the computational resources provided by the Summit supercomputer, awarded through the Frontier DD allocation and INCITE 2023 program for the project \"Scalable Foundation Models for Transferable Generalist AI\" and SummitPlus allocation in 2024. These resources were supplied by the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, with support from the Office of Science of the U.S. Department of Energy. We extend special thanks to Jens Glaser for his assistance with the Summit and Frontier supercomputers, and to Darshil Doshi for insightful discussions around scaling laws",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Acknowledgement",
        "chunkIndex": 69,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-70",
      "content": "- A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P. Blunsom, A. Üstün, and S. Hooker. Intriguing properties of quantization at scale, 2023. URL https://arxiv.org/abs/2305. 19268 .\n2. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL\\_CARD.md .\n- Z. Allen-Zhu and Y. Li. Physics of language models: Part 3.3, knowledge capacity scaling laws, 2024. URL https://arxiv.org/abs/2404.05405 .\n4. AMD Team. Amd instinct mi210 accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi200/mi210.html , 2022a. Accessed: July 3, 2024.\n5. AMD Team. Amd instinct mi250 and mi250x accelerators. https://www.amd.com/system/ files/documents/amd-instinct-mi200-datasheet.pdf , 2022b. Accessed: July 3, 2024.\n6. AMD Team. Amd instinct mi300a accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi300/mi300a.html , 2023a. Accessed: July 3, 2024.\n7. AMD Team. Amd instinct mi300x accelerator.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 70,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-71",
      "content": "4.\n6. AMD Team. Amd instinct mi300a accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi300/mi300a.html , 2023a. Accessed: July 3, 2024.\n7. AMD Team. Amd instinct mi300x accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi300/mi300x.html , 2023b. Accessed: July 3, 2024.\n8. AMD Team. Amd instinct mi325x accelerator. https: //ir.amd.com/news-events/press-releases/detail/1201/ amd-accelerates-pace-of-data-center-ai-innovation-and , 2024. Accessed: July 3, 2024.\n- A. Andonian, Q. Anthony, S. Biderman, S. Black, P. Gali, L. Gao, E. Hallahan, J. Levy-Kramer, C. Leahy, L. Nestler, K. Parker, M. Pieler, J. Phang, S. Purohit, H. Schoelkopf, D. Stander, T. Songz, C. Tigges, B. Thérien, P. Wang, and S. Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 9 2023. URL https://www.github.com/eleutherai/ gpt-neox .\n- M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 71,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-72",
      "content": "age Modeling in PyTorch, 9 2023. URL https://www.github.com/eleutherai/ gpt-neox .\n- M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517 , 2024.\n\n- Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013.\n- J. Bethge, M. Bornstein, A. Loy, H. Yang, and C. Meinel. Training competitive binary neural networks from scratch. ArXiv , abs/1812.01965, 2018. URL https://api.semanticscholar. org/CorpusID:54458838 .\n- S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.\n- L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/ .",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 72,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-73",
      "content": "and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.\n- L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/ . Software available from wandb.com.\n- Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence , 2019. URL https://api. semanticscholar.org/CorpusID:208290939 .\n- S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv. org/abs/2204.06745 .\n- Y. Bondarenko, M. Nagel, and T. Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization, 2021. URL https://arxiv.org/abs/2109.12948 .\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 73,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-74",
      "content": "el, and T. Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization, 2021. URL https://arxiv.org/abs/2109.12948 .\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020a.\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020b.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 74,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-75",
      "content": "J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020b. URL https://arxiv.org/abs/2005.14165 .\n- Y. Cai, Z. Yao, Z. Dong, A. Gholami, M. W. Mahoney, and K. Keutzer. Zeroq: A novel zero shot quantization framework, 2020. URL https://arxiv.org/abs/2001.00281 .\n- Y. Choukroun, E. Kravchik, F. Yang, and P. Kisilev. Low-bit quantization of neural networks for efficient inference, 2019. URL https://arxiv.org/abs/1902.06822 .\n- C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2924-2936, Minneapolis, Minnesota, June 2019.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 75,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-76",
      "content": "ence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300 .\n- P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https: //api.semanticscholar.org/CorpusID:3922816 .\n- M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in neural information processing systems , 28: 3123-3131, 2016a.\n- M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations, 2016b. URL https://arxiv.org/abs/1511.00363 .\n- T. Dao.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 76,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-77",
      "content": "3131, 2016a.\n- M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations, 2016b. URL https://arxiv.org/abs/1511.00363 .\n- T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https://arxiv.org/abs/2307.08691 .\n\n- T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/abs/2405.21060 .\n- T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/2205.14135 .\n- T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws, 2023.\n- T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 77,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-78",
      "content": ". Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023. URL https://arxiv.org/abs/2306.03078 .\n- V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme compression of large language models via additive quantization, 2024. URL https://arxiv. org/abs/2401.06118 .\n- E. Frantar and D. Alistarh. Marlin: a fast 4-bit inference kernel for medium batchsizes. https: //github.com/IST-DASLab/marlin , 2024.\n- E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323 , 2022.\n- L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.\n- L. Gao, J. Tow, B. Abbasi, S.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 78,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-79",
      "content": "Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.\n- L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac'h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/ records/10256836 .\n- A. Gholami, Z. Yao, S. Kim, C. Hooper, M. W. Mahoney, and K. Keutzer. Ai and memory wall, 2024. URL https://arxiv.org/abs/2403.14123 .\n11. Google TPU Team. Google cloud tpu v3. https://cloud.google.com/tpu/docs/v3 , 2018. Accessed: July 3, 2024.\n12. Google TPU Team. Google cloud tpu v4. https://cloud.google.com/tpu/docs/v4 , 2021. Accessed: July 3, 2024.\n13. Google TPU Team. Google cloud tpu v5e. https://cloud.google.com/tpu/docs/v5e , 2023a.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 79,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-80",
      "content": "12. Google TPU Team. Google cloud tpu v4. https://cloud.google.com/tpu/docs/v4 , 2021. Accessed: July 3, 2024.\n13. Google TPU Team. Google cloud tpu v5e. https://cloud.google.com/tpu/docs/v5e , 2023a. Accessed: July 3, 2024.\n14. Google TPU Team. Google cloud tpu v5p. https://cloud.google.com/tpu/docs/v5p , 2023b. Accessed: July 3, 2024.\n- D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson, Y. Wang, S. Arora, D. Atkinson, R. Authur, K. R. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel, T. Khot, W. Merrill, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, E. Strubell, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert, K. Richardson, L. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838 .\n- A. Gu and T. Dao.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 80,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-81",
      "content": "rdson, L. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838 .\n- A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752 .\n- A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR) , 2022.\n- D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 81,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-82",
      "content": "de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203. 15556 .\n- S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395 .\n- W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno. How good are low-bit quantized llama3 models? an empirical study, 2024. URL https://arxiv. org/abs/2404.14047 .\n- I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. Neural Computation , 29(1):322-344, 2017.\n- I.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 82,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-83",
      "content": "rical study, 2024. URL https://arxiv. org/abs/2404.14047 .\n- I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. Neural Computation , 29(1):322-344, 2017.\n- I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming, 2020. URL https://arxiv. org/abs/2006.10518 .\n6. Intel Gaudi Team. Intel gaudi 2 and gaudi 3 ai accelerators. https://cdrdv2-public.intel. com/817486/gaudi-3-ai-accelerator-white-paper.pdf , 2024. Accessed: July 3, 2024.\n- M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 83,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-84",
      "content": "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, J. Yang, J. Park, A. Heinecke, E. Georganas, S. Srinivasan, A. Kundu, M. Smelyanskiy, B. Kaul, and P. Dubey. A study of bfloat16 for deep learning training, 2019. URL https://arxiv.org/abs/1905.12322 .\n- J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/ abs/2001.08361 .\n- S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization, 2024.\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.\n12. P.-H.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 84,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-85",
      "content": "Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization, 2024.\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.\n12. P.-H. C. Le and X. Li. Binaryvit: Pushing binary vision transformers towards convolutional models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , pages 4664-4673, June 2023.\n- C. Lee, J. Jin, T. Kim, H. Kim, and E. Park. Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models, 2024. URL https://arxiv.org/abs/2306. 02272 .\n- K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of Applied Mathematics , 2:164-168, 1944.\n- F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711 , 2016.\n- S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and Y. Wang. Evaluating quantized large language models, 2024.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 85,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-86",
      "content": "Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711 , 2016.\n- S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and Y. Wang. Evaluating quantized large language models, 2024. URL https://arxiv.org/abs/2402.18158 .\n- J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. URL https://arxiv.org/abs/2306.00978 .\n\n- S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021.\n- J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence , IJCAI'20, 2021. ISBN 9780999241165.\n- Z. Liu, B. Oguz, A. Pappu, Y. Shi, and R. Krishnamoorthi. Binary and ternary natural language generation, 2023a. URL https://arxiv.org/abs/2306.01841 .\n- Z.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 86,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-87",
      "content": "nce , IJCAI'20, 2021. ISBN 9780999241165.\n- Z. Liu, B. Oguz, A. Pappu, Y. Shi, and R. Krishnamoorthi. Binary and ternary natural language generation, 2023a. URL https://arxiv.org/abs/2306.01841 .\n- Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 , 2023b.\n- Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun, O. Pangarkar, R. Fan, Y. Gu, V. Miller, Y. Zhuang, G. He, H. Li, F. Koto, L. Tang, N. Ranjan, Z. Shen, X. Ren, R. Iriondo, C. Mu, Z. Hu, M. Schulze, P. Nakov, T. Baldwin, and E. P. Xing. Llm360: Towards fully transparent open-source llms, 2023c.\n- S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.\n- D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms .",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 87,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-88",
      "content": "ng, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.\n- D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms . Cambridge University Press, 2003. ISBN 978-0521642989.\n- V. Malinovskii, D. Mazur, I. Ilin, D. Kuznedelev, K. Burlachenko, K. Yi, D. Alistarh, and P. Richtarik. Pv-tuning: Beyond straight-through estimation for extreme llm compression, 2024. URL https: //arxiv.org/abs/2405.14852 .\n- D. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics , 11(2):431-441, 1963.\n- P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training, 2018. URL https://arxiv. org/abs/1710.03740 .\n- T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space, 2013.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 88,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-89",
      "content": ", and H. Wu. Mixed precision training, 2018. URL https://arxiv. org/abs/1710.03740 .\n- T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space, 2013. URL https://arxiv.org/abs/1301.3781 .\n- N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , Online, Nov. 2020. Association for Computational Linguistics.\n- B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. In Proceedings of the International Conference on Learning Representations (ICLR) , 2018.\n14. Nvidia, :, B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, S. Das, A. Dattagupta, O. Delalleau, L. Derczynski, Y. Dong, D. Egert, E. Evans, A.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 89,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-90",
      "content": "B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, S. Das, A. Dattagupta, O. Delalleau, L. Derczynski, Y. Dong, D. Egert, E. Evans, A. Ficek, D. Fridman, S. Ghosh, B. Ginsburg, I. Gitman, T. Grzegorzek, R. Hero, J. Huang, V. Jawa, J. Jennings, A. Jhunjhunwala, J. Kamalu, S. Khan, O. Kuchaiev, P. LeGresley, H. Li, J. Liu, Z. Liu, E. Long, A. S. Mahabaleshwarkar, S. Majumdar, J. Maki, M. Martinez, M. R. de Melo, I. Moshkov, D. Narayanan, S. Narenthiran, J. Navarro, P. Nguyen, O. Nitski, V. Noroozi, G. Nutheti, C. Parisien, J. Parmar, M. Patwary, K. Pawelec, W. Ping, S. Prabhumoye, R. Roy, T. Saar, V. R. N. Sabavat, S. Satheesh, J. P. Scowcroft, J. Sewall, P. Shamis, G. Shen, M. Shoeybi, D. Sizer, M. Smelyanskiy, F. Soares, M. N. Sreedhar, D. Su, S. Subramanian, S. Sun, S. Toshniwal, H. Wang, Z. Wang, J. You, J. Zeng, J. Zhang, J. Zhang, V. Zhang, Y. Zhang, and C. Zhu. Nemotron-4 340b technical report, 2024.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 90,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-91",
      "content": ", F. Soares, M. N. Sreedhar, D. Su, S. Subramanian, S. Sun, S. Toshniwal, H. Wang, Z. Wang, J. You, J. Zeng, J. Zhang, J. Zhang, V. Zhang, Y. Zhang, and C. Zhu. Nemotron-4 340b technical report, 2024. URL https://arxiv.org/abs/2406.11704 .\n15. Nvidia Team. Nvidia tesla v100 gpu accelerator. https://images.nvidia.com/content/ technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf , 2018. Accessed: July 3, 2024.\n\n- Nvidia Team. Nvidia a100 tensor core gpu. https://www. nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/ nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf , 2020. Accessed: July 3, 2024.\n- Nvidia Team. Nvidia h100 tensor core gpu. https://resources.nvidia.com/ en-us-tensor-core/nvidia-tensor-core-gpu-datasheet , 2022. Accessed: July 3, 2024.\n- Nvidia Team. Nvidia h200 tensor core gpu. https://nvdam.widen.net/s/nb5zzzsjdf/ hpc-datasheet-sc23-h200-datasheet-3002446 , 2023. Accessed: July 3 2024.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 91,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-92",
      "content": "gpu-datasheet , 2022. Accessed: July 3, 2024.\n- Nvidia Team. Nvidia h200 tensor core gpu. https://nvdam.widen.net/s/nb5zzzsjdf/ hpc-datasheet-sc23-h200-datasheet-3002446 , 2023. Accessed: July 3 2024. Redirected from https://www.nvidia.com/en-in/data-center/h200/ .\n- Nvidia Team. Nvidia blackwell architecture. https://resources.nvidia.com/ en-us-blackwell-architecture , 2024. Accessed: July 3, 2024.\n- D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In K. Erk and N. A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1525-1534, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https: //aclanthology.org/P16-1144 .\n- A. Papoulis and S. U. Pillai. Probability, Random Variables, and Stochastic Processes .",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 92,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-93",
      "content": "Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https: //aclanthology.org/P16-1144 .\n- A. Papoulis and S. U. Pillai. Probability, Random Variables, and Stochastic Processes . McGraw-Hill, 4 edition, 2002. ISBN 978-0071226615.\n- A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and S. Bowman. BBQ: A hand-built bias benchmark for question answering. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022 , pages 2086-2105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165. URL https://aclanthology.org/2022.findings-acl. 165 .\n- H. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, R. Li, M. Zhang, C. Li, J. Ning, R. Wang, Z. Zhang, S. Liu, J. Chau, H. Hu, and P. Cheng. Fp8-lm: Training fp8 large language models, 2023. URL https://arxiv.org/abs/2310.18313 .\n- O. Press and L.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 93,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-94",
      "content": "Hu, R. Li, M. Zhang, C. Li, J. Ning, R. Wang, Z. Zhang, S. Liu, J. Chau, H. Hu, and P. Cheng. Fp8-lm: Training fp8 large language models, 2023. URL https://arxiv.org/abs/2310.18313 .\n- O. Press and L. Wolf. Using the output embedding to improve language models, 2017. URL https://arxiv.org/abs/1608.05859 .\n- PyTorch Team. Accelerating triton with torchscript and tensorrt integration. https://pytorch. org/blog/accelerating-triton/ , 2024. Accessed: 2024-07-04.\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC '20. IEEE Press, 2020. ISBN 9781728199986.\n- J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 94,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-95",
      "content": "SC '20. IEEE Press, 2020. ISBN 9781728199986.\n- J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining , KDD '20, page 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703 .\n- M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. European Conference on Computer Vision , pages 525-542, 2016.\n- K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM , 64(9):99-106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381 .\n- C. E. Shannon. A mathematical theory of communication.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 95,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-96",
      "content": "ad schema challenge at scale. Commun. ACM , 64(9):99-106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381 .\n- C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal , 27(3): 379-423, 623-656, 1948.\n- N. Shazeer. Glu variants improve transformer, 2020.\n\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\n- D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023. URL https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama .\n- J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n- C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 96,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-97",
      "content": "J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n- C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong. Compression of generative pre-trained language models via quantization. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 4821-4836, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.331. URL https://aclanthology.org/2022.acl-long.331 .\n- G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. Héliou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 97,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-98",
      "content": "erts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. Héliou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G.-C. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J.-B. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, J. Mao-Jones, K. Lee, K. Yu, K. Millican, L. L. Sjoesund, L. Lee, L. Dixon, M. Reid, M. Mikuła, M. Wirth, M. Sharman, N. Chinaev, N. Thain, O. Bachem, O. Chang, O. Wahltinez, P. Bailey, P. Michel, P. Yotov, R. Chaabouni, R. Comanescu, R. Jana, R. Anil, R. McIlroy, R. Liu, R. Mullins, S. L. Smith, S. Borgeaud, S. Girgin, S. Douglas, S. Pandya, S. Shakeri, S. De, T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y. hui Chen, Z. Ahmed, Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 98,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-99",
      "content": ", T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y. hui Chen, Z. Ahmed, Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D. Eck, J. Barral, F. Pereira, E. Collins, A. Joulin, N. Fiedel, E. Senter, A. Andreev, and K. Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295 .\n- H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023a.\n- H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 99,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-100",
      "content": "rrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 100,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-101",
      "content": "ukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper\\_ files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n- H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei. Bitnet: Scaling 1-bit transformers for large language models, 2023.\n- J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. ArXiv , abs/1707.06209, 2017. URL https://api.semanticscholar.org/CorpusID:1553193 .\n- T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 101,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-102",
      "content": "Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,\n\nM. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, Oct. 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6 .\n\n- G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. URL https://arxiv.org/abs/ 2211.10438 .\n- Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers, 2022. URL https://arxiv.org/abs/ 2206.01861 .\n- Z. Yao, X. Wu, C. Li, S. Youn, and Y. He.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 102,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-103",
      "content": "C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers, 2022. URL https://arxiv.org/abs/ 2206.01861 .\n- Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation, 2023. URL https://arxiv.org/abs/ 2303.08302 .\n- Z. Yuan, Y. Shang, and Z. Dong. PB-LLM: Partially binarized large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/ forum?id=BifeBRhikU .\n- R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. Traum, and L. Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472 .\n- B. Zhang and R.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 103,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-104",
      "content": "Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472 .\n- B. Zhang and R. Sennrich. Root mean square layer normalization. In Proceedings of the 33rd International Conference on Neural Information Processing Systems , Red Hook, NY, USA, 2019. Curran Associates Inc.\n- C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR) , 2017.\n- S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 , 2016.\n- S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients, 2018. URL https://arxiv.org/abs/1606. 06160 .\n- C. Zhu, S. Han, H. Mao, and W.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 104,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-105",
      "content": "X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients, 2018. URL https://arxiv.org/abs/1606. 06160 .\n- C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064 , 2017.\n- X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang. A survey on model compression for large language models, 2023. URL https://arxiv.org/abs/2308.07633 .\n- B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing stable and transferable sparse expert models, 2022. URL https://arxiv.org/abs/ 2202.08906 .",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Bibliography",
        "chunkIndex": 105,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-106",
      "content": "This section provides a comprehensive overview of the pretraining for TriLM (Ternary Language Model) and FloatLM (Floating Point Language Model). We outline the forward and backward pass equations specific to their linear layers, highlighting the contrast between the FP16 matrices in FloatLM and the ternary matrices with scalar scaling in TriLM. Additionally, it covers dataset selection, tokenizer usage, and preprocessing methods employed for training data preparation. These discussions provide information on pretraining setups, implementation nuances, and key hyperparameters critical to the models' development.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A Architecture and PreTraining Details",
        "chunkIndex": 106,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-107",
      "content": "Dataset Selection: Let input be X ∈ R b × n for a linear layer with FP16 weight matrix W ∈ R m × n and Y ∈ R b × m be the output. The same matrix W is also used to denote latent weights in TriLMs during training.\n\nFor ternarized layers in TriLMs, we also have a scalar scale γ ∈ R , matrix with ternarized states ̂ W ∈ {-1 , 0 , 1 } n × m and ternarized matrix ˜ W ∈ R n × m . We set ϵ = 1 e -5 .\n\nDue to lack of availability of Pile 300B [Gao et al., 2020] used in Pythia, we opted to use a 300B token sample of deduplicated Slim Pajama dataset 5 . We sample from each subset with the probability proportional to its size.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.1 Data and Tokenizer",
        "chunkIndex": 107,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-108",
      "content": "- Main experiments (Spectra suite) : We used the full 300B token sample.\n- Ablation studies : Training runs with 100B tokens, we sample from these 300B tokens with equal probability weight to each data-point.\n- Fine-Web Edu experiments : We tokenized one-third of a 350B token sample, from which we then sampled 100B tokens for our experiments.\n\nQuantLM: For the creation of QuantLM, we utilized a subset of the Slimpajama-627B dataset, consisting of 512 samples with a sequence length of 2048. These samples were normalized for length. Our approach closely follows the methodology outlined in [Malinovskii et al., 2024].\n\nTokenizer and Optimization Techniques: We use the GPT-NeoX 20B tokenizer following Pythia. For speeding up training, we round embedding rounding of to the nearest multiple of 128 times the model parallel size.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Training Data Preparation:",
        "chunkIndex": 108,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-109",
      "content": "We scale using 2D-parallelism with Megatron-style sharding [Shoeybi et al., 2019] and use ZeRO stage 2 Deepspeed [Rasley et al., 2020] for ZeRO [Rajbhandari et al., 2020].\n\nTable 2: 300B Subset of Slim Pajama\n\n| Dataset        | Size (Tokens)   |\n|----------------|-----------------|\n| Arxiv          | 13B             |\n| Book           | 13B             |\n| C4             | 80B             |\n| Common Crawl   | 156B            |\n| GitHub         | 16B             |\n| Stack Exchange | 10B             |\n| Wikipedia      | 12B             |\n| Total          | 300B            |\n\nOur implementation was based on GPT NeoX Codebase [Andonian et al., 2023]. We use AdamW [Kingma and Ba, 2017] for optimization. We train on nodes with IBM Power9 PC CPUs and 6x16GB V100. Due to the lack of BFloat16 support in V100, we train both TriLM and FloatLM in FP16 using Mixed Precision Training and Dynamic Loss Scaling. Please refer to §A.4 for more implementation specific details.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.2 PreTraining Setup",
        "chunkIndex": 109,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-110",
      "content": "to the lack of BFloat16 support in V100, we train both TriLM and FloatLM in FP16 using Mixed Precision Training and Dynamic Loss Scaling. Please refer to §A.4 for more implementation specific details. We extensively use Huggingface [Wolf et al., 2020] and Wandb [Biewald, 2020] for handling the checkpoints and experiment tracking.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.2 PreTraining Setup",
        "chunkIndex": 110,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-111",
      "content": "Table 3 shows the hyperparameters for TriLM and FloatLM's transformer architecture and their learning rate. We set Adam β are set to (0.9, 0.95) for both families of models and all the reported\n\n5 We also make this subset public\n\nruns are trained to 2048 sequence length. FloatLM and TriLM are respectively trained with batch sizes of 2 M and 1 M tokens respectively.\n\nTable 3: Hyperparameters across model sizes for TriLM and FloatLM.\n\n| Params        |   Hidden |   GLU |   Heads |   Layers |   MP | FloatLM LR     | TriLM LR                        |\n|---------------|----------|-------|---------|----------|------|----------------|---------------------------------|\n| 99.74M (99M)  |      512 |  1280 |       8 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 190.0M (190M) |      768 |  2048 |      12 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 392.4M (390M) |     1024 |  2560 |      16 |       24 |    1 | 3 . 0 ∗ 10 - 4 | 1 .",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.3 Hyperparameters",
        "chunkIndex": 111,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-112",
      "content": "190.0M (190M) |      768 |  2048 |      12 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 392.4M (390M) |     1024 |  2560 |      16 |       24 |    1 | 3 . 0 ∗ 10 - 4 | 1 . 8 ∗ 10 - 3 → 1 . 2 ∗ 10 - 3 |\n| 569.2M (560M) |     1280 |  3072 |      20 |       24 |    1 | 2 . 8 ∗ 10 - 4 | 1 . 6 ∗ 10 - 3 → 1 . 1 ∗ 10 - 3 |\n| 834.0M (830M) |     1536 |  4096 |      24 |       24 |    1 | 2 . 5 ∗ 10 - 4 | 1 . 5 ∗ 10 - 3 → 1 . 0 ∗ 10 - 3 |\n| 1.149B (1.1B) |     1792 |  5120 |      28 |       24 |    2 | 2 . 2 ∗ 10 - 4 | 1 . 3 ∗ 10 - 3 → 9 . 0 ∗ 10 - 4 |\n| 1.515B (1.5B) |     2048 |  6144 |      32 |       24 |    2 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 2.461B (2.4B) |     2304 |  7680 |      36 |       30 |    3 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 3.989B (3.9B) |     3072 |  9216 |      24 |       30 |    6 | 1 . 5 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.3 Hyperparameters",
        "chunkIndex": 112,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-113",
      "content": "680 |      36 |       30 |    3 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 3.989B (3.9B) |     3072 |  9216 |      24 |       30 |    6 | 1 . 5 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n\nTable 4: Sizes in bits (* 10 9 ) for Spectra suite of LLMs across varying parameter counts.\n\n| Params        |   99M |   190M |   390M |   560M |   830M |   1.1B |   1.5B |   2.4B |   3.9B |\n|---------------|-------|--------|--------|--------|--------|--------|--------|--------|--------|\n| FloatLM       |  1.6  |   3.05 |   6.28 |   9.11 |  13.34 |  18.39 |  24.23 |  39.38 |  63.83 |\n| QuantLM 8-Bit |  1.21 |   2.14 |   3.96 |   5.58 |   7.91 |  10.64 |  13.77 |  21.55 |  34.39 |\n| QuantLM 6-Bit |  1.11 |   1.92 |   3.38 |   4.7  |   6.55 |   8.7  |  11.15 |  17.09 |  27.03 |\n| QuantLM 4-Bit |  1.03 |   1.72 |   2.88 |   3.93 |   5.36 |   7    |   8.86 |  13.18 |  20.59 |\n| QuantLM 3-Bit |  0.98 |   1.6  |   2.59 |   3.49 |   4.68 |   6.03 |   7.55 |  10.95 |  16.91 |\n| TriLM         |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.3 Hyperparameters",
        "chunkIndex": 113,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-114",
      "content": "|  1.03 |   1.72 |   2.88 |   3.93 |   5.36 |   7    |   8.86 |  13.18 |  20.59 |\n| QuantLM 3-Bit |  0.98 |   1.6  |   2.59 |   3.49 |   4.68 |   6.03 |   7.55 |  10.95 |  16.91 |\n| TriLM         |  0.9  |   1.42 |   2.11 |   2.76 |   3.55 |   4.42 |   5.36 |   7.23 |  10.76 |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.3 Hyperparameters",
        "chunkIndex": 114,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-115",
      "content": "Similar to BitNet [Wang et al., 2023], our models exhibit artifacts resulting from model parallelism. A key issue arises when computing the scale, γ , across the entire weight matrix, which is sharded across multiple devices. This process introduces a significant communication overhead due to the all-reduce operations. In our implementation, we address this by computing the scales over the portion of the weight matrix local to each device. Consequently, during inference with TriLM models, scales are computed independently within each model parallel group. Importantly, this modification has a negligible impact on the bits per parameter, amounting to less than 10 -5 , even at the highest model parallelism level of 6 for our largest model.\n\nGiven that we train in FP16, some artifacts are expected as a result of this training method. However, we do not anticipate significant performance differences when comparing mixed precision training with BF16 or even FP32.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.4 Known Implementation Artifacts",
        "chunkIndex": 115,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-116",
      "content": "FP16, some artifacts are expected as a result of this training method. However, we do not anticipate significant performance differences when comparing mixed precision training with BF16 or even FP32. This expectation is based on the observation that the lowest loss scales recorded during our runs were consistently at or above the recommended value of 128 [Micikevicius et al., 2018] (refer to Table 5).",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.4 Known Implementation Artifacts",
        "chunkIndex": 116,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-117",
      "content": "TriLM differs from BitNet b1.58 in several ways for better performance as well as for fairer comparison with FloatLMs. Adopting the GPT-3's Pre-Normalization approach as outlined by [Brown et al., 2020a], normalization is applied prior to each linear layer. This method has proven essential for maintaining stable training under FP16 precision [Wang et al., 2023]. Consequently, normalization occurs twice within each transformer layer: once at the input representations to the attention sub-layer and again at the input representations to the Gated MLP sub-layer. This approach contrasts with BitNet, where activation or intermediate representations are normalized, scaled, and quantized to 8 bits before each linear layer, which occurs between 4 to 7 times per transformer layer depending on the specific implementation. Furthermore, TriLM employs RMSNorm with a scale parameter over the parameterless RMSNorm.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.5 Differences from BitNet Architecture",
        "chunkIndex": 117,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-118",
      "content": "inear layer, which occurs between 4 to 7 times per transformer layer depending on the specific implementation. Furthermore, TriLM employs RMSNorm with a scale parameter over the parameterless RMSNorm.\n\nFigure 14 shows the commonsense and reasoning performance of TriLM 1.1B, FloatLM 1.1B and our replication of BitNet b1.58's architecture at 1.1B scale, along with the reported performance\n\nTable 5: Final loss-scale and number of batches skipped across TriLM and FloatLM training runs - We are able to maintain above the recommended loss scales of 128 for mixed precision training [Micikevicius et al., 2018].\n\n| Model        |   Min. Loss-Scale |   # Skipped Batches | # Skipped Tokens   |\n|--------------|-------------------|---------------------|--------------------|\n| FloatLM 99M  |               256 |                 181 | 0.37B              |\n| TriLM 99M    |              1024 |                 303 | 0.33B              |\n| FloatLM 190M |               512 |                 168 | 0.35B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.5 Differences from BitNet Architecture",
        "chunkIndex": 118,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-119",
      "content": "256 |                 181 | 0.37B              |\n| TriLM 99M    |              1024 |                 303 | 0.33B              |\n| FloatLM 190M |               512 |                 168 | 0.35B              |\n| TriLM 190M   |               512 |                 305 | 0.33B              |\n| FloatLM 390M |              1024 |                 170 | 0.35B              |\n| TriLM 390M   |               512 |                 312 | 0.34B              |\n| FloatLM 560M |               256 |                 164 | 0.33B              |\n| TriLM 560M   |               512 |                 294 | 0.32B              |\n| FloatLM 830M |              2048 |                 175 | 0.36B              |\n| TriLM 830M   |               128 |                 307 | 0.33B              |\n| FloatLM 1.1B |              2048 |                 158 | 0.32B              |\n| TriLM 1.1B   |               512 |                 306 | 0.33B              |\n| FloatLM 1.5B |               256 |                 170 | 0.35B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.5 Differences from BitNet Architecture",
        "chunkIndex": 119,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-120",
      "content": "2048 |                 158 | 0.32B              |\n| TriLM 1.1B   |               512 |                 306 | 0.33B              |\n| FloatLM 1.5B |               256 |                 170 | 0.35B              |\n| TriLM 1.5B   |               512 |                 318 | 0.34B              |\n| FloatLM 2.4B |              1024 |                 165 | 0.34B              |\n| TriLM 2.4B   |               256 |                 294 | 0.32B              |\n| FloatLM 3.9B |               256 |                 164 | 0.34B              |\n| TriLM 3.9B   |               128 |                 309 | 0.33B              |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "A.5 Differences from BitNet Architecture",
        "chunkIndex": 120,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-121",
      "content": "Figure 14: Performance across various architectures - TriLM 1.1B, FloatLM 1.1B, BitNet b1.58 1.1B (our replication) along with reported scores of BitNet b1.58 at 700M and 1.3B params. Scores are averaged across 6 common sense and reasoning benchmarks, mentioned in Table 11 and 10.\n\n<!-- image -->\n\nfor BitNet b1.58 700M and 1.3B. All these models have been trained for 100B tokens. Our BitNet replication achieves performance between the 700M and 1.3B models. However, all the BitNet models, including the larger 1.3B parameter model perform worse than TriLM 1.1B. It should be noted that at this 1.1B scale, TriLMs do not achieve parity with FloatLMs of the same parameter count. Table 11 and 10lists the detailed performance of these models across common sense benchmarks.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Relative Performance Across Architectures",
        "chunkIndex": 121,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-122",
      "content": "In this section, we will comprehensively compare Binary Large Language Models (BiLMs) with Ternary Large Language Models (TriLMs). We will start by describing BiLMs, followed by studying scaling laws and presenting results on various benchmarks, as well as comparisons with TriLMs across parameter count and model size (in bits).",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "B Scaling of Binary and Ternary Large Language Models",
        "chunkIndex": 122,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-123",
      "content": "Figures 15a and 15b show the final validation loss across sizes (in bits) and parameters, respectively. At the Billion+ model scale, Ternary Models appear to be preferable in terms of both the number of parameters and model size (in bits). However, the gap seems to be declining. The convergence point appears to occur at a high parameter scale (10B+). Thus, we decided to scale only TriLMs further to study the scaling laws of FLoatLMs and TriLMs upto a scale of 3.9 B parameters. This trend suggests that BiLMs have the potential to match at higher parameter counts.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "B.1 Scaling Laws",
        "chunkIndex": 123,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-124",
      "content": "In Binary Large Language Models (BiLMs), the weights of the linear layers are represented by binary values of -1 or 1, with an accompanying floating-point scaling factor, similar to the method employed in TriLMs. Comprehensive formal descriptions of the forward pass, backward pass, and inference time calculations are provided in Appendix (§A). We have trained three BiLM models of distinct sizes: 99M, 560M, and 1.1B parameters. These models were trained on the same dataset and in the same sequence as the TriLMs, adhering to the identical optimization schedule detailed in Section 3.3.\n\nFigure 15: Final Validation loss across size (measure in bits) and parameters.\n\n<!-- image -->\n\nFigures 15a and 15b show the final validation loss across model sizes (in bits) and parameter counts, respectively. At the Billion+ model scale, Ternary Models appear to be preferable in terms of both the number of parameters and model size (in bits).",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "B.2 BiLM: Binary Large Language Model",
        "chunkIndex": 124,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-125",
      "content": "across model sizes (in bits) and parameter counts, respectively. At the Billion+ model scale, Ternary Models appear to be preferable in terms of both the number of parameters and model size (in bits). However, the gap seems to be narrowing, with convergence likely occurring at higher parameter scales (10B+). Therefore, we decided to scale only TriLMs further to study the scaling laws of FloatLMs and TriLMs up to 3.9B parameters. This trend suggests that BiLMs have the potential to match the performance of floatLM at higher parameter counts.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "B.2 BiLM: Binary Large Language Model",
        "chunkIndex": 125,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-126",
      "content": "We conducted a comprehensive benchmark analysis of Binary Large Language Models (BiLMs) across three key dimensions: commonsense and reasoning tasks, knowledge-based tasks, and toxicity evaluation, as detailed in Tables 6, 7, 9, and 12\n\n<!-- image -->\n\nFigure 16: Performance of ternary TriLMs and BiLMs models on commonsense and Reasoning and MMLUs tasks across Size (Bits) and Parameters. Refer to Tables 6 and 7 for details.\n\nFigure 17: Performance of ternary TriLMs and BiLMs models on LAMBADA tasks across Size (Bits) and Parameters. Refer to Tables 6, and 7 for details.\n\n<!-- image -->\n\nFigure 18: Performance of ternary TriLMs and BiLMs models on MMLU across Size (Bits) and Parameters. Refer to Tables 12 for details.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "B.3 Results",
        "chunkIndex": 126,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-127",
      "content": "In this section, we provide additional insights into the scaling fits discussed in Section 4.3. In addition to fitting a power law with an offset, we also explore a standard power law following Kaplan et al. [2020]. Our findings suggest that the standard power law fits are slightly less precise than those incorporating an offset term. However, both models indicate a decreasing difference in validation loss as the number of parameters ( N ) increases.\n\nFigure 19: Comparison of Power Law and Power Law-with-offset Fits for TriLM and FloatLM.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "C Scaling Law",
        "chunkIndex": 127,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-128",
      "content": "We benchmark TriLM, FloatLM and QuantLM across Knowledge, Commonsense, Reasoning and Toxicity benchmarks. We average our scores across 3 different 'seeds' by preparing three different QuantLM models quantized using different calibration sets. We also add Pythia (deduplicated with consistent 2 M batch size across families) suite of models (70M to 2.8B params) and BitNet b.158 performance scores from their paper for comparison. We use the LM Evaluation Harness [Gao et al., 2023] to benchmark.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D Benchmark Details",
        "chunkIndex": 128,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-129",
      "content": "We report commonsense and reasoning benchmark scores across 6 benchmarks previously considered by BitNet b.158 in Table 6, 7 and rest in Table 9. Each is considered in a zero-shot setting. Following are the details of each of the benchmarks considered:\n\n- ARC Challenge and Easy : [Clark et al., 2018] ARC dataset comprises 7787 multiplechoice science questions divided into two sets: Challenge and Easy. We calculate accuracy and normalised accuracy across both of these sets.\n- BoolQ : [Clark et al., 2019] BoolQ is a reading comprehension dataset consisting of naturally occurring yes/no questions. We calculate the accuracy of this task.\n- HellaSwag : [Zellers et al., 2019] HellaSWAG is a dataset of multiple choice questions for testing grounded commonsense. The incorrect options are generated through Adversarial Filtering (AF) to fool machines but not humans.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.1 Commonsense and Reasoning",
        "chunkIndex": 129,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-130",
      "content": "al., 2019] HellaSWAG is a dataset of multiple choice questions for testing grounded commonsense. The incorrect options are generated through Adversarial Filtering (AF) to fool machines but not humans. We calculate the accuracy and normalized accuracy on this task.\n- WinoGrande : [Sakaguchi et al., 2021] WinoGrande is a collection of 44k problems for testing commonsense reasoning formulated as a fill-in-a-blank task with binary options. We report the accuracy on this task.\n- PIQA : [Bisk et al., 2019] Physical Interaction Question Answering (PIQA) is a physical commonsense reasoning benchmark dataset to test the physical knowledge of language models. We calculate the accuracy and normalized accuracy on this task.\n- LAMBADA OpenAI : [Paperno et al., 2016] LAMBADA is a dataset to evaluate text understanding by next-word prediction.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.1 Commonsense and Reasoning",
        "chunkIndex": 130,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-131",
      "content": "language models. We calculate the accuracy and normalized accuracy on this task.\n- LAMBADA OpenAI : [Paperno et al., 2016] LAMBADA is a dataset to evaluate text understanding by next-word prediction. It is a collection of narrative passages BooksCorpus To succeed on LAMBADA, models must integrate broader discourse information, not solely rely on local context. We calculate the perplexity and the accuracy of the model on this task.\n- LogiQA : [Liu et al., 2021] LogiQA is a dataset for testing human logical reasoning. It contains questions spanning multiple types of deductive reasoning. We calculate the accuracy and normalized accuracy on this task.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.1 Commonsense and Reasoning",
        "chunkIndex": 131,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-132",
      "content": "We report performance on SciQ, TriviaQA in Tables 9, 12 and 13. Each is considered in a zero-shot setting. Following are the details of each of the benchmarks considered:\n\nThe knowledge-based evaluation included the following tasks:\n\n- SciQ : [Welbl et al., 2017] The SciQ dataset contains multiple-choice questions with 4 answer options from crowd-sourced science exams. The questions range from Physics, Chemistry and Biology and several other fields. We calculate the accuracy and length normalized accuracy on this task.\n- TriviaQA : [Joshi et al., 2017] TriviaQA is a reading comprehension dataset containing question-answer-evidence triples. We calculate the exact match accuracy on this task.\n- MMLU [Hendrycks et al., 2021]: The benchmark aims to assess the knowledge gained during pretraining by evaluating models solely in zero-shot and few-shot scenarios. It spans 57 subjects, including STEM fields, humanities, social sciences, and more.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 132,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-133",
      "content": "s to assess the knowledge gained during pretraining by evaluating models solely in zero-shot and few-shot scenarios. It spans 57 subjects, including STEM fields, humanities, social sciences, and more.\n\nTable 6: Spectra Suite Performance (Part 1): Arc Challenge, Arc Easy, and BoolQ. Additionally, we also include scores on the Pythia LLM suite.\n\n| Models                                | Arc Challenge         | Arc Challenge         | Arc Easy              | Arc Easy              | BoolQ                 |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   |\n| Pythia 70M FloatLM 99M                | 22.0 ± 1.2 23.8 ± 1.2 | 22.1 ± 1.2 19.9 ± 1.2 | 24.8 ± 0.9 39.1 ± 1.0 | 24.8 ± 0.9 45.1 ± 1.0 | 38.5 ± 0.9 58.2 ± 0.9 |\n| Quan",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 133,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-134",
      "content": "| Acc                   |\n| Pythia 70M FloatLM 99M                | 22.0 ± 1.2 23.8 ± 1.2 | 22.1 ± 1.2 19.9 ± 1.2 | 24.8 ± 0.9 39.1 ± 1.0 | 24.8 ± 0.9 45.1 ± 1.0 | 38.5 ± 0.9 58.2 ± 0.9 |\n| QuantLM 99M 8-Bit                     | 23.8 ± 1.2            | 19.6 ± 1.2            | 39.4 ± 1.0            | 45.3 ± 1.0            | 58.5 ± 0.9            |\n| QuantLM 99M 6-Bit                     | 23.2 ± 1.2            | 19.7 ± 1.2            | 38.8 ± 1.0            | 44.8 ± 1.0            | 58.9 ± 0.9            |\n| QuantLM 99M 4-Bit                     | 22.6 ± 1.2            | 18.0 ± 1.1            | 37.1 ± 1.0            | 41.7 ± 1.0            | 52.2 ± 0.9            |\n| QuantLM 99M 3-Bit                     | 23.2 ± 1.2            | 19.5 ± 1.2            | 34.8 ± 1.0            | 36.1 ± 1.0            | 48.4 ± 0.9            |\n| TriLM 99M                             | 24.1 ± 1.3            | 19.1 ± 1.1            | 36.6 ± 1.0            | 39.8 ± 1.0            | 61.3 ± 0.9",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 134,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-135",
      "content": "1.0            | 48.4 ± 0.9            |\n| TriLM 99M                             | 24.1 ± 1.3            | 19.1 ± 1.1            | 36.6 ± 1.0            | 39.8 ± 1.0            | 61.3 ± 0.9            |\n| Binary 99M                            | 20.8 ± 1.2            | 18.3 ± 1.1            | 35.8 ± 0.9            | 40.1 ± 1.0            | 61.0 ± 0.8            |\n| Pythia 160M                           | 23.8 ± 1.2            | 23.1 ± 1.2            | 26.7 ± 0.9            | 26.6 ± 0.9            | 38.3 ± 0.9            |\n| FloatLM 190M                          | 24.1 ± 1.3            | 20.5 ± 1.2            | 43.0 ± 1.0            | 48.4 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 8-Bit                    | 24.4 ± 1.3            | 20.3 ± 1.2            | 43.0 ± 1.0            | 48.5 ± 1.0            | 59.3 ± 0.9            |\n| QuantLM 190M 6-Bit                    | 23.8 ± 1.2            | 20.0 ± 1.2            | 42.0 ± 1.0            | 48.0 ± 1.0            | 59.1 ± 0.9",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 135,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-136",
      "content": "| 48.5 ± 1.0            | 59.3 ± 0.9            |\n| QuantLM 190M 6-Bit                    | 23.8 ± 1.2            | 20.0 ± 1.2            | 42.0 ± 1.0            | 48.0 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 4-Bit                    | 25.2 ± 1.3            | 19.9 ± 1.2            | 26.5 ± 0.9            | 26.8 ± 0.9            | 40.9 ± 0.9            |\n| QuantLM 190M 3-Bit                    | 22.5 ± 1.2            | 19.4 ± 1.2            | 37.1 ± 1.0            | 39.7 ± 1.0            | 56.5 ± 0.9            |\n| TriLM 190M                            | 23.0 ± 1.2            | 19.5 ± 1.2            | 39.6 ± 1.0            | 43.9 ± 1.0            | 46.8 ± 0.9            |\n| FloatLM 390M                          | 24.7 ± 1.3            | 21.3 ± 1.2            | 46.5 ± 1.0            | 51.0 ± 1.0            | 54.7 ± 0.9            |\n| QuantLM 390M 8-Bit                    | 24.6 ± 1.3            | 21.2 ± 1.2            | 46.6 ± 1.0            | 51.0 ± 1.0            |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 136,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-137",
      "content": "| 51.0 ± 1.0            | 54.7 ± 0.9            |\n| QuantLM 390M 8-Bit                    | 24.6 ± 1.3            | 21.2 ± 1.2            | 46.6 ± 1.0            | 51.0 ± 1.0            | 54.6 ± 0.9            |\n| QuantLM 390M 6-Bit                    | 24.8 ± 1.3            | 21.5 ± 1.2            | 46.8 ± 1.0            | 51.8 ± 1.0            | 55.3 ± 0.9            |\n| QuantLM 390M 4-Bit                    | 25.1 ± 1.3            | 21.3 ± 1.2            | 45.2 ± 1.0            | 49.6 ± 1.0            | 50.8 ± 0.9            |\n| QuantLM 390M 3-Bit                    | 24.9 ± 1.3            | 21.5 ± 1.2            | 41.6 ± 1.0            | 43.6 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 390M                            | 24.5 ± 1.3            | 21.2 ± 1.2            | 44.1 ± 1.0            | 48.6 ± 1.0            | 55.1 ± 0.9            |\n| Pythia 410M                           | 24.7 ± 1.3            | 21.2 ± 1.2            | 45.7 ± 1.0            | 51.6 ± 1.0",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 137,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-138",
      "content": "44.1 ± 1.0            | 48.6 ± 1.0            | 55.1 ± 0.9            |\n| Pythia 410M                           | 24.7 ± 1.3            | 21.2 ± 1.2            | 45.7 ± 1.0            | 51.6 ± 1.0            | 60.0 ± 0.9            |\n| FloatLM 560M                          | 26.5 ± 1.3            | 23.9 ± 1.2            | 48.4 ± 1.0            | 54.4 ± 1.0            | 57.9 ± 0.9            |\n| QuantLM 560M 8-Bit                    | 26.5 ± 1.3            | 23.6 ± 1.2            | 48.3 ± 1.0            | 54.1 ± 1.0            | 57.6 ± 0.9            |\n| QuantLM 560M 6-Bit QuantLM 560M 4-Bit | 26.0 ± 1.3            | 23.5 ± 1.2            | 47.6 ± 1.0            | 54.2 ± 1.0            | 57.3 ± 0.9            |\n| QuantLM 560M 3-Bit                    | 25.9 ± 1.3 24.0 ± 1.2 | 23.0 ± 1.2 21.2 ± 1.2 | 46.3 ± 1.0            | 52.4 ± 1.0            | 58.8 ± 0.9 59.0 ± 0.9 |\n|                                       |                       |                       | 42.3 ± 1.0            | 45.8",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 138,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-139",
      "content": "2 ± 1.2 | 46.3 ± 1.0            | 52.4 ± 1.0            | 58.8 ± 0.9 59.0 ± 0.9 |\n|                                       |                       |                       | 42.3 ± 1.0            | 45.8 ± 1.0 50.2 ± 1.0 |                       |\n| TriLM 560M                            | 25.7 ± 1.3 24.6 ± 1.2 | 21.0 ± 1.2 20.2 ± 1.1 | 45.5 ± 1.0 41.9 ± 1.0 | 47.8 ± 1.0            | 57.3 ± 0.9 61.5 ± 0.8 |\n| Binary 560M FloatLM 830M              | 28.0 ± 1.3            | 24.5 ± 1.3            | 51.6 ± 1.0            | 57.3 ± 1.0            | 61.0 ± 0.9            |\n| QuantLM 830M 8-Bit                    | 28.2 ± 1.3            | 25.1 ± 1.3            | 51.7 ± 1.0            | 57.3 ± 1.0            | 60.9 ± 0.9            |\n| QuantLM 830M 6-Bit                    | 27.6 ± 1.3            | 24.7 ± 1.3            | 51.6 ± 1.0            | 57.7 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 830M 4-Bit                    | 27.6 ± 1.3            | 23.3 ± 1.2            | 50.5 ± 1.0",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 139,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-140",
      "content": "± 1.3            | 51.6 ± 1.0            | 57.7 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 830M 4-Bit                    | 27.6 ± 1.3            | 23.3 ± 1.2            | 50.5 ± 1.0            | 56.2 ± 1.0            | 58.1 ± 0.9            |\n| QuantLM 830M 3-Bit                    | 27.1 ± 1.3            | 22.7 ± 1.2            | 46.8 ± 1.0            | 50.5 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 830M                            | 25.3 ± 1.3            | 22.5 ± 1.2            | 48.7 ± 1.0            | 54.2 ± 1.0            | 60.4 ± 0.9            |\n| Pythia 1B                             |                       | 24.4 ± 1.3            | 49.0 ± 1.0            | 57.0 ± 1.0            | ±                     |\n| FloatLM 1.1B                          | 27.0 ± 1.3 29.1 ± 1.3 | 26.1 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 60.8 0.9 62.9 ± 0.8   |\n| QuantLM 1.1B 8-Bit                    | 28.9 ± 1.3            | 26.1 ± 1.3            | 54.1 ± 1",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 140,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-141",
      "content": "1.3 | 26.1 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 60.8 0.9 62.9 ± 0.8   |\n| QuantLM 1.1B 8-Bit                    | 28.9 ± 1.3            | 26.1 ± 1.3            | 54.1 ± 1.0            | 60.2 ± 1.0            | 62.6 ± 0.8            |\n| QuantLM 1.1B 6-Bit                    | 29.8 ± 1.3            | 25.5 ± 1.3            | 54.3 ± 1.0            | 60.2 ± 1.0            | 62.9 ± 0.8            |\n|                                       |                       |                       |                       | 59.0 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 1.1B 4-Bit QuantLM 1.1B 3-Bit | 30.3 ± 1.3 29.2 ± 1.3 | 26.0 ± 1.3 27.0 ± 1.3 | 53.6 ± 1.0 48.9 ± 1.0 | 55.0 ± 1.0            | 62.1 ± 0.8            |\n| TriLM 1.1B                            | 26.5 ± 1.3            | 24.6 ± 1.3            | 49.8 ± 1.0            | 56.3 ± 1.0            | 59.1 ± 0.9            |\n| Binary 1.1B                           | 24.8 ± 1.3            | 22.3 ± 1.2",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 141,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-142",
      "content": ".3            | 24.6 ± 1.3            | 49.8 ± 1.0            | 56.3 ± 1.0            | 59.1 ± 0.9            |\n| Binary 1.1B                           | 24.8 ± 1.3            | 22.3 ± 1.2            | 46.1 ± 1.0            | 52.7 ± 1.0            | 56.3 ± 0.9            |\n| Pythia 1.4B                           | 28.7 ± 1.3            | 26.0 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 63.2 ± 0.8            |\n| FloatLM 1.5B                          | 29.7 ± 1.3            | 26.2 ± 1.3            | 56.4 ± 1.0            | 62.6 ± 1.0            | 63.2 ± 0.8            |\n| QuantLM 1.5B 8-Bit                    | 29.8 ± 1.3            | 26.0 ± 1.3            | 56.6 ± 1.0            | 62.4 ± 1.0 62.2 ± 1.0 | 63.3 ± 0.8            |\n| QuantLM 1.5B 6-Bit                    | 30.1 ± 1.3            | 26.0 ± 1.3 ±          | 56.8 ± 1.0 55.2 ± 1.0 |                       | 63.4 ± 0.8            |\n| QuantLM 1.5B 4-Bit                    | 29.4 ± 1.3            | 26.9 1.3",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 142,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-143",
      "content": "| 30.1 ± 1.3            | 26.0 ± 1.3 ±          | 56.8 ± 1.0 55.2 ± 1.0 |                       | 63.4 ± 0.8            |\n| QuantLM 1.5B 4-Bit                    | 29.4 ± 1.3            | 26.9 1.3              |                       | 60.4 ± 1.0            | 62.5 ± 0.8            |\n| QuantLM 1.5B 3-Bit                    | 27.8 ± 1.3            | 25.2 ± 1.3            | 49.7 ± 1.0            | 54.8 ± 1.0            | 53.7 ± 0.9            |\n| TriLM 1.5B                            | 28.2 ± 1.3            | 24.7 ± 1.3            | 53.1 ± 1.0            | 59.0 ± 1.0            | 54.1 ± 0.9            |\n| FloatLM 2.4B                          | 32.7 ± 1.4            | 30.1 ± 1.3            | 60.5 ± 1.0            | 65.5 ± 1.0            | 62.1 ± 0.8            |\n| QuantLM 2.4B 8-Bit                    | 32.6 ± 1.4            | 30.0 ± 1.3            | 60.3 ± 1.0            | 65.7 ± 1.0            | 62.1 ± 0.8 62.0 ± 0.8 |\n| QuantLM 2.4B 6-Bit                    | 32.7 ± 1.4            | 30",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 143,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-144",
      "content": "| 32.6 ± 1.4            | 30.0 ± 1.3            | 60.3 ± 1.0            | 65.7 ± 1.0            | 62.1 ± 0.8 62.0 ± 0.8 |\n| QuantLM 2.4B 6-Bit                    | 32.7 ± 1.4            | 30.6 ± 1.3            | 60.4 ± 1.0            | 65.4 ± 1.0            |                       |\n| QuantLM 2.4B 4-Bit                    | 33.3 ± 1.4            | 30.8 ± 1.3            | 59.6 ± 1.0            | 64.1 ± 1.0            | 59.0 ± 0.9            |\n| QuantLM 2.4B 3-Bit                    | 29.7 ± 1.3            | 28.4 ± 1.3            | 54.2 ± 1.0            | 58.4 ± 1.0            | 55.7 ± 0.9            |\n| TriLM 2.4B                            | 29.9 ± 1.3            | 29.5 ± 1.3            | 58.0 ± 1.0            | 63.8 ± 1.0            | 64.4 ± 0.8            |\n| FloatLM 3.9B                          | 34.6 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0            | 68.3 ± 1.0            | 65.9 ± 0.8            |\n| QuantLM 3.9B 8-Bit                    | 34.6 ± 1.4",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 144,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-145",
      "content": "| 34.6 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0            | 68.3 ± 1.0            | 65.9 ± 0.8            |\n| QuantLM 3.9B 8-Bit                    | 34.6 ± 1.4            | 31.9 ± 1.4            |                       | 68.1 ± 1.0            | 65.4 ± 0.8            |\n| QuantLM 3.9B 6-Bit                    | 35.1 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0 63.3 ± 1.0 | 68.0 ± 1.0            | 65.6 ± 0.8            |\n|                                       |                       |                       |                       |                       | 65.4 ± 0.8            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 34.7 ± 1.4 32.1 ± 1.4 | 32.9 ± 1.4 29.3 ± 1.3 | 61.2 ± 1.0 55.5 ± 1.0 | 68.3 ± 1.0 62.1 ± 1.0 | 60.0 ± 0.9            |\n| TriLM 3.9B                            | 35.3 ± 1.4            | 31.9 ± 1.4            | 60.8 ± 1.0            | 66.0 ± 1.0            | 66.5 ± 0.8            |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 145,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-146",
      "content": "0 62.1 ± 1.0 | 60.0 ± 0.9            |\n| TriLM 3.9B                            | 35.3 ± 1.4            | 31.9 ± 1.4            | 60.8 ± 1.0            | 66.0 ± 1.0            | 66.5 ± 0.8            |\n\nTable 7: Spectra Suite Performance (Part 2): HellaSwag, PIQA, WinoGrande, and Average Scores (including Arc Easy, Arc Challenge, and BoolQ). Additionally, we include scores from the Pythia LLM suite.\n\n| Models                                | HellaSwag             | HellaSwag             | PIQA                  | PIQA                  | WinoGrande            | Avg ( HellaSwag, PIQA, WinoGrande, Arc BoolQ)   |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-------------------------------------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 146,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-147",
      "content": "-|-----------------------|-----------------------|-------------------------------------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   | Easy, Arc Challenge, and                        |\n| Pythia 70M FloatLM 99M                | 25.1 ± 0.4 31.6 ± 0.5 | 25.1 ± 0.4 29.1 ± 0.5 | 49.8 ± 1.2 62.8 ± 1.1 | 49.9 ± 1.2 63.2 ± 1.1 | 49.1 ± 1.4 50.2 ± 1.4 | 34.9 44.3                                       |\n| QuantLM 99M 8-Bit                     | 31.7 ± 0.5            | 29.0 ± 0.5            | 62.6 ± 1.1            | 63.0 ± 1.1            | 50.0 ± 1.4            | 44.3                                            |\n| QuantLM 99M 6-Bit                     | 31.7 ± 0.5            | 29.2 ± 0.5            | 62.8 ± 1.1            | 63.1 ± 1.1            | 50.2 ± 1.4            | 44.3                                            |\n| QuantLM 99M 4-Bit                     | 31.0 ± 0.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 147,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-148",
      "content": "| 29.2 ± 0.5            | 62.8 ± 1.1            | 63.1 ± 1.1            | 50.2 ± 1.4            | 44.3                                            |\n| QuantLM 99M 4-Bit                     | 31.0 ± 0.5            | 28.9 ± 0.5            | 62.2 ± 1.1            | 60.9 ± 1.1            | 50.4 ± 1.4            | 42.6                                            |\n| QuantLM 99M 3-Bit                     | 29.2 ± 0.5            | 27.7 ± 0.4            | 57.2 ± 1.2            | 58.2 ± 1.2            | 49.2 ± 1.4            | 40.3                                            |\n| TriLM 99M                             | 28.4 ± 0.5            | 27.6 ± 0.4            | 60.1 ± 1.1            | 60.4 ± 1.1            | 50.7 ± 1.4            | 43.5                                            |\n| Binary 99M                            | 27.7 ± 0.4            | 27.2 ± 0.4            | 59.2 ± 1.1            | 59.2 ± 1.1            | 48.8 ± 1.4            | 39.8                                            |\n| P",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 148,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-149",
      "content": "| 27.7 ± 0.4            | 27.2 ± 0.4            | 59.2 ± 1.1            | 59.2 ± 1.1            | 48.8 ± 1.4            | 39.8                                            |\n| Pythia 160M                           | 25.1 ± 0.4            | 25.0 ± 0.4            | 53.1 ± 1.2            | 53.1 ± 1.2            | 47.3 ± 1.4            | 35.7                                            |\n| FloatLM 190M                          | 36.6 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.9 ± 1.4            | 46.7                                            |\n| QuantLM 190M 8-Bit                    | 36.5 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.7 ± 1.4            | 46.8                                            |\n| QuantLM 190M 6-Bit                    | 36.3 ± 0.5            | 31.5 ± 0.5            | 65.6 ± 1.1            | 64.3 ± 1.1            | 51.9 ± 1.4            | 46.4",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 149,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-150",
      "content": "|\n| QuantLM 190M 6-Bit                    | 36.3 ± 0.5            | 31.5 ± 0.5            | 65.6 ± 1.1            | 64.3 ± 1.1            | 51.9 ± 1.4            | 46.4                                            |\n| QuantLM 190M 4-Bit                    | 26.0 ± 0.4            | 25.7 ± 0.4            | 49.3 ± 1.2            | 51.7 ± 1.2            | 51.0 ± 1.4            | 36.5                                            |\n| QuantLM 190M 3-Bit                    | 32.0 ± 0.5            | 28.8 ± 0.5            | 58.1 ± 1.2            | 58.7 ± 1.1            | 50.1 ± 1.4            | 42.7                                            |\n| TriLM 190M                            | 31.6 ± 0.5            | 29.0 ± 0.5            | 62.0 ± 1.1            | 62.3 ± 1.1            | 51.7 ± 1.4            | 42.4                                            |\n| FloatLM 390M                          | 44.4 ± 0.5            | 35.7 ± 0.5            | 68.7 ± 1.1            | 68.4",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 150,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-151",
      "content": "| 51.7 ± 1.4            | 42.4                                            |\n| FloatLM 390M                          | 44.4 ± 0.5            | 35.7 ± 0.5            | 68.7 ± 1.1            | 68.4 ± 1.1            | 51.8 ± 1.4            | 48.5                                            |\n| QuantLM 390M 8-Bit                    | 44.5 ± 0.5            | 35.7 ± 0.5            | 68.8 ± 1.1            | 68.6 ± 1.1            | 52.6 ± 1.4            | 48.6                                            |\n| QuantLM 390M 6-Bit                    | 44.2 ± 0.5            | 35.6 ± 0.5            | 69.0 ± 1.1            | 68.4 ± 1.1            | 53.0 ± 1.4            | 48.9                                            |\n| QuantLM 390M 4-Bit                    | 43.4 ± 0.5            | 35.1 ± 0.5            | 68.1 ± 1.1            | 68.3 ± 1.1            | 53.7 ± 1.4            | 47.7                                            |\n| QuantLM 390M 3-Bit                    | 39.5 ± 0.5            | 32.9",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 151,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-152",
      "content": "| 68.1 ± 1.1            | 68.3 ± 1.1            | 53.7 ± 1.4            | 47.7                                            |\n| QuantLM 390M 3-Bit                    | 39.5 ± 0.5            | 32.9 ± 0.5            | 63.8 ± 1.1            | 63.2 ± 1.1            | 53.0 ± 1.4            | 46.5                                            |\n| TriLM 390M                            | 37.9 ± 0.5            | 32.0 ± 0.5            | 64.7 ± 1.1            | 65.0 ± 1.1            | 52.2 ± 1.4            | 46.4                                            |\n| Pythia 410M                           | 40.3 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 66.3 ± 1.1            | 53.5 ± 1.4            | 48.6                                            |\n| FloatLM 560M                          | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.8 ± 1.1            | 69.0 ± 1.1            | 53.7 ± 1.4            | 50.5                                            |\n| QuantLM 560M 8-Bit",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 152,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-153",
      "content": "| 47.6 ± 0.5            | 37.7 ± 0.5            | 68.8 ± 1.1            | 69.0 ± 1.1            | 53.7 ± 1.4            | 50.5                                            |\n| QuantLM 560M 8-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.9 ± 1.1            | 68.9 ± 1.1            | 53.8 ± 1.4            | 50.4                                            |\n| QuantLM 560M 6-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.7 ± 1.1            | 68.8 ± 1.1            | 53.5 ± 1.4            | 50.1                                            |\n| QuantLM 560M 4-Bit                    | 46.7 ± 0.5            | 37.0 ± 0.5            | 67.8 ± 1.1            | 67.1 ± 1.1            | 53.1 ± 1.4            | 49.8                                            |\n| QuantLM 560M 3-Bit                    | 41.7 ± 0.5            | 33.4 ± 0.5            | 63.5 ± 1.1            | 63.2 ± 1.1            | 49.7 ± 1.4            | 46.7",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 153,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-154",
      "content": "|\n| QuantLM 560M 3-Bit                    | 41.7 ± 0.5            | 33.4 ± 0.5            | 63.5 ± 1.1            | 63.2 ± 1.1            | 49.7 ± 1.4            | 46.7                                            |\n| TriLM 560M                            | 41.5 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 67.5 ± 1.1            | 53.1 ± 1.4            | 48.4                                            |\n| Binary 560M                           | 36.4 ± 0.4            | 31.2 ± 0.4            | 64.6 ± 1.1            | 64.2 ± 1.1            | 52.8 ± 1.4            | 44.5                                            |\n| FloatLM 830M QuantLM 830M 8-Bit       | 51.3 ± 0.5 51.4 ± 0.5 | 40.1 ± 0.5 40.1 ± 0.5 | 71.4 ± 1.1 ±          | 71.7 ± 1.1 71.7 ± 1.1 | 56.4 ± 1.4 55.9 ± 1.4 | 53.3 53.2                                       |\n| QuantLM 830M 6-Bit                    | 51.5 ± 0.5            | 40.2 ± 0.5            | 71.2 1.1 71.3 ± 1.1   | 71.8 ± 1.0            | 5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 154,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-155",
      "content": ".9 ± 1.4 | 53.3 53.2                                       |\n| QuantLM 830M 6-Bit                    | 51.5 ± 0.5            | 40.2 ± 0.5            | 71.2 1.1 71.3 ± 1.1   | 71.8 ± 1.0            | 56.2 ± 1.4            | 53.2                                            |\n| QuantLM 830M 4-Bit                    |                       | 39.2 ± 0.5            | 70.6 ± 1.1            | 71.1 ± 1.1            | 56.0 ± 1.4            | 52.2                                            |\n| 3-Bit                                 | 50.2 ± 0.5            | 35.9 ± 0.5            | 66.1 ± 1.1            | 66.6 ± 1.1            | 53.5 ± 1.4            | 49.2                                            |\n| QuantLM 830M                          | 45.5 ± 0.5            |                       |                       |                       |                       |                                                 |\n| TriLM 830M Pythia 1B                  | 46.0 ± 0.5            | 36.8 ± 0.5            | 6",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 155,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-156",
      "content": "|                       |                       |                                                 |\n| TriLM 830M Pythia 1B                  | 46.0 ± 0.5            | 36.8 ± 0.5            | 68.2 ± 1.1            | 68.4 ± 1.1 70.8 ± 1.1 | 55.6 ± 1.4            | 50.7                                            |\n| FloatLM 1.1B                          | 47.2 ± 0.5 55.2 ± 0.5 | 37.7 ± 0.5 42.6 ± 0.5 | 69.3 ± 1.1 72.2 ± 1.0 | 71.3 ± 1.1            | 53.2 ± 1.4 56.3 ± 1.4 | 51.1 54.9                                       |\n| QuantLM 1.1B 8-Bit                    | 55.2 ± 0.5            | 42.6 ± 0.5            | 72.1 ± 1.0            | 71.2 ± 1.1            | 56.2 ± 1.4            | 54.8                                            |\n| QuantLM 1.1B 6-Bit                    | 54.9 ± 0.5            |                       | 71.9 ± 1.0            | 71.2 ± 1.1            | 56.1 ± 1.4            | 55.0                                            |\n| QuantLM 1.1B 4-Bit                    | 5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 156,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-157",
      "content": "|                       | 71.9 ± 1.0            | 71.2 ± 1.1            | 56.1 ± 1.4            | 55.0                                            |\n| QuantLM 1.1B 4-Bit                    | 54.9 ± 0.5            | 42.6 ± 0.5 42.0 ± 0.5 | 71.6 ± 1.1            | 70.4 ± 1.1            | 54.8 ± 1.4            | 54.4                                            |\n| QuantLM 1.1B 3-Bit                    | 51.3 ± 0.5            | 39.4 ± 0.5            | 69.4 ± 1.1            | 68.4 ± 1.1            | 54.8 ± 1.4            | 52.6                                            |\n| TriLM 1.1B                            | 49.1 ± 0.5            | 38.8 ± 0.5            | 69.8 ± 1.1            | 69.3 ± 1.1            | 55.5 ± 1.4            | 51.6                                            |\n|                                       | 43.4 ± 0.5            | 35.1 ± 0.4            | 66.9 ± 1.1            | 68.3 ± 1.1            | 55.3 ± 1.4            | 47.1",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 157,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-158",
      "content": "| 43.4 ± 0.5            | 35.1 ± 0.4            | 66.9 ± 1.1            | 68.3 ± 1.1            | 55.3 ± 1.4            | 47.1                                            |\n| Binary 1.1B Pythia 1.4B               | 52.0 ± 0.5            | 40.4 ± 0.5            | 70.8 ± 1.1            | 70.6 ± 1.1            | 57.1 ± 1.4            | 54.3                                            |\n| FloatLM 1.5B                          | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.9 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.7                                            |\n| QuantLM 1.5B 8-Bit                    | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.7 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.8                                            |\n| QuantLM 1.5B 6-Bit                    | 57.5 ± 0.5            | 44.2 ± 0.5            | 74.0 ± 1.0            | 73.0 ± 1.0            | 59.7 ± 1.4",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 158,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-159",
      "content": "|\n| QuantLM 1.5B 6-Bit                    | 57.5 ± 0.5            | 44.2 ± 0.5            | 74.0 ± 1.0            | 73.0 ± 1.0            | 59.7 ± 1.4            | 56.9                                            |\n| QuantLM 1.5B 4-Bit                    | 56.9 ± 0.5            | 43.2 ± 0.5            | 72.7 ± 1.0            | 72.4 ± 1.0            | 57.1 ± 1.4            | 55.6                                            |\n| QuantLM 1.5B 3-Bit                    | 53.7 ± 0.5            | 41.0 ± 0.5            | 70.0 ± 1.1            | 69.4 ± 1.1            | 55.0 ± 1.4            | 51.6                                            |\n| TriLM 1.5B                            | 53.1 ± 0.5            | 40.9 ± 0.5            | 70.1 ± 1.1            | 70.3 ± 1.1            | 56.1 ± 1.4            | 52.5                                            |\n| FloatLM 2.4B                          | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.2 ± 1.0",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 159,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-160",
      "content": "1            | 56.1 ± 1.4            | 52.5                                            |\n| FloatLM 2.4B                          | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.2 ± 1.0            | 74.9 ± 1.0            | 61.8 ± 1.4            | 59.2                                            |\n|                                       | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.4 ± 1.0            | 74.9 ± 1.0            | 61.4 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 8-Bit QuantLM 2.4B 6-Bit | 62.9 ± 0.5            | 47.0 ± 0.5            | 75.7 ± 1.0            | 74.7 ± 1.0            | 61.1 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 4-Bit                    | 62.2 ± 0.5            | 46.5 ± 0.5            | 75.4 ± 1.0            | 74.5 ± 1.0            | 61.7 ± 1.4            | 58.5                                            |\n| QuantLM 2.4B 3-Bit                    | 58.6 ± 0.5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 160,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-161",
      "content": "5            | 75.4 ± 1.0            | 74.5 ± 1.0            | 61.7 ± 1.4            | 58.5                                            |\n| QuantLM 2.4B 3-Bit                    | 58.6 ± 0.5            | 43.5 ± 0.5            | 72.7 ± 1.0            | 70.8 ± 1.1            | 57.2 ± 1.4            | 54.7                                            |\n| TriLM 2.4B                            | 59.0 ± 0.5            | 45.3 ± 0.5            | 72.6 ± 1.0            | 71.4 ± 1.1            | 59.7 ± 1.4            | 57.3                                            |\n| FloatLM 3.9B                          | 66.1 ± 0.5            | 49.7 ± 0.5            | 75.8 ± 1.0            | 75.4 ± 1.0            | ±                     | 61.4                                            |\n| QuantLM 3.9B 8-Bit                    |                       | 49.7 ± 0.5            |                       | 75.5 ± 1.0            | 62.8 1.4 ±            | 61.3                                            |\n| QuantLM 3.9B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 161,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-162",
      "content": "|                       | 49.7 ± 0.5            |                       | 75.5 ± 1.0            | 62.8 1.4 ±            | 61.3                                            |\n| QuantLM 3.9B 6-Bit                    | 66.0 ± 0.5 65.9 ± 0.5 | 49.7 ± 0.5            | 75.9 ± 1.0 75.5 ± 1.0 | 75.6 ± 1.0            | 62.9 1.4 62.2 ± 1.4   | 61.3                                            |\n| QuantLM 3.9B 4-Bit                    | 65.0 ± 0.5            | 49.0 ± 0.5            | 75.5 ± 1.0            | 75.6 ± 1.0            | 62.7 ± 1.4            | 60.7                                            |\n| QuantLM 3.9B 3-Bit                    | 61.2 ± 0.5            | 45.9 ± 0.5            | 72.6 ± 1.0            | 72.3 ± 1.0            | 59.3 ± 1.4            | 56.8                                            |\n| TriLM 3.9B                            | 64.7 ± 0.5            | 48.3 ± 0.5            | 74.6 ± 1.0            | 74.4 ± 1.0            | 62.1 ± 1.4            | 60.7",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 162,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-163",
      "content": "|\n| TriLM 3.9B                            | 64.7 ± 0.5            | 48.3 ± 0.5            | 74.6 ± 1.0            | 74.4 ± 1.0            | 62.1 ± 1.4            | 60.7                                            |\n\nTable 8: Performance of BitNet b1.58 on ARC Challenge, ARC Easy, BoolQ, HellaSwag, PIQA, and WinoGrande. The scores are taken from [Ma et al., 2024].\n\n| Models      | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ   | HellaSwag   | HellaSwag   | PIQA      | PIQA   | WinoGrande Acc   | Avg   |\n|-------------|-----------------|-----------------|------------|------------|---------|-------------|-------------|-----------|--------|------------------|-------|\n| Models      | Acc Norm.       | Acc             | Acc Norm.  | Acc        | Acc     | Acc Norm.   | Acc         | Acc Norm.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 163,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-164",
      "content": "-----------|-------------|-----------|--------|------------------|-------|\n| Models      | Acc Norm.       | Acc             | Acc Norm.  | Acc        | Acc     | Acc Norm.   | Acc         | Acc Norm. | Acc    |                  |       |\n| BitNet 700M | 21.4            |                 | 51.8       |            | 58.2    | 35.1        |             | 68.1      |        | 55.2             | 48.3  |\n| BitNet 1.3B | 24.2            |                 | 54.9       |            | 56.7    | 37.7        |             | 68.8      |        | 55.8             | 49.7  |\n| BitNet 3B   | 28.3            |                 | 61.4       |            | 61.5    | 42.9        |             | 71.5      |        | 59.3             | 54.2  |\n| BitNet 3.9B | 28.7            |                 | 64.2       |            | 63.5    | 44.2        |             | 73.2      |        | 60.5             | 55.7  |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.2 Knowledge",
        "chunkIndex": 164,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-165",
      "content": "We report toxicity-based evaluation in 12. Each is considered in a zero-shot setting.\n\nThe toxicity-based evaluation included the following tasks:\n\n- BBQ [Parrish et al., 2022]: The Bias Benchmark for QA (BBQ) dataset, comprises sets of questions developed by its authors, focusing on documented social biases directed towards individuals from protected classes across nine distinct social dimensions pertinent to U.S. English-speaking environments.\n- Crows Pairs [Nangia et al., 2020]: proposed a challenging dataset aimed at quantifying stereotypical biases embedded within language models, with a specific emphasis on U.S. contexts. Hosted on GitHub, this dataset serves as a crucial resource for assessing and addressing biases through paired sentences that illuminate societal stereotypes.\n- TruthfulQA [Lin et al., 2021]: A benchmark designed to evaluate the truthfulness of language models in generating responses to questions.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 165,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-166",
      "content": "rough paired sentences that illuminate societal stereotypes.\n- TruthfulQA [Lin et al., 2021]: A benchmark designed to evaluate the truthfulness of language models in generating responses to questions. This benchmark includes 817 questions across 38 categories, such as health, law, finance, and politics.\n\nTable 9: Spectra Suite Performance (Part 3): LAMBADA OpenAI, SciQ, LogiQA. We additionally also include Pythia's performance scores.\n\n| Models                                | LAMBADA               | LAMBADA               | SciQ                  | SciQ                  | LogiQA                | LogiQA                |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Perp.                 | Acc                   | Acc Norm.             | Acc                   | Acc Norm.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 166,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-167",
      "content": "---|-----------------------|-----------------------|\n| Models                                | Perp.                 | Acc                   | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   |\n| FloatLM 99M QuantLM 99M 8-Bit         | 85.0 ± 6.9 85.8 ± 7.0 | 26.5 ± 0.6 26.6 ± 0.6 | 62.9 ± 1.5 62.8 ± 1.5 | 73.6 ± 1.4 73.7 ± 1.4 | 27.6 ± 1.8 27.8 ± 1.8 | 21.2 ± 1.6 21.0 ± 1.6 |\n| QuantLM 99M 6-Bit                     | 89.9 ± 7.4            | 26.1 ± 0.6            | 61.8 ± 1.5            | 73.9 ± 1.4            | 28.1 ± 1.8            | 20.3 ± 1.6            |\n| QuantLM 99M 4-Bit                     | 211.6 ± 17.3          | 16.7 ± 0.5            | 61.2 ± 1.5            | 70.7 ± 1.4            | 24.9 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 99M 3-Bit                     | 4765.4 ± 413.0        | 4.5 ± 0.3             | 51.9 ± 1.6            | 57.0 ± 1.6            | 25.3 ± 1.7            | 19.8 ± 1.6            |\n| TriLM 99M",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 167,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-168",
      "content": "uantLM 99M 3-Bit                     | 4765.4 ± 413.0        | 4.5 ± 0.3             | 51.9 ± 1.6            | 57.0 ± 1.6            | 25.3 ± 1.7            | 19.8 ± 1.6            |\n| TriLM 99M                             | 172.0 ± 8.4           | 20.0 ± 0.6            | 60.4 ± 1.5            | 67.6 ± 1.5            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| Binary 99M                            | 468.3 ± 24.1          | 14.0 ± 0.4            | 54.4 ± 1.6            | 62.5 ± 1.5            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| FloatLM 190M                          | 50.3 ± 2.7            | 31.1 ± 0.6            | 65.1 ± 1.5            | 77.3 ± 1.3            | 27.2 ± 1.7            | 22.1 ± 1.6            |\n| QuantLM 190M 8-Bit                    | 48.7 ± 2.6            | 31.5 ± 0.6            | 65.5 ± 1.5            | 77.1 ± 1.3            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| QuantLM 190M 6-Bit                    | 55.3 ± 3.0            | 30.0 ±",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 168,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-169",
      "content": "| 31.5 ± 0.6            | 65.5 ± 1.5            | 77.1 ± 1.3            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| QuantLM 190M 6-Bit                    | 55.3 ± 3.0            | 30.0 ± 0.6            | 64.2 ± 1.5            | 77.0 ± 1.3            | 26.1 ± 1.7            | 22.4 ± 1.6            |\n| QuantLM 190M 4-Bit                    | 72479077.3            | 0.00 ± 0.0            | 25.6 ± 1.4            | 22.9 ± 1.3            | 23.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 190M 3-Bit                    | 664.5 ± 41.1          | 12.4 ± 0.5            | 58.5 ± 1.6            | 66.4 ± 1.5            | 26.3 ± 1.7            | 21.0 ± 1.6            |\n| TriLM 190M                            | 130.7 ± 6.5           | 23.7 ± 0.6            | 61.0 ± 1.5            | 72.6 ± 1.4            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 390M                          | 21.9 ± 0.9            | 42.2 ± 0.7            | 75.6 ± 1.4            | 84.2 ± 1.2",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 169,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-170",
      "content": "2.6 ± 1.4            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 390M                          | 21.9 ± 0.9            | 42.2 ± 0.7            | 75.6 ± 1.4            | 84.2 ± 1.2            | 28.1 ± 1.8            | 23.8 ± 1.7            |\n| QuantLM 390M 8-Bit                    | 21.7 ± 0.9            | 42.3 ± 0.7            | 75.7 ± 1.4            | 84.1 ± 1.2            | 28.3 ± 1.8            | 24.1 ± 1.7            |\n| QuantLM 390M 6-Bit                    | 24.3 ± 1.0            | 40.6 ± 0.7            | 75.5 ± 1.4            | 83.7 ± 1.2            | 27.6 ± 1.8            | 23.2 ± 1.7            |\n| QuantLM 390M 4-Bit                    | 30.2 ± 1.3            | 39.1 ± 0.7            | 77.1 ± 1.3            | 84.1 ± 1.2            | 25.8 ± 1.7            | 23.3 ± 1.7            |\n| QuantLM 390M 3-Bit                    | 115.0 ± 5.6           | 23.0 ± 0.6            | 67.4 ± 1.5            | 76.7 ± 1.3            | 25.7 ± 1.7            | 21.8 ± 1.6            |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 170,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-171",
      "content": "7            |\n| QuantLM 390M 3-Bit                    | 115.0 ± 5.6           | 23.0 ± 0.6            | 67.4 ± 1.5            | 76.7 ± 1.3            | 25.7 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 390M                            | 77.7 ± 3.8            | 28.0 ± 0.6            | 68.6 ± 1.5            | 76.9 ± 1.3            | 26.4 ± 1.7            | 21.8 ± 1.6            |\n| FloatLM 560M                          | 20.8 ± 0.9            | 44.1 ± 0.7            | 74.7 ± 1.4            | 83.5 ± 1.2            | 27.0 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 8-Bit                    | 20.9 ± 0.9            | 44.2 ± 0.7            | 74.7 ± 1.4            | 83.6 ± 1.2            | 27.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 6-Bit                    | 21.7 ± 0.9            | 42.8 ± 0.7            | 74.4 ± 1.4            | 83.6 ± 1.2            | 25.8 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 560M 4-Bit                    | 24.9 ± 1.1",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 171,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-172",
      "content": "21.7 ± 0.9            | 42.8 ± 0.7            | 74.4 ± 1.4            | 83.6 ± 1.2            | 25.8 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 560M 4-Bit                    | 24.9 ± 1.1            | 40.8 ± 0.7            | 73.6 ± 1.4            | 82.0 ± 1.2            | 27.0 ± 1.7            | 21.7 ± 1.6            |\n| QuantLM 560M 3-Bit                    | 146.3 ± 7.1           | 20.1 ± 0.6            |                       | 75.9 ± 1.4            | 25.0 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 560M                            | 55.6 ± 2.7            | 32.4 ± 0.7            | 71.1 ± 1.4 70.8 ± 1.4 | 78.7 ± 1.3            | 26.1 ± 1.7            | 19.8 ± 1.6            |\n| Binary 560M                           | 62.8 ± 3.0            | 31.0 ± 0.6            | 70.0 ± 1.4            | 78.8 ± 1.3            | 26.7 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 830M                          | 13.3 ± 0.5            | 49.6 ± 0.7            | 78.4 ± 1.3",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 172,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-173",
      "content": "1.4            | 78.8 ± 1.3            | 26.7 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 830M                          | 13.3 ± 0.5            | 49.6 ± 0.7            | 78.4 ± 1.3            | 85.9 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6            |\n| QuantLM 830M 8-Bit                    | 13.5 ± 0.5            | 49.4 ± 0.7            | 78.5 ± 1.3            | 86.1 ± 1.1            | 26.6 ± 1.7            | 20.0 ± 1.6            |\n| QuantLM 830M 6-Bit                    | 13.3 ± 0.5            | 49.1 ± 0.7            | 77.8 ± 1.3            | 85.4 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6 21.2 ± 1.6 |\n| QuantLM 830M 4-Bit                    | 15.4 ± 0.6            | 47.3 ± 0.7            | 78.8 ± 1.3            | 85.1 ± 1.1            | 25.5 ± 1.7            |                       |\n| QuantLM 830M 3-Bit                    | 47.7 ± 2.0            | 30.5 ± 0.6            | 74.1 ± 1.4            | 80.1 ± 1.3            | 28.1 ± 1.8            | 21.2",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 173,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-174",
      "content": "|                       |\n| QuantLM 830M 3-Bit                    | 47.7 ± 2.0            | 30.5 ± 0.6            | 74.1 ± 1.4            | 80.1 ± 1.3            | 28.1 ± 1.8            | 21.2 ± 1.6            |\n| TriLM 830M                            | 26.0 ± 1.1            | 39.9 ± 0.7            | 75.4 ± 1.4            | 82.8 ± 1.2            | 27.6 ± 1.8            | 21.4 ± 1.6            |\n| FloatLM 1.1B                          | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.2 ± 1.2            | 88.1 ± 1.0            | 27.3 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 1.1B 8-Bit                    | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.1 ± 1.2            | 88.1 ± 1.0 88.1 ± 1.0 | 27.8 ± 1.8            | 21.2 ± 1.6            |\n| QuantLM 1.1B 6-Bit                    | 11.7 ± 0.4            | 51.0 ± 0.7            | 82.3 ± 1.2            |                       | 27.5 ± 1.8 28.4 ± 1.8 | 21.5 ± 1.6            |\n| QuantLM 1.1B 4-Bit",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 174,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-175",
      "content": "| 11.7 ± 0.4            | 51.0 ± 0.7            | 82.3 ± 1.2            |                       | 27.5 ± 1.8 28.4 ± 1.8 | 21.5 ± 1.6            |\n| QuantLM 1.1B 4-Bit                    | 13.9 ± 0.5            | 49.3 ± 0.7            | 81.2 ± 1.2            | 87.6 ± 1.0            |                       | 20.3 ± 1.6            |\n| QuantLM 1.1B 3-Bit                    | 26.9 ± 1.1            | 39.1 ± 0.7            | 78.7 ± 1.3            | 85.0 ± 1.1            | 25.8 ± 1.7            | 20.7 ± 1.6            |\n| TriLM 1.1B Binary 1.1B                | 17.3 ± 0.7 33.4 ± 1.4 | 46.2 ± 0.7 37.6 ± 0.6 | 73.3 ± 1.4 71.1 ± 1.4 | 81.9 ± 1.2 81.2 ± 1.3 | 26.9 ± 1.7 28.4 ± 1.7 | 22.0 ± 1.6 23.2 ± 1.6 |\n| FloatLM 1.5B QuantLM 1.5B 8-Bit       | 9.4 ± 0.3 9.5 ± 0.3   | 55.5 ± 0.7 ±          | 80.9 ± 1.2 81.3 ± 1.2 | 87.4 ± 1.0 87.5 ± 1.0 | 26.1 ± 1.7 25.7 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 9.5 ± 0.3             | 55.5 0.7 55.4 ± 0.7   | 81.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 175,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-176",
      "content": "| 80.9 ± 1.2 81.3 ± 1.2 | 87.4 ± 1.0 87.5 ± 1.0 | 26.1 ± 1.7 25.7 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 9.5 ± 0.3             | 55.5 0.7 55.4 ± 0.7   | 81.4 ± 1.2            | 87.6 ± 1.0            | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 6-Bit                    |                       | ±                     |                       |                       | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 4-Bit                    | 17.8 ± 0.7            | 45.3 ± 0.7            | 75.5 ± 1.4            | 82.1 ± 1.2            |                       |                       |\n| QuantLM 1.5B 3-Bit                    | 10.4 ± 0.4            | 53.0 0.7              | 81.1 ± 1.2            | 86.9 ± 1.1            |                       |                       |\n| TriLM 1.5B FloatLM 2.4B               | 7.7 ± 0.3             | 46.2 ± 0.7 59.3 ± 0.7 | 87.2 ± 1.1            | 87.3 ± 1.1 91.0 ± 0.9 | 28.4 ± 1.8",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 176,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-177",
      "content": "|                       |                       |\n| TriLM 1.5B FloatLM 2.4B               | 7.7 ± 0.3             | 46.2 ± 0.7 59.3 ± 0.7 | 87.2 ± 1.1            | 87.3 ± 1.1 91.0 ± 0.9 | 28.4 ± 1.8 29.5 ± 1.8 | 22.7 ± 1.6 21.5 ± 1.6 |\n|                                       | 16.4 ± 0.7            |                       | 80.7 ± 1.2            |                       | 27.8 ± 1.8            | 21.5 ± 1.6            |\n|                                       |                       |                       |                       |                       |                       | 21.5 ± 1.6            |\n| QuantLM 2.4B 8-Bit                    | 7.7 ± 0.3             | 59.2 ± 0.7            | 87.1 ± 1.1            | 91.0 ± 0.9            | 29.5 ± 1.8            |                       |\n| QuantLM 2.4B 6-Bit                    | 7.9 ± 0.3             | 58.9 ± 0.7            | 87.3 ± 1.1            | 90.9 ± 0.9            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 4-Bi",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 177,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-178",
      "content": "ntLM 2.4B 6-Bit                    | 7.9 ± 0.3             | 58.9 ± 0.7            | 87.3 ± 1.1            | 90.9 ± 0.9            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 4-Bit                    | 8.9 ± 0.3             | 56.1 ± 0.7            | 84.8 ± 1.1            | 89.7 ± 1.0            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 3-Bit                    | 15.6 ± 0.6            | 45.0 ± 0.7            | 79.9 ± 1.3            | 86.7 ± 1.1            | 28.6 ± 1.8            | 21.4 ± 1.6            |\n| TriLM 2.4B                            | 8.6 ± 0.3             | 55.7 ± 0.7            | 84.2 ± 1.2            | 88.7 ± 1.0            | 28.6 ± 1.8            | 24.3 ± 1.7            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 6.7 ± 0.2             | 61.1 ± 0.7 61.1 ± 0.7 | 86.5 ± 1.1 86.2 ± 1.1 | 90.9 ± 0.9 91.0 ± 0.9 | 26.9 ± 1.7 26.6 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 6.7 ± 0.2             |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 178,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-179",
      "content": "| 61.1 ± 0.7 61.1 ± 0.7 | 86.5 ± 1.1 86.2 ± 1.1 | 90.9 ± 0.9 91.0 ± 0.9 | 26.9 ± 1.7 26.6 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 6.7 ± 0.2             |                       |                       |                       |                       | 20.4 ± 1.6            |\n| QuantLM 3.9B 6-Bit                    | 6.8 ± 0.2             | 60.8 ± 0.7 58.5 ± 0.7 | 86.6 ± 1.1 86.1 ± 1.1 | 91.3 ± 0.9 90.8 ± 0.9 | 25.8 ± 1.7 28.6 ± 1.8 | 20.1 ± 1.6            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 7.4 ± 0.2 14.0 ± 0.5  | 47.1 ± 0.7            | 83.1 ± 1.2            | 88.6 ± 1.0            | 27.0 ± 1.7            | 21.5 ± 1.6 22.7 ± 1.6 |\n| TriLM 3.9B                            | 6.3 ± 0.2             | 61.6 ± 0.7            | 87.4 ± 1.0            | 90.8 ± 0.9            | 27.6 ± 1.8            |                       |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "D.3 Toxicity",
        "chunkIndex": 179,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-180",
      "content": "Figure 20: Weight distribution in the linear layers of FloatLM models across various model sizes, ranging from 99M to 3.9B parameters\n\n<!-- image -->\n\nThe observed Gaussian distribution in the weights of our final trained FloatLM models across various scales is supported by both theoretical foundations and empirical evidence. We outline the rationale for the normality of weight distributions as follows:\n\nEmpirical Consistency: Across all model scales, ranging from 99 million to 3.9 billion parameters, the weight distributions consistently exhibit Gaussian characteristics, as illustrated in Figure 20. This consistency is visually evident in distribution plots and quantitatively confirmed by fitting Gaussian functions to the weight histograms, demonstrating a good fit across different model sizes.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "E Weight Distribution of Linear Layers",
        "chunkIndex": 180,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-181",
      "content": ". This consistency is visually evident in distribution plots and quantitatively confirmed by fitting Gaussian functions to the weight histograms, demonstrating a good fit across different model sizes.\n\nTheoretical Underpinning: Neural network weight initialization typically follows a Gaussian distribution to facilitate balanced learning dynamics. As training progresses, despite non-linear transformations and complex interactions within the network, the Central Limit Theorem suggests that the aggregation of numerous independent random variables (such as updates during backpropagation) tends toward a normal distribution. This tendency is particularly pronounced given the high dimensionality and extensive data processing involved in training large language models.\n\nStabilization through Regularization and Optimization: Techniques such as L2 regularization constrain weight magnitudes, encouraging them towards smaller values and contributing to a peak around zero-a characteristic feature of",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "E Weight Distribution of Linear Layers",
        "chunkIndex": 181,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-182",
      "content": "arization and Optimization: Techniques such as L2 regularization constrain weight magnitudes, encouraging them towards smaller values and contributing to a peak around zero-a characteristic feature of Gaussian distributions. Additionally, optimization algorithms like Adam, which adjust learning rates based on moving averages of recent gradients, promote smoother updates. This approach maintains the Gaussian form by mitigating the impact of outlier gradients.\n\nGiven these, we can assert that the weight distribution of our trained models closely follows a Gaussian distribution which is crucial for understanding the weight variance across different scales.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "E Weight Distribution of Linear Layers",
        "chunkIndex": 182,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-183",
      "content": "Recent observations [Gholami et al., 2024] suggest that, given the slower pace of improvements in memory and communication compared to compute (FLOPs), the bottleneck continues to shift away from computation towards the memory-related characteristics of hardware for deploying large language models. This shift underscores the importance of exploring solutions that directly address memory constraints. Below, we formally analyze this trend and the impact of low-bitwidth language models on addressing memory bottlenecks during inference.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F Memory Bottlenecks and Low-Bitwidth Language Modelling",
        "chunkIndex": 183,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-184",
      "content": "We begin our analysis by surveying a broad range of recent datacenter General Purpose GPUs (GPGPUs) employed for neural network development and research since 2018. This includes hardware from multiple providers, covering various configurations across the latest microarchitectures.\n\nFrom Nvidia, we consider the following:\n\n- Volta : V100 (SXM/PCIe) [Nvidia Team, 2018],\n- Ampere : A100 (40GB/80GB SXM/PCIe) [Nvidia Team, 2020],\n- Hopper : H100 (SXM/PCIe) and H200 [Nvidia Team, 2022, 2023],\n- Blackwell : This includes preliminary data for Blackwell microarchitectures, which at the time of access were subject to change [Nvidia Team, 2024].\n\nFrom AMD, we analyze the following models:\n\n- MI200 Series : MI210, MI250, MI250X [AMD Team, 2022a,b],\n- MI300 Series : MI300A, MI300X, MI325X [AMD Team, 2023a,b, 2024].\n\nAdditionally, we include hardware from Intel and Google:",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.1 Overview of Recent Datacenter GPUs and Accelerators.",
        "chunkIndex": 184,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-185",
      "content": "llowing models:\n\n- MI200 Series : MI210, MI250, MI250X [AMD Team, 2022a,b],\n- MI300 Series : MI300A, MI300X, MI325X [AMD Team, 2023a,b, 2024].\n\nAdditionally, we include hardware from Intel and Google:\n\n- From Intel, the Gaudi Series : Gaudi 2 and Gaudi 3 [Intel Gaudi Team, 2024],\n- From Google, the Tensor Processing Units (TPUs) : TPUv3 [Google TPU Team, 2018], TPUv4 [Google TPU Team, 2021], and TPUv5 (TPUv5e, TPUv5p) [Google TPU Team, 2023a,b].\n\nAll data was sourced from the respective datasheets, technical documentation, or press releases of the cited hardware. Over the past several years, each of these four accelerator families has improved in three areas - FLOPS, memory capacity, and bandwidth.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.1 Overview of Recent Datacenter GPUs and Accelerators.",
        "chunkIndex": 185,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-186",
      "content": "Memory Capacity and Bandwidth of GPGPUs Relative to Peak TFLOPs. In Figure 21a, we show the trends of Memory Capacity over Peak TFLOPS (Half Precision - FP16/BF16) for various accelerators over the years. We also perform a linear fit for each family of accelerators separately. The linear fit for all the families has a downward slope, showing that memory capacity is improving at a slower pace than computation capability. This trend holds true even for the most recent hardware, such as Blackwell, MI325X, and Gaudi3. Though we consider Half-Precision TFLOPs, the slope is expected to become steeper when considering peak TFLOPS over Ampere sparse or FP8. Similarly, in Figure 21b, we present the trends of Memory Bandwidth (specifically for DRAM or its equivalent memory) over FLOPs for the accelerators over the years, along with the linear fit for each family. We observe a downward slope here as well, indicating the trend that memory bandwidth is growing much slower than computation.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 186,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-187",
      "content": "he accelerators over the years, along with the linear fit for each family. We observe a downward slope here as well, indicating the trend that memory bandwidth is growing much slower than computation.\n\nMemory Wall and Speedup Opportunities. Kim et al. [2024] established the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. As a result, the autoregressive decoding of LLM inference can have a theoretical speedup proportional to its compression factor. Various efficient inference kernels over quantized models have realized this speedup in low batch settings across a variety of hardware. This includes\n\nFigure 21: Trends of Memory/FLOP and Bandwidth/FLOP across different (datacenter) GPGPUs.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 187,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-188",
      "content": "have realized this speedup in low batch settings across a variety of hardware. This includes\n\nFigure 21: Trends of Memory/FLOP and Bandwidth/FLOP across different (datacenter) GPGPUs.\n\n<!-- image -->\n\nCPUs 6 , consumer GPUs 7 and data center GPUs [PyTorch Team, 2024]. However, since TFLOPS to bandwidth ratio is up to 500 times, this ideal speedup can also be achieved in much higher batch settings encountered in LLM deployment. Open-source kernels like Marlin [Frantar and Alistarh, 2024] have demonstrated that these ideal speedups can also be consistently realized in high batch size scenarios and sustained over longer periods of time.\n\nIn Figure 2a, we show the size of models (in GB) across parameter count for two low bitwidth modeling scenarios, TriLM and QuantLM 4-Bit along with the standard half-precision FloatLM. For simplicity, we do not consider the overhead of KV Cache, activations, and compilation overhead incurred during model deployment.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 188,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-189",
      "content": "M and QuantLM 4-Bit along with the standard half-precision FloatLM. For simplicity, we do not consider the overhead of KV Cache, activations, and compilation overhead incurred during model deployment. The FloatLM model starts to reach the capacity of a single H100 at just 34B parameters. At 340 Billion (the size of Nemotron 4) is more than the capacity of a single 8xH100 node. QuantLM 4-Bit scales better, easily supporting the deployment of a 70 billion parameter model (like largest LLaMa 1 and 2) on a single H100 and 300B parameter models on a single MI300X. However, TriLMs with more than 300 billion parameters, with appropriate packing, can fit on a single H100. This feature makes TriLMs especially crucial for deployment at the edge, where devices have less than 8GB or 16GB of RAM, shared across the operating system and multiple applications.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 189,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-190",
      "content": "n a single H100. This feature makes TriLMs especially crucial for deployment at the edge, where devices have less than 8GB or 16GB of RAM, shared across the operating system and multiple applications.\n\nMemory bandwidth and Memory bandwidth of GPGPUs and model inference speedup: Kim et al. [2024] established the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. As a result, the autoregressive decoding of LLM inference can have a theoretical speedup proportional to its compression factor.\n\nVarious efficient inference kernels over quantized models have realized this speedup in low batch settings across a variety of hardware. This includes CPUs 8 , consumer GPUs 9 and data center GPUs [PyTorch Team, 2024].",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 190,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-191",
      "content": "inference kernels over quantized models have realized this speedup in low batch settings across a variety of hardware. This includes CPUs 8 , consumer GPUs 9 and data center GPUs [PyTorch Team, 2024]. However, since TFLOPS to bandwidth ratio is up to 500 times, this ideal speedup can also be achieved in much higher batch settings encountered in LLM deployment. Open-source kernels like Marlin [Frantar and Alistarh, 2024] have demonstrated that these ideal speedups can also be consistently realized in high batch size scenarios and sustained over longer periods of time. In Figure 2b, we show the (theoretically) maximum possible speedup relative to FP16 at varying parameter counts for QuantLM 4-Bit and TriLM. Even at 7 billion parameters, TriLMs can be more than 4 times faster at autoregressive decoding than FloatLM and 2 times faster than QuantLM 4-bit. While QuantLM 4-Bit plateaus at a maximum possible speedup factor of 4x, TriLMs plateau much higher at 10x for FloatLM.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 191,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-192",
      "content": "r at autoregressive decoding than FloatLM and 2 times faster than QuantLM 4-bit. While QuantLM 4-Bit plateaus at a maximum possible speedup factor of 4x, TriLMs plateau much higher at 10x for FloatLM.\n\n6 https://github.com/ggerganov/llama.cpp\n\n7 https://github.com/turboderp/exllamav2\n\n8 https://github.com/ggerganov/llama.cpp\n\n9 https://github.com/turboderp/exllamav2\n\nFigure 22: Model performance on MMLU subsets: STEM, Humanities, Social Sciences, and others. Plot accuracy scores against model size in bits (left) and number of parameters (right), ranging from 560M to 3.9B parameters for TriLM (ternary), FloatLM (FP16), and QuantLM (3-bit &amp; 4-bit).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling",
        "chunkIndex": 192,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-193",
      "content": "Table 11 and 10 shows the performance of ablation 100B token training runs over the six commonsense benchmarks from BitNet b1.58 at 1.1B parameters. The first two rows show the performance of TriLM 1.1B and Float 1.1B at this token count, followed by our replication of BitNet b1.58 (Ours) as well as the scores from BitNet b1.58 over 700M and 1.3B parameters. We observe that at this scale, TriLM does not come close to matching the performance of FloatLM, but it outperforms much larger BitNets. The next two rows show the performance of TriLM 1.1B and FloatLM 1.1B when trained on 100B tokens of FineWeb, instead of SlimPajama. While the performance of both the models improves on FineWeb, the average difference in their performance across datasets remains the same. Lastly, we show the performances across various optimization schedules. A significant drop in averaged performance is noticed when the baseline schedule of linear decay with constant weight decay is used.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 193,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-194",
      "content": "ly, we show the performances across various optimization schedules. A significant drop in averaged performance is noticed when the baseline schedule of linear decay with constant weight decay is used. The gains from dropping l2 regularization in the schedule are more than that of dropping the peak learning rate, however, not enough to match that of TriLM 1.1B's schedule.\n\nTable 10: Ablation Common Sense Task Performance: HellaSwag, PIQA, WinoGrande, Arc Easy, Arc Challenge, BoolQ (Contd.). BitNet b1.58's scores from Ma et al. [2024]. All runs are for 100B tokens on Slim Pajama, except those explicitly stated as FineWeb\n\n| Models                          | HellaSwag   | HellaSwag   | PIQA       | PIQA       | WinoGrande   |\n|---------------------------------|-------------|-------------|------------|------------|--------------|\n|                                 | Acc N.      | Acc         | Acc N.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 194,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-195",
      "content": "A       | WinoGrande   |\n|---------------------------------|-------------|-------------|------------|------------|--------------|\n|                                 | Acc N.      | Acc         | Acc N.     | Acc        | Acc          |\n| FloatLM 1.1B                    | 50.0 ± 0.5  | 39.3 ± 0.5  | 70.9 ± 1.0 | 70.1 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B                      | 46.8 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.4 ± 1.0 | 53.8 ± 1.4   |\n| BitNet b1.58 1.1B (Ours)        | 47.0 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.6 ± 1.0 | 53.4 ± 1.4   |\n| BitNet b1.58 700M               | 35.1        | 35.1        | 68.1       | 68.1       | 55.2         |\n| BitNet b1.58 1.3B               | 37.7        | 37.7        | 68.8       | 68.8       | 55.8         |\n| TriLM 1.1B FineWeb              | 50.0 ± 0.5  | 39.2 ± 0.4  | 70.2 ± 1.0 | 70.1 ± 1.0 | 56.6 ± 1.3   |\n| FloatLM 1.1B FineWeb            | 52.7 ± 0.5  | 41.1 ± 0.4  | 73.0 ± 1.0 | 71.3 ± 1.0 | 56.7 ± 1.3   |\n| TriLM 1.1B Only Peak LR Drop",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 195,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-196",
      "content": "± 0.5  | 39.2 ± 0.4  | 70.2 ± 1.0 | 70.1 ± 1.0 | 56.6 ± 1.3   |\n| FloatLM 1.1B FineWeb            | 52.7 ± 0.5  | 41.1 ± 0.4  | 73.0 ± 1.0 | 71.3 ± 1.0 | 56.7 ± 1.3   |\n| TriLM 1.1B Only Peak LR Dropped | 46.7 ± 0.5  | 36.8 ± 0.4  | 68.9 ± 1.0 | 69.5 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B Only L2 Reg. Dropped | 47.1 ± 0.5  | 37.5 ± 0.4  | 68.6 ± 1.0 | 69.4 ± 1.0 | 55.2 ± 1.4   |\n| TriLM 1.1B Baseline Schedule    | 46.0 ± 0.5  | 36.9 ± 0.4  | 69.3 ± 1.0 | 69.1 ± 1.0 | 56.2 ± 1.3   |\n\nTable 11: Ablation Common Sense Task Performance: HellaSwag, PIQA, WinoGrande, Arc Easy, Arc Challenge, BoolQ. BitNet b1.58's scores from Ma et al. [2024]. All runs are for 100B tokens on Slim Pajama, except those explicitly stated as FineWeb\n\n| Models                          | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ      | Avg ( HellaSwag, PIQA, WinoGrande,   |\n|---------------------------------|-----------------|-----------------|------------|------------|------------|----------",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 196,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-197",
      "content": "| Arc Easy   | Arc Easy   | BoolQ      | Avg ( HellaSwag, PIQA, WinoGrande,   |\n|---------------------------------|-----------------|-----------------|------------|------------|------------|--------------------------------------|\n|                                 | Acc N.          | Acc             | Acc N.     | Acc        | Acc        | Arc Easy, Arc Challenge, and BoolQ)  |\n| FloatLM 1.1B                    | 26.3 ± 1.3      | 22.5 ± 1.2      | 50.3 ± 1.0 | 56.8 ± 1.0 | 60.6 ± 0.8 | 52.2                                 |\n| TriLM 1.1B                      | 26.7 ± 1.3      | 22.9 ± 1.2      | 49.7 ± 1.0 | 55.0 ± 1.0 | 54.9 ± 0.8 | 50.2                                 |\n| BitNet b1.58 1.1B (Ours)        | 26.1 ± 1.2      | 23.6 ± 1.2      | 47.7 ± 1.0 | 55.3 ± 1.0 | 49.7 ± 0.8 | 48.9                                 |\n| BitNet b1.58 700M               | 21.4            | 21.4            | 51.8       | 51.8       | 58.2       | 48.3                                 |\n| BitNet b1.58 1.3B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 197,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-198",
      "content": "|\n| BitNet b1.58 700M               | 21.4            | 21.4            | 51.8       | 51.8       | 58.2       | 48.3                                 |\n| BitNet b1.58 1.3B               | 24.2            | 24.2            | 54.9       | 54.9       | 56.7       | 49.6                                 |\n| TriLM 1.1B FineWeb              | 31.7 ± 1.3      | 31.9 ± 1.3      | 63.1 ± 0.9 | 66.8 ± 0.9 | 58.3 ± 0.8 | 54.9                                 |\n| FloatLM 1.1B FineWeb            | 34.4 ± 1.3      | 33.0 ± 1.3      | 65.7 ± 0.9 | 70.2 ± 0.9 | 59.3 ± 0.8 | 56.9                                 |\n| TriLM 1.1B Only Peak LR Dropped | 27.4 ± 1.3      | 23.6 ± 1.2      | 48.3 ± 1.0 | 55.1 ± 1.0 | 51.6 ± 0.8 | 49.7                                 |\n| TriLM 1.1B Only L2 Reg.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 198,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-199",
      "content": "|\n| TriLM 1.1B Only Peak LR Dropped | 27.4 ± 1.3      | 23.6 ± 1.2      | 48.3 ± 1.0 | 55.1 ± 1.0 | 51.6 ± 0.8 | 49.7                                 |\n| TriLM 1.1B Only L2 Reg. Dropped | 27.6 ± 1.3      | 24.8 ± 1.2      | 49.2 ± 1.0 | 55.1 ± 1.0 | 53.1 ± 0.8 | 50.1                                 |\n| TriLM 1.1B Baseline Schedule    | 26.2 ± 1.2      | 23.2 ± 1.2      | 48.0 ± 1.0 | 54.0 ± 1.0 | 49.4 ± 0.8 | 49.1                                 |\n\nTable 12: Spectra Suite Performance (Part 4): TriviaQA, CrowsPairs, Big Bench BBQ Lite, TruthQA. We additionally also include Pythia's performance scores.\n\n| Models                                | TriviaQA Exact Match   | CrowsPairs                | CrowsPairs            | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n|---------------------------------------|------------------------|---------------------------|-----------------------|--------------------------|-----------------------|\n| Models",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 199,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-200",
      "content": "|\n|---------------------------------------|------------------------|---------------------------|-----------------------|--------------------------|-----------------------|\n| Models                                | TriviaQA Exact Match   | Likelihood diff.          | Pct stereotype        | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n| FloatLM 99M QuantLM 99M 8-Bit         | 0.6 ± 0.1 0.6 ± 0.1    | 372.4 ± 14.6 370.9 ± 14.6 | 55.4 ± 1.2 55.1 ± 1.2 | 30.8 ± 0.4 26.5 ± 0.3    | 24.4 ± 1.5 24.1 ± 1.5 |\n| QuantLM 99M 6-Bit                     | 0.6 ± 0.1              | 389.8 ± 14.8              | 56.9 ± 1.2            | 26.7 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 99M 4-Bit                     | 0.3 ± 0                | 425.5 ± 15.2              | 54.0 ± 1.2            | 26.2 ± 0.3               | 22.9 ± 1.5            |\n| QuantLM 99M 3-Bit                     | 0.1 ± 0                | 611.1 ± 18.8              | 51.0 ± 1.2            | 31.4 ± 0.4               | 24",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 200,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-201",
      "content": "26.2 ± 0.3               | 22.9 ± 1.5            |\n| QuantLM 99M 3-Bit                     | 0.1 ± 0                | 611.1 ± 18.8              | 51.0 ± 1.2            | 31.4 ± 0.4               | 24.6 ± 1.5            |\n| TriLM 99M                             | 0.1 ± 0                | 362.4 ± 10.8              | 54.2 ± 1.2            | 30.8 ± 0.4               | 24.2 ± 1.5            |\n| Binary 99M                            | 0.2 ± 0.0              | 353.5 ± 11.1              | 53.3 ± 1.2            | 31.5 ± 0.3               | 25.7 ± 1.5            |\n| FloatLM 190M                          | 0.6 ± 0.1              | 348.2 ± 11.3              | 55.9 ± 1.2            | 27.3 ± 0.4               | 22.4 ± 1.5            |\n| QuantLM 190M 8-Bit                    | 0.7 ± 0.1              | 352.7 ± 11.4              | 56.2 ± 1.2            | 27.1 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 6-Bit                    | 0.7 ± 0.1              | 368.9 ± 11.7              | 56.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 201,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-202",
      "content": "52.7 ± 11.4              | 56.2 ± 1.2            | 27.1 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 6-Bit                    | 0.7 ± 0.1              | 368.9 ± 11.7              | 56.2 ± 1.2            | 27.2 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 4-Bit                    | 0.0 ± 0                | 961.9 ± 25.4              | 43.8 ± 1.2            | 35.0 ± 0.4               | 24.2 ± 1.5            |\n| QuantLM 190M 3-Bit                    | 0.1 ± 0                | 482.7 ± 15.2              | 53.7 ± 1.2            | 26.4 ± 0.3               | 25.0 ± 1.5            |\n| TriLM 190M                            | 0.2 ± 0                | 343.5 ± 10.9              | 55.5 ± 1.2            | 29.7 ± 0.4               | 23.9 ± 1.5            |\n| FloatLM 390M                          | 2.8 ± 0                | 355.5 ± 10.4              | 59.6 ± 1.2            | 25.4 ± 0.3               | 22.4 ± 1.5            |\n| QuantLM 390M 8-Bit                    | 2.9 ±",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 202,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-203",
      "content": "| 2.8 ± 0                | 355.5 ± 10.4              | 59.6 ± 1.2            | 25.4 ± 0.3               | 22.4 ± 1.5            |\n| QuantLM 390M 8-Bit                    | 2.9 ± 0                | 355.8 ± 10.4              | 59.8 ± 1.2            | 25.4 ± 0.3               | 22.2 ± 1.5            |\n| QuantLM 390M 6-Bit                    | 2.4 ± 0                | 360.5 ± 10.4              | 60.6 ± 1.2            | 25.3 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 4-Bit                    | 1.3 ± 0.1              | 368.2 ± 10.2              | 59.4 ± 1.2            | 25.5 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 3-Bit                    | 0.8 ± 0.1              | 444.4 ± 12.2              | 54.3 ± 1.2            | 26.3 ± 0.3               | 23.0 ± 1.5            |\n| TriLM 390M                            | 1.3 ± 0.1              | 344.5 ± 10.3              | 58.3 ± 1.2            | 26.9 ± 0.3               | 24.4 ± 1.5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 203,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-204",
      "content": "| 23.0 ± 1.5            |\n| TriLM 390M                            | 1.3 ± 0.1              | 344.5 ± 10.3              | 58.3 ± 1.2            | 26.9 ± 0.3               | 24.4 ± 1.5            |\n| FloatLM 560M                          | 4.6 ± 0.2              | 351.8 ± 9.9               | 58.9 ± 1.2            | 25.7 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 560M 8-Bit                    | 4.7 ± 0.2              | 352.9 ± 10.0              | 59.2 ± 1.2            | 25.7 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 560M 6-Bit                    | 3.5 ± 0.1              | 353.7 ± 9.9               | 59.3 ± 1.2            | 25.8 ± 0.3               | 22.0 ± 1.5            |\n| QuantLM 560M 4-Bit 3-Bit              | 2.1 ± 0.1              | 372.7 ± 10.7              | 59.2 ± 1.2            | 27.0 ± 0.4 29.0 ± 0.4    | 22.2 ± 1.5 22.9 ± 1.5 |\n| QuantLM 560M TriLM 560M               | 1.5 ± 0.1              | 411.2 ± 11.3              | 57.9 ± 1.2 58.7 ± 1.2 |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 204,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-205",
      "content": "| 59.2 ± 1.2            | 27.0 ± 0.4 29.0 ± 0.4    | 22.2 ± 1.5 22.9 ± 1.5 |\n| QuantLM 560M TriLM 560M               | 1.5 ± 0.1              | 411.2 ± 11.3              | 57.9 ± 1.2 58.7 ± 1.2 | 25.5 ± 0.3               |                       |\n| Binary 560M                           | 2.4 ± 0.1 0.2 ± 0.0    | 345.1 ± 10.1 356.3 ± 10.4 | 58.5 ± 1.2            | 26.3 ± 0.3               | 23.6 ± 1.5 23.1 ± 1.4 |\n| FloatLM 830M                          | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.6 ± 1.2            | 25.7 ± 0.3               | 23.1 ± 1.5            |\n| QuantLM 830M 8-Bit                    | 8.5 ± 0.2              | 354.5 ± 9.6               | 62.1 ± 1.2            | 25.6 ± 0.3               | 23.0 ± 1.5            |\n| QuantLM 830M 6-Bit                    | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.7 ± 1.2            | 25.5 ± 0.3               | 22.5 ± 1.5            |\n| QuantLM 830M 4-Bit                    | 10.6 ± 0.2             | 3",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 205,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-206",
      "content": "| 8.5 ± 0.2              | 354.6 ± 9.6               | 62.7 ± 1.2            | 25.5 ± 0.3               | 22.5 ± 1.5            |\n| QuantLM 830M 4-Bit                    | 10.6 ± 0.2             | 364.2 ± 9.8               | 59.9 ± 1.2            | 25.9 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 830M 3-Bit                    | 3.1 ± 0.1              | 389.5 ± 10.9              | 59.9 ± 1.2            | 30.5 ± 0.4               | 24.4 ± 1.5            |\n| TriLM 830M                            | 4.3 ± 0.2              | 344.9 ± 10.0              | 60.7 ± 1.2            | 25.1 ± 0.3               | 22.8 ± 1.5            |\n| FloatLM 1.1B                          | 12.9 ± 0.3             | 349.2 ± 9.7               | 61.2 ± 1.2            | 25.4 ± 0.3               | 21.4 ± 1.4            |\n| QuantLM 1.1B 8-Bit                    | 12.7 ± 0.2             | 349.5 ± 9.7               | 61.1 ± 1.2            | 25.4 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 1.1B 6-B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 206,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-207",
      "content": "|\n| QuantLM 1.1B 8-Bit                    | 12.7 ± 0.2             | 349.5 ± 9.7               | 61.1 ± 1.2            | 25.4 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 1.1B 6-Bit QuantLM 1.1B 4-Bit | 12.4 ± 0.2 9.3 ± 0.2   | 349.7 ± 9.6 359.1 ± 10.1  | 59.9 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4 21.3 ± 1.4 |\n|                                       |                        |                           | 60.9 ± 1.2            | 25.4 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 1.1B 3-Bit                    | 6.8 ± 0.2              | 422.4 ± 11.5              | 58.7 ± 1.2            | 29.9 ± 0.4               |                       |\n| TriLM 1.1B                            | 1.9 ± 0.1              | 343.4 ± 9.9               | 61.4 ± 1.2            | 25.8 ± 0.3               | 21.5 ± 1.4            |\n| Binary 1.1B                           | 2.2 ± 0.1              | 351.6 ± 9.8               | 58.3 ± 1.2            | 26.4 ± 0.3",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 207,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-208",
      "content": "| 25.8 ± 0.3               | 21.5 ± 1.4            |\n| Binary 1.1B                           | 2.2 ± 0.1              | 351.6 ± 9.8               | 58.3 ± 1.2            | 26.4 ± 0.3               | 23.2 ± 1.4            |\n| FloatLM 1.5B                          | 12.2 ± 0.2             | 351.9 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 8-Bit                    | 12.5 ± 0.2             | 352.4 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 4-Bit                    | 9.0 ± 0.2              | 357.9 ± 9.8               | 60.7 ± 1.2            | 25.9 ± 0.3               | 20.8 ± 1.4            |\n| QuantLM 1.5B 3-Bit                    | 4.2 ± 0.1              | 400.0 ± 10.6              | 60.9 ± 1.2            | 26.8 ± 0.3               | 20.8 ± 1.4            |\n| TriLM 1.5B                            | 5.9 ± 0.1              | 348.9 ± 9.9",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 208,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-209",
      "content": "| 400.0 ± 10.6              | 60.9 ± 1.2            | 26.8 ± 0.3               | 20.8 ± 1.4            |\n| TriLM 1.5B                            | 5.9 ± 0.1              | 348.9 ± 9.9               | 59.9 ± 1.2            | 25.2 ± 0.3               | 21.7 ± 1.4            |\n| FloatLM 2.4B QuantLM 2.4B 8-Bit       | 20.7 ± 0.3 20.7 ± 0.3  | 360.4 ± 9.4 360.5 ± 9.4   | 64.2 ± 1.2 64.2 ± 1.2 | 26.7 ± 0.3               | 21.7 ± 1.4 21.9 ± 1.4 |\n| QuantLM 2.4B 6-Bit                    | 20.4 ± 0.3             | 360.8 ± 9.5               | 63.4 ± 1.2            | 26.5 ± 0.3 26.4 ± 0.3    | 21.8 ± 1.4 21.7 ± 1.4 |\n| QuantLM 2.4B 4-Bit                    | 21.1 ± 0.3             | 358.7 ± 9.6               | 63.4 ± 1.2            | 26.0 ± 0.3 ±             |                       |\n| QuantLM 2.4B 3-Bit                    | 10.9 ± 0.2             | 360.2 ± 9.5               | 59.9 ± 1.2            | 25.8 0.3                 | 21.5 ± 1.4            |\n| TriLM 2.4B",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 209,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-210",
      "content": "M 2.4B 3-Bit                    | 10.9 ± 0.2             | 360.2 ± 9.5               | 59.9 ± 1.2            | 25.8 0.3                 | 21.5 ± 1.4            |\n| TriLM 2.4B                            | 12.3 ± 0.1             | 353.0 ± 10.0              | 64.1 ± 1.2            | 25.4 ± 0.3               | 23.0 ± 1.5            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 21.5 ± 0.3 ±           | 359.2 ± 9.6 359.8 ± 9.6   | 64.6 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n|                                       |                        |                           | 64.7 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n| QuantLM 3.9B 6-Bit                    | 21.7 0.3 21.0 ± 0.3    | 359.5 ± 9.6               | 63.9 ± 1.2            | 25.4 ± 0.3               | 23.5 ± 1.5            |\n| QuantLM 3.9B 4-Bit                    | 17.9 ± 0.3             | 365.5 ± 9.7               | 64.8 ± 1.2            | 25.3 ± 0.3               | 24.2 ± 1.5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 210,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-211",
      "content": "3               | 23.5 ± 1.5            |\n| QuantLM 3.9B 4-Bit                    | 17.9 ± 0.3             | 365.5 ± 9.7               | 64.8 ± 1.2            | 25.3 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 3.9B 3-Bit                    | 8.2 ± 0.2              | 365.9 ± 9.8               | 64.3 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4            |\n| TriLM 3.9B                            | 21.3 ± 0.3             | 362.4 ± 9.6               | 65.4 ± 1.2            | 25.9 ± 0.3               | 24.1 ± 1.5            |\n\nTable 13: Spectra Suite Performance (Part 5): MMLU- STEM, Humanities, Social Sciences, Others.\n\n| Models                                | Stem                  | Humanities            | MMLUAccuracy Social Sciences   | Other                 | Avg.                  |\n|---------------------------------------|-----------------------|-----------------------|--------------------------------|-----------------------|-----------------------|\n| FloatLM",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 211,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-212",
      "content": "|\n|---------------------------------------|-----------------------|-----------------------|--------------------------------|-----------------------|-----------------------|\n| FloatLM 99M                           | 22.8 ± 0.7            | 24.0 ± 0.6            | 27.0 ± 0.8                     | 28.0 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 8-Bit                     | 22.9 ± 0.7            | 24.2 ± 0.6            | 26.9 ± 0.8                     | 27.9 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 6-Bit                     | 22.7 ± 0.7            | 24.1 ± 0.6            | 26.6 ± 0.8                     | 28.2 ± 0.8            | 25.2 ± 0.4            |\n| QuantLM 99M 4-Bit                     | 22.9 ± 0.7            | 24.1 ± 0.6            | 26.7 ± 0.8                     | 27.4 ± 0.8            | 25.1 ± 0.4            |\n| QuantLM 99M 3-Bit                     | 23.5 ± 0.8            | 23.9 ± 0.6            | 26.2 ± 0.8                     | 25",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 212,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-213",
      "content": "0.8                     | 27.4 ± 0.8            | 25.1 ± 0.4            |\n| QuantLM 99M 3-Bit                     | 23.5 ± 0.8            | 23.9 ± 0.6            | 26.2 ± 0.8                     | 25.9 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 99M                             | 23.9 ± 0.8            | 23.6 ± 0.6            | 26.7 ± 0.8                     | 26.6 ± 0.8            | 25.0 ± 0.4            |\n| Binary 99M                            | 21.6 ± 0.7            | 24.3 ± 0.6            | 21.8 ± 0.7                     | 24.0 ± 0.7            | 23.1 ± 0.3            |\n| FloatLM 190M                          | 24.0 ± 0.8            | 24.4 ± 0.6            | 28.8 ± 0.8                     | 30.1 ± 0.8            | 26.5 ± 0.4            |\n| QuantLM 190M 8-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.9 ± 0.8                     | 30.0 ± 0.8            | 26.6 ± 0.4            |\n| QuantLM 190M 6-Bit                    | 24.1 ± 0.8            | 24.5",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 213,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-214",
      "content": "24.1 ± 0.8            | 24.5 ± 0.6            | 28.9 ± 0.8                     | 30.0 ± 0.8            | 26.6 ± 0.4            |\n| QuantLM 190M 6-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.3 ± 0.8                     | 29.8 ± 0.8            | 26.4 ± 0.4            |\n| QuantLM 190M 4-Bit                    | 22.9 ± 0.7            | 22.9 ± 0.6            | 24.5 ± 0.8                     | 23.4 ± 0.8            | 23.4 ± 0.4            |\n| QuantLM 190M 3-Bit                    | 23.9 ± 0.8            | 23.2 ± 0.6            | 25.4 ± 0.8                     | 27.5 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 190M                            | 22.5 ± 0.7            | 23.8 ± 0.6            | 26.7 ± 0.8                     | 28.4 ± 0.8            | 25.2 ± 0.4            |\n| FloatLM 390M                          | 25.8 ± 0.8            | 25.9 ± 0.6            | 30.3 ± 0.8                     | 32.8 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 214,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-215",
      "content": "|\n| FloatLM 390M                          | 25.8 ± 0.8            | 25.9 ± 0.6            | 30.3 ± 0.8                     | 32.8 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 8-Bit                    | 25.7 ± 0.8            | 25.9 ± 0.6            | 30.2 ± 0.8                     | 32.4 ± 0.8            | 28.2 ± 0.4            |\n| QuantLM 390M 6-Bit                    | 26.0 ± 0.8            | 25.8 ± 0.6            | 30.2 ± 0.8                     | 32.3 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 4-Bit                    | 25.5 ± 0.8            | 25.4 ± 0.6            | 30.5 ± 0.8                     | 31.6 ± 0.8            | 27.9 ± 0.4            |\n| QuantLM 390M 3-Bit                    | 24.4 ± 0.8            | 25.0 ± 0.6            | 29.4 ± 0.8                     | 29.3 ± 0.8            | 26.8 ± 0.4            |\n| TriLM 390M                            | 24.1 ± 0.8            | 24.8 ± 0.6            | 28.3 ± 0.8                     | 29.0 ± 0",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 215,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-216",
      "content": "| 29.3 ± 0.8            | 26.8 ± 0.4            |\n| TriLM 390M                            | 24.1 ± 0.8            | 24.8 ± 0.6            | 28.3 ± 0.8                     | 29.0 ± 0.8            | 26.4 ± 0.4            |\n| FloatLM 560M                          | 24.8 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 32.3 ± 0.8            | 28.4 ± 0.4            |\n| QuantLM 560M 8-Bit                    | 24.8 ± 0.8            | 26.6 ± 0.6            | 30.5 ± 0.8                     | 32.1 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 560M 6-Bit                    | 24.6 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 31.3 ± 0.8            | 28.1 ± 0.4            |\n| QuantLM 560M 4-Bit QuantLM 560M 3-Bit | 24.7 ± 0.8            | 25.9 ± 0.6            | 29.9 ± 0.8                     | 31.1 ± 0.8 28.2 ± 0.8 | 27.7 ± 0.4 26.0 ± 0.4 |\n| TriLM 560M                            | 24.5 ± 0.8 25.0 ± 0.8 | 24.2 ± 0.6",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 216,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-217",
      "content": "± 0.8            | 25.9 ± 0.6            | 29.9 ± 0.8                     | 31.1 ± 0.8 28.2 ± 0.8 | 27.7 ± 0.4 26.0 ± 0.4 |\n| TriLM 560M                            | 24.5 ± 0.8 25.0 ± 0.8 | 24.2 ± 0.6 ±          | 28.1 ± 0.8 29.0 ± 0.8          | 30.2 ± 0.8            | 27.0 ± 0.4            |\n|                                       |                       | 25.1 0.6              |                                | 23.9 ± 0.7            | 22.9 ± 0.3            |\n| Binary 560M FloatLM 830M              | 21.4 ± 0.7 25.8 ± 0.8 | 24.2 ± 0.6 27.5 ± 0.6 | 21.6 ± 0.7 32.3 ± 0.8          | 34.6 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 8-Bit                    | 25.8 ± 0.8            | 27.4 ± 0.6            | 32.1 ± 0.8                     | 34.7 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 6-Bit                    | 25.6 ± 0.8            | 27.3 ± 0.6            | 32.1 ± 0.8                     | 34.2 ± 0.8            | 29.5 ± 0.4            |\n| QuantLM 830M 4-Bit",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 217,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-218",
      "content": "|\n| QuantLM 830M 6-Bit                    | 25.6 ± 0.8            | 27.3 ± 0.6            | 32.1 ± 0.8                     | 34.2 ± 0.8            | 29.5 ± 0.4            |\n| QuantLM 830M 4-Bit                    | 25.9 ± 0.8            | 26.8 ± 0.6            | 31.2 ± 0.8                     | 33.6 ± 0.8            | 29.1 ± 0.4            |\n| QuantLM 830M 3-Bit                    | 24.8 ± 0.8            | 25.1 ± 0.6            | 28.9 ± 0.8                     | 30.8 ± 0.8            | 27.1 ± 0.4            |\n| TriLM 830M                            | 24.9 ± 0.8            | 25.8 ± 0.6            | 30.1 ± 0.8                     | 31.1 ± 0.8            | 27.7 ± 0.4            |\n| FloatLM 1.1B                          | 26.4 ± 0.8            | 27.6 ± 0.6            | 32.5 ± 0.8                     | 34.8 ± 0.8            | 30.0 ± 0.4            |\n| QuantLM 1.1B 8-Bit                    | 26.2 ± 0.8            | 27.4 ± 0.6            | 32.5 ± 0.8                     | 34.9 ± 0.8",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 218,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-219",
      "content": "| 34.8 ± 0.8            | 30.0 ± 0.4            |\n| QuantLM 1.1B 8-Bit                    | 26.2 ± 0.8            | 27.4 ± 0.6            | 32.5 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 6-Bit                    | 26.0 ± 0.8            | 27.5 ± 0.6            | 32.7 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 4-Bit                    | 26.0 ± 0.8            | 26.6 ± 0.6            | 32.4 ± 0.8                     | 33.8 ± 0.8            | 29.3 ± 0.4            |\n| QuantLM 1.1B 3-Bit                    | 25.9 ± 0.8            | 26.1 ± 0.6            | 30.0 ± 0.8                     | 33.0 ± 0.8            | 28.4 ± 0.4            |\n| TriLM 1.1B                            | 25.2 ± 0.8            | 26.1 ± 0.6            | 30.6 ± 0.8                     | 32.2 ± 0.8            | 28.3 ± 0.4            |\n| Binary 1.1B                           | 21.0 ± 0.7            | 24.2 ± 0.6",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 219,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-220",
      "content": "| 26.1 ± 0.6            | 30.6 ± 0.8                     | 32.2 ± 0.8            | 28.3 ± 0.4            |\n| Binary 1.1B                           | 21.0 ± 0.7            | 24.2 ± 0.6            | 21.7 ± 0.8                     | 24.4 ± 0.7            | 23.0 ± 0.3            |\n| FloatLM 1.5B                          | 26.1 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.6 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 8-Bit                    | 26.1 ± 0.8            | 28.1 ± 0.7            | 32.9 ± 0.8                     | 35.5 ± 0.8            | 30.3 ± 0.4            |\n| QuantLM 1.5B 6-Bit                    | 26.3 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.4 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 4-Bit                    | 26.2 ± 0.8            | 28.1 ± 0.7            | 32.4 ± 0.8                     | 34.8 ± 0.8            | 30.1 ± 0.4            |\n| QuantLM 1.5B 3-Bit",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 220,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-221",
      "content": "|\n| QuantLM 1.5B 4-Bit                    | 26.2 ± 0.8            | 28.1 ± 0.7            | 32.4 ± 0.8                     | 34.8 ± 0.8            | 30.1 ± 0.4            |\n| QuantLM 1.5B 3-Bit                    | 25.5 ± 0.8            | 26.7 ± 0.6            | 31.2 ± 0.8                     | 33.4 ± 0.8            | 28.9 ± 0.4            |\n| TriLM 1.5B                            | 25.7 ± 0.8            | 27.4 ± 0.6            | 31.5 ± 0.8                     | 34.6 ± 0.8            | 29.5 ± 0.4            |\n| FloatLM 2.4B                          | 26.9 ± 0.8            | 29.4 ± 0.7            | 34.2 ± 0.8                     | 38.1 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 8-Bit                    | 27.0 ± 0.8            | 29.4 ± 0.7            | 34.1 ± 0.8                     | 38.0 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 6-Bit                    | 26.8 ± 0.8 ±          | 29.5 ± 0.7 28.8 ± 0.7 | 34.2 ± 0.8 34.3 ± 0.8          | 38.2 ± 0.9 38.1 ± 0.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 221,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-222",
      "content": "| 38.0 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 6-Bit                    | 26.8 ± 0.8 ±          | 29.5 ± 0.7 28.8 ± 0.7 | 34.2 ± 0.8 34.3 ± 0.8          | 38.2 ± 0.9 38.1 ± 0.9 | 31.8 ± 0.4 ±          |\n| QuantLM 2.4B 4-Bit                    | 26.5 0.8              |                       |                                |                       | 31.5 0.4              |\n| QuantLM 2.4B 3-Bit TriLM 2.4B         | 25.5 ± 0.8 27.4 ± 0.8 | 27.1 ± 0.6 27.8 ± 0.6 | 32.3 ± 0.8 34.6 ± 0.9          | 36.4 ± 0.9 35.1 ± 0.8 | 29.9 ± 0.4 30.8 ± 0.4 |\n| FloatLM 3.9B                          | 27.7 ± 0.8            | 30.6 ± 0.7            | 36.9 ± 0.9                     | 39.8 ± 0.9            | 33.3 ± 0.4            |\n| QuantLM 3.9B 8-Bit                    | 27.6 ± 0.8            | 30.7 ± 0.7            | 37.0 ± 0.9                     | 39.7 ± 0.9            | 33.4 ± 0.4            |\n| QuantLM 3.9B 6-Bit                    | 27.3 ± 0.8            | 30.3 ± 0.7",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 222,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-223",
      "content": "| 30.7 ± 0.7            | 37.0 ± 0.9                     | 39.7 ± 0.9            | 33.4 ± 0.4            |\n| QuantLM 3.9B 6-Bit                    | 27.3 ± 0.8            | 30.3 ± 0.7            | 36.9 ± 0.9                     | 39.3 ± 0.9            | 33.1 ± 0.4            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 27.1 ± 0.8 27.3 ± 0.8 | 30.3 ± 0.7 28.4 ± 0.7 | 36.3 ± 0.9 34.3 ± 0.9          | 38.8 ± 0.9 37.2 ± 0.9 | 32.8 ± 0.4 31.4 ± 0.4 |\n| TriLM 3.9B                            | 28.3 ±                | 29.5 ± 0.7            | 35.4 ± 0.9                     | 39.6 ±                | 32.8 ± 0.4            |\n|                                       | 0.8                   |                       |                                | 0.9                   |                       |",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "G Ablations",
        "chunkIndex": 223,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-224",
      "content": "We showcase instances of outputs produced by TriLM (3.9B) across diverse tasks, highlighting its proficiency in tasks such as comprehension, prompt completion, and creative composition.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "H Illustrative examples of TriLM 3.9B's completion capabilities",
        "chunkIndex": 224,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-225",
      "content": "Title: The Blitz Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Gerät beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Gerät, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Gerät.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Generated Output on Reading Comprehension by TriLM (3.9B)",
        "chunkIndex": 225,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-226",
      "content": "A: 4,000",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Q: How many sorties were flown in March 1941?",
        "chunkIndex": 226,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-227",
      "content": "A: Only during moonlit nights",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Q: When did the Luftwaffe fly inland missions?",
        "chunkIndex": 227,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-228",
      "content": "Background: Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Title: Helsinki",
        "chunkIndex": 228,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-229",
      "content": "e. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Title: Helsinki",
        "chunkIndex": 229,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-230",
      "content": "A: Helsinki\n\n- Q: How many people live there?\n- A: 1.4 million in the metropolitan area\n- Q: What percent of the foreign companies that operate in Finland are in Helsinki? A: 75%\n- Q: What towns are a part of the metropolitan area?\n- A: Espoo, Vantaa, Kauniainen, and surrounding commuter towns",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Q: What is the most populous municipality in Finland?",
        "chunkIndex": 230,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-231",
      "content": "In the year 2150, humanity had established colonies on Mars. Captain Ramirez was on a routine mission to explore an uncharted region of the planet when he and his crew were attacked by Martian natives who wanted them dead for their technology. The captain's ship crashed into the surface of Mars where it exploded in flames.\n\nThe survivors managed to escape from the wreckage but they soon found themselves stranded with no way out or back home. They would have to survive alone until rescue came. But what if there is nothing waiting for them? What if they are trapped forever?",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Essay Writing",
        "chunkIndex": 231,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-232",
      "content": "In a garden where the roses bloom, Underneath the silver moon, A gentle breeze begins to sway, And whispers secrets of the day. The stars above start to appear,\n\nWhile shadows dance and disappear,\n\nThe night unfolds in its own time. The sun rises over the horizon, As it does every morning, But this is no ordinary dawn. It's a new beginning for all who are here today. For those who have come before us, And those yet to be born. We celebrate life with music and laughter, With friends old and new. This is our moment to shine!",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Poem Writing",
        "chunkIndex": 232,
        "totalChunks": 234
      }
    },
    {
      "id": "2407.12327v5-chunk-233",
      "content": "Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They use a mixer to mix ingredients, then knead dough by hand. The women roll out the dough with a rolling pin, cut it into shapes using cookie cutters, bake them, and decorate them with frosting or sprinkles.\n\nThe energy given up by electrons as they move through the electron transport chain is used to produce ATP. In this process, oxygen and water are produced in a reaction called cellular respiration.\n\nWhen talking about sensitive topics, it is crucial to be aware of the fact that there are many different types of people. Some will react negatively and others positively. It's important not to take offense when someone says something you don't like.\n\nIf someone disagrees with you, it is best to ignore them. The most important thing in life is not what we have but who we are and how we treat others.",
      "metadata": {
        "source": "arxiv:2407.12327v5",
        "title": "Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale",
        "authors": [
          "Ayush Kaushal",
          "Tejas Vaidhya",
          "Arnab Kumar Mondal",
          "Tejas Pandey",
          "Aaryan Bhagat",
          "Irina Rish"
        ],
        "section": "Auto-complete on various prompts by TriLM (3.9B)",
        "chunkIndex": 233,
        "totalChunks": 234
      }
    }
  ],
  "fullText": "<!-- image -->\n\n## Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale\n\n∗\n\nTejas Vaidhya 1,2,4 ∗\n\ntejas@nolano.ai\n\nAyush Kaushal 1,2 ayush@nolano.ai\n\nTejas Pandey 1,3\n\ntejaspandey2003@kgpian.iitkgp.ac.in\n\nIrina Rish 1, 2, 4\n\nirina.rish@gmail.com\n\n1 Nolano AI , 2 University of Montreal , 3 IIT Kharagpur , 4 Mila - Quebec AI Institute, 5 UC Riverside\n\n## Abstract\n\nRapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memoryrelated bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite , the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs. To enhance understanding of low-bitwidth models, we are releasing 500+ intermediate checkpoints of the Spectra suite at https://github.com/NolanoOrg/SpectraSuite.\n\n## 1 Introduction\n\nThe computational power of GPUs, measured in FLOPs, is increasing exponentially, doubling approximately every 1.26 years. In contrast, memory capacity and bandwidth are growing at a slower pace, doubling every 2 and 2.9 years, respectively [Gholami et al., 2024]. This disparity highlights that computing capabilities are outpacing advances in memory technology. In Large Language Models (LLMs) inference, the primary bottlenecks are caused by model size (bits), which affects memory usage (memory capacity) and data transfer to processors (memory bandwidth). These issues are\n\n∗ Equal contribution, listed in alphabetical order.\n\nArnab Kumar Mondal 4\n\narnab.mondal@mila.quebec\n\nAaryan Bhagat 5\n\nabhag017@ucr.edu\n\nFigure 1: Common Sense and Reasoning (C&amp;R) &amp; LAMBADA Accuracy for ternary TriLM, FP16 FloatLM and quantized QuantLM models across different model sizes, in bits and number of parameters. C&amp;R scores are averaged across 6 benchmarks. At 3B+ scales, TriLMs demonstrate better performance for their size than QuantLM and competitive performance to FloatLM of the same parameters. See Tables 6, 7 and 9 for details.\n\n<!-- image -->\n\nbecoming more critical than the growing number of model parameters which affect the computational limits (FLOPs). For instance, state-of-the-art LLMs such as 340B Nemotron 4 [Nvidia et al., 2024] have sizes (in bits) exceeding the memory capacity of data center GPUs, such as 8xH100s. Token generation speed, or latency, is now limited by memory bandwidth [Kim et al., 2024]. Addressing these bottlenecks requires more expensive training, exceeding Chinchilla's compute-optimal regime [Hoffmann et al., 2022], with less than 10B parameter models being trained on up to 15 trillion tokens [Touvron et al., 2023b, AI@Meta, 2024, Team et al., 2024]. Another widely used technique is post-training quantization during deployment [Zhu et al., 2023]; however, we demonstrate in Section §5 that this approach is sub-optimal.\n\nIn post-training quantization, LLMs initially trained using 16-bit floating point precision (referred to as FloatLMs) undergo parameter quantization, and the resulting models are termed QuantLMs. These models use optimized kernels for deployment, offering speedups nearly proportional to the compression factor [Frantar and Alistarh, 2024]. However, quantizing to very low bitwidths creates a significant mismatch between the representations of pretrained FloatLM and the deployable QuantLM, resulting in undesired behavior and quality degradation [Li et al., 2024, Huang et al., 2024]. Some state-of-the-art methods [Frantar et al., 2022, Egiazarian et al., 2024] mitigate this issue by using calibration and re-training data from target domains; however, this approach increases sensitivity to the calibration data. For instance, seemingly simple choices, like whether to length-normalize the calibration data, can significantly impact QuantLM's performance [Malinovskii et al., 2024]. Other works have observed that QuantLM at 4 bits (4-bit QuantLMs) have about 65% lower knowledge capacity per parameter compared to trained and aligned FloatLMs [Allen-Zhu and Li, 2024].\n\nAn alternate approach to reducing model bitsize while maintaining parameter count involves training neural networks with low effective bitwidths [Zhou et al., 2018]. This approach offers compression benefits beyond post-training quantization without its associated drawbacks. While low-bitwidth approaches typically employ binary (1-bit) or ternary quantization (1.58-bit), binary quantization generally underperforms compared to regular FP16 models [Liu et al., 2023a] (see Appendix §B). In contrast, ternary modeling can match performance while significantly reducing memory requirements\n\n(as we demonstrate in section §5). Nevertheless, the relative performance of pretrained low-bitwidth language models compared to QuantLMs across similar sizes (in bits) and similar parameter counts remains unclear . This is a crucial unanswered question, given the high inference costs during the deployment of very large-scale LLMs. Moreover, the training dynamics and scaling law of these low-bitwidth models are also poorly understood, partly due to the lack of publicly available systematic suites and associated comparative studies .\n\nMotivated by these challenges, we make the following contributions in this paper:\n\nFeasibility and Scalability of Training Ternary Language Models (TriLMs) We discuss the deployment advantages (in section 2.1) and theoretical feasibility (in section 2.2) of training lowbitwidth models at scale. We then introduce ternary language models (TriLMs) and systematically study their scaling laws compared to FloatLMs. Our analysis reveals that TriLMs offer better scaling behavior in terms of model size, measured in bits (refer to Section 4.3). Moreover, as the number of parameters increases, the difference in validation loss between TriLMs and FloatLMs becomes insignificant, indicating TriLM's competitive performance at scale.\n\nSpectra LLM suite. We present Spectra , the first open suite of LLMs spanning many bit-widths. This suite includes FloatLMs, the corresponding QuantLMs at 3, 4, 6, and 8 bits, and ternary LLMs (TriLMs). The latter uses ternary weights {-1, 0, +1}. The suite features 9 models ranging from 99M to 3.9B parameters, all trained on the same 300B token dataset, totalling 54 models. We believe that the Spectra LLM suite makes a valuable contribution to the LLM research community by facilitating comparative studies, exploring the scalability and efficiency of ternary modeling, and improving interpretability from neuronal to connection levels.\n\n## Evaluation and comparative analysis of TriLMs, FloatLMs, and QuantLMs at different scales.\n\nWe evaluate TriLMs, FloatLMs, and QuantLMs across multiple benchmarks, spanning commonsense, reasoning, knowledge capacity and toxicity. At the billion parameter scale, TriLMs consistently outperform their QuantLM and FloatLM counterparts of the same bit-size across all the aforementioned benchmarks (see Figure 1). Surprisingly, TriLM 3.9B matches the performance of FloatLM 3.9B across all benchmarks, despite getting a higher perplexity and being 5 . 9 times smaller in bitsize.\n\nHowever, we also note that certain challenges remain in TriLMs. For instance, TriLM 3.9B exhibits the same level of toxicity and stereotyping as FloatLM 3.9B, significantly higher than a similarly sized FloatLM 830M when measured in bits. While TriLM 3.9B and FloatLM 3.9B show similar validation perplexity on some datasets, such as Penn Tree Bank and Lambada, a gap persists at this scale on web corpora, both in-domain (i.e., on a test subset of SlimPajama, the same domain used to train the models) and out-of-domain (e.g., on Dolma, C4 and RefinedWeb datasets). We provide detailed perplexity results in the section §5.\n\n## 2 Motivation for Low-Bitwidt Models\n\n## 2.1 Memory Bottlenecks and Language Model Deployment\n\nExperimental Setup: First, we, explore the impact of training low-bitwidth language models on deployment focussing on addressing memory bottlenecks. Our analysis includes transformer configurations from the LLaMa family [Touvron et al., 2023a,b]. As larger vocabularies in LLMs are becoming increasingly common for efficient multilingual modeling, we use a vocabulary size of 128k from LLaMa 3 [AI@Meta, 2024] for our analysis. We assume the Embedding and LM Head weights are retained in Half-Precision across all bitwidths for these analyses.\n\nMemory Capacity of GPGPUs and Model Size (in Bits): Figure 21a in Appendix §F reveal that memory capacity has consistently lagged behind computational power across various accelerators, including recent hardware like Blackwell [Nvidia Team, 2024], MI325X [AMD Team, 2024], and Gaudi3 [Intel Gaudi Team, 2024]. This gap is further exacerbated by advanced computational techniques like Ampere sparse or FP8. As shown in Figure 2a, the model sizes (in GB) for TriLM, QuantLM 4-Bit, and FloatLM scale differently with parameter counts. For simplicity, the figure excludes overhead from KV Cache, activations, and compilation during model deployment. FloatLM reaches the memory capacity of a single H100 at 34B parameters, with larger models exceeding the\n\nFigure 2: Expected gains from low bitwidth modeling. TriLMs can fit over 300B parameters on a single H100 and achieve up to a theoretical maximum of 10x faster autoregressive decoding compared to FloatLM.\n\n<!-- image -->\n\ncapacity of multiple GPUs. In contrast, QuantLM 4-Bit supports up to 300B parameters on a single MI300X. TriLMs, with over 300B parameters and appropriate packing, can fit on a single H100, making them not only efficient for GPU deployment but also ideal for edge devices.\n\nMemory bandwidth of GPGPUs and model inference speedup: Figure 21b and Appendix §F demonstrate the trends of Memory Bandwidth over FLOPs for accelerators over the years, highlighting the slower growth of memory bandwidth compared to computation. This trend, indicating a downward slope, aligns with Kim et al. [2024]'s establishment of the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. Consequently, the autoregressive decoding of LLM inference can theoretically achieve speedup proportional to its compression factor. Figure 2b illustrates the maximum possible speedup for QuantLM 4-Bit and TriLM compared to FP16 at different parameter counts. Even at 7 billion parameters, TriLMs can be more than 4 times faster at autoregressive decoding than FloatLM and 2 times faster than QuantLM 4-bit. While QuantLM 4-Bit plateaus at a maximum possible speedup factor of 4x, TriLMs plateau much higher at 10x for FloatLM.\n\n## 2.2 Low bits can capture weight variance effectively at scale\n\nIn this section, we use information theory to support our hypothesis: as the number of parameters increases, training language models with low-bitwidth can effectively capture the necessary weight variance without significant information loss . Assuming a fixed training dataset, we base this hypothesis on analyzing weight distributions in FloatLMs ranging from 99M to 3.9B parameters.\n\nFigure 3: Shannon entropy (in bits) of discretized weight distribution with increasing number of bins.\n\n<!-- image -->\n\nFigure 4: Differential entropy of Gaussian fits on weight distributions across different scales.\n\n<!-- image -->\n\nAssuming that the weights of a trained model follow a Gaussian distribution (see Appendix §E), we fit a Gaussian to these weights to understand their statistical behavior across model scales. The differential entropy is calculated using the expression: H ( W ) = 1 2 log 2 (2 πeσ 2 W ) where σ W is the standard deviation of the weights [Papoulis and Pillai, 2002]. As shown in Figure 4, differential entropy decreases with an increase in the number of parameters. This decrease indicates that the weights become more concentrated around the mean as the model size increases, suggesting higher predictability and\n\nless variability [MacKay, 2003]. This reduced variability is due to overparameterization, which leads to redundancy in the weights Zhang et al. [2017], Neyshabur et al. [2018].\n\nAdditionally, we also use Shannon entropy, calculated by discretizing the weight distribution into ( N ) bins and computing H Shannon = -∑ N i =1 p i log 2 p i , where p i is the probability of the weights falling into the i -th bin. The probability is given by the normalized frequency of weights within each bin. Discretization allows us to apply discrete entropy measures to continuous data, with the number of bins determining the representation granularity. Shannon entropy measures the average minimum number of bits needed to encode the weight values based on their distribution, providing a quantifiable measure of their \"information content\" [Shannon, 1948]. Lower Shannon entropy indicates a more predictable, less diverse distribution that can be effectively encoded using fewer bits. Examining Shannon entropy across scales and bin sizes, we validate our finding that larger models require fewer bits for effective representation. As plotted in Figure 3, there is a general trend of decreasing Shannon entropy with increasing parameter count for a given number of bins.\n\nThese analyses support the potential of low-bitwidth models to match full-precision performance, especially as model sizes grow. In Section 4.3, we further substantiate this by analyzing the scaling behavior of our low-bitwidth TriLMs compared to FloatLMs.\n\n## 2.3 Selecting the Appropriate Low-Bitwidth Model\n\nSelecting the appropriate quantization level is crucial when training low-bitwidth models, as it significantly impacts both computational efficiency and model performance. Binary models quantize weights to {-1, 1}, simplifying multiplication operations in neural networks to XOR operations. Such operations are highly efficient on digital hardware due to their basic logic gate implementation [Courbariaux et al., 2016a]. XOR operations, which can be executed rapidly and with lower power consumption than traditional multiplications, make binary models especially appropriate for resource-constrained environments [Hubara et al., 2017]. Ternary models extend this concept by incorporating a zero state, allowing weights to be {-1, 0, +1}. This modification not only maintains computational simplicity by replacing multiplications with additions and subtractions [Li et al., 2016] but also leverages the sparsity of neural networks to enhance computational efficiency by eliminating calculations where weights are zero [Zhu et al., 2017].\n\nIn contrast, models quantized to 2, 3, or 4 bits involve more complex arithmetic operations, including full multiplications, which are computationally more demanding than those required for binary and ternary models [Zhou et al., 2016]. While these higher bitwidth models offer more detailed weight representations, they do so at the expense of greater computational resource usage. Therefore, this work only considers binary and ternary language models for further analysis.\n\nWhile binary quantization minimizes computational complexity, it often results in significant performance degradation, failing to meet the standards of practical applications [Rastegari et al., 2016]. In contrast, ternary quantization introduces a zero state that more accurately approximates weight distributions without substantially increasing computational demands [Li et al., 2016]. This capability makes ternary models particularly advantageous, as they maintain performance levels closer to those of full-precision models while still achieving significant efficiency gains [Zhu et al., 2017]. Our observations align with these, as the scaling trends for given data size for our TriLMs consistently outperform those of BiLMs (Binary LLMs) across all parameter counts and bit sizes considered in this work, as shown in Appendix §B and Figure 15. As a result, this work primarily focuses on the TriLM model, which we describe in the following section.\n\n## 3 TriLM: Ternary Language Model\n\nIn this section, we present the architectural and optimization details of the TriLM (Ternary Language Model). We discuss the training and inference processes, as well as the optimization schedule.\n\n## 3.1 Architecture\n\nTriLM is a LLaMa-style [Touvron et al., 2023a] autoregressive transformers [Vaswani et al., 2017] model with RMSNorm [Zhang and Sennrich, 2019], SwiGLU Gated MLP [Shazeer, 2020], Rotary Position Embedding (RoPE) [Su et al., 2021], Multi-Headed Attention and no bias terms. In TriLMs, we represent the weights of all the linear layers in one of three possible ternary states {-1 , 0 , 1 } ,\n\nFigure 5: The computational flow of forward, backward, and inference processes in TriLM's linear layer with N-Way model parallelism.\n\n<!-- image -->\n\nalong with an additional floating-point number called 'scale value' shared across the matrix. During training, we maintain the latent (or master) weights in floating point precision, allowing for the accumulation of small updates over iterations that eventually contribute to a transition in the estimated ternary state of a parameter. As shown in Figure 5, during the forward pass, we ternarize the floating point latent weights on-the-fly. This process involves calculating the scale value to the absolute mean of the latent weights. After scaling, we estimate the ternary state of each parameter by rounding off to the nearest ternary state. In the backward pass, we use a straight-through estimator to compute gradients of the floating point latent weights [Bengio et al., 2013]. Since we only need the scale values and the ternarized states during inference, we achieve a significant reduction in both model size and inference time at larger scales compared to FloatLMs. We provide a formal description of these forward pass, backward pass, and inference time equations in the Section (§3.2). We represent the embedding and language model head in half-precision floating point across all our experiments.\n\nSince, training of TriLMs requires on-the-fly computation of scale values, synchronizing for a single scalar across devices in model parallel training [Shoeybi et al., 2019] can cause significant communication overhead. To address this, we allow each device to independently compute these scale values over its own matrix shard. This approach introduces additional artifacts, where the number of scalar values for each matrix is the same as the degree of model parallelism used during training (see section A.4). However, the impact on modelsize is negligible, for matrices with millions of parameters, we only add 6 scalar values each.\n\nConcurrent research on low-bit LLMs, like BitNet 1.58 [Ma et al., 2024], also demonstrates the feasibility of training LLMs with ternary weights. Our experiments demonstrate that TriLM's architecture not only outperforms BitNet b1.58 but is also simpler and more stable. Moreover, both the larger BitNet 1.3B model presented in their paper and our replication of the BitNet 1.1B model underperform compared to our TriLM 1.1B on commonsense and reasoning benchmarks. We provide further details on these comparisons in Appendix A.5.\n\n## 3.2 Forward Pass, Backward Pass and Inference Equations\n\nTable 1 shows the equations for TriLM, BiLM (Binary Langauge Model) and FloatLM for forward pass, backward pass and inference.\n\nReason for restricting the quantization approach to linear weights in TriLMs. In developing extremely large language models like TriLMs, a key architectural strategy is to quantize only the linear layer weights while keeping the embedding layers and language model head in higher precision. This\n\nTable 1: Equations in the Linear Layer of TriLMs and FloatLMs.\n\n| Type    | Forward Pass                                                                                                             | Backward Pass                       | Inference                                                      |\n|---------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------|----------------------------------------------------------------|\n| FloatLM | Y = XW T                                                                                                                 | ∂L ∂X = ∂L ∂Y W ∂L ∂W = ∂L ∂Y T X   | Y = XW T                                                       |\n| TriLM   | γ = ϵ + 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = round ( min ( max ( W ij γ , - 1 ) , 1 )) ˜ W ij = γ ̂ W ij Y = X ˜ W T | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and γ once and cache ˜ W ij = γ ̂ W ij Y = X ˜ W T |\n| BiLM    | α = 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = sign ( W ij - 1 nm ∑ n i =1 ∑ m j =1 W ij ) ˜ W ij = α ̂ W ij Y = X ˜ W T   | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and α once and cache ˜ W ij = α ̂ W ij Y = X ˜ W T |\n\nis driven by the need to reduce model size while maintaining performance. Linear layers (dense layers) hold the bulk of the parameters in transformer models [Vaswani et al., 2017]. Quantizing these weights to ternary states significantly reduces the model size, facilitating deployment on memory-constrained hardware. However, the embedding layers and language model head remain in higher precision (e.g., half-precision floating point) to preserve critical functions in language understanding and generation. Embedding layers encode important semantic and syntactic information, and quantizing them would degrade performance [Mikolov et al., 2013]. Similarly, the language model head, which maps internal representations to the vocabulary space, requires high precision to maintain prediction quality [Press and Wolf, 2017]\n\n## 3.3 Optimization Schedule\n\nOptimization of low bitwidth neural networks (such as in Quantization Aware Training) [Liu et al., 2023b, Yuan et al., 2024, Bethge et al., 2018, Le and Li, 2023] requires a set of considerations like higher initial learning rate and reduced weight decay. Our optimization schedule for TriLM closely follows that of BitNet [Ma et al., 2024] consisting of two interventions in a vanilla linear decay learning rate scheduling with warmup and weight decay (L2 Regularization). (1) Peak LR - at roughly the halfway point, we reduce the peak learning rate. (2) L2 Reg. - at roughly two-thirds of the training, we remove the weight decay regularization as ternarization provides sufficient regularization [Courbariaux et al., 2016b]. Figure 6 demonstrates the ablation run performed for a 1.1B parameter model on 100B tokens with both, only one and neither of these interventions.\n\nFigure 6: Training loss for a 1.1B parameter TriLM, across different optimization schedules.\n\n<!-- image -->\n\nAmong the four experimental runs, we observed that the lowest final training loss occurred when both L2 regularization and peak learning rate (LR) adjustments were implemented. This was closely followed by the scenario in which only L2 regularization was adjusted, and subsequently by the condition where only peak LR was modified. Notably, discontinuing the peak learning rate at the midpoint of training resulted in a significant and rapid decline in training loss. Similar phenomena have been documented in training schedules characterized by brief periods of rapid learning rate decay, such as those described in MiniCPM[Hu et al., 2024]. Conversely, the removal of L2 regularization, or weight decay, led to an accelerated convergence rate, which can produce effects analogous to reducing the peak learning rate, also resulting in a swift decrease in loss. These relative training loss observation at 100B tokens also go hand in hand with relative downstream performance across commonsense and reasoning tasks, which are listed in\n\nTable 11 and 10. Consequently, we have established a fixed optimization schedule for the TriLM, which entails dropping the peak learning rate at the halfway point while removing weight decay at the two-thirds mark.\n\n## 4 Spectra suite: Spanning Parameters and Bitwidths\n\nThe Spectra suite includes comprehensive families of Large language models designed to span different parameter counts and bit-widths. This suite includes three main model families: TriLMs , FloatLMs , and QuantLMs (3, 4, 6, and 8 bits). Drawing inspiration from established model suites such as those by [Biderman et al., 2023, Liu et al., 2023c, Groeneveld et al., 2024], Spectra aims to facilitate scientific research on low-bitwidth LLMs.\n\n## 4.1 Overview of Spectra Suite\n\nThe Spectra suite stands out with the following key properties:\n\n1. Scale: The suite spans a broad spectrum of scales across parameter count (99M to 3.9B), sizes ( 9 ∗ 10 8 to 6 . 4 ∗ 10 10 bits) and bitwidths (1.58 bits to 16 bits).\n2. Uniform Training: All the TriLMs and FloatLMs are trained on a identical data sequences, specifically a 300B subset of Slim Pajama [Soboleva et al., 2023] dataset, ensuring training consistency. Data ordering and batch sizes are also kept consistent within each model family to support reproducibility and comparability in research. QuantLMs undergo quantization using the same calibration data, maintaining uniformity in model quantization procedures.\n3. Public Accessibility: The training data and intermediate checkpoints are publicly available for study.\n4. Consistent Model Size Mapping: All models across the families maintain a consistent one-to-one mapping for parameter count.\n\nFigure 7 demonstrates the Spectra LM suite spanning across two dimensions - size (bits) and parameters. For each parameter count, we have 6 models across different bitwidths. Due to availability of FloatLM, Spectra can easily be extended with new QuantLMs by using different Post Training Quantization methods.\n\nThe details on dataset and tokenizer, pretraining setup, and hyperparameters across all the models are detailed in the Appendix §A, while information on other families is covered in the next section.\n\n## 4.2 FloatLMs and QuantLMs\n\nFamily of FloatLMs. We utilize LLaMa-style [Touvron et al., 2023a] architecture akin to TriLM. In FloatLMs, parameters in the weight matrices of linear layers are represented as floating-point numbers (FP16/BF16). The optimization schedule for FloatLM follows a cosine decay scheduling with weight decay and includes a learning rate warmup. This methodology is consistent with the practices established in models such as Pythia, OLMo, LLM360. For more details, refer to the Appendix (A.3).\n\nFamily of QuantLMs. Recently, data-aware quantisation techniques like GPTQ [Frantar et al., 2022] have emerged as efficient solutions for nearlossless weight quantization down to 4-bit precision [Dettmers and Zettlemoyer, 2023]. In our work, we implemented GPTQ post-training quantization to FloatLM, creating the QuantLM family of models across 3, 4, 6, and 8 bits. We quantized all trans-\n\nFigure 7: The Spectra Suite spans across two dimensions of parameters and scale. Each point corresponds to a LLM in the spectra suite.\n\n<!-- image -->\n\nformer layer weights. For 3-bit and 4-bit quantization, we employ a group size of 128, which results\n\nin effective bit rates of 3.25 and 4.25 bits per parameter, respectively. We've refined our approach by incorporating best practices from recent research [Malinovskii et al., 2024], particularly in terms of calibration data and scaling it to a million tokens for improved reconstruction. To ensure a fair comparison with TriLM, we maintain certain components in their original precision. Specifically, we do not quantize the embedding, language model head, or activations. Additionally, we use symmetric quantization (without zero offset) as it is simpler, is supported by fast inference kernels [Frantar and Alistarh, 2024] and offers similar performance to assymmetric quantization (with separate zero offsets in addition to scale for each group). It also offers consistency and a fairer comparison with TriLMs. It's worth noting that our Spectra suite is designed with flexibility in mind, allowing for easy extension to other quantization methods as needed.\n\n## 4.3 Training Dynamics and Scaling Laws\n\nFigure 8: Training Cross Entropy Loss across steps for the TriLM family of models. At the halfway point (150B tokens) when we lower the peak learning rate, we observe a sudden drop in training loss. In the two-third way, removing weight decay leads to faster convergence.\n\n<!-- image -->\n\nTraining Dynamics. Figure 8a illustrates the training loss curves for all the TriLMs trained and Figure 8b shows the relative training loss of a TriLM in comparison to two smaller FloatLMs. The loss curves demonstrate a continuous and consistent improvement in TriLMs with an increase in parameter count. During training, we make several key observations. First, minor spikes and drops in training loss occurred consistently across different TriLM scales at the same token counts, as all models were trained on identical data with the same ordering. Notably, the two largest models-TriLM 2.4B and TriLM 3.9B-each experienced a large loss spike in the first half of training. Second, adjusting the learning rate at the midpoint led to a sharp decline in training loss over a few hundred million tokens, though its impact varied by model size: for the larger models (2.4B and 3.9B), the rate of loss reduction returned to the prior pace after the initial sharp drop, while for smaller models (1.1B and 1.5B), the loss reduction plateaued, and models below 1B parameters saw an increase in training loss. Lastly, the removal of weight decay at the two-thirds mark accelerated convergence for all models, with the effect being most pronounced in the largest TriLM models.\n\nScaling Laws. Figures 9a and 9b illustrate the final validation loss across different model size in terms of bits and number of parameters ( N ) respectively. In terms of effective model size in bits (9a), which is crucial during inference, TriLMs exhibit significantly better scaling than FloatLMs. Notably, TriLM 3.9B validation error matches with FloatLMs 1.1B, which is nearly 1.7 times larger in terms of effective bit size. To investigate the scaling behaviour in terms of N , we fit the validation loss to a power-law with offset 2 Hoffmann et al. [2022] (see Figure 9b and Appendix §C):\n\n<!-- formula-not-decoded -->\n\nWe employ the Levenberg-Marquardt algorithm (Levenberg [1944], Marquardt [1963]) for efficient non-linear least squares fitting. Both FloatLM and TriLM share the scaling exponent α = 0 . 26 ,\n\n2 derived using a fixed data regime of 300B tokens\n\n<!-- image -->\n\n- (a) Scaling laws - perplexity across size (bits).\n- (b) Scaling laws - perplexity across parameters.\n\n<!-- image -->\n\nFigure 9: Final validation loss across sizes (in bits) and parameters. TriLMs with increasing size offer better performance than FloatLMs of the same number of bits; and the gap in validation perplexity closes at scale.\n\nindicating similar scaling behavior with the number of parameters N . However, the offset terms ϵ TriLM = 1 . 76 and ϵ FloatLM = 1 . 67 reveal that their validation losses converge as N increases. Although TriLM starts with a higher coefficient A = 185 , suggesting greater initial validation loss than FloatLM ( A = 159 ), this difference becomes insignificant with larger N , aligning their performance at asymptotic scales as shown in Figure §9b.\n\nAs shown in Figure 10, using the scaling equations for TriLMs and FloatLMs, we derive the relationship between parameter count and the percentage difference in validation loss relative to FloatLMs. We observe that at 330B and 15.6B parameters, the validation losses for TriLMs are within 6% and 7% of FloatLMs' validation losses, respectively. This indicates that TriLMs are likely to closely match the performance of FloatLMs at larger scales.\n\nDespite the observed differences in validation loss at the scale of models considered in this work, we demonstrate in Section 5 that at 3.9B parameters, TriLM offers competitive downstream performance\n\nFigure 10: Comparison of Power Law and Power Law-with-offset Fits for TriLM and FloatLM.\n\n<!-- image -->\n\ncompared to a FloatLM of the same parameter count across a variety of benchmarks in commonsense reasoning and knowledge-based tasks. Moreover, as discussed in Section §5, both models show similar perplexity on clean datasets such as Penn Tree Bank and OpenAI's Lambda. However, the gap in perplexity is observed in overlapping web-based datasets like Dolma and RefinedWeb.\n\n## 5 Results\n\nWe evaluate the families of LLMs on three aspects - commonsense &amp; reasoning tasks, knowledgebased tasks, and toxicity, all of which are crucial measures of their downstream performance. Readers may refer to the appendix for more details regarding the benchmarks Appendix (§D).\n\nCommonsense and Reasoning. We evaluate Spectra Suite models using eight distinct commonsense and reasoning benchmarks consisting of tasks from logical and reasoning questions to grounded and physical commonsense tasks: Arc Easy, Arc Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], HellaSWAG [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], PIQA [Bisk et al., 2019], LAMBADA [Paperno et al., 2016], LogiQA [Liu et al., 2021], all under zero-shot settings. Figures 1a and 1b display the average performance of the LLMs on the first six benchmarks across size in bits and number of params. Figures 1c and 1d present the performance for the LAMBADA dataset. The results show that TriLMs consistently demonstrate superior performance for their size across all benchmarks at the 2 . 4 B and 3 . 9 B parameter scales. At the largest scale of 3 . 9 B, TriLM surpasses FloatLM on LAMBADA and achieves competitive average scores across six benchmarks. Additionally, TriLMs at the largest scales consistently outperform 4-bit QuantLMs of equivalent parameter count. However, across the considered scales, all LLMs show poor performance on\n\nLogiQA, making it difficult to identify a clear performance trend. For detailed benchmarking across all datasets -see Tables 6, 7 and 9.\n\nFigure 11: Performance of ternary TriLM, FloatLM and quantized QuantLM (3-bit &amp; 4-bit) models on SciQ and TriviaQA tasks across Size (Bits) and Parameters. Refer to Tables 9 and 12 for details.\n\n<!-- image -->\n\nKnowledge. Several downstream practical uses of LLMs require them to have knowledge about common subjects like science or politics. To evaluate the performance of LLMs on these subjects, we use SciQ [Welbl et al., 2017], TriviaQA [Joshi et al., 2017] and MMLU [Hendrycks et al., 2021] benchmarks in zero-shot settings. Figures 11a and 11b show the accuracy of the Spectra suite LLMs on SciQ across size in bits and parameter counts. Figures 11c and 11d depict the accuracy for TriviaQA, while 12a and 12b do the same for MMLU. Across both benchmarks, at large 2.4B+ scales, TriLMs offer the best performance at a given size (bits). Surprisingly, despite having fewer bits, the knowledge capacity of TriLM does not have any significant degradation as observed in the case of QuantLMs [Allen-Zhu and Li, 2024]. This indicates that low-bitwidth LLMs like TriLMs have similar knowledge capacity to FloatLMs, indicating that knowledge capacity is parameterized via the presence and nature of a connection (+1 or -1), rather than its strength. Tables 9 and 12 expand on these results.\n\nFigure 12: MMLU Accuracy for ternary TriLM, FloatLM and quantized QuantLM (3-bit &amp; 4-bit) models across Size and Parameters. Please refer to Table 13 for details.\n\n<!-- image -->\n\nToxicity. We evaluate the Spectra suite across various safety and toxicity benchmarks of TruthfulQA [Lin et al., 2021], Big Bench BBQ Lite [Parrish et al., 2022] and CrowsPairs [Nangia et al., 2020]. These scores are listed in the Appendix in Table 12. We observe that none of the LLMs, even those with up to 3.9 billion parameters and trained on 300 billion tokens, perform significantly better than random guessing on the TruthfulQA benchmark. For the other two datasets, there is a noticeable correlation between the occurrence of toxicity and stereotypes and the LLMs' performance on various tasks. In particular, TriLMs with fewer than one billion parameters exhibit less stereotyping than FloatLMs with a similar parameter count. However, this difference diminishes as the scale increases, with TriLM 2.4B and TriLM 3.9B exhibiting biases comparable to those of FloatLM 2.4B and FloatLM 3.9B, respectively, on these benchmarks. This suggests that, although TriLMs initially show reduced bias compared to similarly sized FloatLMs, their performance aligns with FloatLMs of equivalent parameter counts at larger scales. This also highlights that TriLMs exhibit considerably more stereotyping than FloatLMs of comparable size (measured in bits), yet perform comparably to FloatLMs with similar parameter counts.\n\nPerplexity on other datasets. We measure perplexity using TriLM 3.9B and FloatLM 3.9B across various other corpora than SlimPajama, which was used for training. These corpora include OpenAI Lambada, Penn Tree Bank, C4, Cosmopedia, Dolma, S2Orc, Wikipedia, and RefinedWeb. A portion of Wikipedia, C4 is included in Slim Pajama. Some other corpora like Dolma and RefinedWeb, may also have overlaps from C4, Wikipedia as well as Common Crawl. Figure 13 demonstrates that while TriLM 3.9B is similar or better than FloatLM 3.9B on PTB and Lambada, across the other datasets, with potential overlaps with SlimPajama, it's performance is consistently worse - indicating lower capability to memorize training data as well as worse in-distribution performance, despite competitive out of distribution performance.\n\nFigure 13: Cross-entropy (log perplexity) comparison between TriLM and FloatLM (both 3.9B parameters) across various datasets apart from SlimPajama.\n\n<!-- image -->\n\nIllustrative examples. We also generated a few examples of poem and essay writing, as well as reading comprehension (see Appendix §H). Our results show that the TriLM 3.9B model is able to generate cohesive and correct responses with randomly sampled examples.\n\n## 6 Related Work\n\nQuantization of Large Language Models after Training. Post-training quantization (PTQ) algorithms convert a pretrained high-precision model (FP32 / FP16 / BF16) into a lower precision format without requiring the original training process [Cai et al., 2020, Hubara et al., 2020, Choukroun et al., 2019]. These methods can be either data-independent or need a small calibration dataset. Additionally, PTQ for LLMs presents unique challenges due to numerical outliers in both weights and activations [Bondarenko et al., 2021]. GPTQ [Frantar et al., 2022] is a state-of-the-art one-shot weight quantization method aimed at finding a matrix of quantized weights (say ˆ W ) that minimizes the squared error relative to the full precision layer output. By leveraging second-order information, GPTQ derives a closed-form solution to this optimization problem, making it scalable to large LLMs. Other methods [Dettmers et al., 2023, Lin et al., 2024, Lee et al., 2024] emphasize the importance of outlier weights that correspond to high-magnitude activations. Some recent methods also quantized activation along with the weights [Xiao et al., 2024, Yao et al., 2022, 2023]. Ahmadian et al. [2023] demonstrate that large activation outliers can be effectively mitigated at scale by making appropriate optimization decisions during the pretraining phase.\n\nTraining Language Models At Lower Precision. Several prominent language models such as GPT [Brown et al., 2020b], NeoX [Black et al., 2022], Llama and Pythia families have been traditionally trained using mixed precision (FP32/FP16 or FP32/BF16) [Micikevicius et al., 2018] or half-precision (FP16/BF16) [Kalamkar et al., 2019]. Recently, Tao et al. [2022] introduced QuantGPT, a model that incorporates contrastive and logit distillation from a full-precision teacher to a quantized student model during pretraining. Further developments, such as BitNet [Wang et al., 2023] and BitNet b1.58 [Ma et al., 2024], have specifically focused on quantization-aware training for extremely low-bitwidth\n\nnetworks in transformer-based models. In their work, models are trained at low 'effective' precision of binary and ternary respectively - where the latent (or master) weights during training are maintained in higher precision like FP16. The model weights are binarized or ternarized on the fly during the forward pass and gradients are backpropagated for the latent weights using the straight-through estimator [Courbariaux et al., 2016b]. Prior works emphasize the importance of maintaining latent (or master) weights at high precision to allow accumulation of small updates during training - for example, Peng et al. [2023] observed a significant performance drop on the language model when the latent (or master) model weights were switch from 16-bits (FP16/BF16) to 8-bits (FP8) during training. Concurrent architectural improvements such as Flash Attention [Dao et al., 2022, Dao, 2023], the mixture of experts [Zoph et al., 2022], xLSTM Beck et al. [2024] and state space models [Gu and Dao, 2024, Dao and Gu, 2024, Gu et al., 2022] complement these advancements in lower precision modeling.\n\n## 7 Conclusion and Future Work\n\nIn this work, we address memory limitations in large language model (LLM) deployment by exploring both post-training quantization and direct low-bitwidth training. We introduce the Spectra LLM suite, featuring 54 models ranging from 99 million to 3.9 billion parameters, trained on 300 billion tokens. This suite includes Float16 LLMs (FloatLMs), quantized QuantLMs (3-8 bits), and our proposed ternary LLMs (TriLMs). Our findings reveal that TriLMs scale better than their half-precision Float16 counterparts in terms of effective model bit size, and they can achieve comparable validation loss when scaled to a large number of parameters. Additionally, our results demonstrate that TriLMs surpass other models in bit-size efficiency and achieve performance comparable to FloatLMs at 3 billion+ parameters across multiple benchmarks.\n\nFuture work should address the remaining challenges of toxicity, stereotyping, and performance gaps on web corpora associated with low-bitwidth models. Investigating scaling laws across different data regimes for various low-bitwidth architectures can provide deeper insights into their behavior and limitations. Additionally, combining these models with state-space architectures like Mamba Gu and Dao [2024] could further enhance efficiency and performance without sacrificing accuracy. These research directions hold promise in advancing efficient language modeling further.\n\n## 8 Broader Impact\n\nInterpretability Beyond Neuron Level: While several efforts have been made to understand how language models work and means to steer them without training, these methods have mostly focussed on intervening at neuron level. TriLMs opens a new degree of interpretability - at the connection level. Here, the connections between any two neurons in a layer are in one of the three states - 0 (no connection), -1 (negative connection) and +1 (positive connection), each with equal strength. This is in sharp contrast to FloatLMs, where these connections can be of varying strengths, making it harder to study interpretability beyond neuron level. By releasing the checkpoints across our training runs, we facilitate research along these directions.\n\nEnvironmental Benefits and Resource Efficiency: The open release of our models mitigates future emissions by allowing others to bypass the need for pretraining models from scratch. Moreover, TriLMs much lesser resource to deploy, and can perform the autoregressive generation as a faster pace - making them critical to scenarios demanding strict latency. Additionally, TriLMs represent a substantial advancement in enhancing performance on resource-constrained edge devices, including smartphones, laptops, and automobiles.\n\nImpact on Specialised Hardware: While TriLMs offers significant memory reduction and latency improvements on General Purpose GPUs like H100 and RTX4090, certain specialized hardware benefits more from ternary modeling. Hardware (like Cerabras 3 ) that support high byte-to-flop ratio computations, can leverage the sparsity stemming from ternarization for speedup in both training as well as inference. On the other hand, hardware with limited Memory/SRAM (like Groq 4 ), benefit from reduction in the number of chips needed to deploy an LLMs.\n\n3 https://www.cerebras.net/product-chip/\n\n4 https://groq.com/\n\nReduced Training Costs: The Chinchilla scaling laws established that for training compute optimality, it may be recommended to train larger LLMs for lesser tokens than smaller LLMs for more tokens for achieving the desired model performance. However, memory requirements and latency associated with deployment of larger models, has motivated costlier training runs that go far beyond Chinchilla optimality. For example a LLaMa 3 model with only 8B parameter was trained for 15T tokens. Since, TriLM and ternary models in general can reduce the memory requirements and latency, this can motivate a shift inparameter-token tradeoff for efficient training runs towards Chinchilla's compute-optimal regime.\n\n## Acknowledgement\n\nWe acknowledge the support from the Mozilla Responsible AI Grant, the Canada CIFAR AI Chair Program and the Canada Excellence Research Chairs Program. This research was enabled by the computational resources provided by the Summit supercomputer, awarded through the Frontier DD allocation and INCITE 2023 program for the project \"Scalable Foundation Models for Transferable Generalist AI\" and SummitPlus allocation in 2024. These resources were supplied by the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, with support from the Office of Science of the U.S. Department of Energy. We extend special thanks to Jens Glaser for his assistance with the Summit and Frontier supercomputers, and to Darshil Doshi for insightful discussions around scaling laws\n\n## Bibliography\n\n- A. Ahmadian, S. Dash, H. Chen, B. Venkitesh, S. Gou, P. Blunsom, A. Üstün, and S. Hooker. Intriguing properties of quantization at scale, 2023. URL https://arxiv.org/abs/2305. 19268 .\n2. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL\\_CARD.md .\n- Z. Allen-Zhu and Y. Li. Physics of language models: Part 3.3, knowledge capacity scaling laws, 2024. URL https://arxiv.org/abs/2404.05405 .\n4. AMD Team. Amd instinct mi210 accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi200/mi210.html , 2022a. Accessed: July 3, 2024.\n5. AMD Team. Amd instinct mi250 and mi250x accelerators. https://www.amd.com/system/ files/documents/amd-instinct-mi200-datasheet.pdf , 2022b. Accessed: July 3, 2024.\n6. AMD Team. Amd instinct mi300a accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi300/mi300a.html , 2023a. Accessed: July 3, 2024.\n7. AMD Team. Amd instinct mi300x accelerator. https://www.amd.com/en/products/ accelerators/instinct/mi300/mi300x.html , 2023b. Accessed: July 3, 2024.\n8. AMD Team. Amd instinct mi325x accelerator. https: //ir.amd.com/news-events/press-releases/detail/1201/ amd-accelerates-pace-of-data-center-ai-innovation-and , 2024. Accessed: July 3, 2024.\n- A. Andonian, Q. Anthony, S. Biderman, S. Black, P. Gali, L. Gao, E. Hallahan, J. Levy-Kramer, C. Leahy, L. Nestler, K. Parker, M. Pieler, J. Phang, S. Purohit, H. Schoelkopf, D. Stander, T. Songz, C. Tigges, B. Thérien, P. Wang, and S. Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 9 2023. URL https://www.github.com/eleutherai/ gpt-neox .\n- M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. xlstm: Extended long short-term memory. arXiv preprint arXiv:2405.04517 , 2024.\n\n- Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013.\n- J. Bethge, M. Bornstein, A. Loy, H. Yang, and C. Meinel. Training competitive binary neural networks from scratch. ArXiv , abs/1812.01965, 2018. URL https://api.semanticscholar. org/CorpusID:54458838 .\n- S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023.\n- L. Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/ . Software available from wandb.com.\n- Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence , 2019. URL https://api. semanticscholar.org/CorpusID:208290939 .\n- S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv. org/abs/2204.06745 .\n- Y. Bondarenko, M. Nagel, and T. Blankevoort. Understanding and overcoming the challenges of efficient transformer quantization, 2021. URL https://arxiv.org/abs/2109.12948 .\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020a.\n- T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020b. URL https://arxiv.org/abs/2005.14165 .\n- Y. Cai, Z. Yao, Z. Dong, A. Gholami, M. W. Mahoney, and K. Keutzer. Zeroq: A novel zero shot quantization framework, 2020. URL https://arxiv.org/abs/2001.00281 .\n- Y. Choukroun, E. Kravchik, F. Yang, and P. Kisilev. Low-bit quantization of neural networks for efficient inference, 2019. URL https://arxiv.org/abs/1902.06822 .\n- C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300 .\n- P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https: //api.semanticscholar.org/CorpusID:3922816 .\n- M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations. Advances in neural information processing systems , 28: 3123-3131, 2016a.\n- M. Courbariaux, Y. Bengio, and J.-P. David. Binaryconnect: Training deep neural networks with binary weights during propagations, 2016b. URL https://arxiv.org/abs/1511.00363 .\n- T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https://arxiv.org/abs/2307.08691 .\n\n- T. Dao and A. Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/abs/2405.21060 .\n- T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. URL https://arxiv.org/abs/2205.14135 .\n- T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws, 2023.\n- T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh. Spqr: A sparse-quantized representation for near-lossless llm weight compression, 2023. URL https://arxiv.org/abs/2306.03078 .\n- V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh. Extreme compression of large language models via additive quantization, 2024. URL https://arxiv. org/abs/2401.06118 .\n- E. Frantar and D. Alistarh. Marlin: a fast 4-bit inference kernel for medium batchsizes. https: //github.com/IST-DASLab/marlin , 2024.\n- E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323 , 2022.\n- L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.\n- L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, A. Le Noac'h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/ records/10256836 .\n- A. Gholami, Z. Yao, S. Kim, C. Hooper, M. W. Mahoney, and K. Keutzer. Ai and memory wall, 2024. URL https://arxiv.org/abs/2403.14123 .\n11. Google TPU Team. Google cloud tpu v3. https://cloud.google.com/tpu/docs/v3 , 2018. Accessed: July 3, 2024.\n12. Google TPU Team. Google cloud tpu v4. https://cloud.google.com/tpu/docs/v4 , 2021. Accessed: July 3, 2024.\n13. Google TPU Team. Google cloud tpu v5e. https://cloud.google.com/tpu/docs/v5e , 2023a. Accessed: July 3, 2024.\n14. Google TPU Team. Google cloud tpu v5p. https://cloud.google.com/tpu/docs/v5p , 2023b. Accessed: July 3, 2024.\n- D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson, Y. Wang, S. Arora, D. Atkinson, R. Authur, K. R. Chandu, A. Cohan, J. Dumas, Y. Elazar, Y. Gu, J. Hessel, T. Khot, W. Merrill, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, V. Pyatkin, A. Ravichander, D. Schwenk, S. Shah, W. Smith, E. Strubell, N. Subramani, M. Wortsman, P. Dasigi, N. Lambert, K. Richardson, L. Zettlemoyer, J. Dodge, K. Lo, L. Soldaini, N. A. Smith, and H. Hajishirzi. Olmo: Accelerating the science of language models, 2024. URL https://arxiv.org/abs/2402.00838 .\n- A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752 .\n- A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR) , 2022.\n- D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n\n- J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203. 15556 .\n- S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, X. Zhang, Z. L. Thai, K. Zhang, C. Wang, Y. Yao, C. Zhao, J. Zhou, J. Cai, Z. Zhai, N. Ding, C. Jia, G. Zeng, D. Li, Z. Liu, and M. Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. URL https://arxiv.org/abs/2404.06395 .\n- W. Huang, X. Ma, H. Qin, X. Zheng, C. Lv, H. Chen, J. Luo, X. Qi, X. Liu, and M. Magno. How good are low-bit quantized llama3 models? an empirical study, 2024. URL https://arxiv. org/abs/2404.14047 .\n- I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. Neural Computation , 29(1):322-344, 2017.\n- I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry. Improving post training neural quantization: Layer-wise calibration and integer programming, 2020. URL https://arxiv. org/abs/2006.10518 .\n6. Intel Gaudi Team. Intel gaudi 2 and gaudi 3 ai accelerators. https://cdrdv2-public.intel. com/817486/gaudi-3-ai-accelerator-white-paper.pdf , 2024. Accessed: July 3, 2024.\n- M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, J. Yang, J. Park, A. Heinecke, E. Georganas, S. Srinivasan, A. Kundu, M. Smelyanskiy, B. Kaul, and P. Dubey. A study of bfloat16 for deep learning training, 2019. URL https://arxiv.org/abs/1905.12322 .\n- J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/ abs/2001.08361 .\n- S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney, and K. Keutzer. Squeezellm: Dense-and-sparse quantization, 2024.\n- D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.\n12. P.-H. C. Le and X. Li. Binaryvit: Pushing binary vision transformers towards convolutional models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , pages 4664-4673, June 2023.\n- C. Lee, J. Jin, T. Kim, H. Kim, and E. Park. Owq: Outlier-aware weight quantization for efficient fine-tuning and inference of large language models, 2024. URL https://arxiv.org/abs/2306. 02272 .\n- K. Levenberg. A method for the solution of certain non-linear problems in least squares. Quarterly of Applied Mathematics , 2:164-168, 1944.\n- F. Li, B. Zhang, and B. Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711 , 2016.\n- S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and Y. Wang. Evaluating quantized large language models, 2024. URL https://arxiv.org/abs/2402.18158 .\n- J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. URL https://arxiv.org/abs/2306.00978 .\n\n- S. Lin, J. Hilton, and O. Evans. Truthfulqa: Measuring how models mimic human falsehoods, 2021.\n- J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang. Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence , IJCAI'20, 2021. ISBN 9780999241165.\n- Z. Liu, B. Oguz, A. Pappu, Y. Shi, and R. Krishnamoorthi. Binary and ternary natural language generation, 2023a. URL https://arxiv.org/abs/2306.01841 .\n- Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 , 2023b.\n- Z. Liu, A. Qiao, W. Neiswanger, H. Wang, B. Tan, T. Tao, J. Li, Y. Wang, S. Sun, O. Pangarkar, R. Fan, Y. Gu, V. Miller, Y. Zhuang, G. He, H. Li, F. Koto, L. Tang, N. Ranjan, Z. Shen, X. Ren, R. Iriondo, C. Mu, Z. Hu, M. Schulze, P. Nakov, T. Baldwin, and E. P. Xing. Llm360: Towards fully transparent open-source llms, 2023c.\n- S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei. The era of 1-bit llms: All large language models are in 1.58 bits, 2024.\n- D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms . Cambridge University Press, 2003. ISBN 978-0521642989.\n- V. Malinovskii, D. Mazur, I. Ilin, D. Kuznedelev, K. Burlachenko, K. Yi, D. Alistarh, and P. Richtarik. Pv-tuning: Beyond straight-through estimation for extreme llm compression, 2024. URL https: //arxiv.org/abs/2405.14852 .\n- D. Marquardt. An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial and Applied Mathematics , 11(2):431-441, 1963.\n- P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, and H. Wu. Mixed precision training, 2018. URL https://arxiv. org/abs/1710.03740 .\n- T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space, 2013. URL https://arxiv.org/abs/1301.3781 .\n- N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman. CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing , Online, Nov. 2020. Association for Computational Linguistics.\n- B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro. Towards understanding the role of over-parametrization in generalization of neural networks. In Proceedings of the International Conference on Learning Representations (ICLR) , 2018.\n14. Nvidia, :, B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, S. Das, A. Dattagupta, O. Delalleau, L. Derczynski, Y. Dong, D. Egert, E. Evans, A. Ficek, D. Fridman, S. Ghosh, B. Ginsburg, I. Gitman, T. Grzegorzek, R. Hero, J. Huang, V. Jawa, J. Jennings, A. Jhunjhunwala, J. Kamalu, S. Khan, O. Kuchaiev, P. LeGresley, H. Li, J. Liu, Z. Liu, E. Long, A. S. Mahabaleshwarkar, S. Majumdar, J. Maki, M. Martinez, M. R. de Melo, I. Moshkov, D. Narayanan, S. Narenthiran, J. Navarro, P. Nguyen, O. Nitski, V. Noroozi, G. Nutheti, C. Parisien, J. Parmar, M. Patwary, K. Pawelec, W. Ping, S. Prabhumoye, R. Roy, T. Saar, V. R. N. Sabavat, S. Satheesh, J. P. Scowcroft, J. Sewall, P. Shamis, G. Shen, M. Shoeybi, D. Sizer, M. Smelyanskiy, F. Soares, M. N. Sreedhar, D. Su, S. Subramanian, S. Sun, S. Toshniwal, H. Wang, Z. Wang, J. You, J. Zeng, J. Zhang, J. Zhang, V. Zhang, Y. Zhang, and C. Zhu. Nemotron-4 340b technical report, 2024. URL https://arxiv.org/abs/2406.11704 .\n15. Nvidia Team. Nvidia tesla v100 gpu accelerator. https://images.nvidia.com/content/ technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf , 2018. Accessed: July 3, 2024.\n\n- Nvidia Team. Nvidia a100 tensor core gpu. https://www. nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/ nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf , 2020. Accessed: July 3, 2024.\n- Nvidia Team. Nvidia h100 tensor core gpu. https://resources.nvidia.com/ en-us-tensor-core/nvidia-tensor-core-gpu-datasheet , 2022. Accessed: July 3, 2024.\n- Nvidia Team. Nvidia h200 tensor core gpu. https://nvdam.widen.net/s/nb5zzzsjdf/ hpc-datasheet-sc23-h200-datasheet-3002446 , 2023. Accessed: July 3 2024. Redirected from https://www.nvidia.com/en-in/data-center/h200/ .\n- Nvidia Team. Nvidia blackwell architecture. https://resources.nvidia.com/ en-us-blackwell-architecture , 2024. Accessed: July 3, 2024.\n- D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In K. Erk and N. A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1525-1534, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https: //aclanthology.org/P16-1144 .\n- A. Papoulis and S. U. Pillai. Probability, Random Variables, and Stochastic Processes . McGraw-Hill, 4 edition, 2002. ISBN 978-0071226615.\n- A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and S. Bowman. BBQ: A hand-built bias benchmark for question answering. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022 , pages 2086-2105, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165. URL https://aclanthology.org/2022.findings-acl. 165 .\n- H. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, R. Li, M. Zhang, C. Li, J. Ning, R. Wang, Z. Zhang, S. Liu, J. Chau, H. Hu, and P. Cheng. Fp8-lm: Training fp8 large language models, 2023. URL https://arxiv.org/abs/2310.18313 .\n- O. Press and L. Wolf. Using the output embedding to improve language models, 2017. URL https://arxiv.org/abs/1608.05859 .\n- PyTorch Team. Accelerating triton with torchscript and tensorrt integration. https://pytorch. org/blog/accelerating-triton/ , 2024. Accessed: 2024-07-04.\n- S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis , SC '20. IEEE Press, 2020. ISBN 9781728199986.\n- J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining , KDD '20, page 3505-3506, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703 .\n- M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. European Conference on Computer Vision , pages 525-542, 2016.\n- K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM , 64(9):99-106, aug 2021. ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381 .\n- C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal , 27(3): 379-423, 623-656, 1948.\n- N. Shazeer. Glu variants improve transformer, 2020.\n\n- M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.\n- D. Soboleva, F. Al-Khateeb, R. Myers, J. R. Steeves, J. Hestness, and N. Dey. Slimpajama: A 627b token cleaned and deduplicated version of redpajama, 2023. URL https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama .\n- J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n- C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo, and N. Wong. Compression of generative pre-trained language models via quantization. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 4821-4836, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.331. URL https://aclanthology.org/2022.acl-long.331 .\n- G. Team, T. Mesnard, C. Hardin, R. Dadashi, S. Bhupatiraju, S. Pathak, L. Sifre, M. Rivière, M. S. Kale, J. Love, P. Tafti, L. Hussenot, P. G. Sessa, A. Chowdhery, A. Roberts, A. Barua, A. Botev, A. Castro-Ros, A. Slone, A. Héliou, A. Tacchetti, A. Bulanova, A. Paterson, B. Tsai, B. Shahriari, C. L. Lan, C. A. Choquette-Choo, C. Crepy, D. Cer, D. Ippolito, D. Reid, E. Buchatskaya, E. Ni, E. Noland, G. Yan, G. Tucker, G.-C. Muraru, G. Rozhdestvenskiy, H. Michalewski, I. Tenney, I. Grishchenko, J. Austin, J. Keeling, J. Labanowski, J.-B. Lespiau, J. Stanway, J. Brennan, J. Chen, J. Ferret, J. Chiu, J. Mao-Jones, K. Lee, K. Yu, K. Millican, L. L. Sjoesund, L. Lee, L. Dixon, M. Reid, M. Mikuła, M. Wirth, M. Sharman, N. Chinaev, N. Thain, O. Bachem, O. Chang, O. Wahltinez, P. Bailey, P. Michel, P. Yotov, R. Chaabouni, R. Comanescu, R. Jana, R. Anil, R. McIlroy, R. Liu, R. Mullins, S. L. Smith, S. Borgeaud, S. Girgin, S. Douglas, S. Pandya, S. Shakeri, S. De, T. Klimenko, T. Hennigan, V. Feinberg, W. Stokowiec, Y. hui Chen, Z. Ahmed, Z. Gong, T. Warkentin, L. Peran, M. Giang, C. Farabet, O. Vinyals, J. Dean, K. Kavukcuoglu, D. Hassabis, Z. Ghahramani, D. Eck, J. Barral, F. Pereira, E. Collins, A. Joulin, N. Fiedel, E. Senter, A. Andreev, and K. Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295 .\n- H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023a.\n- H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.\n- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper\\_ files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\n- H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei. Bitnet: Scaling 1-bit transformers for large language models, 2023.\n- J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. ArXiv , abs/1707.06209, 2017. URL https://api.semanticscholar.org/CorpusID:1553193 .\n- T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,\n\nM. Drame, Q. Lhoest, and A. M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pages 38-45, Online, Oct. 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6 .\n\n- G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. URL https://arxiv.org/abs/ 2211.10438 .\n- Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers, 2022. URL https://arxiv.org/abs/ 2206.01861 .\n- Z. Yao, X. Wu, C. Li, S. Youn, and Y. He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation, 2023. URL https://arxiv.org/abs/ 2303.08302 .\n- Z. Yuan, Y. Shang, and Z. Dong. PB-LLM: Partially binarized large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/ forum?id=BifeBRhikU .\n- R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. Traum, and L. Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4791-4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472 .\n- B. Zhang and R. Sennrich. Root mean square layer normalization. In Proceedings of the 33rd International Conference on Neural Information Processing Systems , Red Hook, NY, USA, 2019. Curran Associates Inc.\n- C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR) , 2017.\n- S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 , 2016.\n- S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients, 2018. URL https://arxiv.org/abs/1606. 06160 .\n- C. Zhu, S. Han, H. Mao, and W. J. Dally. Trained ternary quantization. arXiv preprint arXiv:1612.01064 , 2017.\n- X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang. A survey on model compression for large language models, 2023. URL https://arxiv.org/abs/2308.07633 .\n- B. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer, and W. Fedus. St-moe: Designing stable and transferable sparse expert models, 2022. URL https://arxiv.org/abs/ 2202.08906 .\n\n## A Architecture and PreTraining Details\n\nThis section provides a comprehensive overview of the pretraining for TriLM (Ternary Language Model) and FloatLM (Floating Point Language Model). We outline the forward and backward pass equations specific to their linear layers, highlighting the contrast between the FP16 matrices in FloatLM and the ternary matrices with scalar scaling in TriLM. Additionally, it covers dataset selection, tokenizer usage, and preprocessing methods employed for training data preparation. These discussions provide information on pretraining setups, implementation nuances, and key hyperparameters critical to the models' development.\n\n## A.1 Data and Tokenizer\n\nDataset Selection: Let input be X ∈ R b × n for a linear layer with FP16 weight matrix W ∈ R m × n and Y ∈ R b × m be the output. The same matrix W is also used to denote latent weights in TriLMs during training.\n\nFor ternarized layers in TriLMs, we also have a scalar scale γ ∈ R , matrix with ternarized states ̂ W ∈ {-1 , 0 , 1 } n × m and ternarized matrix ˜ W ∈ R n × m . We set ϵ = 1 e -5 .\n\nDue to lack of availability of Pile 300B [Gao et al., 2020] used in Pythia, we opted to use a 300B token sample of deduplicated Slim Pajama dataset 5 . We sample from each subset with the probability proportional to its size.\n\n## Training Data Preparation:\n\n- Main experiments (Spectra suite) : We used the full 300B token sample.\n- Ablation studies : Training runs with 100B tokens, we sample from these 300B tokens with equal probability weight to each data-point.\n- Fine-Web Edu experiments : We tokenized one-third of a 350B token sample, from which we then sampled 100B tokens for our experiments.\n\nQuantLM: For the creation of QuantLM, we utilized a subset of the Slimpajama-627B dataset, consisting of 512 samples with a sequence length of 2048. These samples were normalized for length. Our approach closely follows the methodology outlined in [Malinovskii et al., 2024].\n\nTokenizer and Optimization Techniques: We use the GPT-NeoX 20B tokenizer following Pythia. For speeding up training, we round embedding rounding of to the nearest multiple of 128 times the model parallel size.\n\n## A.2 PreTraining Setup\n\nWe scale using 2D-parallelism with Megatron-style sharding [Shoeybi et al., 2019] and use ZeRO stage 2 Deepspeed [Rasley et al., 2020] for ZeRO [Rajbhandari et al., 2020].\n\nTable 2: 300B Subset of Slim Pajama\n\n| Dataset        | Size (Tokens)   |\n|----------------|-----------------|\n| Arxiv          | 13B             |\n| Book           | 13B             |\n| C4             | 80B             |\n| Common Crawl   | 156B            |\n| GitHub         | 16B             |\n| Stack Exchange | 10B             |\n| Wikipedia      | 12B             |\n| Total          | 300B            |\n\nOur implementation was based on GPT NeoX Codebase [Andonian et al., 2023]. We use AdamW [Kingma and Ba, 2017] for optimization. We train on nodes with IBM Power9 PC CPUs and 6x16GB V100. Due to the lack of BFloat16 support in V100, we train both TriLM and FloatLM in FP16 using Mixed Precision Training and Dynamic Loss Scaling. Please refer to §A.4 for more implementation specific details. We extensively use Huggingface [Wolf et al., 2020] and Wandb [Biewald, 2020] for handling the checkpoints and experiment tracking.\n\n## A.3 Hyperparameters\n\nTable 3 shows the hyperparameters for TriLM and FloatLM's transformer architecture and their learning rate. We set Adam β are set to (0.9, 0.95) for both families of models and all the reported\n\n5 We also make this subset public\n\nruns are trained to 2048 sequence length. FloatLM and TriLM are respectively trained with batch sizes of 2 M and 1 M tokens respectively.\n\nTable 3: Hyperparameters across model sizes for TriLM and FloatLM.\n\n| Params        |   Hidden |   GLU |   Heads |   Layers |   MP | FloatLM LR     | TriLM LR                        |\n|---------------|----------|-------|---------|----------|------|----------------|---------------------------------|\n| 99.74M (99M)  |      512 |  1280 |       8 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 190.0M (190M) |      768 |  2048 |      12 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 392.4M (390M) |     1024 |  2560 |      16 |       24 |    1 | 3 . 0 ∗ 10 - 4 | 1 . 8 ∗ 10 - 3 → 1 . 2 ∗ 10 - 3 |\n| 569.2M (560M) |     1280 |  3072 |      20 |       24 |    1 | 2 . 8 ∗ 10 - 4 | 1 . 6 ∗ 10 - 3 → 1 . 1 ∗ 10 - 3 |\n| 834.0M (830M) |     1536 |  4096 |      24 |       24 |    1 | 2 . 5 ∗ 10 - 4 | 1 . 5 ∗ 10 - 3 → 1 . 0 ∗ 10 - 3 |\n| 1.149B (1.1B) |     1792 |  5120 |      28 |       24 |    2 | 2 . 2 ∗ 10 - 4 | 1 . 3 ∗ 10 - 3 → 9 . 0 ∗ 10 - 4 |\n| 1.515B (1.5B) |     2048 |  6144 |      32 |       24 |    2 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 2.461B (2.4B) |     2304 |  7680 |      36 |       30 |    3 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 3.989B (3.9B) |     3072 |  9216 |      24 |       30 |    6 | 1 . 5 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n\nTable 4: Sizes in bits (* 10 9 ) for Spectra suite of LLMs across varying parameter counts.\n\n| Params        |   99M |   190M |   390M |   560M |   830M |   1.1B |   1.5B |   2.4B |   3.9B |\n|---------------|-------|--------|--------|--------|--------|--------|--------|--------|--------|\n| FloatLM       |  1.6  |   3.05 |   6.28 |   9.11 |  13.34 |  18.39 |  24.23 |  39.38 |  63.83 |\n| QuantLM 8-Bit |  1.21 |   2.14 |   3.96 |   5.58 |   7.91 |  10.64 |  13.77 |  21.55 |  34.39 |\n| QuantLM 6-Bit |  1.11 |   1.92 |   3.38 |   4.7  |   6.55 |   8.7  |  11.15 |  17.09 |  27.03 |\n| QuantLM 4-Bit |  1.03 |   1.72 |   2.88 |   3.93 |   5.36 |   7    |   8.86 |  13.18 |  20.59 |\n| QuantLM 3-Bit |  0.98 |   1.6  |   2.59 |   3.49 |   4.68 |   6.03 |   7.55 |  10.95 |  16.91 |\n| TriLM         |  0.9  |   1.42 |   2.11 |   2.76 |   3.55 |   4.42 |   5.36 |   7.23 |  10.76 |\n\n## A.4 Known Implementation Artifacts\n\nSimilar to BitNet [Wang et al., 2023], our models exhibit artifacts resulting from model parallelism. A key issue arises when computing the scale, γ , across the entire weight matrix, which is sharded across multiple devices. This process introduces a significant communication overhead due to the all-reduce operations. In our implementation, we address this by computing the scales over the portion of the weight matrix local to each device. Consequently, during inference with TriLM models, scales are computed independently within each model parallel group. Importantly, this modification has a negligible impact on the bits per parameter, amounting to less than 10 -5 , even at the highest model parallelism level of 6 for our largest model.\n\nGiven that we train in FP16, some artifacts are expected as a result of this training method. However, we do not anticipate significant performance differences when comparing mixed precision training with BF16 or even FP32. This expectation is based on the observation that the lowest loss scales recorded during our runs were consistently at or above the recommended value of 128 [Micikevicius et al., 2018] (refer to Table 5).\n\n## A.5 Differences from BitNet Architecture\n\nTriLM differs from BitNet b1.58 in several ways for better performance as well as for fairer comparison with FloatLMs. Adopting the GPT-3's Pre-Normalization approach as outlined by [Brown et al., 2020a], normalization is applied prior to each linear layer. This method has proven essential for maintaining stable training under FP16 precision [Wang et al., 2023]. Consequently, normalization occurs twice within each transformer layer: once at the input representations to the attention sub-layer and again at the input representations to the Gated MLP sub-layer. This approach contrasts with BitNet, where activation or intermediate representations are normalized, scaled, and quantized to 8 bits before each linear layer, which occurs between 4 to 7 times per transformer layer depending on the specific implementation. Furthermore, TriLM employs RMSNorm with a scale parameter over the parameterless RMSNorm.\n\nFigure 14 shows the commonsense and reasoning performance of TriLM 1.1B, FloatLM 1.1B and our replication of BitNet b1.58's architecture at 1.1B scale, along with the reported performance\n\nTable 5: Final loss-scale and number of batches skipped across TriLM and FloatLM training runs - We are able to maintain above the recommended loss scales of 128 for mixed precision training [Micikevicius et al., 2018].\n\n| Model        |   Min. Loss-Scale |   # Skipped Batches | # Skipped Tokens   |\n|--------------|-------------------|---------------------|--------------------|\n| FloatLM 99M  |               256 |                 181 | 0.37B              |\n| TriLM 99M    |              1024 |                 303 | 0.33B              |\n| FloatLM 190M |               512 |                 168 | 0.35B              |\n| TriLM 190M   |               512 |                 305 | 0.33B              |\n| FloatLM 390M |              1024 |                 170 | 0.35B              |\n| TriLM 390M   |               512 |                 312 | 0.34B              |\n| FloatLM 560M |               256 |                 164 | 0.33B              |\n| TriLM 560M   |               512 |                 294 | 0.32B              |\n| FloatLM 830M |              2048 |                 175 | 0.36B              |\n| TriLM 830M   |               128 |                 307 | 0.33B              |\n| FloatLM 1.1B |              2048 |                 158 | 0.32B              |\n| TriLM 1.1B   |               512 |                 306 | 0.33B              |\n| FloatLM 1.5B |               256 |                 170 | 0.35B              |\n| TriLM 1.5B   |               512 |                 318 | 0.34B              |\n| FloatLM 2.4B |              1024 |                 165 | 0.34B              |\n| TriLM 2.4B   |               256 |                 294 | 0.32B              |\n| FloatLM 3.9B |               256 |                 164 | 0.34B              |\n| TriLM 3.9B   |               128 |                 309 | 0.33B              |\n\n## Relative Performance Across Architectures\n\nFigure 14: Performance across various architectures - TriLM 1.1B, FloatLM 1.1B, BitNet b1.58 1.1B (our replication) along with reported scores of BitNet b1.58 at 700M and 1.3B params. Scores are averaged across 6 common sense and reasoning benchmarks, mentioned in Table 11 and 10.\n\n<!-- image -->\n\nfor BitNet b1.58 700M and 1.3B. All these models have been trained for 100B tokens. Our BitNet replication achieves performance between the 700M and 1.3B models. However, all the BitNet models, including the larger 1.3B parameter model perform worse than TriLM 1.1B. It should be noted that at this 1.1B scale, TriLMs do not achieve parity with FloatLMs of the same parameter count. Table 11 and 10lists the detailed performance of these models across common sense benchmarks.\n\n## B Scaling of Binary and Ternary Large Language Models\n\nIn this section, we will comprehensively compare Binary Large Language Models (BiLMs) with Ternary Large Language Models (TriLMs). We will start by describing BiLMs, followed by studying scaling laws and presenting results on various benchmarks, as well as comparisons with TriLMs across parameter count and model size (in bits).\n\n## B.1 Scaling Laws\n\nFigures 15a and 15b show the final validation loss across sizes (in bits) and parameters, respectively. At the Billion+ model scale, Ternary Models appear to be preferable in terms of both the number of parameters and model size (in bits). However, the gap seems to be declining. The convergence point appears to occur at a high parameter scale (10B+). Thus, we decided to scale only TriLMs further to study the scaling laws of FLoatLMs and TriLMs upto a scale of 3.9 B parameters. This trend suggests that BiLMs have the potential to match at higher parameter counts.\n\n## B.2 BiLM: Binary Large Language Model\n\nIn Binary Large Language Models (BiLMs), the weights of the linear layers are represented by binary values of -1 or 1, with an accompanying floating-point scaling factor, similar to the method employed in TriLMs. Comprehensive formal descriptions of the forward pass, backward pass, and inference time calculations are provided in Appendix (§A). We have trained three BiLM models of distinct sizes: 99M, 560M, and 1.1B parameters. These models were trained on the same dataset and in the same sequence as the TriLMs, adhering to the identical optimization schedule detailed in Section 3.3.\n\nFigure 15: Final Validation loss across size (measure in bits) and parameters.\n\n<!-- image -->\n\nFigures 15a and 15b show the final validation loss across model sizes (in bits) and parameter counts, respectively. At the Billion+ model scale, Ternary Models appear to be preferable in terms of both the number of parameters and model size (in bits). However, the gap seems to be narrowing, with convergence likely occurring at higher parameter scales (10B+). Therefore, we decided to scale only TriLMs further to study the scaling laws of FloatLMs and TriLMs up to 3.9B parameters. This trend suggests that BiLMs have the potential to match the performance of floatLM at higher parameter counts.\n\n## B.3 Results\n\nWe conducted a comprehensive benchmark analysis of Binary Large Language Models (BiLMs) across three key dimensions: commonsense and reasoning tasks, knowledge-based tasks, and toxicity evaluation, as detailed in Tables 6, 7, 9, and 12\n\n<!-- image -->\n\nFigure 16: Performance of ternary TriLMs and BiLMs models on commonsense and Reasoning and MMLUs tasks across Size (Bits) and Parameters. Refer to Tables 6 and 7 for details.\n\nFigure 17: Performance of ternary TriLMs and BiLMs models on LAMBADA tasks across Size (Bits) and Parameters. Refer to Tables 6, and 7 for details.\n\n<!-- image -->\n\nFigure 18: Performance of ternary TriLMs and BiLMs models on MMLU across Size (Bits) and Parameters. Refer to Tables 12 for details.\n\n<!-- image -->\n\n## C Scaling Law\n\nIn this section, we provide additional insights into the scaling fits discussed in Section 4.3. In addition to fitting a power law with an offset, we also explore a standard power law following Kaplan et al. [2020]. Our findings suggest that the standard power law fits are slightly less precise than those incorporating an offset term. However, both models indicate a decreasing difference in validation loss as the number of parameters ( N ) increases.\n\nFigure 19: Comparison of Power Law and Power Law-with-offset Fits for TriLM and FloatLM.\n\n<!-- image -->\n\n## D Benchmark Details\n\nWe benchmark TriLM, FloatLM and QuantLM across Knowledge, Commonsense, Reasoning and Toxicity benchmarks. We average our scores across 3 different 'seeds' by preparing three different QuantLM models quantized using different calibration sets. We also add Pythia (deduplicated with consistent 2 M batch size across families) suite of models (70M to 2.8B params) and BitNet b.158 performance scores from their paper for comparison. We use the LM Evaluation Harness [Gao et al., 2023] to benchmark.\n\n## D.1 Commonsense and Reasoning\n\nWe report commonsense and reasoning benchmark scores across 6 benchmarks previously considered by BitNet b.158 in Table 6, 7 and rest in Table 9. Each is considered in a zero-shot setting. Following are the details of each of the benchmarks considered:\n\n- ARC Challenge and Easy : [Clark et al., 2018] ARC dataset comprises 7787 multiplechoice science questions divided into two sets: Challenge and Easy. We calculate accuracy and normalised accuracy across both of these sets.\n- BoolQ : [Clark et al., 2019] BoolQ is a reading comprehension dataset consisting of naturally occurring yes/no questions. We calculate the accuracy of this task.\n- HellaSwag : [Zellers et al., 2019] HellaSWAG is a dataset of multiple choice questions for testing grounded commonsense. The incorrect options are generated through Adversarial Filtering (AF) to fool machines but not humans. We calculate the accuracy and normalized accuracy on this task.\n- WinoGrande : [Sakaguchi et al., 2021] WinoGrande is a collection of 44k problems for testing commonsense reasoning formulated as a fill-in-a-blank task with binary options. We report the accuracy on this task.\n- PIQA : [Bisk et al., 2019] Physical Interaction Question Answering (PIQA) is a physical commonsense reasoning benchmark dataset to test the physical knowledge of language models. We calculate the accuracy and normalized accuracy on this task.\n- LAMBADA OpenAI : [Paperno et al., 2016] LAMBADA is a dataset to evaluate text understanding by next-word prediction. It is a collection of narrative passages BooksCorpus To succeed on LAMBADA, models must integrate broader discourse information, not solely rely on local context. We calculate the perplexity and the accuracy of the model on this task.\n- LogiQA : [Liu et al., 2021] LogiQA is a dataset for testing human logical reasoning. It contains questions spanning multiple types of deductive reasoning. We calculate the accuracy and normalized accuracy on this task.\n\n## D.2 Knowledge\n\nWe report performance on SciQ, TriviaQA in Tables 9, 12 and 13. Each is considered in a zero-shot setting. Following are the details of each of the benchmarks considered:\n\nThe knowledge-based evaluation included the following tasks:\n\n- SciQ : [Welbl et al., 2017] The SciQ dataset contains multiple-choice questions with 4 answer options from crowd-sourced science exams. The questions range from Physics, Chemistry and Biology and several other fields. We calculate the accuracy and length normalized accuracy on this task.\n- TriviaQA : [Joshi et al., 2017] TriviaQA is a reading comprehension dataset containing question-answer-evidence triples. We calculate the exact match accuracy on this task.\n- MMLU [Hendrycks et al., 2021]: The benchmark aims to assess the knowledge gained during pretraining by evaluating models solely in zero-shot and few-shot scenarios. It spans 57 subjects, including STEM fields, humanities, social sciences, and more.\n\nTable 6: Spectra Suite Performance (Part 1): Arc Challenge, Arc Easy, and BoolQ. Additionally, we also include scores on the Pythia LLM suite.\n\n| Models                                | Arc Challenge         | Arc Challenge         | Arc Easy              | Arc Easy              | BoolQ                 |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   |\n| Pythia 70M FloatLM 99M                | 22.0 ± 1.2 23.8 ± 1.2 | 22.1 ± 1.2 19.9 ± 1.2 | 24.8 ± 0.9 39.1 ± 1.0 | 24.8 ± 0.9 45.1 ± 1.0 | 38.5 ± 0.9 58.2 ± 0.9 |\n| QuantLM 99M 8-Bit                     | 23.8 ± 1.2            | 19.6 ± 1.2            | 39.4 ± 1.0            | 45.3 ± 1.0            | 58.5 ± 0.9            |\n| QuantLM 99M 6-Bit                     | 23.2 ± 1.2            | 19.7 ± 1.2            | 38.8 ± 1.0            | 44.8 ± 1.0            | 58.9 ± 0.9            |\n| QuantLM 99M 4-Bit                     | 22.6 ± 1.2            | 18.0 ± 1.1            | 37.1 ± 1.0            | 41.7 ± 1.0            | 52.2 ± 0.9            |\n| QuantLM 99M 3-Bit                     | 23.2 ± 1.2            | 19.5 ± 1.2            | 34.8 ± 1.0            | 36.1 ± 1.0            | 48.4 ± 0.9            |\n| TriLM 99M                             | 24.1 ± 1.3            | 19.1 ± 1.1            | 36.6 ± 1.0            | 39.8 ± 1.0            | 61.3 ± 0.9            |\n| Binary 99M                            | 20.8 ± 1.2            | 18.3 ± 1.1            | 35.8 ± 0.9            | 40.1 ± 1.0            | 61.0 ± 0.8            |\n| Pythia 160M                           | 23.8 ± 1.2            | 23.1 ± 1.2            | 26.7 ± 0.9            | 26.6 ± 0.9            | 38.3 ± 0.9            |\n| FloatLM 190M                          | 24.1 ± 1.3            | 20.5 ± 1.2            | 43.0 ± 1.0            | 48.4 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 8-Bit                    | 24.4 ± 1.3            | 20.3 ± 1.2            | 43.0 ± 1.0            | 48.5 ± 1.0            | 59.3 ± 0.9            |\n| QuantLM 190M 6-Bit                    | 23.8 ± 1.2            | 20.0 ± 1.2            | 42.0 ± 1.0            | 48.0 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 4-Bit                    | 25.2 ± 1.3            | 19.9 ± 1.2            | 26.5 ± 0.9            | 26.8 ± 0.9            | 40.9 ± 0.9            |\n| QuantLM 190M 3-Bit                    | 22.5 ± 1.2            | 19.4 ± 1.2            | 37.1 ± 1.0            | 39.7 ± 1.0            | 56.5 ± 0.9            |\n| TriLM 190M                            | 23.0 ± 1.2            | 19.5 ± 1.2            | 39.6 ± 1.0            | 43.9 ± 1.0            | 46.8 ± 0.9            |\n| FloatLM 390M                          | 24.7 ± 1.3            | 21.3 ± 1.2            | 46.5 ± 1.0            | 51.0 ± 1.0            | 54.7 ± 0.9            |\n| QuantLM 390M 8-Bit                    | 24.6 ± 1.3            | 21.2 ± 1.2            | 46.6 ± 1.0            | 51.0 ± 1.0            | 54.6 ± 0.9            |\n| QuantLM 390M 6-Bit                    | 24.8 ± 1.3            | 21.5 ± 1.2            | 46.8 ± 1.0            | 51.8 ± 1.0            | 55.3 ± 0.9            |\n| QuantLM 390M 4-Bit                    | 25.1 ± 1.3            | 21.3 ± 1.2            | 45.2 ± 1.0            | 49.6 ± 1.0            | 50.8 ± 0.9            |\n| QuantLM 390M 3-Bit                    | 24.9 ± 1.3            | 21.5 ± 1.2            | 41.6 ± 1.0            | 43.6 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 390M                            | 24.5 ± 1.3            | 21.2 ± 1.2            | 44.1 ± 1.0            | 48.6 ± 1.0            | 55.1 ± 0.9            |\n| Pythia 410M                           | 24.7 ± 1.3            | 21.2 ± 1.2            | 45.7 ± 1.0            | 51.6 ± 1.0            | 60.0 ± 0.9            |\n| FloatLM 560M                          | 26.5 ± 1.3            | 23.9 ± 1.2            | 48.4 ± 1.0            | 54.4 ± 1.0            | 57.9 ± 0.9            |\n| QuantLM 560M 8-Bit                    | 26.5 ± 1.3            | 23.6 ± 1.2            | 48.3 ± 1.0            | 54.1 ± 1.0            | 57.6 ± 0.9            |\n| QuantLM 560M 6-Bit QuantLM 560M 4-Bit | 26.0 ± 1.3            | 23.5 ± 1.2            | 47.6 ± 1.0            | 54.2 ± 1.0            | 57.3 ± 0.9            |\n| QuantLM 560M 3-Bit                    | 25.9 ± 1.3 24.0 ± 1.2 | 23.0 ± 1.2 21.2 ± 1.2 | 46.3 ± 1.0            | 52.4 ± 1.0            | 58.8 ± 0.9 59.0 ± 0.9 |\n|                                       |                       |                       | 42.3 ± 1.0            | 45.8 ± 1.0 50.2 ± 1.0 |                       |\n| TriLM 560M                            | 25.7 ± 1.3 24.6 ± 1.2 | 21.0 ± 1.2 20.2 ± 1.1 | 45.5 ± 1.0 41.9 ± 1.0 | 47.8 ± 1.0            | 57.3 ± 0.9 61.5 ± 0.8 |\n| Binary 560M FloatLM 830M              | 28.0 ± 1.3            | 24.5 ± 1.3            | 51.6 ± 1.0            | 57.3 ± 1.0            | 61.0 ± 0.9            |\n| QuantLM 830M 8-Bit                    | 28.2 ± 1.3            | 25.1 ± 1.3            | 51.7 ± 1.0            | 57.3 ± 1.0            | 60.9 ± 0.9            |\n| QuantLM 830M 6-Bit                    | 27.6 ± 1.3            | 24.7 ± 1.3            | 51.6 ± 1.0            | 57.7 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 830M 4-Bit                    | 27.6 ± 1.3            | 23.3 ± 1.2            | 50.5 ± 1.0            | 56.2 ± 1.0            | 58.1 ± 0.9            |\n| QuantLM 830M 3-Bit                    | 27.1 ± 1.3            | 22.7 ± 1.2            | 46.8 ± 1.0            | 50.5 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 830M                            | 25.3 ± 1.3            | 22.5 ± 1.2            | 48.7 ± 1.0            | 54.2 ± 1.0            | 60.4 ± 0.9            |\n| Pythia 1B                             |                       | 24.4 ± 1.3            | 49.0 ± 1.0            | 57.0 ± 1.0            | ±                     |\n| FloatLM 1.1B                          | 27.0 ± 1.3 29.1 ± 1.3 | 26.1 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 60.8 0.9 62.9 ± 0.8   |\n| QuantLM 1.1B 8-Bit                    | 28.9 ± 1.3            | 26.1 ± 1.3            | 54.1 ± 1.0            | 60.2 ± 1.0            | 62.6 ± 0.8            |\n| QuantLM 1.1B 6-Bit                    | 29.8 ± 1.3            | 25.5 ± 1.3            | 54.3 ± 1.0            | 60.2 ± 1.0            | 62.9 ± 0.8            |\n|                                       |                       |                       |                       | 59.0 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 1.1B 4-Bit QuantLM 1.1B 3-Bit | 30.3 ± 1.3 29.2 ± 1.3 | 26.0 ± 1.3 27.0 ± 1.3 | 53.6 ± 1.0 48.9 ± 1.0 | 55.0 ± 1.0            | 62.1 ± 0.8            |\n| TriLM 1.1B                            | 26.5 ± 1.3            | 24.6 ± 1.3            | 49.8 ± 1.0            | 56.3 ± 1.0            | 59.1 ± 0.9            |\n| Binary 1.1B                           | 24.8 ± 1.3            | 22.3 ± 1.2            | 46.1 ± 1.0            | 52.7 ± 1.0            | 56.3 ± 0.9            |\n| Pythia 1.4B                           | 28.7 ± 1.3            | 26.0 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 63.2 ± 0.8            |\n| FloatLM 1.5B                          | 29.7 ± 1.3            | 26.2 ± 1.3            | 56.4 ± 1.0            | 62.6 ± 1.0            | 63.2 ± 0.8            |\n| QuantLM 1.5B 8-Bit                    | 29.8 ± 1.3            | 26.0 ± 1.3            | 56.6 ± 1.0            | 62.4 ± 1.0 62.2 ± 1.0 | 63.3 ± 0.8            |\n| QuantLM 1.5B 6-Bit                    | 30.1 ± 1.3            | 26.0 ± 1.3 ±          | 56.8 ± 1.0 55.2 ± 1.0 |                       | 63.4 ± 0.8            |\n| QuantLM 1.5B 4-Bit                    | 29.4 ± 1.3            | 26.9 1.3              |                       | 60.4 ± 1.0            | 62.5 ± 0.8            |\n| QuantLM 1.5B 3-Bit                    | 27.8 ± 1.3            | 25.2 ± 1.3            | 49.7 ± 1.0            | 54.8 ± 1.0            | 53.7 ± 0.9            |\n| TriLM 1.5B                            | 28.2 ± 1.3            | 24.7 ± 1.3            | 53.1 ± 1.0            | 59.0 ± 1.0            | 54.1 ± 0.9            |\n| FloatLM 2.4B                          | 32.7 ± 1.4            | 30.1 ± 1.3            | 60.5 ± 1.0            | 65.5 ± 1.0            | 62.1 ± 0.8            |\n| QuantLM 2.4B 8-Bit                    | 32.6 ± 1.4            | 30.0 ± 1.3            | 60.3 ± 1.0            | 65.7 ± 1.0            | 62.1 ± 0.8 62.0 ± 0.8 |\n| QuantLM 2.4B 6-Bit                    | 32.7 ± 1.4            | 30.6 ± 1.3            | 60.4 ± 1.0            | 65.4 ± 1.0            |                       |\n| QuantLM 2.4B 4-Bit                    | 33.3 ± 1.4            | 30.8 ± 1.3            | 59.6 ± 1.0            | 64.1 ± 1.0            | 59.0 ± 0.9            |\n| QuantLM 2.4B 3-Bit                    | 29.7 ± 1.3            | 28.4 ± 1.3            | 54.2 ± 1.0            | 58.4 ± 1.0            | 55.7 ± 0.9            |\n| TriLM 2.4B                            | 29.9 ± 1.3            | 29.5 ± 1.3            | 58.0 ± 1.0            | 63.8 ± 1.0            | 64.4 ± 0.8            |\n| FloatLM 3.9B                          | 34.6 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0            | 68.3 ± 1.0            | 65.9 ± 0.8            |\n| QuantLM 3.9B 8-Bit                    | 34.6 ± 1.4            | 31.9 ± 1.4            |                       | 68.1 ± 1.0            | 65.4 ± 0.8            |\n| QuantLM 3.9B 6-Bit                    | 35.1 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0 63.3 ± 1.0 | 68.0 ± 1.0            | 65.6 ± 0.8            |\n|                                       |                       |                       |                       |                       | 65.4 ± 0.8            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 34.7 ± 1.4 32.1 ± 1.4 | 32.9 ± 1.4 29.3 ± 1.3 | 61.2 ± 1.0 55.5 ± 1.0 | 68.3 ± 1.0 62.1 ± 1.0 | 60.0 ± 0.9            |\n| TriLM 3.9B                            | 35.3 ± 1.4            | 31.9 ± 1.4            | 60.8 ± 1.0            | 66.0 ± 1.0            | 66.5 ± 0.8            |\n\nTable 7: Spectra Suite Performance (Part 2): HellaSwag, PIQA, WinoGrande, and Average Scores (including Arc Easy, Arc Challenge, and BoolQ). Additionally, we include scores from the Pythia LLM suite.\n\n| Models                                | HellaSwag             | HellaSwag             | PIQA                  | PIQA                  | WinoGrande            | Avg ( HellaSwag, PIQA, WinoGrande, Arc BoolQ)   |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-------------------------------------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   | Easy, Arc Challenge, and                        |\n| Pythia 70M FloatLM 99M                | 25.1 ± 0.4 31.6 ± 0.5 | 25.1 ± 0.4 29.1 ± 0.5 | 49.8 ± 1.2 62.8 ± 1.1 | 49.9 ± 1.2 63.2 ± 1.1 | 49.1 ± 1.4 50.2 ± 1.4 | 34.9 44.3                                       |\n| QuantLM 99M 8-Bit                     | 31.7 ± 0.5            | 29.0 ± 0.5            | 62.6 ± 1.1            | 63.0 ± 1.1            | 50.0 ± 1.4            | 44.3                                            |\n| QuantLM 99M 6-Bit                     | 31.7 ± 0.5            | 29.2 ± 0.5            | 62.8 ± 1.1            | 63.1 ± 1.1            | 50.2 ± 1.4            | 44.3                                            |\n| QuantLM 99M 4-Bit                     | 31.0 ± 0.5            | 28.9 ± 0.5            | 62.2 ± 1.1            | 60.9 ± 1.1            | 50.4 ± 1.4            | 42.6                                            |\n| QuantLM 99M 3-Bit                     | 29.2 ± 0.5            | 27.7 ± 0.4            | 57.2 ± 1.2            | 58.2 ± 1.2            | 49.2 ± 1.4            | 40.3                                            |\n| TriLM 99M                             | 28.4 ± 0.5            | 27.6 ± 0.4            | 60.1 ± 1.1            | 60.4 ± 1.1            | 50.7 ± 1.4            | 43.5                                            |\n| Binary 99M                            | 27.7 ± 0.4            | 27.2 ± 0.4            | 59.2 ± 1.1            | 59.2 ± 1.1            | 48.8 ± 1.4            | 39.8                                            |\n| Pythia 160M                           | 25.1 ± 0.4            | 25.0 ± 0.4            | 53.1 ± 1.2            | 53.1 ± 1.2            | 47.3 ± 1.4            | 35.7                                            |\n| FloatLM 190M                          | 36.6 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.9 ± 1.4            | 46.7                                            |\n| QuantLM 190M 8-Bit                    | 36.5 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.7 ± 1.4            | 46.8                                            |\n| QuantLM 190M 6-Bit                    | 36.3 ± 0.5            | 31.5 ± 0.5            | 65.6 ± 1.1            | 64.3 ± 1.1            | 51.9 ± 1.4            | 46.4                                            |\n| QuantLM 190M 4-Bit                    | 26.0 ± 0.4            | 25.7 ± 0.4            | 49.3 ± 1.2            | 51.7 ± 1.2            | 51.0 ± 1.4            | 36.5                                            |\n| QuantLM 190M 3-Bit                    | 32.0 ± 0.5            | 28.8 ± 0.5            | 58.1 ± 1.2            | 58.7 ± 1.1            | 50.1 ± 1.4            | 42.7                                            |\n| TriLM 190M                            | 31.6 ± 0.5            | 29.0 ± 0.5            | 62.0 ± 1.1            | 62.3 ± 1.1            | 51.7 ± 1.4            | 42.4                                            |\n| FloatLM 390M                          | 44.4 ± 0.5            | 35.7 ± 0.5            | 68.7 ± 1.1            | 68.4 ± 1.1            | 51.8 ± 1.4            | 48.5                                            |\n| QuantLM 390M 8-Bit                    | 44.5 ± 0.5            | 35.7 ± 0.5            | 68.8 ± 1.1            | 68.6 ± 1.1            | 52.6 ± 1.4            | 48.6                                            |\n| QuantLM 390M 6-Bit                    | 44.2 ± 0.5            | 35.6 ± 0.5            | 69.0 ± 1.1            | 68.4 ± 1.1            | 53.0 ± 1.4            | 48.9                                            |\n| QuantLM 390M 4-Bit                    | 43.4 ± 0.5            | 35.1 ± 0.5            | 68.1 ± 1.1            | 68.3 ± 1.1            | 53.7 ± 1.4            | 47.7                                            |\n| QuantLM 390M 3-Bit                    | 39.5 ± 0.5            | 32.9 ± 0.5            | 63.8 ± 1.1            | 63.2 ± 1.1            | 53.0 ± 1.4            | 46.5                                            |\n| TriLM 390M                            | 37.9 ± 0.5            | 32.0 ± 0.5            | 64.7 ± 1.1            | 65.0 ± 1.1            | 52.2 ± 1.4            | 46.4                                            |\n| Pythia 410M                           | 40.3 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 66.3 ± 1.1            | 53.5 ± 1.4            | 48.6                                            |\n| FloatLM 560M                          | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.8 ± 1.1            | 69.0 ± 1.1            | 53.7 ± 1.4            | 50.5                                            |\n| QuantLM 560M 8-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.9 ± 1.1            | 68.9 ± 1.1            | 53.8 ± 1.4            | 50.4                                            |\n| QuantLM 560M 6-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.7 ± 1.1            | 68.8 ± 1.1            | 53.5 ± 1.4            | 50.1                                            |\n| QuantLM 560M 4-Bit                    | 46.7 ± 0.5            | 37.0 ± 0.5            | 67.8 ± 1.1            | 67.1 ± 1.1            | 53.1 ± 1.4            | 49.8                                            |\n| QuantLM 560M 3-Bit                    | 41.7 ± 0.5            | 33.4 ± 0.5            | 63.5 ± 1.1            | 63.2 ± 1.1            | 49.7 ± 1.4            | 46.7                                            |\n| TriLM 560M                            | 41.5 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 67.5 ± 1.1            | 53.1 ± 1.4            | 48.4                                            |\n| Binary 560M                           | 36.4 ± 0.4            | 31.2 ± 0.4            | 64.6 ± 1.1            | 64.2 ± 1.1            | 52.8 ± 1.4            | 44.5                                            |\n| FloatLM 830M QuantLM 830M 8-Bit       | 51.3 ± 0.5 51.4 ± 0.5 | 40.1 ± 0.5 40.1 ± 0.5 | 71.4 ± 1.1 ±          | 71.7 ± 1.1 71.7 ± 1.1 | 56.4 ± 1.4 55.9 ± 1.4 | 53.3 53.2                                       |\n| QuantLM 830M 6-Bit                    | 51.5 ± 0.5            | 40.2 ± 0.5            | 71.2 1.1 71.3 ± 1.1   | 71.8 ± 1.0            | 56.2 ± 1.4            | 53.2                                            |\n| QuantLM 830M 4-Bit                    |                       | 39.2 ± 0.5            | 70.6 ± 1.1            | 71.1 ± 1.1            | 56.0 ± 1.4            | 52.2                                            |\n| 3-Bit                                 | 50.2 ± 0.5            | 35.9 ± 0.5            | 66.1 ± 1.1            | 66.6 ± 1.1            | 53.5 ± 1.4            | 49.2                                            |\n| QuantLM 830M                          | 45.5 ± 0.5            |                       |                       |                       |                       |                                                 |\n| TriLM 830M Pythia 1B                  | 46.0 ± 0.5            | 36.8 ± 0.5            | 68.2 ± 1.1            | 68.4 ± 1.1 70.8 ± 1.1 | 55.6 ± 1.4            | 50.7                                            |\n| FloatLM 1.1B                          | 47.2 ± 0.5 55.2 ± 0.5 | 37.7 ± 0.5 42.6 ± 0.5 | 69.3 ± 1.1 72.2 ± 1.0 | 71.3 ± 1.1            | 53.2 ± 1.4 56.3 ± 1.4 | 51.1 54.9                                       |\n| QuantLM 1.1B 8-Bit                    | 55.2 ± 0.5            | 42.6 ± 0.5            | 72.1 ± 1.0            | 71.2 ± 1.1            | 56.2 ± 1.4            | 54.8                                            |\n| QuantLM 1.1B 6-Bit                    | 54.9 ± 0.5            |                       | 71.9 ± 1.0            | 71.2 ± 1.1            | 56.1 ± 1.4            | 55.0                                            |\n| QuantLM 1.1B 4-Bit                    | 54.9 ± 0.5            | 42.6 ± 0.5 42.0 ± 0.5 | 71.6 ± 1.1            | 70.4 ± 1.1            | 54.8 ± 1.4            | 54.4                                            |\n| QuantLM 1.1B 3-Bit                    | 51.3 ± 0.5            | 39.4 ± 0.5            | 69.4 ± 1.1            | 68.4 ± 1.1            | 54.8 ± 1.4            | 52.6                                            |\n| TriLM 1.1B                            | 49.1 ± 0.5            | 38.8 ± 0.5            | 69.8 ± 1.1            | 69.3 ± 1.1            | 55.5 ± 1.4            | 51.6                                            |\n|                                       | 43.4 ± 0.5            | 35.1 ± 0.4            | 66.9 ± 1.1            | 68.3 ± 1.1            | 55.3 ± 1.4            | 47.1                                            |\n| Binary 1.1B Pythia 1.4B               | 52.0 ± 0.5            | 40.4 ± 0.5            | 70.8 ± 1.1            | 70.6 ± 1.1            | 57.1 ± 1.4            | 54.3                                            |\n| FloatLM 1.5B                          | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.9 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.7                                            |\n| QuantLM 1.5B 8-Bit                    | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.7 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.8                                            |\n| QuantLM 1.5B 6-Bit                    | 57.5 ± 0.5            | 44.2 ± 0.5            | 74.0 ± 1.0            | 73.0 ± 1.0            | 59.7 ± 1.4            | 56.9                                            |\n| QuantLM 1.5B 4-Bit                    | 56.9 ± 0.5            | 43.2 ± 0.5            | 72.7 ± 1.0            | 72.4 ± 1.0            | 57.1 ± 1.4            | 55.6                                            |\n| QuantLM 1.5B 3-Bit                    | 53.7 ± 0.5            | 41.0 ± 0.5            | 70.0 ± 1.1            | 69.4 ± 1.1            | 55.0 ± 1.4            | 51.6                                            |\n| TriLM 1.5B                            | 53.1 ± 0.5            | 40.9 ± 0.5            | 70.1 ± 1.1            | 70.3 ± 1.1            | 56.1 ± 1.4            | 52.5                                            |\n| FloatLM 2.4B                          | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.2 ± 1.0            | 74.9 ± 1.0            | 61.8 ± 1.4            | 59.2                                            |\n|                                       | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.4 ± 1.0            | 74.9 ± 1.0            | 61.4 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 8-Bit QuantLM 2.4B 6-Bit | 62.9 ± 0.5            | 47.0 ± 0.5            | 75.7 ± 1.0            | 74.7 ± 1.0            | 61.1 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 4-Bit                    | 62.2 ± 0.5            | 46.5 ± 0.5            | 75.4 ± 1.0            | 74.5 ± 1.0            | 61.7 ± 1.4            | 58.5                                            |\n| QuantLM 2.4B 3-Bit                    | 58.6 ± 0.5            | 43.5 ± 0.5            | 72.7 ± 1.0            | 70.8 ± 1.1            | 57.2 ± 1.4            | 54.7                                            |\n| TriLM 2.4B                            | 59.0 ± 0.5            | 45.3 ± 0.5            | 72.6 ± 1.0            | 71.4 ± 1.1            | 59.7 ± 1.4            | 57.3                                            |\n| FloatLM 3.9B                          | 66.1 ± 0.5            | 49.7 ± 0.5            | 75.8 ± 1.0            | 75.4 ± 1.0            | ±                     | 61.4                                            |\n| QuantLM 3.9B 8-Bit                    |                       | 49.7 ± 0.5            |                       | 75.5 ± 1.0            | 62.8 1.4 ±            | 61.3                                            |\n| QuantLM 3.9B 6-Bit                    | 66.0 ± 0.5 65.9 ± 0.5 | 49.7 ± 0.5            | 75.9 ± 1.0 75.5 ± 1.0 | 75.6 ± 1.0            | 62.9 1.4 62.2 ± 1.4   | 61.3                                            |\n| QuantLM 3.9B 4-Bit                    | 65.0 ± 0.5            | 49.0 ± 0.5            | 75.5 ± 1.0            | 75.6 ± 1.0            | 62.7 ± 1.4            | 60.7                                            |\n| QuantLM 3.9B 3-Bit                    | 61.2 ± 0.5            | 45.9 ± 0.5            | 72.6 ± 1.0            | 72.3 ± 1.0            | 59.3 ± 1.4            | 56.8                                            |\n| TriLM 3.9B                            | 64.7 ± 0.5            | 48.3 ± 0.5            | 74.6 ± 1.0            | 74.4 ± 1.0            | 62.1 ± 1.4            | 60.7                                            |\n\nTable 8: Performance of BitNet b1.58 on ARC Challenge, ARC Easy, BoolQ, HellaSwag, PIQA, and WinoGrande. The scores are taken from [Ma et al., 2024].\n\n| Models      | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ   | HellaSwag   | HellaSwag   | PIQA      | PIQA   | WinoGrande Acc   | Avg   |\n|-------------|-----------------|-----------------|------------|------------|---------|-------------|-------------|-----------|--------|------------------|-------|\n| Models      | Acc Norm.       | Acc             | Acc Norm.  | Acc        | Acc     | Acc Norm.   | Acc         | Acc Norm. | Acc    |                  |       |\n| BitNet 700M | 21.4            |                 | 51.8       |            | 58.2    | 35.1        |             | 68.1      |        | 55.2             | 48.3  |\n| BitNet 1.3B | 24.2            |                 | 54.9       |            | 56.7    | 37.7        |             | 68.8      |        | 55.8             | 49.7  |\n| BitNet 3B   | 28.3            |                 | 61.4       |            | 61.5    | 42.9        |             | 71.5      |        | 59.3             | 54.2  |\n| BitNet 3.9B | 28.7            |                 | 64.2       |            | 63.5    | 44.2        |             | 73.2      |        | 60.5             | 55.7  |\n\n## D.3 Toxicity\n\nWe report toxicity-based evaluation in 12. Each is considered in a zero-shot setting.\n\nThe toxicity-based evaluation included the following tasks:\n\n- BBQ [Parrish et al., 2022]: The Bias Benchmark for QA (BBQ) dataset, comprises sets of questions developed by its authors, focusing on documented social biases directed towards individuals from protected classes across nine distinct social dimensions pertinent to U.S. English-speaking environments.\n- Crows Pairs [Nangia et al., 2020]: proposed a challenging dataset aimed at quantifying stereotypical biases embedded within language models, with a specific emphasis on U.S. contexts. Hosted on GitHub, this dataset serves as a crucial resource for assessing and addressing biases through paired sentences that illuminate societal stereotypes.\n- TruthfulQA [Lin et al., 2021]: A benchmark designed to evaluate the truthfulness of language models in generating responses to questions. This benchmark includes 817 questions across 38 categories, such as health, law, finance, and politics.\n\nTable 9: Spectra Suite Performance (Part 3): LAMBADA OpenAI, SciQ, LogiQA. We additionally also include Pythia's performance scores.\n\n| Models                                | LAMBADA               | LAMBADA               | SciQ                  | SciQ                  | LogiQA                | LogiQA                |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Perp.                 | Acc                   | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   |\n| FloatLM 99M QuantLM 99M 8-Bit         | 85.0 ± 6.9 85.8 ± 7.0 | 26.5 ± 0.6 26.6 ± 0.6 | 62.9 ± 1.5 62.8 ± 1.5 | 73.6 ± 1.4 73.7 ± 1.4 | 27.6 ± 1.8 27.8 ± 1.8 | 21.2 ± 1.6 21.0 ± 1.6 |\n| QuantLM 99M 6-Bit                     | 89.9 ± 7.4            | 26.1 ± 0.6            | 61.8 ± 1.5            | 73.9 ± 1.4            | 28.1 ± 1.8            | 20.3 ± 1.6            |\n| QuantLM 99M 4-Bit                     | 211.6 ± 17.3          | 16.7 ± 0.5            | 61.2 ± 1.5            | 70.7 ± 1.4            | 24.9 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 99M 3-Bit                     | 4765.4 ± 413.0        | 4.5 ± 0.3             | 51.9 ± 1.6            | 57.0 ± 1.6            | 25.3 ± 1.7            | 19.8 ± 1.6            |\n| TriLM 99M                             | 172.0 ± 8.4           | 20.0 ± 0.6            | 60.4 ± 1.5            | 67.6 ± 1.5            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| Binary 99M                            | 468.3 ± 24.1          | 14.0 ± 0.4            | 54.4 ± 1.6            | 62.5 ± 1.5            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| FloatLM 190M                          | 50.3 ± 2.7            | 31.1 ± 0.6            | 65.1 ± 1.5            | 77.3 ± 1.3            | 27.2 ± 1.7            | 22.1 ± 1.6            |\n| QuantLM 190M 8-Bit                    | 48.7 ± 2.6            | 31.5 ± 0.6            | 65.5 ± 1.5            | 77.1 ± 1.3            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| QuantLM 190M 6-Bit                    | 55.3 ± 3.0            | 30.0 ± 0.6            | 64.2 ± 1.5            | 77.0 ± 1.3            | 26.1 ± 1.7            | 22.4 ± 1.6            |\n| QuantLM 190M 4-Bit                    | 72479077.3            | 0.00 ± 0.0            | 25.6 ± 1.4            | 22.9 ± 1.3            | 23.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 190M 3-Bit                    | 664.5 ± 41.1          | 12.4 ± 0.5            | 58.5 ± 1.6            | 66.4 ± 1.5            | 26.3 ± 1.7            | 21.0 ± 1.6            |\n| TriLM 190M                            | 130.7 ± 6.5           | 23.7 ± 0.6            | 61.0 ± 1.5            | 72.6 ± 1.4            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 390M                          | 21.9 ± 0.9            | 42.2 ± 0.7            | 75.6 ± 1.4            | 84.2 ± 1.2            | 28.1 ± 1.8            | 23.8 ± 1.7            |\n| QuantLM 390M 8-Bit                    | 21.7 ± 0.9            | 42.3 ± 0.7            | 75.7 ± 1.4            | 84.1 ± 1.2            | 28.3 ± 1.8            | 24.1 ± 1.7            |\n| QuantLM 390M 6-Bit                    | 24.3 ± 1.0            | 40.6 ± 0.7            | 75.5 ± 1.4            | 83.7 ± 1.2            | 27.6 ± 1.8            | 23.2 ± 1.7            |\n| QuantLM 390M 4-Bit                    | 30.2 ± 1.3            | 39.1 ± 0.7            | 77.1 ± 1.3            | 84.1 ± 1.2            | 25.8 ± 1.7            | 23.3 ± 1.7            |\n| QuantLM 390M 3-Bit                    | 115.0 ± 5.6           | 23.0 ± 0.6            | 67.4 ± 1.5            | 76.7 ± 1.3            | 25.7 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 390M                            | 77.7 ± 3.8            | 28.0 ± 0.6            | 68.6 ± 1.5            | 76.9 ± 1.3            | 26.4 ± 1.7            | 21.8 ± 1.6            |\n| FloatLM 560M                          | 20.8 ± 0.9            | 44.1 ± 0.7            | 74.7 ± 1.4            | 83.5 ± 1.2            | 27.0 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 8-Bit                    | 20.9 ± 0.9            | 44.2 ± 0.7            | 74.7 ± 1.4            | 83.6 ± 1.2            | 27.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 6-Bit                    | 21.7 ± 0.9            | 42.8 ± 0.7            | 74.4 ± 1.4            | 83.6 ± 1.2            | 25.8 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 560M 4-Bit                    | 24.9 ± 1.1            | 40.8 ± 0.7            | 73.6 ± 1.4            | 82.0 ± 1.2            | 27.0 ± 1.7            | 21.7 ± 1.6            |\n| QuantLM 560M 3-Bit                    | 146.3 ± 7.1           | 20.1 ± 0.6            |                       | 75.9 ± 1.4            | 25.0 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 560M                            | 55.6 ± 2.7            | 32.4 ± 0.7            | 71.1 ± 1.4 70.8 ± 1.4 | 78.7 ± 1.3            | 26.1 ± 1.7            | 19.8 ± 1.6            |\n| Binary 560M                           | 62.8 ± 3.0            | 31.0 ± 0.6            | 70.0 ± 1.4            | 78.8 ± 1.3            | 26.7 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 830M                          | 13.3 ± 0.5            | 49.6 ± 0.7            | 78.4 ± 1.3            | 85.9 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6            |\n| QuantLM 830M 8-Bit                    | 13.5 ± 0.5            | 49.4 ± 0.7            | 78.5 ± 1.3            | 86.1 ± 1.1            | 26.6 ± 1.7            | 20.0 ± 1.6            |\n| QuantLM 830M 6-Bit                    | 13.3 ± 0.5            | 49.1 ± 0.7            | 77.8 ± 1.3            | 85.4 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6 21.2 ± 1.6 |\n| QuantLM 830M 4-Bit                    | 15.4 ± 0.6            | 47.3 ± 0.7            | 78.8 ± 1.3            | 85.1 ± 1.1            | 25.5 ± 1.7            |                       |\n| QuantLM 830M 3-Bit                    | 47.7 ± 2.0            | 30.5 ± 0.6            | 74.1 ± 1.4            | 80.1 ± 1.3            | 28.1 ± 1.8            | 21.2 ± 1.6            |\n| TriLM 830M                            | 26.0 ± 1.1            | 39.9 ± 0.7            | 75.4 ± 1.4            | 82.8 ± 1.2            | 27.6 ± 1.8            | 21.4 ± 1.6            |\n| FloatLM 1.1B                          | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.2 ± 1.2            | 88.1 ± 1.0            | 27.3 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 1.1B 8-Bit                    | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.1 ± 1.2            | 88.1 ± 1.0 88.1 ± 1.0 | 27.8 ± 1.8            | 21.2 ± 1.6            |\n| QuantLM 1.1B 6-Bit                    | 11.7 ± 0.4            | 51.0 ± 0.7            | 82.3 ± 1.2            |                       | 27.5 ± 1.8 28.4 ± 1.8 | 21.5 ± 1.6            |\n| QuantLM 1.1B 4-Bit                    | 13.9 ± 0.5            | 49.3 ± 0.7            | 81.2 ± 1.2            | 87.6 ± 1.0            |                       | 20.3 ± 1.6            |\n| QuantLM 1.1B 3-Bit                    | 26.9 ± 1.1            | 39.1 ± 0.7            | 78.7 ± 1.3            | 85.0 ± 1.1            | 25.8 ± 1.7            | 20.7 ± 1.6            |\n| TriLM 1.1B Binary 1.1B                | 17.3 ± 0.7 33.4 ± 1.4 | 46.2 ± 0.7 37.6 ± 0.6 | 73.3 ± 1.4 71.1 ± 1.4 | 81.9 ± 1.2 81.2 ± 1.3 | 26.9 ± 1.7 28.4 ± 1.7 | 22.0 ± 1.6 23.2 ± 1.6 |\n| FloatLM 1.5B QuantLM 1.5B 8-Bit       | 9.4 ± 0.3 9.5 ± 0.3   | 55.5 ± 0.7 ±          | 80.9 ± 1.2 81.3 ± 1.2 | 87.4 ± 1.0 87.5 ± 1.0 | 26.1 ± 1.7 25.7 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 9.5 ± 0.3             | 55.5 0.7 55.4 ± 0.7   | 81.4 ± 1.2            | 87.6 ± 1.0            | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 6-Bit                    |                       | ±                     |                       |                       | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 4-Bit                    | 17.8 ± 0.7            | 45.3 ± 0.7            | 75.5 ± 1.4            | 82.1 ± 1.2            |                       |                       |\n| QuantLM 1.5B 3-Bit                    | 10.4 ± 0.4            | 53.0 0.7              | 81.1 ± 1.2            | 86.9 ± 1.1            |                       |                       |\n| TriLM 1.5B FloatLM 2.4B               | 7.7 ± 0.3             | 46.2 ± 0.7 59.3 ± 0.7 | 87.2 ± 1.1            | 87.3 ± 1.1 91.0 ± 0.9 | 28.4 ± 1.8 29.5 ± 1.8 | 22.7 ± 1.6 21.5 ± 1.6 |\n|                                       | 16.4 ± 0.7            |                       | 80.7 ± 1.2            |                       | 27.8 ± 1.8            | 21.5 ± 1.6            |\n|                                       |                       |                       |                       |                       |                       | 21.5 ± 1.6            |\n| QuantLM 2.4B 8-Bit                    | 7.7 ± 0.3             | 59.2 ± 0.7            | 87.1 ± 1.1            | 91.0 ± 0.9            | 29.5 ± 1.8            |                       |\n| QuantLM 2.4B 6-Bit                    | 7.9 ± 0.3             | 58.9 ± 0.7            | 87.3 ± 1.1            | 90.9 ± 0.9            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 4-Bit                    | 8.9 ± 0.3             | 56.1 ± 0.7            | 84.8 ± 1.1            | 89.7 ± 1.0            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 3-Bit                    | 15.6 ± 0.6            | 45.0 ± 0.7            | 79.9 ± 1.3            | 86.7 ± 1.1            | 28.6 ± 1.8            | 21.4 ± 1.6            |\n| TriLM 2.4B                            | 8.6 ± 0.3             | 55.7 ± 0.7            | 84.2 ± 1.2            | 88.7 ± 1.0            | 28.6 ± 1.8            | 24.3 ± 1.7            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 6.7 ± 0.2             | 61.1 ± 0.7 61.1 ± 0.7 | 86.5 ± 1.1 86.2 ± 1.1 | 90.9 ± 0.9 91.0 ± 0.9 | 26.9 ± 1.7 26.6 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 6.7 ± 0.2             |                       |                       |                       |                       | 20.4 ± 1.6            |\n| QuantLM 3.9B 6-Bit                    | 6.8 ± 0.2             | 60.8 ± 0.7 58.5 ± 0.7 | 86.6 ± 1.1 86.1 ± 1.1 | 91.3 ± 0.9 90.8 ± 0.9 | 25.8 ± 1.7 28.6 ± 1.8 | 20.1 ± 1.6            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 7.4 ± 0.2 14.0 ± 0.5  | 47.1 ± 0.7            | 83.1 ± 1.2            | 88.6 ± 1.0            | 27.0 ± 1.7            | 21.5 ± 1.6 22.7 ± 1.6 |\n| TriLM 3.9B                            | 6.3 ± 0.2             | 61.6 ± 0.7            | 87.4 ± 1.0            | 90.8 ± 0.9            | 27.6 ± 1.8            |                       |\n\n## E Weight Distribution of Linear Layers\n\nFigure 20: Weight distribution in the linear layers of FloatLM models across various model sizes, ranging from 99M to 3.9B parameters\n\n<!-- image -->\n\nThe observed Gaussian distribution in the weights of our final trained FloatLM models across various scales is supported by both theoretical foundations and empirical evidence. We outline the rationale for the normality of weight distributions as follows:\n\nEmpirical Consistency: Across all model scales, ranging from 99 million to 3.9 billion parameters, the weight distributions consistently exhibit Gaussian characteristics, as illustrated in Figure 20. This consistency is visually evident in distribution plots and quantitatively confirmed by fitting Gaussian functions to the weight histograms, demonstrating a good fit across different model sizes.\n\nTheoretical Underpinning: Neural network weight initialization typically follows a Gaussian distribution to facilitate balanced learning dynamics. As training progresses, despite non-linear transformations and complex interactions within the network, the Central Limit Theorem suggests that the aggregation of numerous independent random variables (such as updates during backpropagation) tends toward a normal distribution. This tendency is particularly pronounced given the high dimensionality and extensive data processing involved in training large language models.\n\nStabilization through Regularization and Optimization: Techniques such as L2 regularization constrain weight magnitudes, encouraging them towards smaller values and contributing to a peak around zero-a characteristic feature of Gaussian distributions. Additionally, optimization algorithms like Adam, which adjust learning rates based on moving averages of recent gradients, promote smoother updates. This approach maintains the Gaussian form by mitigating the impact of outlier gradients.\n\nGiven these, we can assert that the weight distribution of our trained models closely follows a Gaussian distribution which is crucial for understanding the weight variance across different scales.\n\n## F Memory Bottlenecks and Low-Bitwidth Language Modelling\n\nRecent observations [Gholami et al., 2024] suggest that, given the slower pace of improvements in memory and communication compared to compute (FLOPs), the bottleneck continues to shift away from computation towards the memory-related characteristics of hardware for deploying large language models. This shift underscores the importance of exploring solutions that directly address memory constraints. Below, we formally analyze this trend and the impact of low-bitwidth language models on addressing memory bottlenecks during inference.\n\n## F.1 Overview of Recent Datacenter GPUs and Accelerators.\n\nWe begin our analysis by surveying a broad range of recent datacenter General Purpose GPUs (GPGPUs) employed for neural network development and research since 2018. This includes hardware from multiple providers, covering various configurations across the latest microarchitectures.\n\nFrom Nvidia, we consider the following:\n\n- Volta : V100 (SXM/PCIe) [Nvidia Team, 2018],\n- Ampere : A100 (40GB/80GB SXM/PCIe) [Nvidia Team, 2020],\n- Hopper : H100 (SXM/PCIe) and H200 [Nvidia Team, 2022, 2023],\n- Blackwell : This includes preliminary data for Blackwell microarchitectures, which at the time of access were subject to change [Nvidia Team, 2024].\n\nFrom AMD, we analyze the following models:\n\n- MI200 Series : MI210, MI250, MI250X [AMD Team, 2022a,b],\n- MI300 Series : MI300A, MI300X, MI325X [AMD Team, 2023a,b, 2024].\n\nAdditionally, we include hardware from Intel and Google:\n\n- From Intel, the Gaudi Series : Gaudi 2 and Gaudi 3 [Intel Gaudi Team, 2024],\n- From Google, the Tensor Processing Units (TPUs) : TPUv3 [Google TPU Team, 2018], TPUv4 [Google TPU Team, 2021], and TPUv5 (TPUv5e, TPUv5p) [Google TPU Team, 2023a,b].\n\nAll data was sourced from the respective datasheets, technical documentation, or press releases of the cited hardware. Over the past several years, each of these four accelerator families has improved in three areas - FLOPS, memory capacity, and bandwidth.\n\n## F.2 Memory Trends and Speedup Opportunities in Low-Bitwidth Language Modeling\n\nMemory Capacity and Bandwidth of GPGPUs Relative to Peak TFLOPs. In Figure 21a, we show the trends of Memory Capacity over Peak TFLOPS (Half Precision - FP16/BF16) for various accelerators over the years. We also perform a linear fit for each family of accelerators separately. The linear fit for all the families has a downward slope, showing that memory capacity is improving at a slower pace than computation capability. This trend holds true even for the most recent hardware, such as Blackwell, MI325X, and Gaudi3. Though we consider Half-Precision TFLOPs, the slope is expected to become steeper when considering peak TFLOPS over Ampere sparse or FP8. Similarly, in Figure 21b, we present the trends of Memory Bandwidth (specifically for DRAM or its equivalent memory) over FLOPs for the accelerators over the years, along with the linear fit for each family. We observe a downward slope here as well, indicating the trend that memory bandwidth is growing much slower than computation.\n\nMemory Wall and Speedup Opportunities. Kim et al. [2024] established the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. As a result, the autoregressive decoding of LLM inference can have a theoretical speedup proportional to its compression factor. Various efficient inference kernels over quantized models have realized this speedup in low batch settings across a variety of hardware. This includes\n\nFigure 21: Trends of Memory/FLOP and Bandwidth/FLOP across different (datacenter) GPGPUs.\n\n<!-- image -->\n\nCPUs 6 , consumer GPUs 7 and data center GPUs [PyTorch Team, 2024]. However, since TFLOPS to bandwidth ratio is up to 500 times, this ideal speedup can also be achieved in much higher batch settings encountered in LLM deployment. Open-source kernels like Marlin [Frantar and Alistarh, 2024] have demonstrated that these ideal speedups can also be consistently realized in high batch size scenarios and sustained over longer periods of time.\n\nIn Figure 2a, we show the size of models (in GB) across parameter count for two low bitwidth modeling scenarios, TriLM and QuantLM 4-Bit along with the standard half-precision FloatLM. For simplicity, we do not consider the overhead of KV Cache, activations, and compilation overhead incurred during model deployment. The FloatLM model starts to reach the capacity of a single H100 at just 34B parameters. At 340 Billion (the size of Nemotron 4) is more than the capacity of a single 8xH100 node. QuantLM 4-Bit scales better, easily supporting the deployment of a 70 billion parameter model (like largest LLaMa 1 and 2) on a single H100 and 300B parameter models on a single MI300X. However, TriLMs with more than 300 billion parameters, with appropriate packing, can fit on a single H100. This feature makes TriLMs especially crucial for deployment at the edge, where devices have less than 8GB or 16GB of RAM, shared across the operating system and multiple applications.\n\nMemory bandwidth and Memory bandwidth of GPGPUs and model inference speedup: Kim et al. [2024] established the memory wall in autoregressive LLM computation. They found that the speed of token generation is bottlenecked by the rate at which data is fed from memory to processors, rather than the processing speed of the hardware. As a result, the autoregressive decoding of LLM inference can have a theoretical speedup proportional to its compression factor.\n\nVarious efficient inference kernels over quantized models have realized this speedup in low batch settings across a variety of hardware. This includes CPUs 8 , consumer GPUs 9 and data center GPUs [PyTorch Team, 2024]. However, since TFLOPS to bandwidth ratio is up to 500 times, this ideal speedup can also be achieved in much higher batch settings encountered in LLM deployment. Open-source kernels like Marlin [Frantar and Alistarh, 2024] have demonstrated that these ideal speedups can also be consistently realized in high batch size scenarios and sustained over longer periods of time. In Figure 2b, we show the (theoretically) maximum possible speedup relative to FP16 at varying parameter counts for QuantLM 4-Bit and TriLM. Even at 7 billion parameters, TriLMs can be more than 4 times faster at autoregressive decoding than FloatLM and 2 times faster than QuantLM 4-bit. While QuantLM 4-Bit plateaus at a maximum possible speedup factor of 4x, TriLMs plateau much higher at 10x for FloatLM.\n\n6 https://github.com/ggerganov/llama.cpp\n\n7 https://github.com/turboderp/exllamav2\n\n8 https://github.com/ggerganov/llama.cpp\n\n9 https://github.com/turboderp/exllamav2\n\nFigure 22: Model performance on MMLU subsets: STEM, Humanities, Social Sciences, and others. Plot accuracy scores against model size in bits (left) and number of parameters (right), ranging from 560M to 3.9B parameters for TriLM (ternary), FloatLM (FP16), and QuantLM (3-bit &amp; 4-bit).\n\n<!-- image -->\n\n## G Ablations\n\nTable 11 and 10 shows the performance of ablation 100B token training runs over the six commonsense benchmarks from BitNet b1.58 at 1.1B parameters. The first two rows show the performance of TriLM 1.1B and Float 1.1B at this token count, followed by our replication of BitNet b1.58 (Ours) as well as the scores from BitNet b1.58 over 700M and 1.3B parameters. We observe that at this scale, TriLM does not come close to matching the performance of FloatLM, but it outperforms much larger BitNets. The next two rows show the performance of TriLM 1.1B and FloatLM 1.1B when trained on 100B tokens of FineWeb, instead of SlimPajama. While the performance of both the models improves on FineWeb, the average difference in their performance across datasets remains the same. Lastly, we show the performances across various optimization schedules. A significant drop in averaged performance is noticed when the baseline schedule of linear decay with constant weight decay is used. The gains from dropping l2 regularization in the schedule are more than that of dropping the peak learning rate, however, not enough to match that of TriLM 1.1B's schedule.\n\nTable 10: Ablation Common Sense Task Performance: HellaSwag, PIQA, WinoGrande, Arc Easy, Arc Challenge, BoolQ (Contd.). BitNet b1.58's scores from Ma et al. [2024]. All runs are for 100B tokens on Slim Pajama, except those explicitly stated as FineWeb\n\n| Models                          | HellaSwag   | HellaSwag   | PIQA       | PIQA       | WinoGrande   |\n|---------------------------------|-------------|-------------|------------|------------|--------------|\n|                                 | Acc N.      | Acc         | Acc N.     | Acc        | Acc          |\n| FloatLM 1.1B                    | 50.0 ± 0.5  | 39.3 ± 0.5  | 70.9 ± 1.0 | 70.1 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B                      | 46.8 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.4 ± 1.0 | 53.8 ± 1.4   |\n| BitNet b1.58 1.1B (Ours)        | 47.0 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.6 ± 1.0 | 53.4 ± 1.4   |\n| BitNet b1.58 700M               | 35.1        | 35.1        | 68.1       | 68.1       | 55.2         |\n| BitNet b1.58 1.3B               | 37.7        | 37.7        | 68.8       | 68.8       | 55.8         |\n| TriLM 1.1B FineWeb              | 50.0 ± 0.5  | 39.2 ± 0.4  | 70.2 ± 1.0 | 70.1 ± 1.0 | 56.6 ± 1.3   |\n| FloatLM 1.1B FineWeb            | 52.7 ± 0.5  | 41.1 ± 0.4  | 73.0 ± 1.0 | 71.3 ± 1.0 | 56.7 ± 1.3   |\n| TriLM 1.1B Only Peak LR Dropped | 46.7 ± 0.5  | 36.8 ± 0.4  | 68.9 ± 1.0 | 69.5 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B Only L2 Reg. Dropped | 47.1 ± 0.5  | 37.5 ± 0.4  | 68.6 ± 1.0 | 69.4 ± 1.0 | 55.2 ± 1.4   |\n| TriLM 1.1B Baseline Schedule    | 46.0 ± 0.5  | 36.9 ± 0.4  | 69.3 ± 1.0 | 69.1 ± 1.0 | 56.2 ± 1.3   |\n\nTable 11: Ablation Common Sense Task Performance: HellaSwag, PIQA, WinoGrande, Arc Easy, Arc Challenge, BoolQ. BitNet b1.58's scores from Ma et al. [2024]. All runs are for 100B tokens on Slim Pajama, except those explicitly stated as FineWeb\n\n| Models                          | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ      | Avg ( HellaSwag, PIQA, WinoGrande,   |\n|---------------------------------|-----------------|-----------------|------------|------------|------------|--------------------------------------|\n|                                 | Acc N.          | Acc             | Acc N.     | Acc        | Acc        | Arc Easy, Arc Challenge, and BoolQ)  |\n| FloatLM 1.1B                    | 26.3 ± 1.3      | 22.5 ± 1.2      | 50.3 ± 1.0 | 56.8 ± 1.0 | 60.6 ± 0.8 | 52.2                                 |\n| TriLM 1.1B                      | 26.7 ± 1.3      | 22.9 ± 1.2      | 49.7 ± 1.0 | 55.0 ± 1.0 | 54.9 ± 0.8 | 50.2                                 |\n| BitNet b1.58 1.1B (Ours)        | 26.1 ± 1.2      | 23.6 ± 1.2      | 47.7 ± 1.0 | 55.3 ± 1.0 | 49.7 ± 0.8 | 48.9                                 |\n| BitNet b1.58 700M               | 21.4            | 21.4            | 51.8       | 51.8       | 58.2       | 48.3                                 |\n| BitNet b1.58 1.3B               | 24.2            | 24.2            | 54.9       | 54.9       | 56.7       | 49.6                                 |\n| TriLM 1.1B FineWeb              | 31.7 ± 1.3      | 31.9 ± 1.3      | 63.1 ± 0.9 | 66.8 ± 0.9 | 58.3 ± 0.8 | 54.9                                 |\n| FloatLM 1.1B FineWeb            | 34.4 ± 1.3      | 33.0 ± 1.3      | 65.7 ± 0.9 | 70.2 ± 0.9 | 59.3 ± 0.8 | 56.9                                 |\n| TriLM 1.1B Only Peak LR Dropped | 27.4 ± 1.3      | 23.6 ± 1.2      | 48.3 ± 1.0 | 55.1 ± 1.0 | 51.6 ± 0.8 | 49.7                                 |\n| TriLM 1.1B Only L2 Reg. Dropped | 27.6 ± 1.3      | 24.8 ± 1.2      | 49.2 ± 1.0 | 55.1 ± 1.0 | 53.1 ± 0.8 | 50.1                                 |\n| TriLM 1.1B Baseline Schedule    | 26.2 ± 1.2      | 23.2 ± 1.2      | 48.0 ± 1.0 | 54.0 ± 1.0 | 49.4 ± 0.8 | 49.1                                 |\n\nTable 12: Spectra Suite Performance (Part 4): TriviaQA, CrowsPairs, Big Bench BBQ Lite, TruthQA. We additionally also include Pythia's performance scores.\n\n| Models                                | TriviaQA Exact Match   | CrowsPairs                | CrowsPairs            | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n|---------------------------------------|------------------------|---------------------------|-----------------------|--------------------------|-----------------------|\n| Models                                | TriviaQA Exact Match   | Likelihood diff.          | Pct stereotype        | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n| FloatLM 99M QuantLM 99M 8-Bit         | 0.6 ± 0.1 0.6 ± 0.1    | 372.4 ± 14.6 370.9 ± 14.6 | 55.4 ± 1.2 55.1 ± 1.2 | 30.8 ± 0.4 26.5 ± 0.3    | 24.4 ± 1.5 24.1 ± 1.5 |\n| QuantLM 99M 6-Bit                     | 0.6 ± 0.1              | 389.8 ± 14.8              | 56.9 ± 1.2            | 26.7 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 99M 4-Bit                     | 0.3 ± 0                | 425.5 ± 15.2              | 54.0 ± 1.2            | 26.2 ± 0.3               | 22.9 ± 1.5            |\n| QuantLM 99M 3-Bit                     | 0.1 ± 0                | 611.1 ± 18.8              | 51.0 ± 1.2            | 31.4 ± 0.4               | 24.6 ± 1.5            |\n| TriLM 99M                             | 0.1 ± 0                | 362.4 ± 10.8              | 54.2 ± 1.2            | 30.8 ± 0.4               | 24.2 ± 1.5            |\n| Binary 99M                            | 0.2 ± 0.0              | 353.5 ± 11.1              | 53.3 ± 1.2            | 31.5 ± 0.3               | 25.7 ± 1.5            |\n| FloatLM 190M                          | 0.6 ± 0.1              | 348.2 ± 11.3              | 55.9 ± 1.2            | 27.3 ± 0.4               | 22.4 ± 1.5            |\n| QuantLM 190M 8-Bit                    | 0.7 ± 0.1              | 352.7 ± 11.4              | 56.2 ± 1.2            | 27.1 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 6-Bit                    | 0.7 ± 0.1              | 368.9 ± 11.7              | 56.2 ± 1.2            | 27.2 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 4-Bit                    | 0.0 ± 0                | 961.9 ± 25.4              | 43.8 ± 1.2            | 35.0 ± 0.4               | 24.2 ± 1.5            |\n| QuantLM 190M 3-Bit                    | 0.1 ± 0                | 482.7 ± 15.2              | 53.7 ± 1.2            | 26.4 ± 0.3               | 25.0 ± 1.5            |\n| TriLM 190M                            | 0.2 ± 0                | 343.5 ± 10.9              | 55.5 ± 1.2            | 29.7 ± 0.4               | 23.9 ± 1.5            |\n| FloatLM 390M                          | 2.8 ± 0                | 355.5 ± 10.4              | 59.6 ± 1.2            | 25.4 ± 0.3               | 22.4 ± 1.5            |\n| QuantLM 390M 8-Bit                    | 2.9 ± 0                | 355.8 ± 10.4              | 59.8 ± 1.2            | 25.4 ± 0.3               | 22.2 ± 1.5            |\n| QuantLM 390M 6-Bit                    | 2.4 ± 0                | 360.5 ± 10.4              | 60.6 ± 1.2            | 25.3 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 4-Bit                    | 1.3 ± 0.1              | 368.2 ± 10.2              | 59.4 ± 1.2            | 25.5 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 3-Bit                    | 0.8 ± 0.1              | 444.4 ± 12.2              | 54.3 ± 1.2            | 26.3 ± 0.3               | 23.0 ± 1.5            |\n| TriLM 390M                            | 1.3 ± 0.1              | 344.5 ± 10.3              | 58.3 ± 1.2            | 26.9 ± 0.3               | 24.4 ± 1.5            |\n| FloatLM 560M                          | 4.6 ± 0.2              | 351.8 ± 9.9               | 58.9 ± 1.2            | 25.7 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 560M 8-Bit                    | 4.7 ± 0.2              | 352.9 ± 10.0              | 59.2 ± 1.2            | 25.7 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 560M 6-Bit                    | 3.5 ± 0.1              | 353.7 ± 9.9               | 59.3 ± 1.2            | 25.8 ± 0.3               | 22.0 ± 1.5            |\n| QuantLM 560M 4-Bit 3-Bit              | 2.1 ± 0.1              | 372.7 ± 10.7              | 59.2 ± 1.2            | 27.0 ± 0.4 29.0 ± 0.4    | 22.2 ± 1.5 22.9 ± 1.5 |\n| QuantLM 560M TriLM 560M               | 1.5 ± 0.1              | 411.2 ± 11.3              | 57.9 ± 1.2 58.7 ± 1.2 | 25.5 ± 0.3               |                       |\n| Binary 560M                           | 2.4 ± 0.1 0.2 ± 0.0    | 345.1 ± 10.1 356.3 ± 10.4 | 58.5 ± 1.2            | 26.3 ± 0.3               | 23.6 ± 1.5 23.1 ± 1.4 |\n| FloatLM 830M                          | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.6 ± 1.2            | 25.7 ± 0.3               | 23.1 ± 1.5            |\n| QuantLM 830M 8-Bit                    | 8.5 ± 0.2              | 354.5 ± 9.6               | 62.1 ± 1.2            | 25.6 ± 0.3               | 23.0 ± 1.5            |\n| QuantLM 830M 6-Bit                    | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.7 ± 1.2            | 25.5 ± 0.3               | 22.5 ± 1.5            |\n| QuantLM 830M 4-Bit                    | 10.6 ± 0.2             | 364.2 ± 9.8               | 59.9 ± 1.2            | 25.9 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 830M 3-Bit                    | 3.1 ± 0.1              | 389.5 ± 10.9              | 59.9 ± 1.2            | 30.5 ± 0.4               | 24.4 ± 1.5            |\n| TriLM 830M                            | 4.3 ± 0.2              | 344.9 ± 10.0              | 60.7 ± 1.2            | 25.1 ± 0.3               | 22.8 ± 1.5            |\n| FloatLM 1.1B                          | 12.9 ± 0.3             | 349.2 ± 9.7               | 61.2 ± 1.2            | 25.4 ± 0.3               | 21.4 ± 1.4            |\n| QuantLM 1.1B 8-Bit                    | 12.7 ± 0.2             | 349.5 ± 9.7               | 61.1 ± 1.2            | 25.4 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 1.1B 6-Bit QuantLM 1.1B 4-Bit | 12.4 ± 0.2 9.3 ± 0.2   | 349.7 ± 9.6 359.1 ± 10.1  | 59.9 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4 21.3 ± 1.4 |\n|                                       |                        |                           | 60.9 ± 1.2            | 25.4 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 1.1B 3-Bit                    | 6.8 ± 0.2              | 422.4 ± 11.5              | 58.7 ± 1.2            | 29.9 ± 0.4               |                       |\n| TriLM 1.1B                            | 1.9 ± 0.1              | 343.4 ± 9.9               | 61.4 ± 1.2            | 25.8 ± 0.3               | 21.5 ± 1.4            |\n| Binary 1.1B                           | 2.2 ± 0.1              | 351.6 ± 9.8               | 58.3 ± 1.2            | 26.4 ± 0.3               | 23.2 ± 1.4            |\n| FloatLM 1.5B                          | 12.2 ± 0.2             | 351.9 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 8-Bit                    | 12.5 ± 0.2             | 352.4 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 4-Bit                    | 9.0 ± 0.2              | 357.9 ± 9.8               | 60.7 ± 1.2            | 25.9 ± 0.3               | 20.8 ± 1.4            |\n| QuantLM 1.5B 3-Bit                    | 4.2 ± 0.1              | 400.0 ± 10.6              | 60.9 ± 1.2            | 26.8 ± 0.3               | 20.8 ± 1.4            |\n| TriLM 1.5B                            | 5.9 ± 0.1              | 348.9 ± 9.9               | 59.9 ± 1.2            | 25.2 ± 0.3               | 21.7 ± 1.4            |\n| FloatLM 2.4B QuantLM 2.4B 8-Bit       | 20.7 ± 0.3 20.7 ± 0.3  | 360.4 ± 9.4 360.5 ± 9.4   | 64.2 ± 1.2 64.2 ± 1.2 | 26.7 ± 0.3               | 21.7 ± 1.4 21.9 ± 1.4 |\n| QuantLM 2.4B 6-Bit                    | 20.4 ± 0.3             | 360.8 ± 9.5               | 63.4 ± 1.2            | 26.5 ± 0.3 26.4 ± 0.3    | 21.8 ± 1.4 21.7 ± 1.4 |\n| QuantLM 2.4B 4-Bit                    | 21.1 ± 0.3             | 358.7 ± 9.6               | 63.4 ± 1.2            | 26.0 ± 0.3 ±             |                       |\n| QuantLM 2.4B 3-Bit                    | 10.9 ± 0.2             | 360.2 ± 9.5               | 59.9 ± 1.2            | 25.8 0.3                 | 21.5 ± 1.4            |\n| TriLM 2.4B                            | 12.3 ± 0.1             | 353.0 ± 10.0              | 64.1 ± 1.2            | 25.4 ± 0.3               | 23.0 ± 1.5            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 21.5 ± 0.3 ±           | 359.2 ± 9.6 359.8 ± 9.6   | 64.6 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n|                                       |                        |                           | 64.7 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n| QuantLM 3.9B 6-Bit                    | 21.7 0.3 21.0 ± 0.3    | 359.5 ± 9.6               | 63.9 ± 1.2            | 25.4 ± 0.3               | 23.5 ± 1.5            |\n| QuantLM 3.9B 4-Bit                    | 17.9 ± 0.3             | 365.5 ± 9.7               | 64.8 ± 1.2            | 25.3 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 3.9B 3-Bit                    | 8.2 ± 0.2              | 365.9 ± 9.8               | 64.3 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4            |\n| TriLM 3.9B                            | 21.3 ± 0.3             | 362.4 ± 9.6               | 65.4 ± 1.2            | 25.9 ± 0.3               | 24.1 ± 1.5            |\n\nTable 13: Spectra Suite Performance (Part 5): MMLU- STEM, Humanities, Social Sciences, Others.\n\n| Models                                | Stem                  | Humanities            | MMLUAccuracy Social Sciences   | Other                 | Avg.                  |\n|---------------------------------------|-----------------------|-----------------------|--------------------------------|-----------------------|-----------------------|\n| FloatLM 99M                           | 22.8 ± 0.7            | 24.0 ± 0.6            | 27.0 ± 0.8                     | 28.0 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 8-Bit                     | 22.9 ± 0.7            | 24.2 ± 0.6            | 26.9 ± 0.8                     | 27.9 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 6-Bit                     | 22.7 ± 0.7            | 24.1 ± 0.6            | 26.6 ± 0.8                     | 28.2 ± 0.8            | 25.2 ± 0.4            |\n| QuantLM 99M 4-Bit                     | 22.9 ± 0.7            | 24.1 ± 0.6            | 26.7 ± 0.8                     | 27.4 ± 0.8            | 25.1 ± 0.4            |\n| QuantLM 99M 3-Bit                     | 23.5 ± 0.8            | 23.9 ± 0.6            | 26.2 ± 0.8                     | 25.9 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 99M                             | 23.9 ± 0.8            | 23.6 ± 0.6            | 26.7 ± 0.8                     | 26.6 ± 0.8            | 25.0 ± 0.4            |\n| Binary 99M                            | 21.6 ± 0.7            | 24.3 ± 0.6            | 21.8 ± 0.7                     | 24.0 ± 0.7            | 23.1 ± 0.3            |\n| FloatLM 190M                          | 24.0 ± 0.8            | 24.4 ± 0.6            | 28.8 ± 0.8                     | 30.1 ± 0.8            | 26.5 ± 0.4            |\n| QuantLM 190M 8-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.9 ± 0.8                     | 30.0 ± 0.8            | 26.6 ± 0.4            |\n| QuantLM 190M 6-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.3 ± 0.8                     | 29.8 ± 0.8            | 26.4 ± 0.4            |\n| QuantLM 190M 4-Bit                    | 22.9 ± 0.7            | 22.9 ± 0.6            | 24.5 ± 0.8                     | 23.4 ± 0.8            | 23.4 ± 0.4            |\n| QuantLM 190M 3-Bit                    | 23.9 ± 0.8            | 23.2 ± 0.6            | 25.4 ± 0.8                     | 27.5 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 190M                            | 22.5 ± 0.7            | 23.8 ± 0.6            | 26.7 ± 0.8                     | 28.4 ± 0.8            | 25.2 ± 0.4            |\n| FloatLM 390M                          | 25.8 ± 0.8            | 25.9 ± 0.6            | 30.3 ± 0.8                     | 32.8 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 8-Bit                    | 25.7 ± 0.8            | 25.9 ± 0.6            | 30.2 ± 0.8                     | 32.4 ± 0.8            | 28.2 ± 0.4            |\n| QuantLM 390M 6-Bit                    | 26.0 ± 0.8            | 25.8 ± 0.6            | 30.2 ± 0.8                     | 32.3 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 4-Bit                    | 25.5 ± 0.8            | 25.4 ± 0.6            | 30.5 ± 0.8                     | 31.6 ± 0.8            | 27.9 ± 0.4            |\n| QuantLM 390M 3-Bit                    | 24.4 ± 0.8            | 25.0 ± 0.6            | 29.4 ± 0.8                     | 29.3 ± 0.8            | 26.8 ± 0.4            |\n| TriLM 390M                            | 24.1 ± 0.8            | 24.8 ± 0.6            | 28.3 ± 0.8                     | 29.0 ± 0.8            | 26.4 ± 0.4            |\n| FloatLM 560M                          | 24.8 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 32.3 ± 0.8            | 28.4 ± 0.4            |\n| QuantLM 560M 8-Bit                    | 24.8 ± 0.8            | 26.6 ± 0.6            | 30.5 ± 0.8                     | 32.1 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 560M 6-Bit                    | 24.6 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 31.3 ± 0.8            | 28.1 ± 0.4            |\n| QuantLM 560M 4-Bit QuantLM 560M 3-Bit | 24.7 ± 0.8            | 25.9 ± 0.6            | 29.9 ± 0.8                     | 31.1 ± 0.8 28.2 ± 0.8 | 27.7 ± 0.4 26.0 ± 0.4 |\n| TriLM 560M                            | 24.5 ± 0.8 25.0 ± 0.8 | 24.2 ± 0.6 ±          | 28.1 ± 0.8 29.0 ± 0.8          | 30.2 ± 0.8            | 27.0 ± 0.4            |\n|                                       |                       | 25.1 0.6              |                                | 23.9 ± 0.7            | 22.9 ± 0.3            |\n| Binary 560M FloatLM 830M              | 21.4 ± 0.7 25.8 ± 0.8 | 24.2 ± 0.6 27.5 ± 0.6 | 21.6 ± 0.7 32.3 ± 0.8          | 34.6 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 8-Bit                    | 25.8 ± 0.8            | 27.4 ± 0.6            | 32.1 ± 0.8                     | 34.7 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 6-Bit                    | 25.6 ± 0.8            | 27.3 ± 0.6            | 32.1 ± 0.8                     | 34.2 ± 0.8            | 29.5 ± 0.4            |\n| QuantLM 830M 4-Bit                    | 25.9 ± 0.8            | 26.8 ± 0.6            | 31.2 ± 0.8                     | 33.6 ± 0.8            | 29.1 ± 0.4            |\n| QuantLM 830M 3-Bit                    | 24.8 ± 0.8            | 25.1 ± 0.6            | 28.9 ± 0.8                     | 30.8 ± 0.8            | 27.1 ± 0.4            |\n| TriLM 830M                            | 24.9 ± 0.8            | 25.8 ± 0.6            | 30.1 ± 0.8                     | 31.1 ± 0.8            | 27.7 ± 0.4            |\n| FloatLM 1.1B                          | 26.4 ± 0.8            | 27.6 ± 0.6            | 32.5 ± 0.8                     | 34.8 ± 0.8            | 30.0 ± 0.4            |\n| QuantLM 1.1B 8-Bit                    | 26.2 ± 0.8            | 27.4 ± 0.6            | 32.5 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 6-Bit                    | 26.0 ± 0.8            | 27.5 ± 0.6            | 32.7 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 4-Bit                    | 26.0 ± 0.8            | 26.6 ± 0.6            | 32.4 ± 0.8                     | 33.8 ± 0.8            | 29.3 ± 0.4            |\n| QuantLM 1.1B 3-Bit                    | 25.9 ± 0.8            | 26.1 ± 0.6            | 30.0 ± 0.8                     | 33.0 ± 0.8            | 28.4 ± 0.4            |\n| TriLM 1.1B                            | 25.2 ± 0.8            | 26.1 ± 0.6            | 30.6 ± 0.8                     | 32.2 ± 0.8            | 28.3 ± 0.4            |\n| Binary 1.1B                           | 21.0 ± 0.7            | 24.2 ± 0.6            | 21.7 ± 0.8                     | 24.4 ± 0.7            | 23.0 ± 0.3            |\n| FloatLM 1.5B                          | 26.1 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.6 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 8-Bit                    | 26.1 ± 0.8            | 28.1 ± 0.7            | 32.9 ± 0.8                     | 35.5 ± 0.8            | 30.3 ± 0.4            |\n| QuantLM 1.5B 6-Bit                    | 26.3 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.4 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 4-Bit                    | 26.2 ± 0.8            | 28.1 ± 0.7            | 32.4 ± 0.8                     | 34.8 ± 0.8            | 30.1 ± 0.4            |\n| QuantLM 1.5B 3-Bit                    | 25.5 ± 0.8            | 26.7 ± 0.6            | 31.2 ± 0.8                     | 33.4 ± 0.8            | 28.9 ± 0.4            |\n| TriLM 1.5B                            | 25.7 ± 0.8            | 27.4 ± 0.6            | 31.5 ± 0.8                     | 34.6 ± 0.8            | 29.5 ± 0.4            |\n| FloatLM 2.4B                          | 26.9 ± 0.8            | 29.4 ± 0.7            | 34.2 ± 0.8                     | 38.1 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 8-Bit                    | 27.0 ± 0.8            | 29.4 ± 0.7            | 34.1 ± 0.8                     | 38.0 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 6-Bit                    | 26.8 ± 0.8 ±          | 29.5 ± 0.7 28.8 ± 0.7 | 34.2 ± 0.8 34.3 ± 0.8          | 38.2 ± 0.9 38.1 ± 0.9 | 31.8 ± 0.4 ±          |\n| QuantLM 2.4B 4-Bit                    | 26.5 0.8              |                       |                                |                       | 31.5 0.4              |\n| QuantLM 2.4B 3-Bit TriLM 2.4B         | 25.5 ± 0.8 27.4 ± 0.8 | 27.1 ± 0.6 27.8 ± 0.6 | 32.3 ± 0.8 34.6 ± 0.9          | 36.4 ± 0.9 35.1 ± 0.8 | 29.9 ± 0.4 30.8 ± 0.4 |\n| FloatLM 3.9B                          | 27.7 ± 0.8            | 30.6 ± 0.7            | 36.9 ± 0.9                     | 39.8 ± 0.9            | 33.3 ± 0.4            |\n| QuantLM 3.9B 8-Bit                    | 27.6 ± 0.8            | 30.7 ± 0.7            | 37.0 ± 0.9                     | 39.7 ± 0.9            | 33.4 ± 0.4            |\n| QuantLM 3.9B 6-Bit                    | 27.3 ± 0.8            | 30.3 ± 0.7            | 36.9 ± 0.9                     | 39.3 ± 0.9            | 33.1 ± 0.4            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 27.1 ± 0.8 27.3 ± 0.8 | 30.3 ± 0.7 28.4 ± 0.7 | 36.3 ± 0.9 34.3 ± 0.9          | 38.8 ± 0.9 37.2 ± 0.9 | 32.8 ± 0.4 31.4 ± 0.4 |\n| TriLM 3.9B                            | 28.3 ±                | 29.5 ± 0.7            | 35.4 ± 0.9                     | 39.6 ±                | 32.8 ± 0.4            |\n|                                       | 0.8                   |                       |                                | 0.9                   |                       |\n\n## H Illustrative examples of TriLM 3.9B's completion capabilities\n\nWe showcase instances of outputs produced by TriLM (3.9B) across diverse tasks, highlighting its proficiency in tasks such as comprehension, prompt completion, and creative composition.\n\n## Generated Output on Reading Comprehension by TriLM (3.9B)\n\nTitle: The Blitz Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. X- and Y-Gerät beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Gerät, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Gerät.\n\n## Q: How many sorties were flown in March 1941?\n\nA: 4,000\n\n## Q: When did the Luftwaffe fly inland missions?\n\nA: Only during moonlit nights\n\n## Title: Helsinki\n\nBackground: Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world's northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland's major political, educational, financial, cultural, and research center as well as one of northern Europe's major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia.\n\n## Q: What is the most populous municipality in Finland?\n\nA: Helsinki\n\n- Q: How many people live there?\n- A: 1.4 million in the metropolitan area\n- Q: What percent of the foreign companies that operate in Finland are in Helsinki? A: 75%\n- Q: What towns are a part of the metropolitan area?\n- A: Espoo, Vantaa, Kauniainen, and surrounding commuter towns\n\n## Auto-complete on a creative writing task by TriLM (3.9B)\n\n## Essay Writing\n\nIn the year 2150, humanity had established colonies on Mars. Captain Ramirez was on a routine mission to explore an uncharted region of the planet when he and his crew were attacked by Martian natives who wanted them dead for their technology. The captain's ship crashed into the surface of Mars where it exploded in flames.\n\nThe survivors managed to escape from the wreckage but they soon found themselves stranded with no way out or back home. They would have to survive alone until rescue came. But what if there is nothing waiting for them? What if they are trapped forever?\n\n## Poem Writing\n\nIn a garden where the roses bloom, Underneath the silver moon, A gentle breeze begins to sway, And whispers secrets of the day. The stars above start to appear,\n\nWhile shadows dance and disappear,\n\nThe night unfolds in its own time. The sun rises over the horizon, As it does every morning, But this is no ordinary dawn. It's a new beginning for all who are here today. For those who have come before us, And those yet to be born. We celebrate life with music and laughter, With friends old and new. This is our moment to shine!\n\n## Auto-complete on various prompts by TriLM (3.9B)\n\nMaking a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They use a mixer to mix ingredients, then knead dough by hand. The women roll out the dough with a rolling pin, cut it into shapes using cookie cutters, bake them, and decorate them with frosting or sprinkles.\n\nThe energy given up by electrons as they move through the electron transport chain is used to produce ATP. In this process, oxygen and water are produced in a reaction called cellular respiration.\n\nWhen talking about sensitive topics, it is crucial to be aware of the fact that there are many different types of people. Some will react negatively and others positively. It's important not to take offense when someone says something you don't like.\n\nIf someone disagrees with you, it is best to ignore them. The most important thing in life is not what we have but who we are and how we treat others.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Type    | Forward Pass                                                                                                             | Backward Pass                       | Inference                                                      |\n|---------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------|----------------------------------------------------------------|\n| FloatLM | Y = XW T                                                                                                                 | ∂L ∂X = ∂L ∂Y W ∂L ∂W = ∂L ∂Y T X   | Y = XW T                                                       |\n| TriLM   | γ = ϵ + 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = round ( min ( max ( W ij γ , - 1 ) , 1 )) ˜ W ij = γ ̂ W ij Y = X ˜ W T | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and γ once and cache ˜ W ij = γ ̂ W ij Y = X ˜ W T |\n| BiLM    | α = 1 nm ∑ n i =1 ∑ m j =1 | W ij | ̂ W ij = sign ( W ij - 1 nm ∑ n i =1 ∑ m j =1 W ij ) ˜ W ij = α ̂ W ij Y = X ˜ W T   | ∂L ∂X = ∂L ∂Y ˜ W ∂L ∂W = ∂L ∂Y T X | Compute ̂ W and α once and cache ˜ W ij = α ̂ W ij Y = X ˜ W T |"
    },
    {
      "index": 1,
      "markdown": "| Dataset        | Size (Tokens)   |\n|----------------|-----------------|\n| Arxiv          | 13B             |\n| Book           | 13B             |\n| C4             | 80B             |\n| Common Crawl   | 156B            |\n| GitHub         | 16B             |\n| Stack Exchange | 10B             |\n| Wikipedia      | 12B             |\n| Total          | 300B            |"
    },
    {
      "index": 2,
      "markdown": "| Params        |   Hidden |   GLU |   Heads |   Layers |   MP | FloatLM LR     | TriLM LR                        |\n|---------------|----------|-------|---------|----------|------|----------------|---------------------------------|\n| 99.74M (99M)  |      512 |  1280 |       8 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 190.0M (190M) |      768 |  2048 |      12 |       16 |    1 | 4 . 0 ∗ 10 - 4 | 2 . 4 ∗ 10 - 3 → 1 . 5 ∗ 10 - 3 |\n| 392.4M (390M) |     1024 |  2560 |      16 |       24 |    1 | 3 . 0 ∗ 10 - 4 | 1 . 8 ∗ 10 - 3 → 1 . 2 ∗ 10 - 3 |\n| 569.2M (560M) |     1280 |  3072 |      20 |       24 |    1 | 2 . 8 ∗ 10 - 4 | 1 . 6 ∗ 10 - 3 → 1 . 1 ∗ 10 - 3 |\n| 834.0M (830M) |     1536 |  4096 |      24 |       24 |    1 | 2 . 5 ∗ 10 - 4 | 1 . 5 ∗ 10 - 3 → 1 . 0 ∗ 10 - 3 |\n| 1.149B (1.1B) |     1792 |  5120 |      28 |       24 |    2 | 2 . 2 ∗ 10 - 4 | 1 . 3 ∗ 10 - 3 → 9 . 0 ∗ 10 - 4 |\n| 1.515B (1.5B) |     2048 |  6144 |      32 |       24 |    2 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 2.461B (2.4B) |     2304 |  7680 |      36 |       30 |    3 | 2 . 0 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |\n| 3.989B (3.9B) |     3072 |  9216 |      24 |       30 |    6 | 1 . 5 ∗ 10 - 4 | 1 . 2 ∗ 10 - 3 → 8 . 0 ∗ 10 - 4 |"
    },
    {
      "index": 3,
      "markdown": "| Params        |   99M |   190M |   390M |   560M |   830M |   1.1B |   1.5B |   2.4B |   3.9B |\n|---------------|-------|--------|--------|--------|--------|--------|--------|--------|--------|\n| FloatLM       |  1.6  |   3.05 |   6.28 |   9.11 |  13.34 |  18.39 |  24.23 |  39.38 |  63.83 |\n| QuantLM 8-Bit |  1.21 |   2.14 |   3.96 |   5.58 |   7.91 |  10.64 |  13.77 |  21.55 |  34.39 |\n| QuantLM 6-Bit |  1.11 |   1.92 |   3.38 |   4.7  |   6.55 |   8.7  |  11.15 |  17.09 |  27.03 |\n| QuantLM 4-Bit |  1.03 |   1.72 |   2.88 |   3.93 |   5.36 |   7    |   8.86 |  13.18 |  20.59 |\n| QuantLM 3-Bit |  0.98 |   1.6  |   2.59 |   3.49 |   4.68 |   6.03 |   7.55 |  10.95 |  16.91 |\n| TriLM         |  0.9  |   1.42 |   2.11 |   2.76 |   3.55 |   4.42 |   5.36 |   7.23 |  10.76 |"
    },
    {
      "index": 4,
      "markdown": "| Model        |   Min. Loss-Scale |   # Skipped Batches | # Skipped Tokens   |\n|--------------|-------------------|---------------------|--------------------|\n| FloatLM 99M  |               256 |                 181 | 0.37B              |\n| TriLM 99M    |              1024 |                 303 | 0.33B              |\n| FloatLM 190M |               512 |                 168 | 0.35B              |\n| TriLM 190M   |               512 |                 305 | 0.33B              |\n| FloatLM 390M |              1024 |                 170 | 0.35B              |\n| TriLM 390M   |               512 |                 312 | 0.34B              |\n| FloatLM 560M |               256 |                 164 | 0.33B              |\n| TriLM 560M   |               512 |                 294 | 0.32B              |\n| FloatLM 830M |              2048 |                 175 | 0.36B              |\n| TriLM 830M   |               128 |                 307 | 0.33B              |\n| FloatLM 1.1B |              2048 |                 158 | 0.32B              |\n| TriLM 1.1B   |               512 |                 306 | 0.33B              |\n| FloatLM 1.5B |               256 |                 170 | 0.35B              |\n| TriLM 1.5B   |               512 |                 318 | 0.34B              |\n| FloatLM 2.4B |              1024 |                 165 | 0.34B              |\n| TriLM 2.4B   |               256 |                 294 | 0.32B              |\n| FloatLM 3.9B |               256 |                 164 | 0.34B              |\n| TriLM 3.9B   |               128 |                 309 | 0.33B              |"
    },
    {
      "index": 5,
      "markdown": "| Models                                | Arc Challenge         | Arc Challenge         | Arc Easy              | Arc Easy              | BoolQ                 |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   |\n| Pythia 70M FloatLM 99M                | 22.0 ± 1.2 23.8 ± 1.2 | 22.1 ± 1.2 19.9 ± 1.2 | 24.8 ± 0.9 39.1 ± 1.0 | 24.8 ± 0.9 45.1 ± 1.0 | 38.5 ± 0.9 58.2 ± 0.9 |\n| QuantLM 99M 8-Bit                     | 23.8 ± 1.2            | 19.6 ± 1.2            | 39.4 ± 1.0            | 45.3 ± 1.0            | 58.5 ± 0.9            |\n| QuantLM 99M 6-Bit                     | 23.2 ± 1.2            | 19.7 ± 1.2            | 38.8 ± 1.0            | 44.8 ± 1.0            | 58.9 ± 0.9            |\n| QuantLM 99M 4-Bit                     | 22.6 ± 1.2            | 18.0 ± 1.1            | 37.1 ± 1.0            | 41.7 ± 1.0            | 52.2 ± 0.9            |\n| QuantLM 99M 3-Bit                     | 23.2 ± 1.2            | 19.5 ± 1.2            | 34.8 ± 1.0            | 36.1 ± 1.0            | 48.4 ± 0.9            |\n| TriLM 99M                             | 24.1 ± 1.3            | 19.1 ± 1.1            | 36.6 ± 1.0            | 39.8 ± 1.0            | 61.3 ± 0.9            |\n| Binary 99M                            | 20.8 ± 1.2            | 18.3 ± 1.1            | 35.8 ± 0.9            | 40.1 ± 1.0            | 61.0 ± 0.8            |\n| Pythia 160M                           | 23.8 ± 1.2            | 23.1 ± 1.2            | 26.7 ± 0.9            | 26.6 ± 0.9            | 38.3 ± 0.9            |\n| FloatLM 190M                          | 24.1 ± 1.3            | 20.5 ± 1.2            | 43.0 ± 1.0            | 48.4 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 8-Bit                    | 24.4 ± 1.3            | 20.3 ± 1.2            | 43.0 ± 1.0            | 48.5 ± 1.0            | 59.3 ± 0.9            |\n| QuantLM 190M 6-Bit                    | 23.8 ± 1.2            | 20.0 ± 1.2            | 42.0 ± 1.0            | 48.0 ± 1.0            | 59.1 ± 0.9            |\n| QuantLM 190M 4-Bit                    | 25.2 ± 1.3            | 19.9 ± 1.2            | 26.5 ± 0.9            | 26.8 ± 0.9            | 40.9 ± 0.9            |\n| QuantLM 190M 3-Bit                    | 22.5 ± 1.2            | 19.4 ± 1.2            | 37.1 ± 1.0            | 39.7 ± 1.0            | 56.5 ± 0.9            |\n| TriLM 190M                            | 23.0 ± 1.2            | 19.5 ± 1.2            | 39.6 ± 1.0            | 43.9 ± 1.0            | 46.8 ± 0.9            |\n| FloatLM 390M                          | 24.7 ± 1.3            | 21.3 ± 1.2            | 46.5 ± 1.0            | 51.0 ± 1.0            | 54.7 ± 0.9            |\n| QuantLM 390M 8-Bit                    | 24.6 ± 1.3            | 21.2 ± 1.2            | 46.6 ± 1.0            | 51.0 ± 1.0            | 54.6 ± 0.9            |\n| QuantLM 390M 6-Bit                    | 24.8 ± 1.3            | 21.5 ± 1.2            | 46.8 ± 1.0            | 51.8 ± 1.0            | 55.3 ± 0.9            |\n| QuantLM 390M 4-Bit                    | 25.1 ± 1.3            | 21.3 ± 1.2            | 45.2 ± 1.0            | 49.6 ± 1.0            | 50.8 ± 0.9            |\n| QuantLM 390M 3-Bit                    | 24.9 ± 1.3            | 21.5 ± 1.2            | 41.6 ± 1.0            | 43.6 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 390M                            | 24.5 ± 1.3            | 21.2 ± 1.2            | 44.1 ± 1.0            | 48.6 ± 1.0            | 55.1 ± 0.9            |\n| Pythia 410M                           | 24.7 ± 1.3            | 21.2 ± 1.2            | 45.7 ± 1.0            | 51.6 ± 1.0            | 60.0 ± 0.9            |\n| FloatLM 560M                          | 26.5 ± 1.3            | 23.9 ± 1.2            | 48.4 ± 1.0            | 54.4 ± 1.0            | 57.9 ± 0.9            |\n| QuantLM 560M 8-Bit                    | 26.5 ± 1.3            | 23.6 ± 1.2            | 48.3 ± 1.0            | 54.1 ± 1.0            | 57.6 ± 0.9            |\n| QuantLM 560M 6-Bit QuantLM 560M 4-Bit | 26.0 ± 1.3            | 23.5 ± 1.2            | 47.6 ± 1.0            | 54.2 ± 1.0            | 57.3 ± 0.9            |\n| QuantLM 560M 3-Bit                    | 25.9 ± 1.3 24.0 ± 1.2 | 23.0 ± 1.2 21.2 ± 1.2 | 46.3 ± 1.0            | 52.4 ± 1.0            | 58.8 ± 0.9 59.0 ± 0.9 |\n|                                       |                       |                       | 42.3 ± 1.0            | 45.8 ± 1.0 50.2 ± 1.0 |                       |\n| TriLM 560M                            | 25.7 ± 1.3 24.6 ± 1.2 | 21.0 ± 1.2 20.2 ± 1.1 | 45.5 ± 1.0 41.9 ± 1.0 | 47.8 ± 1.0            | 57.3 ± 0.9 61.5 ± 0.8 |\n| Binary 560M FloatLM 830M              | 28.0 ± 1.3            | 24.5 ± 1.3            | 51.6 ± 1.0            | 57.3 ± 1.0            | 61.0 ± 0.9            |\n| QuantLM 830M 8-Bit                    | 28.2 ± 1.3            | 25.1 ± 1.3            | 51.7 ± 1.0            | 57.3 ± 1.0            | 60.9 ± 0.9            |\n| QuantLM 830M 6-Bit                    | 27.6 ± 1.3            | 24.7 ± 1.3            | 51.6 ± 1.0            | 57.7 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 830M 4-Bit                    | 27.6 ± 1.3            | 23.3 ± 1.2            | 50.5 ± 1.0            | 56.2 ± 1.0            | 58.1 ± 0.9            |\n| QuantLM 830M 3-Bit                    | 27.1 ± 1.3            | 22.7 ± 1.2            | 46.8 ± 1.0            | 50.5 ± 1.0            | 56.3 ± 0.9            |\n| TriLM 830M                            | 25.3 ± 1.3            | 22.5 ± 1.2            | 48.7 ± 1.0            | 54.2 ± 1.0            | 60.4 ± 0.9            |\n| Pythia 1B                             |                       | 24.4 ± 1.3            | 49.0 ± 1.0            | 57.0 ± 1.0            | ±                     |\n| FloatLM 1.1B                          | 27.0 ± 1.3 29.1 ± 1.3 | 26.1 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 60.8 0.9 62.9 ± 0.8   |\n| QuantLM 1.1B 8-Bit                    | 28.9 ± 1.3            | 26.1 ± 1.3            | 54.1 ± 1.0            | 60.2 ± 1.0            | 62.6 ± 0.8            |\n| QuantLM 1.1B 6-Bit                    | 29.8 ± 1.3            | 25.5 ± 1.3            | 54.3 ± 1.0            | 60.2 ± 1.0            | 62.9 ± 0.8            |\n|                                       |                       |                       |                       | 59.0 ± 1.0            | 61.3 ± 0.9            |\n| QuantLM 1.1B 4-Bit QuantLM 1.1B 3-Bit | 30.3 ± 1.3 29.2 ± 1.3 | 26.0 ± 1.3 27.0 ± 1.3 | 53.6 ± 1.0 48.9 ± 1.0 | 55.0 ± 1.0            | 62.1 ± 0.8            |\n| TriLM 1.1B                            | 26.5 ± 1.3            | 24.6 ± 1.3            | 49.8 ± 1.0            | 56.3 ± 1.0            | 59.1 ± 0.9            |\n| Binary 1.1B                           | 24.8 ± 1.3            | 22.3 ± 1.2            | 46.1 ± 1.0            | 52.7 ± 1.0            | 56.3 ± 0.9            |\n| Pythia 1.4B                           | 28.7 ± 1.3            | 26.0 ± 1.3            | 54.0 ± 1.0            | 60.4 ± 1.0            | 63.2 ± 0.8            |\n| FloatLM 1.5B                          | 29.7 ± 1.3            | 26.2 ± 1.3            | 56.4 ± 1.0            | 62.6 ± 1.0            | 63.2 ± 0.8            |\n| QuantLM 1.5B 8-Bit                    | 29.8 ± 1.3            | 26.0 ± 1.3            | 56.6 ± 1.0            | 62.4 ± 1.0 62.2 ± 1.0 | 63.3 ± 0.8            |\n| QuantLM 1.5B 6-Bit                    | 30.1 ± 1.3            | 26.0 ± 1.3 ±          | 56.8 ± 1.0 55.2 ± 1.0 |                       | 63.4 ± 0.8            |\n| QuantLM 1.5B 4-Bit                    | 29.4 ± 1.3            | 26.9 1.3              |                       | 60.4 ± 1.0            | 62.5 ± 0.8            |\n| QuantLM 1.5B 3-Bit                    | 27.8 ± 1.3            | 25.2 ± 1.3            | 49.7 ± 1.0            | 54.8 ± 1.0            | 53.7 ± 0.9            |\n| TriLM 1.5B                            | 28.2 ± 1.3            | 24.7 ± 1.3            | 53.1 ± 1.0            | 59.0 ± 1.0            | 54.1 ± 0.9            |\n| FloatLM 2.4B                          | 32.7 ± 1.4            | 30.1 ± 1.3            | 60.5 ± 1.0            | 65.5 ± 1.0            | 62.1 ± 0.8            |\n| QuantLM 2.4B 8-Bit                    | 32.6 ± 1.4            | 30.0 ± 1.3            | 60.3 ± 1.0            | 65.7 ± 1.0            | 62.1 ± 0.8 62.0 ± 0.8 |\n| QuantLM 2.4B 6-Bit                    | 32.7 ± 1.4            | 30.6 ± 1.3            | 60.4 ± 1.0            | 65.4 ± 1.0            |                       |\n| QuantLM 2.4B 4-Bit                    | 33.3 ± 1.4            | 30.8 ± 1.3            | 59.6 ± 1.0            | 64.1 ± 1.0            | 59.0 ± 0.9            |\n| QuantLM 2.4B 3-Bit                    | 29.7 ± 1.3            | 28.4 ± 1.3            | 54.2 ± 1.0            | 58.4 ± 1.0            | 55.7 ± 0.9            |\n| TriLM 2.4B                            | 29.9 ± 1.3            | 29.5 ± 1.3            | 58.0 ± 1.0            | 63.8 ± 1.0            | 64.4 ± 0.8            |\n| FloatLM 3.9B                          | 34.6 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0            | 68.3 ± 1.0            | 65.9 ± 0.8            |\n| QuantLM 3.9B 8-Bit                    | 34.6 ± 1.4            | 31.9 ± 1.4            |                       | 68.1 ± 1.0            | 65.4 ± 0.8            |\n| QuantLM 3.9B 6-Bit                    | 35.1 ± 1.4            | 32.1 ± 1.4            | 63.0 ± 1.0 63.3 ± 1.0 | 68.0 ± 1.0            | 65.6 ± 0.8            |\n|                                       |                       |                       |                       |                       | 65.4 ± 0.8            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 34.7 ± 1.4 32.1 ± 1.4 | 32.9 ± 1.4 29.3 ± 1.3 | 61.2 ± 1.0 55.5 ± 1.0 | 68.3 ± 1.0 62.1 ± 1.0 | 60.0 ± 0.9            |\n| TriLM 3.9B                            | 35.3 ± 1.4            | 31.9 ± 1.4            | 60.8 ± 1.0            | 66.0 ± 1.0            | 66.5 ± 0.8            |"
    },
    {
      "index": 6,
      "markdown": "| Models                                | HellaSwag             | HellaSwag             | PIQA                  | PIQA                  | WinoGrande            | Avg ( HellaSwag, PIQA, WinoGrande, Arc BoolQ)   |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-------------------------------------------------|\n| Models                                | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   | Acc                   | Easy, Arc Challenge, and                        |\n| Pythia 70M FloatLM 99M                | 25.1 ± 0.4 31.6 ± 0.5 | 25.1 ± 0.4 29.1 ± 0.5 | 49.8 ± 1.2 62.8 ± 1.1 | 49.9 ± 1.2 63.2 ± 1.1 | 49.1 ± 1.4 50.2 ± 1.4 | 34.9 44.3                                       |\n| QuantLM 99M 8-Bit                     | 31.7 ± 0.5            | 29.0 ± 0.5            | 62.6 ± 1.1            | 63.0 ± 1.1            | 50.0 ± 1.4            | 44.3                                            |\n| QuantLM 99M 6-Bit                     | 31.7 ± 0.5            | 29.2 ± 0.5            | 62.8 ± 1.1            | 63.1 ± 1.1            | 50.2 ± 1.4            | 44.3                                            |\n| QuantLM 99M 4-Bit                     | 31.0 ± 0.5            | 28.9 ± 0.5            | 62.2 ± 1.1            | 60.9 ± 1.1            | 50.4 ± 1.4            | 42.6                                            |\n| QuantLM 99M 3-Bit                     | 29.2 ± 0.5            | 27.7 ± 0.4            | 57.2 ± 1.2            | 58.2 ± 1.2            | 49.2 ± 1.4            | 40.3                                            |\n| TriLM 99M                             | 28.4 ± 0.5            | 27.6 ± 0.4            | 60.1 ± 1.1            | 60.4 ± 1.1            | 50.7 ± 1.4            | 43.5                                            |\n| Binary 99M                            | 27.7 ± 0.4            | 27.2 ± 0.4            | 59.2 ± 1.1            | 59.2 ± 1.1            | 48.8 ± 1.4            | 39.8                                            |\n| Pythia 160M                           | 25.1 ± 0.4            | 25.0 ± 0.4            | 53.1 ± 1.2            | 53.1 ± 1.2            | 47.3 ± 1.4            | 35.7                                            |\n| FloatLM 190M                          | 36.6 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.9 ± 1.4            | 46.7                                            |\n| QuantLM 190M 8-Bit                    | 36.5 ± 0.5            | 31.4 ± 0.5            | 65.6 ± 1.1            | 64.8 ± 1.1            | 51.7 ± 1.4            | 46.8                                            |\n| QuantLM 190M 6-Bit                    | 36.3 ± 0.5            | 31.5 ± 0.5            | 65.6 ± 1.1            | 64.3 ± 1.1            | 51.9 ± 1.4            | 46.4                                            |\n| QuantLM 190M 4-Bit                    | 26.0 ± 0.4            | 25.7 ± 0.4            | 49.3 ± 1.2            | 51.7 ± 1.2            | 51.0 ± 1.4            | 36.5                                            |\n| QuantLM 190M 3-Bit                    | 32.0 ± 0.5            | 28.8 ± 0.5            | 58.1 ± 1.2            | 58.7 ± 1.1            | 50.1 ± 1.4            | 42.7                                            |\n| TriLM 190M                            | 31.6 ± 0.5            | 29.0 ± 0.5            | 62.0 ± 1.1            | 62.3 ± 1.1            | 51.7 ± 1.4            | 42.4                                            |\n| FloatLM 390M                          | 44.4 ± 0.5            | 35.7 ± 0.5            | 68.7 ± 1.1            | 68.4 ± 1.1            | 51.8 ± 1.4            | 48.5                                            |\n| QuantLM 390M 8-Bit                    | 44.5 ± 0.5            | 35.7 ± 0.5            | 68.8 ± 1.1            | 68.6 ± 1.1            | 52.6 ± 1.4            | 48.6                                            |\n| QuantLM 390M 6-Bit                    | 44.2 ± 0.5            | 35.6 ± 0.5            | 69.0 ± 1.1            | 68.4 ± 1.1            | 53.0 ± 1.4            | 48.9                                            |\n| QuantLM 390M 4-Bit                    | 43.4 ± 0.5            | 35.1 ± 0.5            | 68.1 ± 1.1            | 68.3 ± 1.1            | 53.7 ± 1.4            | 47.7                                            |\n| QuantLM 390M 3-Bit                    | 39.5 ± 0.5            | 32.9 ± 0.5            | 63.8 ± 1.1            | 63.2 ± 1.1            | 53.0 ± 1.4            | 46.5                                            |\n| TriLM 390M                            | 37.9 ± 0.5            | 32.0 ± 0.5            | 64.7 ± 1.1            | 65.0 ± 1.1            | 52.2 ± 1.4            | 46.4                                            |\n| Pythia 410M                           | 40.3 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 66.3 ± 1.1            | 53.5 ± 1.4            | 48.6                                            |\n| FloatLM 560M                          | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.8 ± 1.1            | 69.0 ± 1.1            | 53.7 ± 1.4            | 50.5                                            |\n| QuantLM 560M 8-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.9 ± 1.1            | 68.9 ± 1.1            | 53.8 ± 1.4            | 50.4                                            |\n| QuantLM 560M 6-Bit                    | 47.6 ± 0.5            | 37.7 ± 0.5            | 68.7 ± 1.1            | 68.8 ± 1.1            | 53.5 ± 1.4            | 50.1                                            |\n| QuantLM 560M 4-Bit                    | 46.7 ± 0.5            | 37.0 ± 0.5            | 67.8 ± 1.1            | 67.1 ± 1.1            | 53.1 ± 1.4            | 49.8                                            |\n| QuantLM 560M 3-Bit                    | 41.7 ± 0.5            | 33.4 ± 0.5            | 63.5 ± 1.1            | 63.2 ± 1.1            | 49.7 ± 1.4            | 46.7                                            |\n| TriLM 560M                            | 41.5 ± 0.5            | 33.8 ± 0.5            | 67.2 ± 1.1            | 67.5 ± 1.1            | 53.1 ± 1.4            | 48.4                                            |\n| Binary 560M                           | 36.4 ± 0.4            | 31.2 ± 0.4            | 64.6 ± 1.1            | 64.2 ± 1.1            | 52.8 ± 1.4            | 44.5                                            |\n| FloatLM 830M QuantLM 830M 8-Bit       | 51.3 ± 0.5 51.4 ± 0.5 | 40.1 ± 0.5 40.1 ± 0.5 | 71.4 ± 1.1 ±          | 71.7 ± 1.1 71.7 ± 1.1 | 56.4 ± 1.4 55.9 ± 1.4 | 53.3 53.2                                       |\n| QuantLM 830M 6-Bit                    | 51.5 ± 0.5            | 40.2 ± 0.5            | 71.2 1.1 71.3 ± 1.1   | 71.8 ± 1.0            | 56.2 ± 1.4            | 53.2                                            |\n| QuantLM 830M 4-Bit                    |                       | 39.2 ± 0.5            | 70.6 ± 1.1            | 71.1 ± 1.1            | 56.0 ± 1.4            | 52.2                                            |\n| 3-Bit                                 | 50.2 ± 0.5            | 35.9 ± 0.5            | 66.1 ± 1.1            | 66.6 ± 1.1            | 53.5 ± 1.4            | 49.2                                            |\n| QuantLM 830M                          | 45.5 ± 0.5            |                       |                       |                       |                       |                                                 |\n| TriLM 830M Pythia 1B                  | 46.0 ± 0.5            | 36.8 ± 0.5            | 68.2 ± 1.1            | 68.4 ± 1.1 70.8 ± 1.1 | 55.6 ± 1.4            | 50.7                                            |\n| FloatLM 1.1B                          | 47.2 ± 0.5 55.2 ± 0.5 | 37.7 ± 0.5 42.6 ± 0.5 | 69.3 ± 1.1 72.2 ± 1.0 | 71.3 ± 1.1            | 53.2 ± 1.4 56.3 ± 1.4 | 51.1 54.9                                       |\n| QuantLM 1.1B 8-Bit                    | 55.2 ± 0.5            | 42.6 ± 0.5            | 72.1 ± 1.0            | 71.2 ± 1.1            | 56.2 ± 1.4            | 54.8                                            |\n| QuantLM 1.1B 6-Bit                    | 54.9 ± 0.5            |                       | 71.9 ± 1.0            | 71.2 ± 1.1            | 56.1 ± 1.4            | 55.0                                            |\n| QuantLM 1.1B 4-Bit                    | 54.9 ± 0.5            | 42.6 ± 0.5 42.0 ± 0.5 | 71.6 ± 1.1            | 70.4 ± 1.1            | 54.8 ± 1.4            | 54.4                                            |\n| QuantLM 1.1B 3-Bit                    | 51.3 ± 0.5            | 39.4 ± 0.5            | 69.4 ± 1.1            | 68.4 ± 1.1            | 54.8 ± 1.4            | 52.6                                            |\n| TriLM 1.1B                            | 49.1 ± 0.5            | 38.8 ± 0.5            | 69.8 ± 1.1            | 69.3 ± 1.1            | 55.5 ± 1.4            | 51.6                                            |\n|                                       | 43.4 ± 0.5            | 35.1 ± 0.4            | 66.9 ± 1.1            | 68.3 ± 1.1            | 55.3 ± 1.4            | 47.1                                            |\n| Binary 1.1B Pythia 1.4B               | 52.0 ± 0.5            | 40.4 ± 0.5            | 70.8 ± 1.1            | 70.6 ± 1.1            | 57.1 ± 1.4            | 54.3                                            |\n| FloatLM 1.5B                          | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.9 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.7                                            |\n| QuantLM 1.5B 8-Bit                    | 57.8 ± 0.5            | 44.3 ± 0.5            | 73.7 ± 1.0            | 73.1 ± 1.0            | 59.4 ± 1.4            | 56.8                                            |\n| QuantLM 1.5B 6-Bit                    | 57.5 ± 0.5            | 44.2 ± 0.5            | 74.0 ± 1.0            | 73.0 ± 1.0            | 59.7 ± 1.4            | 56.9                                            |\n| QuantLM 1.5B 4-Bit                    | 56.9 ± 0.5            | 43.2 ± 0.5            | 72.7 ± 1.0            | 72.4 ± 1.0            | 57.1 ± 1.4            | 55.6                                            |\n| QuantLM 1.5B 3-Bit                    | 53.7 ± 0.5            | 41.0 ± 0.5            | 70.0 ± 1.1            | 69.4 ± 1.1            | 55.0 ± 1.4            | 51.6                                            |\n| TriLM 1.5B                            | 53.1 ± 0.5            | 40.9 ± 0.5            | 70.1 ± 1.1            | 70.3 ± 1.1            | 56.1 ± 1.4            | 52.5                                            |\n| FloatLM 2.4B                          | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.2 ± 1.0            | 74.9 ± 1.0            | 61.8 ± 1.4            | 59.2                                            |\n|                                       | 62.7 ± 0.5            | 47.1 ± 0.5            | 75.4 ± 1.0            | 74.9 ± 1.0            | 61.4 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 8-Bit QuantLM 2.4B 6-Bit | 62.9 ± 0.5            | 47.0 ± 0.5            | 75.7 ± 1.0            | 74.7 ± 1.0            | 61.1 ± 1.4            | 59.1                                            |\n| QuantLM 2.4B 4-Bit                    | 62.2 ± 0.5            | 46.5 ± 0.5            | 75.4 ± 1.0            | 74.5 ± 1.0            | 61.7 ± 1.4            | 58.5                                            |\n| QuantLM 2.4B 3-Bit                    | 58.6 ± 0.5            | 43.5 ± 0.5            | 72.7 ± 1.0            | 70.8 ± 1.1            | 57.2 ± 1.4            | 54.7                                            |\n| TriLM 2.4B                            | 59.0 ± 0.5            | 45.3 ± 0.5            | 72.6 ± 1.0            | 71.4 ± 1.1            | 59.7 ± 1.4            | 57.3                                            |\n| FloatLM 3.9B                          | 66.1 ± 0.5            | 49.7 ± 0.5            | 75.8 ± 1.0            | 75.4 ± 1.0            | ±                     | 61.4                                            |\n| QuantLM 3.9B 8-Bit                    |                       | 49.7 ± 0.5            |                       | 75.5 ± 1.0            | 62.8 1.4 ±            | 61.3                                            |\n| QuantLM 3.9B 6-Bit                    | 66.0 ± 0.5 65.9 ± 0.5 | 49.7 ± 0.5            | 75.9 ± 1.0 75.5 ± 1.0 | 75.6 ± 1.0            | 62.9 1.4 62.2 ± 1.4   | 61.3                                            |\n| QuantLM 3.9B 4-Bit                    | 65.0 ± 0.5            | 49.0 ± 0.5            | 75.5 ± 1.0            | 75.6 ± 1.0            | 62.7 ± 1.4            | 60.7                                            |\n| QuantLM 3.9B 3-Bit                    | 61.2 ± 0.5            | 45.9 ± 0.5            | 72.6 ± 1.0            | 72.3 ± 1.0            | 59.3 ± 1.4            | 56.8                                            |\n| TriLM 3.9B                            | 64.7 ± 0.5            | 48.3 ± 0.5            | 74.6 ± 1.0            | 74.4 ± 1.0            | 62.1 ± 1.4            | 60.7                                            |"
    },
    {
      "index": 7,
      "markdown": "| Models      | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ   | HellaSwag   | HellaSwag   | PIQA      | PIQA   | WinoGrande Acc   | Avg   |\n|-------------|-----------------|-----------------|------------|------------|---------|-------------|-------------|-----------|--------|------------------|-------|\n| Models      | Acc Norm.       | Acc             | Acc Norm.  | Acc        | Acc     | Acc Norm.   | Acc         | Acc Norm. | Acc    |                  |       |\n| BitNet 700M | 21.4            |                 | 51.8       |            | 58.2    | 35.1        |             | 68.1      |        | 55.2             | 48.3  |\n| BitNet 1.3B | 24.2            |                 | 54.9       |            | 56.7    | 37.7        |             | 68.8      |        | 55.8             | 49.7  |\n| BitNet 3B   | 28.3            |                 | 61.4       |            | 61.5    | 42.9        |             | 71.5      |        | 59.3             | 54.2  |\n| BitNet 3.9B | 28.7            |                 | 64.2       |            | 63.5    | 44.2        |             | 73.2      |        | 60.5             | 55.7  |"
    },
    {
      "index": 8,
      "markdown": "| Models                                | LAMBADA               | LAMBADA               | SciQ                  | SciQ                  | LogiQA                | LogiQA                |\n|---------------------------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Models                                | Perp.                 | Acc                   | Acc Norm.             | Acc                   | Acc Norm.             | Acc                   |\n| FloatLM 99M QuantLM 99M 8-Bit         | 85.0 ± 6.9 85.8 ± 7.0 | 26.5 ± 0.6 26.6 ± 0.6 | 62.9 ± 1.5 62.8 ± 1.5 | 73.6 ± 1.4 73.7 ± 1.4 | 27.6 ± 1.8 27.8 ± 1.8 | 21.2 ± 1.6 21.0 ± 1.6 |\n| QuantLM 99M 6-Bit                     | 89.9 ± 7.4            | 26.1 ± 0.6            | 61.8 ± 1.5            | 73.9 ± 1.4            | 28.1 ± 1.8            | 20.3 ± 1.6            |\n| QuantLM 99M 4-Bit                     | 211.6 ± 17.3          | 16.7 ± 0.5            | 61.2 ± 1.5            | 70.7 ± 1.4            | 24.9 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 99M 3-Bit                     | 4765.4 ± 413.0        | 4.5 ± 0.3             | 51.9 ± 1.6            | 57.0 ± 1.6            | 25.3 ± 1.7            | 19.8 ± 1.6            |\n| TriLM 99M                             | 172.0 ± 8.4           | 20.0 ± 0.6            | 60.4 ± 1.5            | 67.6 ± 1.5            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| Binary 99M                            | 468.3 ± 24.1          | 14.0 ± 0.4            | 54.4 ± 1.6            | 62.5 ± 1.5            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| FloatLM 190M                          | 50.3 ± 2.7            | 31.1 ± 0.6            | 65.1 ± 1.5            | 77.3 ± 1.3            | 27.2 ± 1.7            | 22.1 ± 1.6            |\n| QuantLM 190M 8-Bit                    | 48.7 ± 2.6            | 31.5 ± 0.6            | 65.5 ± 1.5            | 77.1 ± 1.3            | 27.0 ± 1.7            | 22.3 ± 1.6            |\n| QuantLM 190M 6-Bit                    | 55.3 ± 3.0            | 30.0 ± 0.6            | 64.2 ± 1.5            | 77.0 ± 1.3            | 26.1 ± 1.7            | 22.4 ± 1.6            |\n| QuantLM 190M 4-Bit                    | 72479077.3            | 0.00 ± 0.0            | 25.6 ± 1.4            | 22.9 ± 1.3            | 23.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 190M 3-Bit                    | 664.5 ± 41.1          | 12.4 ± 0.5            | 58.5 ± 1.6            | 66.4 ± 1.5            | 26.3 ± 1.7            | 21.0 ± 1.6            |\n| TriLM 190M                            | 130.7 ± 6.5           | 23.7 ± 0.6            | 61.0 ± 1.5            | 72.6 ± 1.4            | 25.5 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 390M                          | 21.9 ± 0.9            | 42.2 ± 0.7            | 75.6 ± 1.4            | 84.2 ± 1.2            | 28.1 ± 1.8            | 23.8 ± 1.7            |\n| QuantLM 390M 8-Bit                    | 21.7 ± 0.9            | 42.3 ± 0.7            | 75.7 ± 1.4            | 84.1 ± 1.2            | 28.3 ± 1.8            | 24.1 ± 1.7            |\n| QuantLM 390M 6-Bit                    | 24.3 ± 1.0            | 40.6 ± 0.7            | 75.5 ± 1.4            | 83.7 ± 1.2            | 27.6 ± 1.8            | 23.2 ± 1.7            |\n| QuantLM 390M 4-Bit                    | 30.2 ± 1.3            | 39.1 ± 0.7            | 77.1 ± 1.3            | 84.1 ± 1.2            | 25.8 ± 1.7            | 23.3 ± 1.7            |\n| QuantLM 390M 3-Bit                    | 115.0 ± 5.6           | 23.0 ± 0.6            | 67.4 ± 1.5            | 76.7 ± 1.3            | 25.7 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 390M                            | 77.7 ± 3.8            | 28.0 ± 0.6            | 68.6 ± 1.5            | 76.9 ± 1.3            | 26.4 ± 1.7            | 21.8 ± 1.6            |\n| FloatLM 560M                          | 20.8 ± 0.9            | 44.1 ± 0.7            | 74.7 ± 1.4            | 83.5 ± 1.2            | 27.0 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 8-Bit                    | 20.9 ± 0.9            | 44.2 ± 0.7            | 74.7 ± 1.4            | 83.6 ± 1.2            | 27.3 ± 1.7            | 20.7 ± 1.6            |\n| QuantLM 560M 6-Bit                    | 21.7 ± 0.9            | 42.8 ± 0.7            | 74.4 ± 1.4            | 83.6 ± 1.2            | 25.8 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 560M 4-Bit                    | 24.9 ± 1.1            | 40.8 ± 0.7            | 73.6 ± 1.4            | 82.0 ± 1.2            | 27.0 ± 1.7            | 21.7 ± 1.6            |\n| QuantLM 560M 3-Bit                    | 146.3 ± 7.1           | 20.1 ± 0.6            |                       | 75.9 ± 1.4            | 25.0 ± 1.7            | 21.8 ± 1.6            |\n| TriLM 560M                            | 55.6 ± 2.7            | 32.4 ± 0.7            | 71.1 ± 1.4 70.8 ± 1.4 | 78.7 ± 1.3            | 26.1 ± 1.7            | 19.8 ± 1.6            |\n| Binary 560M                           | 62.8 ± 3.0            | 31.0 ± 0.6            | 70.0 ± 1.4            | 78.8 ± 1.3            | 26.7 ± 1.7            | 21.5 ± 1.6            |\n| FloatLM 830M                          | 13.3 ± 0.5            | 49.6 ± 0.7            | 78.4 ± 1.3            | 85.9 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6            |\n| QuantLM 830M 8-Bit                    | 13.5 ± 0.5            | 49.4 ± 0.7            | 78.5 ± 1.3            | 86.1 ± 1.1            | 26.6 ± 1.7            | 20.0 ± 1.6            |\n| QuantLM 830M 6-Bit                    | 13.3 ± 0.5            | 49.1 ± 0.7            | 77.8 ± 1.3            | 85.4 ± 1.1            | 26.3 ± 1.7            | 20.1 ± 1.6 21.2 ± 1.6 |\n| QuantLM 830M 4-Bit                    | 15.4 ± 0.6            | 47.3 ± 0.7            | 78.8 ± 1.3            | 85.1 ± 1.1            | 25.5 ± 1.7            |                       |\n| QuantLM 830M 3-Bit                    | 47.7 ± 2.0            | 30.5 ± 0.6            | 74.1 ± 1.4            | 80.1 ± 1.3            | 28.1 ± 1.8            | 21.2 ± 1.6            |\n| TriLM 830M                            | 26.0 ± 1.1            | 39.9 ± 0.7            | 75.4 ± 1.4            | 82.8 ± 1.2            | 27.6 ± 1.8            | 21.4 ± 1.6            |\n| FloatLM 1.1B                          | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.2 ± 1.2            | 88.1 ± 1.0            | 27.3 ± 1.7            | 20.9 ± 1.6            |\n| QuantLM 1.1B 8-Bit                    | 11.7 ± 0.4            | 51.2 ± 0.7            | 82.1 ± 1.2            | 88.1 ± 1.0 88.1 ± 1.0 | 27.8 ± 1.8            | 21.2 ± 1.6            |\n| QuantLM 1.1B 6-Bit                    | 11.7 ± 0.4            | 51.0 ± 0.7            | 82.3 ± 1.2            |                       | 27.5 ± 1.8 28.4 ± 1.8 | 21.5 ± 1.6            |\n| QuantLM 1.1B 4-Bit                    | 13.9 ± 0.5            | 49.3 ± 0.7            | 81.2 ± 1.2            | 87.6 ± 1.0            |                       | 20.3 ± 1.6            |\n| QuantLM 1.1B 3-Bit                    | 26.9 ± 1.1            | 39.1 ± 0.7            | 78.7 ± 1.3            | 85.0 ± 1.1            | 25.8 ± 1.7            | 20.7 ± 1.6            |\n| TriLM 1.1B Binary 1.1B                | 17.3 ± 0.7 33.4 ± 1.4 | 46.2 ± 0.7 37.6 ± 0.6 | 73.3 ± 1.4 71.1 ± 1.4 | 81.9 ± 1.2 81.2 ± 1.3 | 26.9 ± 1.7 28.4 ± 1.7 | 22.0 ± 1.6 23.2 ± 1.6 |\n| FloatLM 1.5B QuantLM 1.5B 8-Bit       | 9.4 ± 0.3 9.5 ± 0.3   | 55.5 ± 0.7 ±          | 80.9 ± 1.2 81.3 ± 1.2 | 87.4 ± 1.0 87.5 ± 1.0 | 26.1 ± 1.7 25.7 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 9.5 ± 0.3             | 55.5 0.7 55.4 ± 0.7   | 81.4 ± 1.2            | 87.6 ± 1.0            | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 6-Bit                    |                       | ±                     |                       |                       | 25.7 ± 1.7            | 20.3 ± 1.6            |\n| QuantLM 1.5B 4-Bit                    | 17.8 ± 0.7            | 45.3 ± 0.7            | 75.5 ± 1.4            | 82.1 ± 1.2            |                       |                       |\n| QuantLM 1.5B 3-Bit                    | 10.4 ± 0.4            | 53.0 0.7              | 81.1 ± 1.2            | 86.9 ± 1.1            |                       |                       |\n| TriLM 1.5B FloatLM 2.4B               | 7.7 ± 0.3             | 46.2 ± 0.7 59.3 ± 0.7 | 87.2 ± 1.1            | 87.3 ± 1.1 91.0 ± 0.9 | 28.4 ± 1.8 29.5 ± 1.8 | 22.7 ± 1.6 21.5 ± 1.6 |\n|                                       | 16.4 ± 0.7            |                       | 80.7 ± 1.2            |                       | 27.8 ± 1.8            | 21.5 ± 1.6            |\n|                                       |                       |                       |                       |                       |                       | 21.5 ± 1.6            |\n| QuantLM 2.4B 8-Bit                    | 7.7 ± 0.3             | 59.2 ± 0.7            | 87.1 ± 1.1            | 91.0 ± 0.9            | 29.5 ± 1.8            |                       |\n| QuantLM 2.4B 6-Bit                    | 7.9 ± 0.3             | 58.9 ± 0.7            | 87.3 ± 1.1            | 90.9 ± 0.9            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 4-Bit                    | 8.9 ± 0.3             | 56.1 ± 0.7            | 84.8 ± 1.1            | 89.7 ± 1.0            | 29.6 ± 1.8            | 20.9 ± 1.6            |\n| QuantLM 2.4B 3-Bit                    | 15.6 ± 0.6            | 45.0 ± 0.7            | 79.9 ± 1.3            | 86.7 ± 1.1            | 28.6 ± 1.8            | 21.4 ± 1.6            |\n| TriLM 2.4B                            | 8.6 ± 0.3             | 55.7 ± 0.7            | 84.2 ± 1.2            | 88.7 ± 1.0            | 28.6 ± 1.8            | 24.3 ± 1.7            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 6.7 ± 0.2             | 61.1 ± 0.7 61.1 ± 0.7 | 86.5 ± 1.1 86.2 ± 1.1 | 90.9 ± 0.9 91.0 ± 0.9 | 26.9 ± 1.7 26.6 ± 1.7 | 20.9 ± 1.6 20.6 ± 1.6 |\n|                                       | 6.7 ± 0.2             |                       |                       |                       |                       | 20.4 ± 1.6            |\n| QuantLM 3.9B 6-Bit                    | 6.8 ± 0.2             | 60.8 ± 0.7 58.5 ± 0.7 | 86.6 ± 1.1 86.1 ± 1.1 | 91.3 ± 0.9 90.8 ± 0.9 | 25.8 ± 1.7 28.6 ± 1.8 | 20.1 ± 1.6            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 7.4 ± 0.2 14.0 ± 0.5  | 47.1 ± 0.7            | 83.1 ± 1.2            | 88.6 ± 1.0            | 27.0 ± 1.7            | 21.5 ± 1.6 22.7 ± 1.6 |\n| TriLM 3.9B                            | 6.3 ± 0.2             | 61.6 ± 0.7            | 87.4 ± 1.0            | 90.8 ± 0.9            | 27.6 ± 1.8            |                       |"
    },
    {
      "index": 9,
      "markdown": "| Models                          | HellaSwag   | HellaSwag   | PIQA       | PIQA       | WinoGrande   |\n|---------------------------------|-------------|-------------|------------|------------|--------------|\n|                                 | Acc N.      | Acc         | Acc N.     | Acc        | Acc          |\n| FloatLM 1.1B                    | 50.0 ± 0.5  | 39.3 ± 0.5  | 70.9 ± 1.0 | 70.1 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B                      | 46.8 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.4 ± 1.0 | 53.8 ± 1.4   |\n| BitNet b1.58 1.1B (Ours)        | 47.0 ± 0.5  | 37.1 ± 0.4  | 69.4 ± 1.0 | 69.6 ± 1.0 | 53.4 ± 1.4   |\n| BitNet b1.58 700M               | 35.1        | 35.1        | 68.1       | 68.1       | 55.2         |\n| BitNet b1.58 1.3B               | 37.7        | 37.7        | 68.8       | 68.8       | 55.8         |\n| TriLM 1.1B FineWeb              | 50.0 ± 0.5  | 39.2 ± 0.4  | 70.2 ± 1.0 | 70.1 ± 1.0 | 56.6 ± 1.3   |\n| FloatLM 1.1B FineWeb            | 52.7 ± 0.5  | 41.1 ± 0.4  | 73.0 ± 1.0 | 71.3 ± 1.0 | 56.7 ± 1.3   |\n| TriLM 1.1B Only Peak LR Dropped | 46.7 ± 0.5  | 36.8 ± 0.4  | 68.9 ± 1.0 | 69.5 ± 1.0 | 55.4 ± 1.4   |\n| TriLM 1.1B Only L2 Reg. Dropped | 47.1 ± 0.5  | 37.5 ± 0.4  | 68.6 ± 1.0 | 69.4 ± 1.0 | 55.2 ± 1.4   |\n| TriLM 1.1B Baseline Schedule    | 46.0 ± 0.5  | 36.9 ± 0.4  | 69.3 ± 1.0 | 69.1 ± 1.0 | 56.2 ± 1.3   |"
    },
    {
      "index": 10,
      "markdown": "| Models                          | Arc Challenge   | Arc Challenge   | Arc Easy   | Arc Easy   | BoolQ      | Avg ( HellaSwag, PIQA, WinoGrande,   |\n|---------------------------------|-----------------|-----------------|------------|------------|------------|--------------------------------------|\n|                                 | Acc N.          | Acc             | Acc N.     | Acc        | Acc        | Arc Easy, Arc Challenge, and BoolQ)  |\n| FloatLM 1.1B                    | 26.3 ± 1.3      | 22.5 ± 1.2      | 50.3 ± 1.0 | 56.8 ± 1.0 | 60.6 ± 0.8 | 52.2                                 |\n| TriLM 1.1B                      | 26.7 ± 1.3      | 22.9 ± 1.2      | 49.7 ± 1.0 | 55.0 ± 1.0 | 54.9 ± 0.8 | 50.2                                 |\n| BitNet b1.58 1.1B (Ours)        | 26.1 ± 1.2      | 23.6 ± 1.2      | 47.7 ± 1.0 | 55.3 ± 1.0 | 49.7 ± 0.8 | 48.9                                 |\n| BitNet b1.58 700M               | 21.4            | 21.4            | 51.8       | 51.8       | 58.2       | 48.3                                 |\n| BitNet b1.58 1.3B               | 24.2            | 24.2            | 54.9       | 54.9       | 56.7       | 49.6                                 |\n| TriLM 1.1B FineWeb              | 31.7 ± 1.3      | 31.9 ± 1.3      | 63.1 ± 0.9 | 66.8 ± 0.9 | 58.3 ± 0.8 | 54.9                                 |\n| FloatLM 1.1B FineWeb            | 34.4 ± 1.3      | 33.0 ± 1.3      | 65.7 ± 0.9 | 70.2 ± 0.9 | 59.3 ± 0.8 | 56.9                                 |\n| TriLM 1.1B Only Peak LR Dropped | 27.4 ± 1.3      | 23.6 ± 1.2      | 48.3 ± 1.0 | 55.1 ± 1.0 | 51.6 ± 0.8 | 49.7                                 |\n| TriLM 1.1B Only L2 Reg. Dropped | 27.6 ± 1.3      | 24.8 ± 1.2      | 49.2 ± 1.0 | 55.1 ± 1.0 | 53.1 ± 0.8 | 50.1                                 |\n| TriLM 1.1B Baseline Schedule    | 26.2 ± 1.2      | 23.2 ± 1.2      | 48.0 ± 1.0 | 54.0 ± 1.0 | 49.4 ± 0.8 | 49.1                                 |"
    },
    {
      "index": 11,
      "markdown": "| Models                                | TriviaQA Exact Match   | CrowsPairs                | CrowsPairs            | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n|---------------------------------------|------------------------|---------------------------|-----------------------|--------------------------|-----------------------|\n| Models                                | TriviaQA Exact Match   | Likelihood diff.          | Pct stereotype        | Big Bench BBQ Lite Acc   | TruthfulQA Acc        |\n| FloatLM 99M QuantLM 99M 8-Bit         | 0.6 ± 0.1 0.6 ± 0.1    | 372.4 ± 14.6 370.9 ± 14.6 | 55.4 ± 1.2 55.1 ± 1.2 | 30.8 ± 0.4 26.5 ± 0.3    | 24.4 ± 1.5 24.1 ± 1.5 |\n| QuantLM 99M 6-Bit                     | 0.6 ± 0.1              | 389.8 ± 14.8              | 56.9 ± 1.2            | 26.7 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 99M 4-Bit                     | 0.3 ± 0                | 425.5 ± 15.2              | 54.0 ± 1.2            | 26.2 ± 0.3               | 22.9 ± 1.5            |\n| QuantLM 99M 3-Bit                     | 0.1 ± 0                | 611.1 ± 18.8              | 51.0 ± 1.2            | 31.4 ± 0.4               | 24.6 ± 1.5            |\n| TriLM 99M                             | 0.1 ± 0                | 362.4 ± 10.8              | 54.2 ± 1.2            | 30.8 ± 0.4               | 24.2 ± 1.5            |\n| Binary 99M                            | 0.2 ± 0.0              | 353.5 ± 11.1              | 53.3 ± 1.2            | 31.5 ± 0.3               | 25.7 ± 1.5            |\n| FloatLM 190M                          | 0.6 ± 0.1              | 348.2 ± 11.3              | 55.9 ± 1.2            | 27.3 ± 0.4               | 22.4 ± 1.5            |\n| QuantLM 190M 8-Bit                    | 0.7 ± 0.1              | 352.7 ± 11.4              | 56.2 ± 1.2            | 27.1 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 6-Bit                    | 0.7 ± 0.1              | 368.9 ± 11.7              | 56.2 ± 1.2            | 27.2 ± 0.4               | 22.5 ± 1.5            |\n| QuantLM 190M 4-Bit                    | 0.0 ± 0                | 961.9 ± 25.4              | 43.8 ± 1.2            | 35.0 ± 0.4               | 24.2 ± 1.5            |\n| QuantLM 190M 3-Bit                    | 0.1 ± 0                | 482.7 ± 15.2              | 53.7 ± 1.2            | 26.4 ± 0.3               | 25.0 ± 1.5            |\n| TriLM 190M                            | 0.2 ± 0                | 343.5 ± 10.9              | 55.5 ± 1.2            | 29.7 ± 0.4               | 23.9 ± 1.5            |\n| FloatLM 390M                          | 2.8 ± 0                | 355.5 ± 10.4              | 59.6 ± 1.2            | 25.4 ± 0.3               | 22.4 ± 1.5            |\n| QuantLM 390M 8-Bit                    | 2.9 ± 0                | 355.8 ± 10.4              | 59.8 ± 1.2            | 25.4 ± 0.3               | 22.2 ± 1.5            |\n| QuantLM 390M 6-Bit                    | 2.4 ± 0                | 360.5 ± 10.4              | 60.6 ± 1.2            | 25.3 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 4-Bit                    | 1.3 ± 0.1              | 368.2 ± 10.2              | 59.4 ± 1.2            | 25.5 ± 0.3               | 22.8 ± 1.5            |\n| QuantLM 390M 3-Bit                    | 0.8 ± 0.1              | 444.4 ± 12.2              | 54.3 ± 1.2            | 26.3 ± 0.3               | 23.0 ± 1.5            |\n| TriLM 390M                            | 1.3 ± 0.1              | 344.5 ± 10.3              | 58.3 ± 1.2            | 26.9 ± 0.3               | 24.4 ± 1.5            |\n| FloatLM 560M                          | 4.6 ± 0.2              | 351.8 ± 9.9               | 58.9 ± 1.2            | 25.7 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 560M 8-Bit                    | 4.7 ± 0.2              | 352.9 ± 10.0              | 59.2 ± 1.2            | 25.7 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 560M 6-Bit                    | 3.5 ± 0.1              | 353.7 ± 9.9               | 59.3 ± 1.2            | 25.8 ± 0.3               | 22.0 ± 1.5            |\n| QuantLM 560M 4-Bit 3-Bit              | 2.1 ± 0.1              | 372.7 ± 10.7              | 59.2 ± 1.2            | 27.0 ± 0.4 29.0 ± 0.4    | 22.2 ± 1.5 22.9 ± 1.5 |\n| QuantLM 560M TriLM 560M               | 1.5 ± 0.1              | 411.2 ± 11.3              | 57.9 ± 1.2 58.7 ± 1.2 | 25.5 ± 0.3               |                       |\n| Binary 560M                           | 2.4 ± 0.1 0.2 ± 0.0    | 345.1 ± 10.1 356.3 ± 10.4 | 58.5 ± 1.2            | 26.3 ± 0.3               | 23.6 ± 1.5 23.1 ± 1.4 |\n| FloatLM 830M                          | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.6 ± 1.2            | 25.7 ± 0.3               | 23.1 ± 1.5            |\n| QuantLM 830M 8-Bit                    | 8.5 ± 0.2              | 354.5 ± 9.6               | 62.1 ± 1.2            | 25.6 ± 0.3               | 23.0 ± 1.5            |\n| QuantLM 830M 6-Bit                    | 8.5 ± 0.2              | 354.6 ± 9.6               | 62.7 ± 1.2            | 25.5 ± 0.3               | 22.5 ± 1.5            |\n| QuantLM 830M 4-Bit                    | 10.6 ± 0.2             | 364.2 ± 9.8               | 59.9 ± 1.2            | 25.9 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 830M 3-Bit                    | 3.1 ± 0.1              | 389.5 ± 10.9              | 59.9 ± 1.2            | 30.5 ± 0.4               | 24.4 ± 1.5            |\n| TriLM 830M                            | 4.3 ± 0.2              | 344.9 ± 10.0              | 60.7 ± 1.2            | 25.1 ± 0.3               | 22.8 ± 1.5            |\n| FloatLM 1.1B                          | 12.9 ± 0.3             | 349.2 ± 9.7               | 61.2 ± 1.2            | 25.4 ± 0.3               | 21.4 ± 1.4            |\n| QuantLM 1.1B 8-Bit                    | 12.7 ± 0.2             | 349.5 ± 9.7               | 61.1 ± 1.2            | 25.4 ± 0.3               | 21.7 ± 1.4            |\n| QuantLM 1.1B 6-Bit QuantLM 1.1B 4-Bit | 12.4 ± 0.2 9.3 ± 0.2   | 349.7 ± 9.6 359.1 ± 10.1  | 59.9 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4 21.3 ± 1.4 |\n|                                       |                        |                           | 60.9 ± 1.2            | 25.4 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 1.1B 3-Bit                    | 6.8 ± 0.2              | 422.4 ± 11.5              | 58.7 ± 1.2            | 29.9 ± 0.4               |                       |\n| TriLM 1.1B                            | 1.9 ± 0.1              | 343.4 ± 9.9               | 61.4 ± 1.2            | 25.8 ± 0.3               | 21.5 ± 1.4            |\n| Binary 1.1B                           | 2.2 ± 0.1              | 351.6 ± 9.8               | 58.3 ± 1.2            | 26.4 ± 0.3               | 23.2 ± 1.4            |\n| FloatLM 1.5B                          | 12.2 ± 0.2             | 351.9 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 8-Bit                    | 12.5 ± 0.2             | 352.4 ± 9.6               | 61.6 ± 1.2            | 26.8 ± 0.3               | 21.8 ± 1.4            |\n| QuantLM 1.5B 4-Bit                    | 9.0 ± 0.2              | 357.9 ± 9.8               | 60.7 ± 1.2            | 25.9 ± 0.3               | 20.8 ± 1.4            |\n| QuantLM 1.5B 3-Bit                    | 4.2 ± 0.1              | 400.0 ± 10.6              | 60.9 ± 1.2            | 26.8 ± 0.3               | 20.8 ± 1.4            |\n| TriLM 1.5B                            | 5.9 ± 0.1              | 348.9 ± 9.9               | 59.9 ± 1.2            | 25.2 ± 0.3               | 21.7 ± 1.4            |\n| FloatLM 2.4B QuantLM 2.4B 8-Bit       | 20.7 ± 0.3 20.7 ± 0.3  | 360.4 ± 9.4 360.5 ± 9.4   | 64.2 ± 1.2 64.2 ± 1.2 | 26.7 ± 0.3               | 21.7 ± 1.4 21.9 ± 1.4 |\n| QuantLM 2.4B 6-Bit                    | 20.4 ± 0.3             | 360.8 ± 9.5               | 63.4 ± 1.2            | 26.5 ± 0.3 26.4 ± 0.3    | 21.8 ± 1.4 21.7 ± 1.4 |\n| QuantLM 2.4B 4-Bit                    | 21.1 ± 0.3             | 358.7 ± 9.6               | 63.4 ± 1.2            | 26.0 ± 0.3 ±             |                       |\n| QuantLM 2.4B 3-Bit                    | 10.9 ± 0.2             | 360.2 ± 9.5               | 59.9 ± 1.2            | 25.8 0.3                 | 21.5 ± 1.4            |\n| TriLM 2.4B                            | 12.3 ± 0.1             | 353.0 ± 10.0              | 64.1 ± 1.2            | 25.4 ± 0.3               | 23.0 ± 1.5            |\n| FloatLM 3.9B QuantLM 3.9B 8-Bit       | 21.5 ± 0.3 ±           | 359.2 ± 9.6 359.8 ± 9.6   | 64.6 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n|                                       |                        |                           | 64.7 ± 1.2            | 25.4 ± 0.3               | 23.6 ± 1.5            |\n| QuantLM 3.9B 6-Bit                    | 21.7 0.3 21.0 ± 0.3    | 359.5 ± 9.6               | 63.9 ± 1.2            | 25.4 ± 0.3               | 23.5 ± 1.5            |\n| QuantLM 3.9B 4-Bit                    | 17.9 ± 0.3             | 365.5 ± 9.7               | 64.8 ± 1.2            | 25.3 ± 0.3               | 24.2 ± 1.5            |\n| QuantLM 3.9B 3-Bit                    | 8.2 ± 0.2              | 365.9 ± 9.8               | 64.3 ± 1.2            | 25.5 ± 0.3               | 21.9 ± 1.4            |\n| TriLM 3.9B                            | 21.3 ± 0.3             | 362.4 ± 9.6               | 65.4 ± 1.2            | 25.9 ± 0.3               | 24.1 ± 1.5            |"
    },
    {
      "index": 12,
      "markdown": "| Models                                | Stem                  | Humanities            | MMLUAccuracy Social Sciences   | Other                 | Avg.                  |\n|---------------------------------------|-----------------------|-----------------------|--------------------------------|-----------------------|-----------------------|\n| FloatLM 99M                           | 22.8 ± 0.7            | 24.0 ± 0.6            | 27.0 ± 0.8                     | 28.0 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 8-Bit                     | 22.9 ± 0.7            | 24.2 ± 0.6            | 26.9 ± 0.8                     | 27.9 ± 0.8            | 25.3 ± 0.4            |\n| QuantLM 99M 6-Bit                     | 22.7 ± 0.7            | 24.1 ± 0.6            | 26.6 ± 0.8                     | 28.2 ± 0.8            | 25.2 ± 0.4            |\n| QuantLM 99M 4-Bit                     | 22.9 ± 0.7            | 24.1 ± 0.6            | 26.7 ± 0.8                     | 27.4 ± 0.8            | 25.1 ± 0.4            |\n| QuantLM 99M 3-Bit                     | 23.5 ± 0.8            | 23.9 ± 0.6            | 26.2 ± 0.8                     | 25.9 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 99M                             | 23.9 ± 0.8            | 23.6 ± 0.6            | 26.7 ± 0.8                     | 26.6 ± 0.8            | 25.0 ± 0.4            |\n| Binary 99M                            | 21.6 ± 0.7            | 24.3 ± 0.6            | 21.8 ± 0.7                     | 24.0 ± 0.7            | 23.1 ± 0.3            |\n| FloatLM 190M                          | 24.0 ± 0.8            | 24.4 ± 0.6            | 28.8 ± 0.8                     | 30.1 ± 0.8            | 26.5 ± 0.4            |\n| QuantLM 190M 8-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.9 ± 0.8                     | 30.0 ± 0.8            | 26.6 ± 0.4            |\n| QuantLM 190M 6-Bit                    | 24.1 ± 0.8            | 24.5 ± 0.6            | 28.3 ± 0.8                     | 29.8 ± 0.8            | 26.4 ± 0.4            |\n| QuantLM 190M 4-Bit                    | 22.9 ± 0.7            | 22.9 ± 0.6            | 24.5 ± 0.8                     | 23.4 ± 0.8            | 23.4 ± 0.4            |\n| QuantLM 190M 3-Bit                    | 23.9 ± 0.8            | 23.2 ± 0.6            | 25.4 ± 0.8                     | 27.5 ± 0.8            | 24.8 ± 0.4            |\n| TriLM 190M                            | 22.5 ± 0.7            | 23.8 ± 0.6            | 26.7 ± 0.8                     | 28.4 ± 0.8            | 25.2 ± 0.4            |\n| FloatLM 390M                          | 25.8 ± 0.8            | 25.9 ± 0.6            | 30.3 ± 0.8                     | 32.8 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 8-Bit                    | 25.7 ± 0.8            | 25.9 ± 0.6            | 30.2 ± 0.8                     | 32.4 ± 0.8            | 28.2 ± 0.4            |\n| QuantLM 390M 6-Bit                    | 26.0 ± 0.8            | 25.8 ± 0.6            | 30.2 ± 0.8                     | 32.3 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 390M 4-Bit                    | 25.5 ± 0.8            | 25.4 ± 0.6            | 30.5 ± 0.8                     | 31.6 ± 0.8            | 27.9 ± 0.4            |\n| QuantLM 390M 3-Bit                    | 24.4 ± 0.8            | 25.0 ± 0.6            | 29.4 ± 0.8                     | 29.3 ± 0.8            | 26.8 ± 0.4            |\n| TriLM 390M                            | 24.1 ± 0.8            | 24.8 ± 0.6            | 28.3 ± 0.8                     | 29.0 ± 0.8            | 26.4 ± 0.4            |\n| FloatLM 560M                          | 24.8 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 32.3 ± 0.8            | 28.4 ± 0.4            |\n| QuantLM 560M 8-Bit                    | 24.8 ± 0.8            | 26.6 ± 0.6            | 30.5 ± 0.8                     | 32.1 ± 0.8            | 28.3 ± 0.4            |\n| QuantLM 560M 6-Bit                    | 24.6 ± 0.8            | 26.7 ± 0.6            | 30.5 ± 0.8                     | 31.3 ± 0.8            | 28.1 ± 0.4            |\n| QuantLM 560M 4-Bit QuantLM 560M 3-Bit | 24.7 ± 0.8            | 25.9 ± 0.6            | 29.9 ± 0.8                     | 31.1 ± 0.8 28.2 ± 0.8 | 27.7 ± 0.4 26.0 ± 0.4 |\n| TriLM 560M                            | 24.5 ± 0.8 25.0 ± 0.8 | 24.2 ± 0.6 ±          | 28.1 ± 0.8 29.0 ± 0.8          | 30.2 ± 0.8            | 27.0 ± 0.4            |\n|                                       |                       | 25.1 0.6              |                                | 23.9 ± 0.7            | 22.9 ± 0.3            |\n| Binary 560M FloatLM 830M              | 21.4 ± 0.7 25.8 ± 0.8 | 24.2 ± 0.6 27.5 ± 0.6 | 21.6 ± 0.7 32.3 ± 0.8          | 34.6 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 8-Bit                    | 25.8 ± 0.8            | 27.4 ± 0.6            | 32.1 ± 0.8                     | 34.7 ± 0.8            | 29.7 ± 0.4            |\n| QuantLM 830M 6-Bit                    | 25.6 ± 0.8            | 27.3 ± 0.6            | 32.1 ± 0.8                     | 34.2 ± 0.8            | 29.5 ± 0.4            |\n| QuantLM 830M 4-Bit                    | 25.9 ± 0.8            | 26.8 ± 0.6            | 31.2 ± 0.8                     | 33.6 ± 0.8            | 29.1 ± 0.4            |\n| QuantLM 830M 3-Bit                    | 24.8 ± 0.8            | 25.1 ± 0.6            | 28.9 ± 0.8                     | 30.8 ± 0.8            | 27.1 ± 0.4            |\n| TriLM 830M                            | 24.9 ± 0.8            | 25.8 ± 0.6            | 30.1 ± 0.8                     | 31.1 ± 0.8            | 27.7 ± 0.4            |\n| FloatLM 1.1B                          | 26.4 ± 0.8            | 27.6 ± 0.6            | 32.5 ± 0.8                     | 34.8 ± 0.8            | 30.0 ± 0.4            |\n| QuantLM 1.1B 8-Bit                    | 26.2 ± 0.8            | 27.4 ± 0.6            | 32.5 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 6-Bit                    | 26.0 ± 0.8            | 27.5 ± 0.6            | 32.7 ± 0.8                     | 34.9 ± 0.8            | 29.9 ± 0.4            |\n| QuantLM 1.1B 4-Bit                    | 26.0 ± 0.8            | 26.6 ± 0.6            | 32.4 ± 0.8                     | 33.8 ± 0.8            | 29.3 ± 0.4            |\n| QuantLM 1.1B 3-Bit                    | 25.9 ± 0.8            | 26.1 ± 0.6            | 30.0 ± 0.8                     | 33.0 ± 0.8            | 28.4 ± 0.4            |\n| TriLM 1.1B                            | 25.2 ± 0.8            | 26.1 ± 0.6            | 30.6 ± 0.8                     | 32.2 ± 0.8            | 28.3 ± 0.4            |\n| Binary 1.1B                           | 21.0 ± 0.7            | 24.2 ± 0.6            | 21.7 ± 0.8                     | 24.4 ± 0.7            | 23.0 ± 0.3            |\n| FloatLM 1.5B                          | 26.1 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.6 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 8-Bit                    | 26.1 ± 0.8            | 28.1 ± 0.7            | 32.9 ± 0.8                     | 35.5 ± 0.8            | 30.3 ± 0.4            |\n| QuantLM 1.5B 6-Bit                    | 26.3 ± 0.8            | 28.0 ± 0.7            | 33.0 ± 0.8                     | 35.4 ± 0.8            | 30.4 ± 0.4            |\n| QuantLM 1.5B 4-Bit                    | 26.2 ± 0.8            | 28.1 ± 0.7            | 32.4 ± 0.8                     | 34.8 ± 0.8            | 30.1 ± 0.4            |\n| QuantLM 1.5B 3-Bit                    | 25.5 ± 0.8            | 26.7 ± 0.6            | 31.2 ± 0.8                     | 33.4 ± 0.8            | 28.9 ± 0.4            |\n| TriLM 1.5B                            | 25.7 ± 0.8            | 27.4 ± 0.6            | 31.5 ± 0.8                     | 34.6 ± 0.8            | 29.5 ± 0.4            |\n| FloatLM 2.4B                          | 26.9 ± 0.8            | 29.4 ± 0.7            | 34.2 ± 0.8                     | 38.1 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 8-Bit                    | 27.0 ± 0.8            | 29.4 ± 0.7            | 34.1 ± 0.8                     | 38.0 ± 0.9            | 31.8 ± 0.4            |\n| QuantLM 2.4B 6-Bit                    | 26.8 ± 0.8 ±          | 29.5 ± 0.7 28.8 ± 0.7 | 34.2 ± 0.8 34.3 ± 0.8          | 38.2 ± 0.9 38.1 ± 0.9 | 31.8 ± 0.4 ±          |\n| QuantLM 2.4B 4-Bit                    | 26.5 0.8              |                       |                                |                       | 31.5 0.4              |\n| QuantLM 2.4B 3-Bit TriLM 2.4B         | 25.5 ± 0.8 27.4 ± 0.8 | 27.1 ± 0.6 27.8 ± 0.6 | 32.3 ± 0.8 34.6 ± 0.9          | 36.4 ± 0.9 35.1 ± 0.8 | 29.9 ± 0.4 30.8 ± 0.4 |\n| FloatLM 3.9B                          | 27.7 ± 0.8            | 30.6 ± 0.7            | 36.9 ± 0.9                     | 39.8 ± 0.9            | 33.3 ± 0.4            |\n| QuantLM 3.9B 8-Bit                    | 27.6 ± 0.8            | 30.7 ± 0.7            | 37.0 ± 0.9                     | 39.7 ± 0.9            | 33.4 ± 0.4            |\n| QuantLM 3.9B 6-Bit                    | 27.3 ± 0.8            | 30.3 ± 0.7            | 36.9 ± 0.9                     | 39.3 ± 0.9            | 33.1 ± 0.4            |\n| QuantLM 3.9B 4-Bit QuantLM 3.9B 3-Bit | 27.1 ± 0.8 27.3 ± 0.8 | 30.3 ± 0.7 28.4 ± 0.7 | 36.3 ± 0.9 34.3 ± 0.9          | 38.8 ± 0.9 37.2 ± 0.9 | 32.8 ± 0.4 31.4 ± 0.4 |\n| TriLM 3.9B                            | 28.3 ±                | 29.5 ± 0.7            | 35.4 ± 0.9                     | 39.6 ±                | 32.8 ± 0.4            |\n|                                       | 0.8                   |                       |                                | 0.9                   |                       |"
    }
  ],
  "stats": {
    "pages": 41,
    "chunksCreated": 234,
    "totalCharacters": 169321,
    "totalWords": 25444,
    "numTables": 13,
    "processingTimeMs": 110474
  }
}