{
  "paper": {
    "id": "2306.08543v5",
    "title": "MiniLLM: Knowledge Distillation of Large Language Models",
    "abstract": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
    "authors": [
      "Yuxian Gu",
      "Li Dong",
      "Furu Wei",
      "Minlie Huang"
    ],
    "published": "2023-06-14T14:44:03.000Z",
    "updated": "2025-11-21T10:20:53.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2306.08543v5",
    "absUrl": "https://arxiv.org/abs/2306.08543v5"
  },
  "chunks": [
    {
      "id": "2306.08543v5-chunk-0",
      "content": "Yuxian Gu 1 , 2 ∗ , Li Dong 2 , Furu Wei 2 , Minlie Huang 1 †\n\n1 The CoAI Group, Tsinghua University 2 Microsoft Research guyx21@mails.tsinghua.edu.cn {lidong1,fuwei}@microsoft.com aihuang@tsinghua.edu.cn",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "MiniLLM: Knowledge Distillation of Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-1",
      "content": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MINILLM .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-2",
      "content": "rom overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MINILLM . Extensive experiments in the instruction-following setting show that MINILLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm .\n\nFigure 1: The comparison of MINILLM with the sequence-level KD (SeqKD; KR16, TGZ + 23, CLL + 23, PLH + 23, GWS + 23, ZLX + 23) in terms of the average GPT-4 feedback score on our evaluation sets. Left : GPT-2-1.5B as the teacher model and GPT-2 125M, 340M, 760M as the student models.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-3",
      "content": "3, CLL + 23, PLH + 23, GWS + 23, ZLX + 23) in terms of the average GPT-4 feedback score on our evaluation sets. Left : GPT-2-1.5B as the teacher model and GPT-2 125M, 340M, 760M as the student models. Middle : GPT-J 6B as the teacher model and GPT-2 760M, 1.5B, GPTNeo 2.7B as the student models. Right : OPT 13B as the teacher and OPT 1.3B, 2.7B, 6.7B as the student models.\n\n<!-- image -->\n\n∗ Contribution during an internship at Microsoft Research.\n\n† Corresponding author.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Abstract",
        "chunkIndex": 3,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-4",
      "content": "With the rapid development of large language models (LLMs; BMR + 20, HZD + 21, BHA + 21, CND + 22, Ope23), a common technique to reduce their high computational resource demand is knowledge distillation (KD; HVD15), where we train a small student model with supervision from a large teacher model. Two categories of KD are commonly applied: black-box KD, where only the teacher-generated texts are accessible, and white-box KD, where the teacher model's output distribution or intermediate hidden states are also available [JBMD21]. Recently, black-box KD has shown promising results in fine-tuning small models on the prompt-response pairs generated by LLM APIs [TGZ + 23, CLL + 23, WWZ + 23, PLH + 23]. With the emergence of more open-source LLMs [ZRG + 22, TLI + 23], white-box KD becomes more valuable for both research communities and industry sectors because student models receive better signals from the output distribution and hidden states of teacher models, thereby potentially resulting i",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-5",
      "content": "uable for both research communities and industry sectors because student models receive better signals from the output distribution and hidden states of teacher models, thereby potentially resulting in higher performance. However, white-box KD approaches are mostly studied for small ( &lt; 1B parameters) language understanding models [SDCW19, WWD + 20], while white-box KD for LLMs is yet to be explored.\n\nIn this work, we investigate white-box KD of LLMs where the output distribution of the teacher model is available. We argue that the standard KD objectives [KR16, SST + 20, CLL + 23, TGZ + 23] are sub-optimal for LLMs that perform tasks in a generative manner. Given the teacher distribution p ( y | x ) and the student distribution q θ ( y | x ) parameterized by θ , standard KD objectives (including several variants for sequence-level models) essentially minimize the approximated forward Kullback-Leibler divergence (KLD) between the teacher and the student distribution, termed as KL[ p",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-6",
      "content": "including several variants for sequence-level models) essentially minimize the approximated forward Kullback-Leibler divergence (KLD) between the teacher and the student distribution, termed as KL[ p || q θ ] , which forces q θ to cover all modes of p . For text classification tasks, KL[ p || q θ ] works well because the output space usually consists of a finite number of classes such that both p ( y | x ) and q θ ( y | x ) have few modes. However, for open-ended text generation tasks, which is usually the case of LLM applications, the output spaces are much more complex and p ( y | x ) can contain many more modes than what q θ ( y | x ) can express due to the limited model capacity. Minimizing forward KLD causes q θ to assign unreasonably high probabilities to the void regions of p [MG19] and produces very unlikely samples under p during free-run generation [Hus15].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-7",
      "content": "del capacity. Minimizing forward KLD causes q θ to assign unreasonably high probabilities to the void regions of p [MG19] and produces very unlikely samples under p during free-run generation [Hus15].\n\nTo alleviate this problem, we propose to minimize reverse KLD, KL[ q θ || p ] , widely used in computer vision [LPSK23] and reinforcement learning [CPO + 19]. Compared to KL[ p || q θ ] , minimizing KL[ q θ || p ] causes q θ to seek the major modes of p , and assign low probabilities to p 's void regions [M + 05], as illustrated in Figure 2 and discussed in Section 2.1. In text generation, this means that the student avoids learning too many longtail variants [HBD + 20] in the teacher's distribution to focuses on the generation correctness, which is critical in practical scenarios that require truthfulness and reliabil-\n\nFigure 2: The toy experiment. We fit a Gaussian mixture distribution with a single Gaussian distribution using forward KLD and reverse KLD.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-8",
      "content": "ios that require truthfulness and reliabil-\n\nFigure 2: The toy experiment. We fit a Gaussian mixture distribution with a single Gaussian distribution using forward KLD and reverse KLD.\n\n<!-- image -->\n\nity [JLF + 23]. To optimize min θ KL[ q θ || p ] , as shown in Section 2.2, we derive its gradient with Policy Gradient [SMSM99] and adopt an on-policy training approach. To further stabilize and accelerate training, we propose (1) single-step decomposition to reduce variance, (2) teacher-mixed sampling to alleviate reward hacking, and (3) length normalization to eliminate the length bias. Finally, we introduce the overall KD algorithm in Section 2.3. Our student models are named MINILLM , indicating our method is suitable for compressing large (generative) language models.\n\nWe apply our method to various generative language models [RWC + 19, ZRG + 22, TLI + 23] with sizes ranging from 120M to 13B in the instruction-following setting [SWR + 22, WBZ + 22] that covers a large range of NLP",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-9",
      "content": "hod to various generative language models [RWC + 19, ZRG + 22, TLI + 23] with sizes ranging from 120M to 13B in the instruction-following setting [SWR + 22, WBZ + 22] that covers a large range of NLP tasks. We use 5 datasets with Rouge-L [Lin04], the GPT-4 feedback, and human judgment for evaluation. Experiments show that MINILLM consistently outperforms standard KD baselines on all the datasets and scales up well from 120M to 13B models (see Figure 1). More analysis shows that MINILLM yields lower exposure bias, better calibration, and higher long response generation performance, with neglectable loss of diversity.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-10",
      "content": "We consider conditional text generation where the model produces a response y = { y t } T t =1 conditioning on a prompt x sampled from the distribution p x , which is typically how LLMs perform tasks.\n\nFigure 3: Comparison between sequence-level KD (left) and MINILLM (right). Sequence-level KD forces the student to memorize all samples generated by the teacher model, while MINILLM improves its generated texts with the teacher model's feedback.\n\n<!-- image -->\n\nWe formulate KD as an optimization problem to minimize the difference between a fixed teacher model distribution p ( y | x ) and a student model distribution q θ ( y | x ) parameterized by θ . The standard KD methods approximately 3 minimize the forward KLD: KL[ p || q θ ] = E x ∼ p x , y ∼ p ′ log p ( y | x ) q θ ( y | x ) , where p ′ can be real data distribution (word-level KD) or teacher distribution p (sequence-level KD).",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2 Method",
        "chunkIndex": 10,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-11",
      "content": "3 minimize the forward KLD: KL[ p || q θ ] = E x ∼ p x , y ∼ p ′ log p ( y | x ) q θ ( y | x ) , where p ′ can be real data distribution (word-level KD) or teacher distribution p (sequence-level KD). Though widely used, KL[ p || q θ ] tends to overestimate the void regions of p in text generation tasks when q θ is insufficiently expressive [JKH + 23]. KD for LLMs fits the case because LLMs perform tasks in a generative manner, such that the low-capacity student models cannot perfectly imitate the complex text generation distribution of the teacher models or humans.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2 Method",
        "chunkIndex": 11,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-12",
      "content": "We consider minimizing the reverse KLD between the student and teacher model distributions as the learning objective for MINILLM:\n\n<!-- formula-not-decoded -->\n\nMinimizing reverse KLD has been shown to cause the mode-seeking behavior in generative modeling [Hus15, NCT16, CDP + 18, LPSK23], where q θ assigns high probabilities to p 's large modes and ignore the small ones (illustrated in a toy experiment in Figure 2). In this work, we first study this property for KD of LLMs in text generation. Minimizing forward KLD causes q θ to place large probability masses on the zero-probability regions of p , corresponding to the generation of lowquality text in practice, while reverse KLD focuses on p 's major modes, which is crucial to ensure the correctness and faithfulness of text generation. As illustrated in Figure 3, unlike sequence-level KD that minimizes forward KLD [KR16, TGZ + 23], MINILLM that minimizes reverse KLD does not force q θ to fit all y sampled from the teacher distribution",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.1 MINILLM: Knowledge Distillation with Reverse KLD",
        "chunkIndex": 12,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-13",
      "content": "illustrated in Figure 3, unlike sequence-level KD that minimizes forward KLD [KR16, TGZ + 23], MINILLM that minimizes reverse KLD does not force q θ to fit all y sampled from the teacher distribution p . Instead, it encourages the student to generate samples preferred by the teacher within its own capacities, which is more possible to achieve. Interestingly, we also find another perspective of understanding MINILLM motivated by Inverse Reinforcement Learning [ZMB + 08]. We present the related derivation in Appendix A.1.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.1 MINILLM: Knowledge Distillation with Reverse KLD",
        "chunkIndex": 13,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-14",
      "content": "Gradient Derivation We notice that the gradient of the objective function L ( θ ) in Equation (1) can be derived using the Policy Gradient Theorem [Wil92, HTAL17] for on-policy optimization:\n\n<!-- formula-not-decoded -->\n\nwhere T = | y | and R t = ∑ T t ′ = t log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) is the accumulation of r t ′ = log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) that measures the quality of each step's generation. Intuitively, the generated texts are supposed to have high probabilities under the teacher distribution by increasing p ( y t ′ | y &lt;t ′ , x ) , but simultaneously stay diverse by lowering q θ ( y t ′ | y &lt;t ′ , x ) . The expectation in Eq. 2 is computed by Monte-Carlo sampling. Full derivation can be found in Appendix A.2. However, policy gradient suffers from high variance and reward hacking [SHKK22], despite some subsequent solutions [SWD + 17].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.2 On-Policy Distillation",
        "chunkIndex": 14,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-15",
      "content": "ed by Monte-Carlo sampling. Full derivation can be found in Appendix A.2. However, policy gradient suffers from high variance and reward hacking [SHKK22], despite some subsequent solutions [SWD + 17]. Besides, we notice that R t favors short sentences, which causes the student model to output empty responses. Therefore, we propose three strategies to mitigate these problems.\n\n3 Wesay 'approximately' because for word-level KD, y is sampled from the real distribution, not the teacher distribution. For a strong enough teacher model, we can consider the two distributions approximately the same.\n\nSingle-Step Decomposition [CPO + 19] has found that the single-step generation quality r t is critical to the training variance because the error in the front tokens accumulates along the whole sentence. To pay more attention to r t , we re-write ∇L ( θ ) to decompose r t from R t and directly compute the gradient of E y t ∼ q θ ( t ) [ r t ] (see Appendix A.3 for the full derivation):",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.2 On-Policy Distillation",
        "chunkIndex": 15,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-16",
      "content": "whole sentence. To pay more attention to r t , we re-write ∇L ( θ ) to decompose r t from R t and directly compute the gradient of E y t ∼ q θ ( t ) [ r t ] (see Appendix A.3 for the full derivation):\n\n<!-- formula-not-decoded -->\n\nwhere q θ ( t ) = q θ ( ·| y &lt;t , x ) . Note that E y t ∼ q θ ( t ) [ r t ] can be computed directly by summing over the vocabulary instead of using Monte-Carlo sampling and is derivable with respect to θ . This decomposition gives a more precise and efficient estimation of the single-step generation quality, which reduces the variance during training and accelerates convergence.\n\nTeacher-Mixed Sampling We observe reward hacking [SHKK22] when training with Eq. 2 because q θ sometimes produces degenerated sentences y that receive high scores from the teacher (e.g., repeated phrases) during sampling, especially for small student models. To create a better sampling distribution, we mix the teacher and the student distribution at each time step:",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.2 On-Policy Distillation",
        "chunkIndex": 16,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-17",
      "content": "the teacher (e.g., repeated phrases) during sampling, especially for small student models. To create a better sampling distribution, we mix the teacher and the student distribution at each time step:\n\n<!-- formula-not-decoded -->\n\nwhere α controls the strength of the teacher mix-in. Sampling from ˜ p suppresses low-quality generation with the teacher's help and alleviates reward hacking. We re-write ( ∇L ) Single and ( ∇L ) Long with importance sampling to get to an unbiased estimator of the gradient [PSS00]:\n\n<!-- formula-not-decoded -->\n\nwhere w t = ∏ t t ′ =1 q θ ( y t ′ | y &lt;t ′ , x ) ˜ p ( y t ′ | y &lt;t ′ , x ) is the importance weight. However, w t brings high variance in practice because it requires multiplying per-token importance weight over multiple time steps, and thus the variance of each step accumulates. Therefore, we approximately set w t ≈ q θ ( y t | y &lt;t , x ) ˜ p ( y t | y &lt;t , x ) to reduce the variance of the estimator in Eq. 5 [SSG + 17, LKTF20].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.2 On-Policy Distillation",
        "chunkIndex": 17,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-18",
      "content": "us the variance of each step accumulates. Therefore, we approximately set w t ≈ q θ ( y t | y &lt;t , x ) ˜ p ( y t | y &lt;t , x ) to reduce the variance of the estimator in Eq. 5 [SSG + 17, LKTF20].\n\nLength Normalization We found that long sequences tend to have small R t +1 , which encourages the model to produce short responses. Therefore, we add length normalization to R t +1 in Eq. 3:\n\n<!-- formula-not-decoded -->\n\nIn Summary Combining the strategies listed above, we have the final optimization gradient:\n\n<!-- formula-not-decoded -->\n\nwhere V is the vocabulary size of the language model and ( ∇L ) Norm Long is ( ∇L ) Long with R Norm t +1 .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.2 On-Policy Distillation",
        "chunkIndex": 18,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-19",
      "content": "We start from a student model pre-trained on a large long-document corpus D PT. Algorithm 2.3 trains MINILLM by adapting the student model to a text generation task with dataset D and supervision from the teacher model, such as an LLM fine-tuned on D [TGZ + 23, CLL + 23] or that with good task-generalization [CHL + 22, Ope23]. In the training algorithm, we first fine-tune the student model on D and pick the checkpoint with the lowest loss as an initialization for the following training. Then, we compute the gradients ( ∇L ) Single and ( ∇L ) Norm Long based on Eq. 5 and Eq.\n\n6, with a clipping strategy [SWD + 17] added to further improve stability. Same as [OWJ + 22], we include a language modeling loss L PT = -E d ∼D PT log q θ ( d ) to preserve the model performance on canonical NLP benchmarks. The student model is finally updated using a combination of gradients ( ∇L ) Single +( ∇L ) Norm Long + ∇L PT.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.3 Training Algorithm",
        "chunkIndex": 19,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-20",
      "content": "= -E d ∼D PT log q θ ( d ) to preserve the model performance on canonical NLP benchmarks. The student model is finally updated using a combination of gradients ( ∇L ) Single +( ∇L ) Norm Long + ∇L PT. The whole on-policy training pipeline is similar to Reinforcement Learning from Human Feedback (RLHF; OWJ + 22).",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "2.3 Training Algorithm",
        "chunkIndex": 20,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-21",
      "content": "Input: Conditional generation dataset D consisting of prompts and ground-truth responses\n\nPre-training corpus D PT consisting of long-document plain texts\n\nA teacher model with output distribution p\n\nAn initial student model pre-trained on D PT, with the output distribution q θ 0\n\nLearning rate η ; Batch size M ; Clipping Threshold ϵ\n\nOutput: A student model with the output distribution q θ\n\nFine-tune the student model from θ 0 on D supervised by the ground truth responses and choose θ with the lowest validation loss.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Algorithm 1 MINILLM: Knowledge Distillation of LLMs",
        "chunkIndex": 21,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-22",
      "content": "```\nSample a mini-batch of prompts from D and collect responses from ˜ p to get S = { ( x m , y m ) } M m =1 Sample a mini-batch D ′ PT = { d m } M m =1 from D PT Compute ( ∇L ) Single = -1 M ∑ x , y ∈S ∑ T t =1 w t ∇ ∑ y t ∈ V q θ ( y t | y <t , x ) log p ( y t | y <t , x ) q θ ( y t | y <t , x ) ▷ Eq. 5 Compute ( ∇L ) Norm Long = -1 | M | ∑ x , y ∈S ∑ T t =1 R Norm t +1 ∇ min[ ρ t ( θ ) , clip( ρ t ( θ ) , 1 -ϵ, 1 + ϵ )] , where ρ t ( θ ) = q θ ( y t | y <t , x ) ˜ p ( y t | y <t , x ) ▷ Eq. 5, Eq. 6 Compute the gradient of the language modeling loss: ∇L PT = -1 M ∑ d ∈ D ′ PT ∇ log q θ ( d ) Update model parameters: θ ← θ -η [ ( ∇L ) Single +( ∇L ) Norm Long + ∇L PT ]\n```\n\nuntil converge and return q θ",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "repeat",
        "chunkIndex": 22,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-23",
      "content": "We take instruction-following [OWJ + 22] as the conditional text generation task, where models are trained to generate responses according to the instructions. We fine-tune a large model on the dataset D consisting of instruction-response pairs as the teacher model. Then, we compare different KD methods on D by evaluating the student model's instruction-following performance.\n\nBase Models Our student models come from three model families with various sizes: GPT2 [RWC + 19] (120M, 340M, 760M), OPT [ZRG + 22] (1.3B, 2.7B, 6.7B), and LLaMA [TLI + 23] (7B). For teacher models of each model family, we use GPT-2-1.5B, OPT-13B, and LLaMA-13B respectively. These models are fine-tuned on D in advance. We also present the results using GPTJ [WK21] as the teacher model in Appendix C.1.\n\nTraining We construct the training data from databricks-dolly-15K 4 consisting of 15K human-written instruction-response pairs. We filter out samples that exceed the context length of the models.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 23,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-24",
      "content": ".1.\n\nTraining We construct the training data from databricks-dolly-15K 4 consisting of 15K human-written instruction-response pairs. We filter out samples that exceed the context length of the models. Then, we randomly split 1K and 0.5K samples for validation and testing, respectively, leaving about 12.5K examples for training. For D PT, we use OpenWebText [GCPT19] for the GPT-2 family and the RoBERTa training corpus [LOG + 19] for other models. We set the teacher-mix-in strength α = 0 . 2 throughout the experiments in Eq. 4. We use Rouge-L [Lin04] scores on the validation set to search for hyper-parameters because it aligns better with human preference than validation losses [WMA + 22]. More details are shown in Appendix B.1.\n\nEvaluation We evaluate the trained models on five instruction-following datasets:",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 24,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-25",
      "content": "use it aligns better with human preference than validation losses [WMA + 22]. More details are shown in Appendix B.1.\n\nEvaluation We evaluate the trained models on five instruction-following datasets:\n\n- DollyEval : the 500-sample test set we split from the databricks-dolly-15k dataset.\n- SelfInst [WKM + 23]: A user-oriented instruction-following set with 252 samples.\n- VicunaEval [CLL + 23]: The 80 challenging questions used in the Vicuna evaluation.\n- S-NI : The test set of SUPER-NATURALINSTRUCTIONS [WMA + 22] consisting of 9K samples ranging from 119 tasks. Following [PLH + 23], we split the set into 3 subsets whose ground truth\n\n4 https://github.com/databrickslabs/dolly/tree/master\n\nresponse lengths lie in [0 , 5] , [6 , 10] , and [11 , + ∞ ] . We use the [11 , + ∞ ] subset in Section 3.2 and conduct an analysis on all subsets in Section 3.3.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 25,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-26",
      "content": "com/databrickslabs/dolly/tree/master\n\nresponse lengths lie in [0 , 5] , [6 , 10] , and [11 , + ∞ ] . We use the [11 , + ∞ ] subset in Section 3.2 and conduct an analysis on all subsets in Section 3.3.\n\n- UnNI : We randomly sample 10K samples from the core set of UNNATURALINSTRUCTIONS [HSLS23] for evaluation. Similar to S-NI , we first conduct the evaluations on the [11 , + ∞ ] subset, followed by an analysis of the performance on all subsets in Appendix C.3.\n\nWe adopt three metrics to evaluate the model-generated responses:\n\n- R-L : The Rouge-L [Lin04] score to measure the precision of the model generation. [WMA + 22] has shown that Rouge-L is suitable for large-scale instruction-following evaluation.\n- GPT4 : The GPT-4 feedback [ZCS + 23] by asking GPT-4 to compare model-generated responses with the ground truth answers 5 and raise 1-10 scores for both responses (see Appendix B.2 for the prompt we use).",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 26,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-27",
      "content": ": The GPT-4 feedback [ZCS + 23] by asking GPT-4 to compare model-generated responses with the ground truth answers 5 and raise 1-10 scores for both responses (see Appendix B.2 for the prompt we use). We report the ratio of the total score of model responses and ground truth answers. This metric is only applied to DollyEval, SelfInst, and VicunaEval.\n- HumanEvaluation : Weconduct human evaluations on the SelfInst dataset following [PLH + 23] by asking volunteers to compare two responses produced by different models and annotate 'Win', 'Tie', or 'Loss'. More human evaluation details can be found in Appendix B.3.\n\nFor all test sets, we sample the responses with the temperature = 1 and report the average scores of 5 generations for each prompt with different random seeds.\n\nBaselines We consider three baselines in our main experiment:",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 27,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-28",
      "content": "sample the responses with the temperature = 1 and report the average scores of 5 generations for each prompt with different random seeds.\n\nBaselines We consider three baselines in our main experiment:\n\n- SFT w/o KD directly fine-tunes the student model on D supervised by the golden responses.\n- KD [SDCW19, SST + 20] fine-tunes the student model on D using the teacher distribution as the supervision at each token step, also known as word-level KD.\n- SeqKD [KR16, CLL + 23, TGZ + 23, PLH + 23, ZLX + 23] fine-tunes the student model on the data generated by the teacher model.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.1 Experimental Setup",
        "chunkIndex": 28,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-29",
      "content": "We present the R-L and GPT4 evaluation results in Table 1, from which we have three observations.\n\nFirst , by comparing the overall performance of MINILLM with the baselines, we observe that the model distilled by our KD method outperforms the baselines in almost all cases, when trained with different base models, tested on various evaluation sets, and scored by both Rouge-L and GPT-4 feedback. This verifies the good generalization and high overall performance of our KD method. We also find that MINILLM generally works much better on datasets other than Dolly compared with the baselines, indicating its good out-of-distribution generalization.\n\nSecond , the Rouge-L scores show that MINILLM produces the most precise responses that have high overlaps with the ground-truth responses. In some cases, especially on Vicuna, S-NI, and UnNI, student models reach even higher Rouge-L scores than the teacher models, which matches the observation in [FLT + 18].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 29,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-30",
      "content": "h the ground-truth responses. In some cases, especially on Vicuna, S-NI, and UnNI, student models reach even higher Rouge-L scores than the teacher models, which matches the observation in [FLT + 18]. We conjecture that the standard teacher-forcing fine-tuning on D brings training-inference discrepancy to the teacher model, also known as exposure bias [BVJS15]. On the contrary, MINILLM is optimized with policy optimization methods, which samples responses from student models during training and thus alleviates exposure bias [PH21]. We include further analysis on exposure bias in Section 3.3.\n\nThird , comparing the results across model sizes and model families, we can see that the improvement of MINILLM is consistent when the base model sizes vary from 120M to 13B across three model families. This tendency is also illustrated in Figure 1, which demonstrates the excellent scalability and generalization of our method in the era of LLMs.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 30,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-31",
      "content": "izes vary from 120M to 13B across three model families. This tendency is also illustrated in Figure 1, which demonstrates the excellent scalability and generalization of our method in the era of LLMs.\n\nThe human evaluation results on the SelfInst dataset based on the LLaMA family are shown in Figure 4. MINILLM obtains better human preference than all the baselines, performing comparably to the teacher model.\n\nFigure 4: Human evaluation results. We use LLaMA-7B as the student and LLaMA-13B as the teacher.\n\n<!-- image -->\n\n5 We use the ChatGPT's generation [Ope22] for VicunaEval's ground truth responses.\n\nTable 1: Evaluation results. GPT4 and R-L stand for the average GPT-4 feedback scores and RougeL scores across 5 random seeds, respectively. The best scores of each model size are boldfaced , and the scores where the student model outperforms the teacher are marked with *.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 31,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-32",
      "content": "dback scores and RougeL scores across 5 random seeds, respectively. The best scores of each model size are boldfaced , and the scores where the student model outperforms the teacher are marked with *.\n\n| Model   | #Params   | Method        | DollyEval   | DollyEval   | SelfInst   | SelfInst   | VicunaEval GPT4   | VicunaEval GPT4   | S-NI R-L   | UnNI       |\n|---------|-----------|---------------|-------------|-------------|------------|------------|-------------------|-------------------|------------|------------|\n|         |           |               | GPT4        | R-L         | GPT4       | R-L        |                   | R-L               |            | R-L        |\n|         | 1.5B      | Teacher       | 58.4        | 27.6        | 42.9       | 14.3       | 48.6              | 16.3              | 27.6       | 31.8       |\n|         |           | SFT w/o KD    | 38.6        | 23.3        | 26.3       | 10.0       | 32.8              | 14.7              | 16.3       | 18.5",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 32,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-33",
      "content": "| 27.6       | 31.8       |\n|         |           | SFT w/o KD    | 38.6        | 23.3        | 26.3       | 10.0       | 32.8              | 14.7              | 16.3       | 18.5       |\n|         |           | KD            | 40.3        | 22.8        | 27.8       | 10.8       | 31.9              | 13.4              | 19.7       | 22.0       |\n|         | 120M      | SeqKD         | 41.2        | 22.7        | 26.2       | 10.1       | 31.0              | 14.3              | 16.4       | 18.8       |\n|         |           | MINILLM       | 44.7        | 24.6        | 29.2       | 13.2       | 34.1              | 16.9 *            | 25.3       | 26.6       |\n| GPT-2   |           | SFT w/o KD    | 51.9        | 25.5        | 39.6       | 13.0       | 42.3              | 16.0              | 25.1       | 32.0       |\n|         |           | KD            | 51.6        | 25.0        | 39.2       | 12.0       | 42.8              | 15.4              | 23.7       | 31.0",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 33,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-34",
      "content": "| 25.1       | 32.0       |\n|         |           | KD            | 51.6        | 25.0        | 39.2       | 12.0       | 42.8              | 15.4              | 23.7       | 31.0       |\n|         | 340M      | SeqKD         | 50.5        | 25.3        | 39.0       | 12.6       | 43.0              | 16.9*             | 22.9       | 30.2       |\n|         |           | MINILLM       | 52.2        | 25.4        | 40.5       | 15.6       | 42.6              | 17.7 *            | 27.4       | 34.5       |\n|         |           | SFT w/o KD    | 50.7        | 25.4        | 38.3       | 12.4       | 43.1              |                   | 21.5       | 27.1       |\n|         |           | KD            | 53.4        | 25.9        | 40.4       | 13.4       |                   | 16.1 16.9*        | 25.3       | 31.7       |\n|         | 760M      | SeqKD         | 52.0        | 25.6        | 38.9       | 14.0       | 43.4              | 15.9              | 26.1       | 32.9",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 34,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-35",
      "content": "16.9*        | 25.3       | 31.7       |\n|         | 760M      | SeqKD         | 52.0        | 25.6        | 38.9       | 14.0       | 43.4              | 15.9              | 26.1       | 32.9       |\n|         |           | MINILLM       | 54.7        | 26.4        | 44.6*      | 15.9       | 42.4 45.7         | 18.3*             | 29.3*      | 37.7*      |\n|         |           | Teacher       | 70.3        | 29.2        | 56.1       | 18.4       | 58.0              | 17.8              | 30.4       | 36.1       |\n|         | 13B       | SFT w/o KD    | 52.6        | 26.0        | 37.7       | 11.4       | 40.5              | 15.6              |            | 28.4       |\n|         |           | KD            | 52.7        | 25.4        | 36.0       | 12.2       | 40.8              | 14.9              | 23.1 21.9  | 27.0       |\n|         | 1.3B      | SeqKD         | 51.0        | 26.1        | 36.6       | 12.7       | 42.6              | 16.6              | 21.4       | 28.2",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 35,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-36",
      "content": "| 23.1 21.9  | 27.0       |\n|         | 1.3B      | SeqKD         | 51.0        | 26.1        | 36.6       | 12.7       | 42.6              | 16.6              | 21.4       | 28.2       |\n|         |           |               |             |             | 47.0       | 14.8       |                   | 17.9*             | 28.6       | 33.4       |\n|         |           | MINILLM       | 60.7        | 26.7        | 38.9       | 13.9       | 50.6              | 16.6              | 24.9       | 32.3       |\n| OPT     |           | SFT w/o KD    | 55.4        | 27.1        | 48.6       | 13.8       | 44.8              | 16.7              | 26.3       | 30.2       |\n|         |           | KD            | 60.5        | 25.9        | 40.5       | 13.3       | 51.3              |                   |            |            |\n|         | 2.7B      | SeqKD         | 57.6        | 27.5 27.4   | 52.7       | 17.2       | 44.5 55.9         | 16.5 19.1*        | 25.3       | 32.3",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 36,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-37",
      "content": "|            |            |\n|         | 2.7B      | SeqKD         | 57.6        | 27.5 27.4   | 52.7       | 17.2       | 44.5 55.9         | 16.5 19.1*        | 25.3       | 32.3       |\n|         |           | MINILLM       | 63.2        | 27.6        | 56.4       | 16.4       | 57.3              | 17.8              | 30.7* 30.3 | 35.1 28.6  |\n|         |           | SFT w/o KD    | 67.9        |             |            |            |                   |                   |            |            |\n|         |           | KD            | 68.6        | 28.3        | 58.0       | 17.0       | 57.0              | 17.5              | 30.7*      |            |\n|         | 6.7B      | SeqKD         | 69.6        | 28.5        | 54.0       | 17.0       | 57.6              | 17.9*             | 30.4       | 26.7 28.2  |\n|         |           | MINILLM       | 70.8*       | 29.0        | 58.5*      | 17.5       | 60.1*             | 18.7*             | 32.5*      | 36.7*",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 37,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-38",
      "content": "*             | 30.4       | 26.7 28.2  |\n|         |           | MINILLM       | 70.8*       | 29.0        | 58.5*      | 17.5       | 60.1*             | 18.7*             | 32.5*      | 36.7*      |\n|         | 13B       | Teacher       | 79.0        | 29.7        | 75.5       | 23.4       | 65.1              | 19.4              | 35.8       | 38.5       |\n| LLaMA   |           | SFT w/o KD    | 73.0        | 26.3        | 69.2       | 20.8       | 61.6              | 17.5              | 32.4       | 35.8       |\n|         |           | SeqKD MINILLM | 73.6 76.4   | 27.5 29.0   | 71.5 73.1  | 20.8 23.2  | 62.6 64.1         | 18.1 20.7*        | 33.7 35.5  | 37.6 40.2* |",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.2 Results",
        "chunkIndex": 38,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-39",
      "content": "Scaling Law of Teacher Although it is intuitive that we can distill better student models from larger teacher models, [MFL + 20] has shown that increasing the teacher models' sizes does not guarantee the improvement of student models, sometimes even harming the distillation performance. It is not clear how MINILLM works when we scale up the teacher models' sizes. Therefore, we compare MINILLM and SeqKD using teacher models with different sizes and fix the size of the student model. We present the results based on the GPT-2 family in Figure 5 and that based on the OPT family in Appendix C.2. We can see that MINILLM constantly outperforms SeqKD, and the student model performance is positively correlated with the teacher model sizes. This shows the potential of our method to compress models with massive parameters.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 39,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-40",
      "content": "onstantly outperforms SeqKD, and the student model performance is positively correlated with the teacher model sizes. This shows the potential of our method to compress models with massive parameters.\n\nFigure 5: The scaling law of teacher based on the GPT-2 family models. We compare MINILLM and SeqKD with GPT-2-125M as the student and GPT-2 340M, 760M, and 1.5B as teachers.\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 6: The excess error caused by the trainingdecoding discrepancy (ExAccErr) accumulated with the generation length. Lower ExAccErr means the method introduces less exposure bias.\n\n<!-- image -->\n\nFigure 7: The Rouge-L scores of the distilled models against SFT on the different subsets of SNI split by the golden responses' length.\n\nTable 2: The ECE and accuracy scores on SST2 and BoolQ datasets. The best scores among student models are boldfaced .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 40,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-41",
      "content": "gainst SFT on the different subsets of SNI split by the golden responses' length.\n\nTable 2: The ECE and accuracy scores on SST2 and BoolQ datasets. The best scores among student models are boldfaced .\n\n|         | SST2   | SST2   | BoolQ   | BoolQ   |\n|---------|--------|--------|---------|---------|\n|         | ECE    | Acc.   | ECE     | Acc.    |\n| Teacher | 0.025  | 93.0   | 0.356   | 74.5    |\n| KD      | 0.191  | 84.7   | 0.682   | 63.5    |\n| SeqKD   | 0.243  | 66.5   | 0.681   | 62.8    |\n| MINILLM | 0.099  | 89.7   | 0.502   | 67.8    |\n\nTable 3: The distinct 4-grams (Dist-4) and language modeling loss (Loss) on the test sets based on the LLaMA family. MINILLM preserves generation diversity.\n\n|         | DollyEval   | DollyEval   | SelfInst   | SelfInst   |\n|---------|-------------|-------------|------------|------------|\n|         | Dist-4      | Loss        | Dist-4     | Loss       |\n| Teacher | 99.3        | 3.55        | 99.1       | 4.44       |\n| SFT     | 99.5        |",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 41,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-42",
      "content": "-|-------------|------------|------------|\n|         | Dist-4      | Loss        | Dist-4     | Loss       |\n| Teacher | 99.3        | 3.55        | 99.1       | 4.44       |\n| SFT     | 99.5        | 3.89        | 99.0       | 5.28       |\n| MINILLM | 99.0        | 3.95        | 98.6       | 5.33       |\n\nExposure Bias Language generation models trained to minimize forward KLD suffer from exposure bias [BVJS15] caused by the discrepancy between teacher-forcing training and free-run generation. When training MINILLM, the student model sees samples generated by itself, alleviating the training-inference mismatch [PH21]. In Figure 6, we use the ExAccErr metric [AEABC22] defined in Appendix B.5 to measure the excess accumulated error due to exposure bias. The experiment is based on GPT-2-125M, with GPT-2-1.5B as the teacher, using Dolly as the test set. For each prompt, we sample 10 responses to reduce the variance.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 42,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-43",
      "content": "umulated error due to exposure bias. The experiment is based on GPT-2-125M, with GPT-2-1.5B as the teacher, using Dolly as the test set. For each prompt, we sample 10 responses to reduce the variance. We can see that the ExAccErrs of the baselines continuously grow during generation, while MINILLM has a much lower ExAccErr, and the error stops accumulating in long-text generation ( &gt; 150 tokens).\n\nCalibration [Ope23] has shown that models trained with policy optimization are likely to be poorly calibrated. We test the calibration of MINILLM and the KD baselines on two widely-used text classification datasets: SST2 [SPW + 13] and BoolQ [CLC + 19], based on LLaMA-7B. We design zero-shot classification instructions (see Appendix B.2) and take the probability of the label words to compute the ECE scores [NDZ + 19].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 43,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-44",
      "content": "[SPW + 13] and BoolQ [CLC + 19], based on LLaMA-7B. We design zero-shot classification instructions (see Appendix B.2) and take the probability of the label words to compute the ECE scores [NDZ + 19]. From Table 2, we observe that KD and SeqKD models are worse calibrated than the teacher model, which potentially explains their low performance on canonical benchmarks [GWS + 23]. We suspect that minimizing forward KLD causes the models to push high probabilities to void regions of the target distribution, which leads to significant distribution differences between the student and the teacher (see the example in Figure 2). In contrast, MINILLM focuses on accurately learning the major parts of the target distribution and narrows the ECE scores gap between the student and the teacher.\n\nPerformance on Different Response Length Westudy the models' performance when the golden response lengths belong to different ranges.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 44,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-45",
      "content": "d narrows the ECE scores gap between the student and the teacher.\n\nPerformance on Different Response Length Westudy the models' performance when the golden response lengths belong to different ranges. In Figure 7, we illustrate the Rouge-L scores of different KD models against the SFT models on three S-NI subsets split by the length of the ground truth responses. We can see that all methods achieve low scores on prompts that expect short responses ( ≤ 5 tokens), probably because most responses in our training set are long sentences, introducing a distribution shift between training and evaluation [PLH + 23]. Furthermore, the output spaces of these prompts are relatively small, allowing the student model to cover most modes of the teacher, and thus reverse KLD and forward KLD have similar performance. For prompts with longer responses ( ≥ 6 tokens), the teacher distribution contains more modes than the students due to the complex out-",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 45,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-46",
      "content": ", and thus reverse KLD and forward KLD have similar performance. For prompts with longer responses ( ≥ 6 tokens), the teacher distribution contains more modes than the students due to the complex out-\n\nTable 4: The performance on the validation and test set when different combinations of MINILLM optimization strategies are applied.\n\n|                   |   Valid. R-L |   Dolly R-L |\n|-------------------|--------------|-------------|\n| MINILLM           |         27.4 |        24.6 |\n| w/o Length Norm.  |         17.4 |        14.7 |\n| w/o Teacher-Mixed |         22.3 |        20.4 |\n| w/o Single-Step   |         27   |        23.7 |\n\nFigure 8: The reverse KLD between the teacher and the students during MINILLM training when different optimization strategies are applied.\n\n<!-- image -->\n\nput spaces, which shows the advantage of MINILLM against standard KD models. Similar results on UnNI are shown in Appendix C.3.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 46,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-47",
      "content": "ing when different optimization strategies are applied.\n\n<!-- image -->\n\nput spaces, which shows the advantage of MINILLM against standard KD models. Similar results on UnNI are shown in Appendix C.3.\n\nGeneration Diversity [CCF + 20] has found that the model optimized by minimizing reverse KLD is likely to lose modes, which affects the generation diversity. We follow [PH21] to discuss generation diversity from three aspects: (i) generating multiple distinct responses given a prompt. (ii) generating linguistically complex responses. (iii) the ability to generate contents that have high coverage of the real data distribution. For (i), we argue that for many NLP applications, generating one correct response is sufficient, especially for those scenarios demanding high truthfulness and reliability [JLF + 23].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 47,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-48",
      "content": "ta distribution. For (i), we argue that for many NLP applications, generating one correct response is sufficient, especially for those scenarios demanding high truthfulness and reliability [JLF + 23]. For (ii) and (iii), we report the responses' distinct 4-gram proportion and the language modeling loss on the test sets in Table 3, using the base models from the LLaMA family (see Appendix B.4 for more details) . We can see that MINILLM preserves the distinct 4-gram proportion in the generated responses and language modeling loss on the test set.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.3 Analysis",
        "chunkIndex": 48,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-49",
      "content": "Weevaluate the effectiveness of the three strategies proposed to stabilize and accelerate optimization in Section 2.2 by distilling a GPT-2-125M model from the GPT-2-1.5B model. More ablation studies can be found in Appendix C.4. In Table 4, we report the best Rouge-L scores on the validation set of each run and the evaluation results of the corresponding checkpoints. We also plot the reverse KLD between the student and the teacher during training in Figure 8, where the curves are smoothed by 32 steps. We can see that Teacher-Mixed Sampling and Length Normalization works for stabilizing training. Although the reverse KLDs also decrease without these strategies, we find that the models quickly learn to generate repeated, short, or meaningless strings that have high probabilities in the teacher distribution (see examples in Appendix D), which is known as reward hacking [SHKK22]. This also leads to the low generation performance in Table 4.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.4 Ablation Studies on Optimization Strategies",
        "chunkIndex": 49,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-50",
      "content": "strings that have high probabilities in the teacher distribution (see examples in Appendix D), which is known as reward hacking [SHKK22]. This also leads to the low generation performance in Table 4. From Figure 8, we also observe that the Single-Step Decomposition effectively reduces the variance of the training process, which also results in higher scores on the validation and test sets.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "3.4 Ablation Studies on Optimization Strategies",
        "chunkIndex": 50,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-51",
      "content": "Large Language Models Large language models (LLMs; BMR + 20, TDFH + 22, CND + 22, Ope23, ADF + 23) have shown superior performance by solving various NLP tasks in a generative manner. Recent works apply instruction tuning [WBZ + 22, SWR + 22, CHL + 22] or learning from human feedback [OWJ + 22, BJN + 22] to improve the alignment of LLMs with humans further and create general AI assistants [Ope22, Goo23]. There are also efforts to build open-source LLMs [ZRG + 22, TLI + 23, BSA + 23] to facilitate research and industry development. Although appealing, the broad capacities of LLMs usually emerge with large model sizes [KMH + 20, WTB + 22] that require massive computational resources. Therefore, model compression is critical for the practical deployment and further research of LLMs.\n\nKnowledge Distillation Knowledge distillation (KD; HVD15), as a widely used model compression technique, aims at training a student model with the guidance of a teacher model [RCG + 15, SDCW19, JBMD21].",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "4 Related Work",
        "chunkIndex": 51,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-52",
      "content": "owledge Distillation Knowledge distillation (KD; HVD15), as a widely used model compression technique, aims at training a student model with the guidance of a teacher model [RCG + 15, SDCW19, JBMD21]. In the NLP community, many works apply KD to text classification tasks by mimicking the teacher model's output distribution [SST + 20, LHS + 21, ZSL + 23], hid-\n\nden states [JYS + 20, SCGL19], or attention scores [WWD + 20, WBH + 21]. For text generation, the standard KD method is to approximately minimize the forward KLD between the student's and the teacher's generation distribution by using the teacher's output at each time step as supervision [SDCW19] or direct training on the teacher's generated texts [KR16, TGZ + 23, CLL + 23, PLH + 23]. In this paper, we minimize the reverse KLD, which is more suitable for LLMs when the teacher distribution is available. Concurrent works [A VS + 23, WLDM23] also explore more the distribution discrepancy metrics in KD.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "4 Related Work",
        "chunkIndex": 52,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-53",
      "content": "minimize the reverse KLD, which is more suitable for LLMs when the teacher distribution is available. Concurrent works [A VS + 23, WLDM23] also explore more the distribution discrepancy metrics in KD.\n\nDistribution Discrepancy Metrics in Text Generation The distribution discrepancy metrics play a significant role in training text generation models. The forward KLD is widely used due to its simplicity when derived as the Maximum Likelihood Estimate (MLE) objective [ZZ19]. However, previous works show that minimizing forward KLDleads to zero-forcing behavior where models try to cover all modes of the target distribution and sacrifice the accuracy of major modes [Hus15]. Some works resort to using other metrics to remedy this problem, such as reverse KLD [JHC + 20], Total Variation Distance [JKH + 23], and Optimal Transport [LLW + 20]. Our paper tackles this problem under the scenario of knowledge distillation for LLMs.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "4 Related Work",
        "chunkIndex": 53,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-54",
      "content": "JKH + 23], and Optimal Transport [LLW + 20]. Our paper tackles this problem under the scenario of knowledge distillation for LLMs.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "4 Related Work",
        "chunkIndex": 54,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-55",
      "content": "In this work, we investigate the problem of distilling the knowledge of LLMs into small language models. We find that the standard distillation methods minimizing the forward KLD is sub-optimal in language generation scenarios because the teacher's output distribution contains more modes than the student's, and forward KLD forces the student distribution to overestimate the low-probability regions of the teacher distribution. Therefore, we propose MINILLM that minimizes the reverse KLD between the teacher and student distribution and design an algorithm to optimize this objective. Extensive experiments show that MINILLM produce more precise responses that have higher overall quality than standard KD models. We also find that MINILLM has lower exposure bias, better calibration, and higher performance in long-text generation with good generation diversity.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "5 Conclusion",
        "chunkIndex": 55,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-56",
      "content": "This work was supported by the National Key Research and Development Program of China (No. 2021ZD0113304), the National Science Foundation for Distinguished Young Scholars (with No. 62125604), and the NSFC projects (Key project with No. 61936010).",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Acknowledgements",
        "chunkIndex": 56,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-57",
      "content": "- [ADF + 23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.\n- [AEABC22] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of ACL , 2022.\n- [AVS + 23] Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. GKD: Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649 , 2023.\n- [BHA + 21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-58",
      "content": "rew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\n- [BJN + 22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n\n- [BLW + 21] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.\n- [BMR + 20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are few-shot learners.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-59",
      "content": "PT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.\n- [BMR + 20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are few-shot learners. In Proceedings of NeurIPS , 2020.\n- [BSA + 23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\n- [BVJS15] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of NeurIPS , 2015.\n- [CCF + 20] Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-60",
      "content": "recurrent neural networks. In Proceedings of NeurIPS , 2015.\n- [CCF + 20] Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. In ICLR , 2020.\n- [CDP + 18] Liqun Chen, Shuyang Dai, Yunchen Pu, Erjin Zhou, Chunyuan Li, Qinliang Su, Changyou Chen, and Lawrence Carin. Symmetric variational autoencoder and connections to adversarial learning. In Proceedings of AISTATS , 2018.\n- [CHL + 22] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n- [Cio21] Kamil Ciosek. Imitation learning by reinforcement learning. In ICLR , 2021.\n- [CLC + 19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-61",
      "content": "LR , 2021.\n- [CLC + 19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL-HLT , 2019.\n- [CLL + 23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n- [CND + 22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- [CPO + 19] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-62",
      "content": "ys. arXiv preprint arXiv:2204.02311 , 2022.\n- [CPO + 19] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In Proceedings of AISTATS , 2019.\n- [CvdS21] Alex James Chan and Mihaela van der Schaar. Scalable bayesian inverse reinforcement learning. In ICLR , 2021.\n- [FLT + 18] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In Proceedings of ICML , 2018.\n- [GCPT19] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019.\n- [Goo23] Google. Bard, 2023.\n- [GWS + 23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 , 2023.\n- [HBD + 20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-63",
      "content": "promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 , 2023.\n- [HBD + 20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In Proceedings of ICLR , 2020.\n\n- [HLM22] Yongchang Hao, Yuxin Liu, and Lili Mou. Teacher forcing recovers reward functions for text generation. In Proceeings of NeurIPS , 2022.\n- [HSLS23] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of ACL , 2023.\n- [HTAL17] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of ICML , 2017.\n- [Hus15] Ferenc Huszár. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101 , 2015.\n- [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-64",
      "content": "erative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101 , 2015.\n- [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015.\n- [HZD + 21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past, present and future. AI Open , 2021.\n- [JBMD21] Gou Jianping, Yu Baosheng, Stephen J Maybank, and Tao Dacheng. Knowledge distillation: A survey. IJCV , 2021.\n- [JHC + 20] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Proceedings ACL , 2020.\n- [JKH + 23] Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. Tailoring language generation models under total variation distance.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-65",
      "content": "regularized optimization. In Proceedings ACL , 2020.\n- [JKH + 23] Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. Tailoring language generation models under total variation distance. In Proceedings of ICLR , 2023.\n- [JLF + 23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys , 2023.\n- [JYS + 20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of EMNLP , 2020.\n- [KMH + 20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- [KR16] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-66",
      "content": "c Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- [KR16] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings of EMNLP , 2016.\n- [LGB + 16] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversitypromoting objective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of NAACL , 2016.\n- [LHS + 21] Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. Mix{kd}: Towards efficient distillation of large-scale language models. In Proceedings of ICLR , 2021.\n- [Lin04] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summarization Branches Out (ACL 2004) , 2004.\n- [LKTF20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-67",
      "content": "Summarization Branches Out (ACL 2004) , 2004.\n- [LKTF20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.\n- [LLW + 20] Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuhchen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, et al. Improving text generation with student-forcing optimal transport. In Proceedings of EMNLP , 2020.\n\n- [LOG + 19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.\n- [LPSK23] Hyoje Lee, Yeachan Park, Hyun Seo, and Myungjoo Kang. Self-knowledge distillation via dropout. Computer Vision and Image Understanding , 2023.\n- [M + 05] Tom Minka et al. Divergence measures and message passing.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-68",
      "content": "Yeachan Park, Hyun Seo, and Myungjoo Kang. Self-knowledge distillation via dropout. Computer Vision and Image Understanding , 2023.\n- [M + 05] Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005.\n- [MFL + 20] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of AAAI , 2020.\n- [MG19] Andrey Malinin and Mark Gales. Reverse KL-divergence training of prior networks: Improved uncertainty and adversarial robustness. In Proceedings of NeurIPS , 2019.\n- [NCT16] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Proceedings of NeurIPS , 2016.\n- [NDZ + 19] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops , 2019.\n- [Ope22] OpenAI.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-69",
      "content": "of NeurIPS , 2016.\n- [NDZ + 19] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops , 2019.\n- [Ope22] OpenAI. OpenAI: Introducing ChatGPT, 2022.\n- [Ope23] OpenAI. GPT-4 technical report, 2023.\n- [OWJ + 22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS , 2022.\n- [PH21] Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations. In Proceedings of ICLR , 2021.\n- [PLH + 23] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023.\n- [PSS00] Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-70",
      "content": "ianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023.\n- [PSS00] Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of ICML , 2000.\n- [RCG + 15] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295 , 2015.\n- [RWC + 19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Technical report , 2019.\n- [SCGL19] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In Proceedings EMNLP , 2019.\n- [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-71",
      "content": "model compression. In Proceedings EMNLP , 2019.\n- [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n- [SHKK22] Joar Max Viktor Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Proceedings of NeurIPS , 2022.\n- [SMSM99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Proceedings of NeurIPS , 1999.\n\n- [SPW + 13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-72",
      "content": "PW + 13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP , October 2013.\n- [SSG + 17] Iulian V Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, et al. A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349 , 2017.\n- [SST + 20] Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. LightPAFF: A two-stage distillation framework for pre-training and fine-tuning. arXiv preprint arXiv:2004.12817 , 2020.\n- [SWD + 17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-73",
      "content": "e-training and fine-tuning. arXiv preprint arXiv:2004.12817 , 2020.\n- [SWD + 17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n- [SWR + 22] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, et al. Multitask prompted training enables zero-shot task generalization. In Proceedings of ICLR , 2022.\n- [TDFH + 22] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.\n- [TGZ + 23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instructionfollowing LLaMA model.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-74",
      "content": "2022.\n- [TGZ + 23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instructionfollowing LLaMA model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.\n- [TLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- [TWS18] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of IJCAI , 2018.\n- [WBH + 21] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-75",
      "content": "ceedings of IJCAI , 2018.\n- [WBH + 21] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of ACL , 2021.\n- [WBZ + 22] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings of ICLR , 2022.\n- [Wil92] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 1992.\n- [WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, 2021.\n- [WKM + 23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of ACL , 2023.\n- [WLDM23] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-76",
      "content": "iel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of ACL , 2023.\n- [WLDM23] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Proceedings of ACL , 2023.\n- [WMA + 22] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. In Proceedings of EMNLP , 2022.\n\n- [WTB + 22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research , 2022.\n- [WWD + 20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-77",
      "content": "Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research , 2022.\n- [WWD + 20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of NeurIPS , 2020.\n- [WWZ + 23] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402 , 2023.\n- [ZCS + 23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of NeurIPS , 2023.\n- [ZLX + 23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-78",
      "content": "oceedings of NeurIPS , 2023.\n- [ZLX + 23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. In Proceedings of NeurIPS , 2023.\n- [ZMB + 08] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI , 2008.\n- [ZRG + 22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.\n- [ZSL + 23] Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation. arXiv preprint arXiv:2305.05010 , 2023.\n- [ZZ19] Huan Zhang and Hai Zhao. Minimum divergence vs. maximum margin: an empirical comparison on seq2seq models.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-79",
      "content": "g perturbed loss for knowledge distillation. arXiv preprint arXiv:2305.05010 , 2023.\n- [ZZ19] Huan Zhang and Hai Zhao. Minimum divergence vs. maximum margin: an empirical comparison on seq2seq models. In International Conference on Learning Representations , 2019.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-80",
      "content": "In Section 2.1, we formulate KD as an optimization problem of minimizing the discrepancy between the teacher distribution and the student distribution and finally reach the objective of minimizing reverse KLD. Alternatively, we can also regard KD as training the student model with the teacher model's guidance, which resembles an agent learning from the feedback from an environment. Following [PH21], we treat token generation as a Markov Decision Process. At each time step t , the student model chooses an action (token) y t from the action space (vocabulary) V conditioning on the state (prefix) ( y &lt;t , x ) based on the policy (generation probability) q θ ( y t | y &lt;t , x ) .\n\nFrom this perspective, standard KD corresponds to behavior cloning (BC; TWS18) in imitation learning [Cio21]. However, BC is known to under-perform Inverse Reinforcement Learning (IRL; ZMB + 08), another imitation learning method that first recovers a reward model from the environment demonstrations and then",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.1 A Perspective of MINILLM from Inverse Reinforcement Learning",
        "chunkIndex": 80,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-81",
      "content": "However, BC is known to under-perform Inverse Reinforcement Learning (IRL; ZMB + 08), another imitation learning method that first recovers a reward model from the environment demonstrations and then trains the policy with the reward using policy optimization algorithms [SMSM99, SWD + 17]. Therefore, in the KD scenario, we seek to first induce a reward r ( y t , ( y &lt;t , x )) from the environment (the teacher model) and then train the student model to maximize the reward as the objective. We take the maximum-entropy inverse reinforcement learning framework [ZMB + 08, CvdS21] and thus the Q-function Q ( y t , ( y &lt;t , x )) in the environment satisfies the soft Bellman Equation:\n\n<!-- formula-not-decoded -->\n\nWe follow [HLM22] to parameterize the Q-function as Q ( y t , ( y &lt;t , x )) = f ( y t , ( y &lt;t , x )) and assume γ = 1 , where f ( y t , ( y &lt;t , x )) is the output logits of the teacher model 6 . Then, the reward is given by:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.1 A Perspective of MINILLM from Inverse Reinforcement Learning",
        "chunkIndex": 81,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-82",
      "content": "&lt;t , x )) = f ( y t , ( y &lt;t , x )) and assume γ = 1 , where f ( y t , ( y &lt;t , x )) is the output logits of the teacher model 6 . Then, the reward is given by:\n\n<!-- formula-not-decoded -->\n\nTo maximize the reward, we apply maximum-entropy reinforcement learning [HTAL17], whose learning objective is\n\n<!-- formula-not-decoded -->\n\nwhere H[ q θ ( ·| y &lt;t , x )] = -E y t ∼ q θ ( ·| y &lt;t , x ) log q θ ( ·| y &lt;t , x ) is the entropy of the student model distribution at the time step t .\n\nEquivalence Between Objectives We prove an approximate equivalence between Eq. 10 and Eq. 1. We first rewrite the summation of the reward ∑ | y | t =1 r ( y t , ( y &lt;t , x )) by the associative law:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n6 The teacher model's distribution satisifies p ( y t | y &lt;t , x ) = exp( f ( y t , ( y &lt;t , x ))) ∑ y ′ ∈ V exp( f ( y ′ , ( y &lt;t , x ))) .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.1 A Perspective of MINILLM from Inverse Reinforcement Learning",
        "chunkIndex": 82,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-83",
      "content": "ula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n6 The teacher model's distribution satisifies p ( y t | y &lt;t , x ) = exp( f ( y t , ( y &lt;t , x ))) ∑ y ′ ∈ V exp( f ( y ′ , ( y &lt;t , x ))) .\n\nThen, J ( θ ) can be approximately rewritten as:\n\n<!-- formula-not-decoded -->\n\nTherefore, maximizing J ( θ ) is approximately equivalent to minimizing L ( θ ) .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.1 A Perspective of MINILLM from Inverse Reinforcement Learning",
        "chunkIndex": 83,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-84",
      "content": "We compute the gradient of L ( θ ) = KL[ q θ || p ] with respect to θ using the Policy Gradient Theorem [SMSM99]:\n\n<!-- formula-not-decoded -->\n\nwhere Eq. 21 is based on the fact that log q θ ( y t | y &lt;t , x ) can only affect tokens at ≥ t positions in y . By setting R t = ∑ T t ′ = t log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) , we obtain Eq. 2.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.2 Derivation of Equation 2",
        "chunkIndex": 84,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-85",
      "content": "To derive Eq. 3, we first denote:\n\n<!-- formula-not-decoded -->\n\nThen, we re-write ∇L ( θ ) as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere Eq. 27 uses the product rule of the gradient and r t = log p ( y t | y &lt;t , x ) q θ ( y t | y &lt;t , x ) .",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "A.3 Derivation of Equation 3",
        "chunkIndex": 85,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-86",
      "content": "Baselines Our baselines include SFT w/o KD , KD , and SeqKD . For models with less than 1.3B parameters, we search for the learning rates in [5e-4, 1e-4, 5e-5], the batch sizes in [32, 64], and train these models for 20 epochs. For other models, we search for the learning rate in [5e-5, 1e5, 5e-6], the batch sizes in [32, 64], and train these models for 10 epochs. For KD , we follow [SST + 20] to mix the distillation loss with the language modeling loss on the ground truth responses by a mixture rate of 0.5. The checkpoints of each baseline are selected by the Rouge-L [Lin04] scores on the validation set because, as stated in previous works [WMA + 22, OWJ + 22], we also find that Rouge-L is better correlated with human judgments.\n\nMINILLM As stated in Section 2.3, training of MINILLM has two phases which is similar to Reinforcement Learning from Human Feedback (RLHF;OWJ + 22).",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.1 Training Details",
        "chunkIndex": 86,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-87",
      "content": "Rouge-L is better correlated with human judgments.\n\nMINILLM As stated in Section 2.3, training of MINILLM has two phases which is similar to Reinforcement Learning from Human Feedback (RLHF;OWJ + 22).\n\n- Phase 1 : We fine-tune the student model on the instruction-response training set D to get a starting point for the subsequent MINILLM training. We fine-tune the model for 3 epochs using the best learning rate and batch size of the corresponding SFT w/o KD baselines. Note that different from the SFT w/o KD baseline, we select the checkpoint with the lowest validation loss , not the Rouge-L score in this phase.\n- Phase 2 : We continuously train the model from Phase 1 as described in Algorithm 2.3 using a learning rate 5e-6, a mini-batch size 64 in all cases. The training and validation set are same as in Phase 1 . Similar to [OWJ + 22], we collect 256 sentences at once and adopt 4 inner epochs when doing the policy optimization.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.1 Training Details",
        "chunkIndex": 87,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-88",
      "content": "atch size 64 in all cases. The training and validation set are same as in Phase 1 . Similar to [OWJ + 22], we collect 256 sentences at once and adopt 4 inner epochs when doing the policy optimization. The clipping rate ϵ is set to 0.2, and the max length of the model is 512. We use temperature = 1 when sampling from q θ . We train the model for 5000 steps and select the final checkpoint using the Rouge-L score on the validation set. Our experiments are based on the NVIDIA V100 32G GPUs. Distilling LLaMA-7B from LLaMA-13B takes less than 10 ours on 16 GPUs.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.1 Training Details",
        "chunkIndex": 88,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-89",
      "content": "During the evaluation, we sample the responses from each model using temperature = 1, a maxlength limit of 512, and random seeds [10, 20, 30, 40, 50]. Similar to [TGZ + 23], we adopt a prompt wrapper shown in Figure 9 to convert each instruction-response pair to a sentence. For the GPT-4\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: {input} ### Response:\n```\n\nFigure 9: The prompt wrapper for training and evaluation.\n\nWe would like to request your feedback on the performance of two AI assistants in response to the user instruction and input displayed above.\n\nPlease rate the helpfulness, relevance, accuracy, and level of detail of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.2 Automatic Evaluation Details",
        "chunkIndex": 89,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-90",
      "content": "the helpfulness, relevance, accuracy, and level of detail of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n\nPlease first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space.\n\nIn the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\n\nFigure 10: GPT-4 evaluation prompt.\n\nfeedback, we apply the prompt in Figure 10 and set the temperature = 0.7. For the classification tasks in the 'Calibration' paragraph of Section 3.3, we prompt the model to do zero-shot text classification with the templates in Figure 11 and 12.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.2 Automatic Evaluation Details",
        "chunkIndex": 90,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-91",
      "content": "Following [PLH + 23], we use SelfInst [WKM + 23] to perform human evaluation. We randomly sampled 50 prompts because we found that more prompts do not affect the results much. We ask the annotators to compare the responses generated by the baseline models with MINILLM and decide which response is preferred or neither of them is significantly better. Note that which model the responses come from is invisible to the annotators. The interface presented to annotators is shown in Figure 13.\n\nFigure 11: Zero-shot text classification prompt for SST2.\n\n<!-- image -->\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Read the input passage and answer the question: {question}? Your answer should be 'Yes' or 'No'. ### Input: {passage} ### Response:\n```\n\nFigure 12: Zero-shot text classification prompt for BoolQ.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.3 Human Evaluation Details",
        "chunkIndex": 91,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-92",
      "content": "ion: Read the input passage and answer the question: {question}? Your answer should be 'Yes' or 'No'. ### Input: {passage} ### Response:\n```\n\nFigure 12: Zero-shot text classification prompt for BoolQ.\n\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n```",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.3 Human Evaluation Details",
        "chunkIndex": 92,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-93",
      "content": "```\n\nFigure 13: The prompt wrapper for training and evaluation.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "Instruction: Desk jobs require writing a lot of emails, so it isn't surprising we get tired of repeating ourselves. Come up with several synonyms for the given word. ### Input: Sincerely ### Response: ##### Answer #1 ##### Fondly, affectionately, lovingly, tenderly, honestly, truly, faithfully, devotedly, passionately ##### Answer #2 ##### Faithfully, Gullibly, Humbly, Piously, Strangely, Weirdly, Yours truly 1: Answer #1 is better 2: Answer #2 is better 3: Tie Your choice:",
        "chunkIndex": 93,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-94",
      "content": "In Table 3, we report the distinct 4-grams (Dist-4) and the language modeling loss (Loss) on the test sets. More details about these two metrics are as follows:\n\n- 'Dist-4' is a fraction: N/C , where N is the number of the distinct 4-grams in the generated responses and C is the total number of 4-grams. It is a widely used metric to measure the generation diversity of a language model [LGB + 16]. The ( N/C ) s on the Dolly test set across 5 random seeds are shown in Table 5. Table 3 reports the average values across the 5 random seeds.\n- 'Loss' is the negative log-likelihood loss on the test set D Test : -∑ x , y ∼D Test log q θ ( y | x ) . It measures the mode coverage of the real data distribution because it is essentially the forward KLDbetween the real data distribution and the model output distribution. This relates to diversity as in the ability to generate different generations given one context with different random seeds.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.4 Details About Generation Diversity Metrics",
        "chunkIndex": 94,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-95",
      "content": "rd KLDbetween the real data distribution and the model output distribution. This relates to diversity as in the ability to generate different generations given one context with different random seeds.\n\nTable 5: The ( N/C )s, where N is the number of the distinct 4-grams in the generated responses and C is the total number of 4-grams. We report the numbers computed on the Dolly test set when evaluated with 5 random seeds: [10, 20, 30, 40, 50].\n\n| Model   | 10            | 20            | 30            | 40            | 50            |\n|---------|---------------|---------------|---------------|---------------|---------------|\n| Teacher | 23562 / 23696 | 23653 / 23834 | 24306 / 24488 | 24207 / 24381 | 23803 / 23967 |\n| KD      | 25889 / 26064 | 24024 / 24197 | 25663 / 25843 | 25611 / 25763 | 26178 / 26339 |\n| SeqKD   | 25358 / 25519 | 25631 / 25822 | 26190 / 26370 | 25574 / 25748 | 26295 / 26522 |\n| MINILLM | 24187 / 24458 | 25011 / 25272 | 25100 / 25436 | 24067 / 24312 | 25205 / 25519 |",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.4 Details About Generation Diversity Metrics",
        "chunkIndex": 95,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-96",
      "content": "26178 / 26339 |\n| SeqKD   | 25358 / 25519 | 25631 / 25822 | 26190 / 26370 | 25574 / 25748 | 26295 / 26522 |\n| MINILLM | 24187 / 24458 | 25011 / 25272 | 25100 / 25436 | 24067 / 24312 | 25205 / 25519 |\n\nFigure 14: The scaling law of teacher model based on the OPT family models. We compare MINILLM and SeqKD with OPT-1.3M as the student and OPT 2.7B, 6.7B, and 13B as teachers.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.4 Details About Generation Diversity Metrics",
        "chunkIndex": 96,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-97",
      "content": "Following [AEABC22], we compute the ExAccErr with the following formula:\n\n<!-- formula-not-decoded -->\n\nwhere R ( l ) is the accumulated regret of imitating the teacher distribution p at the time step l during the free-run generation:\n\n<!-- formula-not-decoded -->\n\nand ϵ ( l ) is the average per-step error between q θ and p using the oracle context sampled from p as the prefix:\n\n<!-- formula-not-decoded -->\n\nIntuitively, the regret of q θ during generation is made of two parts: the error to estimate p given the oracle context and the error caused by the low-quality model-generated prefix. The former is calculated by lϵ ( l ) , and the latter reflects the exposure bias. Therefore, ExAccErr measures the relative error caused only by exposure bias.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "B.5 Exposure Bias Analysis",
        "chunkIndex": 97,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-98",
      "content": "We present the evaluation results when using GPT-J as the teacher model and GPT-2-760M, GPT-21.5B, and GPTNeo -2.7B [BLW + 21] as the student models in Table 6. MINILLM outperforms the baselines in most cases.\n\nFigure 15: The Rouge-L scores of the distilled models against the SFT models on the different evaluation subsets of UnNI split by the golden responses' length.\n\n<!-- image -->\n\nTable 6: Evaluation results when GPT-J is the teacher. GPT4 and R-L stand for the average GPT-4 feedback scores and Rouge-L scores across 5 random seeds. The best scores of each model size are boldfaced , and the scores where the student model outperforms the teacher are marked with *.\n\n| Model          | Method        | DollyEval      | DollyEval   | SelfInst   | SelfInst       | VicunaEval   | VicunaEval   | S-NI      | UnNI R-L   |\n|----------------|---------------|----------------|-------------|------------|----------------|--------------|--------------|-----------|------------|\n| Model          | Met",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.1 GPT-J as the Teacher Model",
        "chunkIndex": 98,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-99",
      "content": "| S-NI      | UnNI R-L   |\n|----------------|---------------|----------------|-------------|------------|----------------|--------------|--------------|-----------|------------|\n| Model          | Method        | GPT4           | R-L         | GPT4       | R-L            | GPT4         | R-L          | R-L       |            |\n| GPT-J-6B       | Teacher       | 65.8           | 27.3        | 57.4       | 17.3           | 55.8         | 17.4         | 28.0      | 33.6       |\n| GPT-2-760M     | SFT w/o KD KD | 50.7 51.6 51.4 | 25.4 26.7   | 38.3 38.9  | 12.4 13.4 14.0 | 43.1 43.4    | 16.1 16.4    | 21.5 25.9 | 27.1       |\n| GPT-2-760M     |               |                |             |            |                |              |              |           | 33.2       |\n| GPT-2-760M     | SeqKD         |                | 26.0        | 39.2       |                | 42.0         | 15.3         | 25.5      | 32.5       |\n| GPT-2-760M     | MINILLM       | 54.0           | 25.8        | 4",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.1 GPT-J as the Teacher Model",
        "chunkIndex": 99,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-100",
      "content": "SeqKD         |                | 26.0        | 39.2       |                | 42.0         | 15.3         | 25.5      | 32.5       |\n| GPT-2-760M     | MINILLM       | 54.0           | 25.8        | 43.7       | 16.3           | 44.3         | 19.1 *       | 27.1      | 35.5 *     |\n| GPT-2-1.5B     | SFT w/o KD    | 58.4           | 27.6 *      | 42.9       | 14.3           | 48.6         | 16.3         | 27.6      | 34.6*      |\n| GPT-2-1.5B     | KD            | 56.5           | 26.6        | 46.0       | 14.5           | 47.2         | 16.5         | 27.6      | 34.9*      |\n| GPT-2-1.5B     | SeqKD         | 58.5           | 27.0        | 43.2       | 13.6           | 46.6         | 16.9         | 28.0      | 34.2*      |\n| GPT-2-1.5B     | MINILLM       | 59.6           | 25.9        | 48.5       | 16.6           | 48.9         | 19.4 *       | 28.5 *    | 35.9 *     |\n| GPT- Neo -2.7B | SFT w/o KD    | 60.7           | 26.8        | 45.4       | 15.8           | 51.5         | 1",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.1 GPT-J as the Teacher Model",
        "chunkIndex": 100,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-101",
      "content": "| 48.5       | 16.6           | 48.9         | 19.4 *       | 28.5 *    | 35.9 *     |\n| GPT- Neo -2.7B | SFT w/o KD    | 60.7           | 26.8        | 45.4       | 15.8           | 51.5         | 17.0         | 26.5      | 31.6       |\n| GPT- Neo -2.7B | KD            | 61.5           | 26.7        | 47.0       | 16.0           | 52.1         | 16.9         | 27.2      | 32.7       |\n| GPT- Neo -2.7B | SeqKD         | 60.8           | 25.6        | 47.2       | 16.2           | 53.0         | 16.9         | 26.1      | 32.9       |\n| GPT- Neo -2.7B | MINILLM       | 63.4           | 28.5*       | 52.5       | 17.1           | 54.1         | 18.6*        | 29.8*     | 35.4*      |\n\nFigure 16: Effect of the α value in the teacher mix-in exploration on the validation Rouge-L score. Larger models to more robust to α .\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.1 GPT-J as the Teacher Model",
        "chunkIndex": 101,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-102",
      "content": "| 18.6*        | 29.8*     | 35.4*      |\n\nFigure 16: Effect of the α value in the teacher mix-in exploration on the validation Rouge-L score. Larger models to more robust to α .\n\n<!-- image -->\n\n|      |                  | CLS       | Inst.     |\n|------|------------------|-----------|-----------|\n| 1.3B | MINILLM w/o L PT | 70.2 65.7 | 52.8 53.2 |\n| 7B   | MINILLM w/o L PT | 78.8 74.3 | 71.2 71.1 |\n\nTable 7: The effect of adding the pre-training loss. 'CLS' is the average accuracy scores on SST2 and BoolQ. 'Inst.' is the average RougeL score on Dolly, SelfInst, and Vicuna.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.1 GPT-J as the Teacher Model",
        "chunkIndex": 102,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-103",
      "content": "We present the performance of MINILLM and SeqKD when we scale up the sizes of teacher models in Figure 14. Similar to the observations in Section 3.3, MINILLM constantly performs better and distills better student models from larger teacher models.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.2 Scaling Law of Teacher based on the OPT family",
        "chunkIndex": 103,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-104",
      "content": "The performance on different U-NI subsets split by the length of the ground truth response is shown in Figure 15. We have the same observation as in Section 3.3 that on short responses, all KD methods perform similarly, and on long responses, MINILLM outperforms other methods.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.3 Performance of Response Length on U-NI",
        "chunkIndex": 104,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-105",
      "content": "Effect of Teacher-Mix-in Strength α In Figure 16, we plot the best Rouge-L scores on the validation set of GPT-2-125M, OPT-1.3B, and LLaMA-7B using GPT-2-1.5B, OPT-13B, and LLAMA13B as the teachers, with different teacher-mix-in strength α in MINILLM. α = 0 . 0 means we only sample from the student distribution, and when α = 1 . 0 , we sample entirely from the teacher distribution. We find that α = 0 . 2 is generally suitable across different model families and sizes, and larger models are more robust to the choice of α .\n\nEffect of Adding Pre-Training Loss In Table 7, we study the effect of adding the pre-training loss in Algorithm 2.3 by comparing MINILLM with its variant where the language modeling loss\n\n| Case #1                   | Case #1                                                                                                                                                                                                                                |\n|-------------------",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 105,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-106",
      "content": "|\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Instruction               | During writing, we added an asterisk for the word that did not come to mind. You will need to provide several examples to demonstrate all the words that can be used in the sentence instead of the asterisk.                          |\n| Input                     | we * to know                                                                                                                                                                                                                           |\n| SFT w/o KD                | W",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 106,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-107",
      "content": "|\n| SFT w/o KD                | We can use 'know' in the sentence instead of 'we * to know'                                                                                                                                                                            |\n| KD                        | we went to know \\ n we will go to know \\ n we can go to know \\ n we may go to know \\ n we should go to know \\ n we must go to know                                                                                                     |\n| SeqKD                     | we do to know \\ n we can do to know \\ n we should do to know \\ n we would do to know \\ n we ought to do to know                                                                                                                        |\n| MINILLM                   | We can use t",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 107,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-108",
      "content": "o to know \\ n we ought to do to know                                                                                                                        |\n| MINILLM                   | We can use the following words in place of the asterisk: \\ n - want \\ n - have \\ n - do \\ n - used \\ n - ought \\ n - desire \\ n - need \\ n - must                                                                                      |\n| MINILLM w/o teacher-mixed | We to know                                                                                                                                                                                                                             |\n| Case #2                   | Case #2                                                                                                                                                                                                                                |\n| Instruction               | Take the title of the p",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 108,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-109",
      "content": "|\n| Instruction               | Take the title of the product and extract its attributes. The attributes in this case refer to the characteristics of the products, such as their brand and color. Your answer must a list following the format of 'attribute: value'. |\n| Input                     | SAMSUNG980 PRO SSD 2TB PCIe NVMeGen 4 Gaming M.2 Internal Solid State Hard Drive Memory Card, MaximumSpeed, Thermal Control, MZ-V8P2T0B                                                                                                |\n| SFT w/o KD                | Attribute: Maximum Speed \\ n Value: 3500 MB/s                                                                                                                                                                                          |\n| KD                        | attribute: brand, attribute: color",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 109,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-110",
      "content": "|\n| KD                        | attribute: brand, attribute: color, attribute: memory card, attribute: maxi- mum speed, attribute: thermal control                                                                                                                     |\n| SeqKD                     | brand: Samsung, color: black, size: 2TB, interface: PCIe NVME Gen 4, design: M.2 Internal                                                                                                                                              |\n| MINILLM                   | brand: SAMSUNG \\ n color: 980 PRO \\ n capacity: 2TB \\ n interface: PCIe NVME Gen 4 \\ n speed: Maximum Speed \\ n threshold: Thermal Control \\ n form factor: M.2 Internal                                                               |\n| MINILLM w/o teacher-mixed | SAMSUNGC 980 Pro SSD 2TB PCIe NVME Gen 4 Gami",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 110,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-111",
      "content": "\\ n threshold: Thermal Control \\ n form factor: M.2 Internal                                                               |\n| MINILLM w/o teacher-mixed | SAMSUNGC 980 Pro SSD 2TB PCIe NVME Gen 4 Gaming M.II Inter- nal Solid State Hard Drive memory Card, Max Speed, Thermal Control, AP1500K                                                                                                |\n\nTable 8: Instruction-following cases from the SelfInst dataset. MINILLM better follows the instructions and provides more detailed and accurate responses. Without the teacher-mixed sampling strategy in Section 2.2, the distilled model outputs short responses (Case #1) or simply repeats the input (Cases #2).\n\non the pre-training corpus is removed (w/o L PT). We have a similar observation as [OWJ + 22] that adding the pre-training loss helps to preserve the abilities on canonical NLP tasks while keeping the performance on instruction-following tasks nearly unchanged.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 111,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-112",
      "content": "ding the pre-training loss helps to preserve the abilities on canonical NLP tasks while keeping the performance on instruction-following tasks nearly unchanged.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "C.4 More Ablation Studies",
        "chunkIndex": 112,
        "totalChunks": 114
      }
    },
    {
      "id": "2306.08543v5-chunk-113",
      "content": "We provide some cases generated by the models distilled by different methods based on the LLaMA model family in Table 8. The prompts are sampled from the SelfInst dataset. We find that MINILLM generates more detailed and accurate responses compared with the baselines.",
      "metadata": {
        "source": "arxiv:2306.08543v5",
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
          "Yuxian Gu",
          "Li Dong",
          "Furu Wei",
          "Minlie Huang"
        ],
        "section": "D Cases",
        "chunkIndex": 113,
        "totalChunks": 114
      }
    }
  ],
  "fullText": "## MiniLLM: Knowledge Distillation of Large Language Models\n\nYuxian Gu 1 , 2 ∗ , Li Dong 2 , Furu Wei 2 , Minlie Huang 1 †\n\n1 The CoAI Group, Tsinghua University 2 Microsoft Research guyx21@mails.tsinghua.edu.cn {lidong1,fuwei}@microsoft.com aihuang@tsinghua.edu.cn\n\n## Abstract\n\nKnowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective on-policy optimization approach to learn this objective. The student models are named MINILLM . Extensive experiments in the instruction-following setting show that MINILLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm .\n\nFigure 1: The comparison of MINILLM with the sequence-level KD (SeqKD; KR16, TGZ + 23, CLL + 23, PLH + 23, GWS + 23, ZLX + 23) in terms of the average GPT-4 feedback score on our evaluation sets. Left : GPT-2-1.5B as the teacher model and GPT-2 125M, 340M, 760M as the student models. Middle : GPT-J 6B as the teacher model and GPT-2 760M, 1.5B, GPTNeo 2.7B as the student models. Right : OPT 13B as the teacher and OPT 1.3B, 2.7B, 6.7B as the student models.\n\n<!-- image -->\n\n∗ Contribution during an internship at Microsoft Research.\n\n† Corresponding author.\n\n## 1 Introduction\n\nWith the rapid development of large language models (LLMs; BMR + 20, HZD + 21, BHA + 21, CND + 22, Ope23), a common technique to reduce their high computational resource demand is knowledge distillation (KD; HVD15), where we train a small student model with supervision from a large teacher model. Two categories of KD are commonly applied: black-box KD, where only the teacher-generated texts are accessible, and white-box KD, where the teacher model's output distribution or intermediate hidden states are also available [JBMD21]. Recently, black-box KD has shown promising results in fine-tuning small models on the prompt-response pairs generated by LLM APIs [TGZ + 23, CLL + 23, WWZ + 23, PLH + 23]. With the emergence of more open-source LLMs [ZRG + 22, TLI + 23], white-box KD becomes more valuable for both research communities and industry sectors because student models receive better signals from the output distribution and hidden states of teacher models, thereby potentially resulting in higher performance. However, white-box KD approaches are mostly studied for small ( &lt; 1B parameters) language understanding models [SDCW19, WWD + 20], while white-box KD for LLMs is yet to be explored.\n\nIn this work, we investigate white-box KD of LLMs where the output distribution of the teacher model is available. We argue that the standard KD objectives [KR16, SST + 20, CLL + 23, TGZ + 23] are sub-optimal for LLMs that perform tasks in a generative manner. Given the teacher distribution p ( y | x ) and the student distribution q θ ( y | x ) parameterized by θ , standard KD objectives (including several variants for sequence-level models) essentially minimize the approximated forward Kullback-Leibler divergence (KLD) between the teacher and the student distribution, termed as KL[ p || q θ ] , which forces q θ to cover all modes of p . For text classification tasks, KL[ p || q θ ] works well because the output space usually consists of a finite number of classes such that both p ( y | x ) and q θ ( y | x ) have few modes. However, for open-ended text generation tasks, which is usually the case of LLM applications, the output spaces are much more complex and p ( y | x ) can contain many more modes than what q θ ( y | x ) can express due to the limited model capacity. Minimizing forward KLD causes q θ to assign unreasonably high probabilities to the void regions of p [MG19] and produces very unlikely samples under p during free-run generation [Hus15].\n\nTo alleviate this problem, we propose to minimize reverse KLD, KL[ q θ || p ] , widely used in computer vision [LPSK23] and reinforcement learning [CPO + 19]. Compared to KL[ p || q θ ] , minimizing KL[ q θ || p ] causes q θ to seek the major modes of p , and assign low probabilities to p 's void regions [M + 05], as illustrated in Figure 2 and discussed in Section 2.1. In text generation, this means that the student avoids learning too many longtail variants [HBD + 20] in the teacher's distribution to focuses on the generation correctness, which is critical in practical scenarios that require truthfulness and reliabil-\n\nFigure 2: The toy experiment. We fit a Gaussian mixture distribution with a single Gaussian distribution using forward KLD and reverse KLD.\n\n<!-- image -->\n\nity [JLF + 23]. To optimize min θ KL[ q θ || p ] , as shown in Section 2.2, we derive its gradient with Policy Gradient [SMSM99] and adopt an on-policy training approach. To further stabilize and accelerate training, we propose (1) single-step decomposition to reduce variance, (2) teacher-mixed sampling to alleviate reward hacking, and (3) length normalization to eliminate the length bias. Finally, we introduce the overall KD algorithm in Section 2.3. Our student models are named MINILLM , indicating our method is suitable for compressing large (generative) language models.\n\nWe apply our method to various generative language models [RWC + 19, ZRG + 22, TLI + 23] with sizes ranging from 120M to 13B in the instruction-following setting [SWR + 22, WBZ + 22] that covers a large range of NLP tasks. We use 5 datasets with Rouge-L [Lin04], the GPT-4 feedback, and human judgment for evaluation. Experiments show that MINILLM consistently outperforms standard KD baselines on all the datasets and scales up well from 120M to 13B models (see Figure 1). More analysis shows that MINILLM yields lower exposure bias, better calibration, and higher long response generation performance, with neglectable loss of diversity.\n\n## 2 Method\n\nWe consider conditional text generation where the model produces a response y = { y t } T t =1 conditioning on a prompt x sampled from the distribution p x , which is typically how LLMs perform tasks.\n\nFigure 3: Comparison between sequence-level KD (left) and MINILLM (right). Sequence-level KD forces the student to memorize all samples generated by the teacher model, while MINILLM improves its generated texts with the teacher model's feedback.\n\n<!-- image -->\n\nWe formulate KD as an optimization problem to minimize the difference between a fixed teacher model distribution p ( y | x ) and a student model distribution q θ ( y | x ) parameterized by θ . The standard KD methods approximately 3 minimize the forward KLD: KL[ p || q θ ] = E x ∼ p x , y ∼ p ′ log p ( y | x ) q θ ( y | x ) , where p ′ can be real data distribution (word-level KD) or teacher distribution p (sequence-level KD). Though widely used, KL[ p || q θ ] tends to overestimate the void regions of p in text generation tasks when q θ is insufficiently expressive [JKH + 23]. KD for LLMs fits the case because LLMs perform tasks in a generative manner, such that the low-capacity student models cannot perfectly imitate the complex text generation distribution of the teacher models or humans.\n\n## 2.1 MINILLM: Knowledge Distillation with Reverse KLD\n\nWe consider minimizing the reverse KLD between the student and teacher model distributions as the learning objective for MINILLM:\n\n<!-- formula-not-decoded -->\n\nMinimizing reverse KLD has been shown to cause the mode-seeking behavior in generative modeling [Hus15, NCT16, CDP + 18, LPSK23], where q θ assigns high probabilities to p 's large modes and ignore the small ones (illustrated in a toy experiment in Figure 2). In this work, we first study this property for KD of LLMs in text generation. Minimizing forward KLD causes q θ to place large probability masses on the zero-probability regions of p , corresponding to the generation of lowquality text in practice, while reverse KLD focuses on p 's major modes, which is crucial to ensure the correctness and faithfulness of text generation. As illustrated in Figure 3, unlike sequence-level KD that minimizes forward KLD [KR16, TGZ + 23], MINILLM that minimizes reverse KLD does not force q θ to fit all y sampled from the teacher distribution p . Instead, it encourages the student to generate samples preferred by the teacher within its own capacities, which is more possible to achieve. Interestingly, we also find another perspective of understanding MINILLM motivated by Inverse Reinforcement Learning [ZMB + 08]. We present the related derivation in Appendix A.1.\n\n## 2.2 On-Policy Distillation\n\nGradient Derivation We notice that the gradient of the objective function L ( θ ) in Equation (1) can be derived using the Policy Gradient Theorem [Wil92, HTAL17] for on-policy optimization:\n\n<!-- formula-not-decoded -->\n\nwhere T = | y | and R t = ∑ T t ′ = t log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) is the accumulation of r t ′ = log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) that measures the quality of each step's generation. Intuitively, the generated texts are supposed to have high probabilities under the teacher distribution by increasing p ( y t ′ | y &lt;t ′ , x ) , but simultaneously stay diverse by lowering q θ ( y t ′ | y &lt;t ′ , x ) . The expectation in Eq. 2 is computed by Monte-Carlo sampling. Full derivation can be found in Appendix A.2. However, policy gradient suffers from high variance and reward hacking [SHKK22], despite some subsequent solutions [SWD + 17]. Besides, we notice that R t favors short sentences, which causes the student model to output empty responses. Therefore, we propose three strategies to mitigate these problems.\n\n3 Wesay 'approximately' because for word-level KD, y is sampled from the real distribution, not the teacher distribution. For a strong enough teacher model, we can consider the two distributions approximately the same.\n\nSingle-Step Decomposition [CPO + 19] has found that the single-step generation quality r t is critical to the training variance because the error in the front tokens accumulates along the whole sentence. To pay more attention to r t , we re-write ∇L ( θ ) to decompose r t from R t and directly compute the gradient of E y t ∼ q θ ( t ) [ r t ] (see Appendix A.3 for the full derivation):\n\n<!-- formula-not-decoded -->\n\nwhere q θ ( t ) = q θ ( ·| y &lt;t , x ) . Note that E y t ∼ q θ ( t ) [ r t ] can be computed directly by summing over the vocabulary instead of using Monte-Carlo sampling and is derivable with respect to θ . This decomposition gives a more precise and efficient estimation of the single-step generation quality, which reduces the variance during training and accelerates convergence.\n\nTeacher-Mixed Sampling We observe reward hacking [SHKK22] when training with Eq. 2 because q θ sometimes produces degenerated sentences y that receive high scores from the teacher (e.g., repeated phrases) during sampling, especially for small student models. To create a better sampling distribution, we mix the teacher and the student distribution at each time step:\n\n<!-- formula-not-decoded -->\n\nwhere α controls the strength of the teacher mix-in. Sampling from ˜ p suppresses low-quality generation with the teacher's help and alleviates reward hacking. We re-write ( ∇L ) Single and ( ∇L ) Long with importance sampling to get to an unbiased estimator of the gradient [PSS00]:\n\n<!-- formula-not-decoded -->\n\nwhere w t = ∏ t t ′ =1 q θ ( y t ′ | y &lt;t ′ , x ) ˜ p ( y t ′ | y &lt;t ′ , x ) is the importance weight. However, w t brings high variance in practice because it requires multiplying per-token importance weight over multiple time steps, and thus the variance of each step accumulates. Therefore, we approximately set w t ≈ q θ ( y t | y &lt;t , x ) ˜ p ( y t | y &lt;t , x ) to reduce the variance of the estimator in Eq. 5 [SSG + 17, LKTF20].\n\nLength Normalization We found that long sequences tend to have small R t +1 , which encourages the model to produce short responses. Therefore, we add length normalization to R t +1 in Eq. 3:\n\n<!-- formula-not-decoded -->\n\nIn Summary Combining the strategies listed above, we have the final optimization gradient:\n\n<!-- formula-not-decoded -->\n\nwhere V is the vocabulary size of the language model and ( ∇L ) Norm Long is ( ∇L ) Long with R Norm t +1 .\n\n## 2.3 Training Algorithm\n\nWe start from a student model pre-trained on a large long-document corpus D PT. Algorithm 2.3 trains MINILLM by adapting the student model to a text generation task with dataset D and supervision from the teacher model, such as an LLM fine-tuned on D [TGZ + 23, CLL + 23] or that with good task-generalization [CHL + 22, Ope23]. In the training algorithm, we first fine-tune the student model on D and pick the checkpoint with the lowest loss as an initialization for the following training. Then, we compute the gradients ( ∇L ) Single and ( ∇L ) Norm Long based on Eq. 5 and Eq.\n\n6, with a clipping strategy [SWD + 17] added to further improve stability. Same as [OWJ + 22], we include a language modeling loss L PT = -E d ∼D PT log q θ ( d ) to preserve the model performance on canonical NLP benchmarks. The student model is finally updated using a combination of gradients ( ∇L ) Single +( ∇L ) Norm Long + ∇L PT. The whole on-policy training pipeline is similar to Reinforcement Learning from Human Feedback (RLHF; OWJ + 22).\n\n## Algorithm 1 MINILLM: Knowledge Distillation of LLMs\n\nInput: Conditional generation dataset D consisting of prompts and ground-truth responses\n\nPre-training corpus D PT consisting of long-document plain texts\n\nA teacher model with output distribution p\n\nAn initial student model pre-trained on D PT, with the output distribution q θ 0\n\nLearning rate η ; Batch size M ; Clipping Threshold ϵ\n\nOutput: A student model with the output distribution q θ\n\nFine-tune the student model from θ 0 on D supervised by the ground truth responses and choose θ with the lowest validation loss.\n\n## repeat\n\n```\nSample a mini-batch of prompts from D and collect responses from ˜ p to get S = { ( x m , y m ) } M m =1 Sample a mini-batch D ′ PT = { d m } M m =1 from D PT Compute ( ∇L ) Single = -1 M ∑ x , y ∈S ∑ T t =1 w t ∇ ∑ y t ∈ V q θ ( y t | y <t , x ) log p ( y t | y <t , x ) q θ ( y t | y <t , x ) ▷ Eq. 5 Compute ( ∇L ) Norm Long = -1 | M | ∑ x , y ∈S ∑ T t =1 R Norm t +1 ∇ min[ ρ t ( θ ) , clip( ρ t ( θ ) , 1 -ϵ, 1 + ϵ )] , where ρ t ( θ ) = q θ ( y t | y <t , x ) ˜ p ( y t | y <t , x ) ▷ Eq. 5, Eq. 6 Compute the gradient of the language modeling loss: ∇L PT = -1 M ∑ d ∈ D ′ PT ∇ log q θ ( d ) Update model parameters: θ ← θ -η [ ( ∇L ) Single +( ∇L ) Norm Long + ∇L PT ]\n```\n\nuntil converge and return q θ\n\n## 3 Experiments\n\n## 3.1 Experimental Setup\n\nWe take instruction-following [OWJ + 22] as the conditional text generation task, where models are trained to generate responses according to the instructions. We fine-tune a large model on the dataset D consisting of instruction-response pairs as the teacher model. Then, we compare different KD methods on D by evaluating the student model's instruction-following performance.\n\nBase Models Our student models come from three model families with various sizes: GPT2 [RWC + 19] (120M, 340M, 760M), OPT [ZRG + 22] (1.3B, 2.7B, 6.7B), and LLaMA [TLI + 23] (7B). For teacher models of each model family, we use GPT-2-1.5B, OPT-13B, and LLaMA-13B respectively. These models are fine-tuned on D in advance. We also present the results using GPTJ [WK21] as the teacher model in Appendix C.1.\n\nTraining We construct the training data from databricks-dolly-15K 4 consisting of 15K human-written instruction-response pairs. We filter out samples that exceed the context length of the models. Then, we randomly split 1K and 0.5K samples for validation and testing, respectively, leaving about 12.5K examples for training. For D PT, we use OpenWebText [GCPT19] for the GPT-2 family and the RoBERTa training corpus [LOG + 19] for other models. We set the teacher-mix-in strength α = 0 . 2 throughout the experiments in Eq. 4. We use Rouge-L [Lin04] scores on the validation set to search for hyper-parameters because it aligns better with human preference than validation losses [WMA + 22]. More details are shown in Appendix B.1.\n\nEvaluation We evaluate the trained models on five instruction-following datasets:\n\n- DollyEval : the 500-sample test set we split from the databricks-dolly-15k dataset.\n- SelfInst [WKM + 23]: A user-oriented instruction-following set with 252 samples.\n- VicunaEval [CLL + 23]: The 80 challenging questions used in the Vicuna evaluation.\n- S-NI : The test set of SUPER-NATURALINSTRUCTIONS [WMA + 22] consisting of 9K samples ranging from 119 tasks. Following [PLH + 23], we split the set into 3 subsets whose ground truth\n\n4 https://github.com/databrickslabs/dolly/tree/master\n\nresponse lengths lie in [0 , 5] , [6 , 10] , and [11 , + ∞ ] . We use the [11 , + ∞ ] subset in Section 3.2 and conduct an analysis on all subsets in Section 3.3.\n\n- UnNI : We randomly sample 10K samples from the core set of UNNATURALINSTRUCTIONS [HSLS23] for evaluation. Similar to S-NI , we first conduct the evaluations on the [11 , + ∞ ] subset, followed by an analysis of the performance on all subsets in Appendix C.3.\n\nWe adopt three metrics to evaluate the model-generated responses:\n\n- R-L : The Rouge-L [Lin04] score to measure the precision of the model generation. [WMA + 22] has shown that Rouge-L is suitable for large-scale instruction-following evaluation.\n- GPT4 : The GPT-4 feedback [ZCS + 23] by asking GPT-4 to compare model-generated responses with the ground truth answers 5 and raise 1-10 scores for both responses (see Appendix B.2 for the prompt we use). We report the ratio of the total score of model responses and ground truth answers. This metric is only applied to DollyEval, SelfInst, and VicunaEval.\n- HumanEvaluation : Weconduct human evaluations on the SelfInst dataset following [PLH + 23] by asking volunteers to compare two responses produced by different models and annotate 'Win', 'Tie', or 'Loss'. More human evaluation details can be found in Appendix B.3.\n\nFor all test sets, we sample the responses with the temperature = 1 and report the average scores of 5 generations for each prompt with different random seeds.\n\nBaselines We consider three baselines in our main experiment:\n\n- SFT w/o KD directly fine-tunes the student model on D supervised by the golden responses.\n- KD [SDCW19, SST + 20] fine-tunes the student model on D using the teacher distribution as the supervision at each token step, also known as word-level KD.\n- SeqKD [KR16, CLL + 23, TGZ + 23, PLH + 23, ZLX + 23] fine-tunes the student model on the data generated by the teacher model.\n\n## 3.2 Results\n\nWe present the R-L and GPT4 evaluation results in Table 1, from which we have three observations.\n\nFirst , by comparing the overall performance of MINILLM with the baselines, we observe that the model distilled by our KD method outperforms the baselines in almost all cases, when trained with different base models, tested on various evaluation sets, and scored by both Rouge-L and GPT-4 feedback. This verifies the good generalization and high overall performance of our KD method. We also find that MINILLM generally works much better on datasets other than Dolly compared with the baselines, indicating its good out-of-distribution generalization.\n\nSecond , the Rouge-L scores show that MINILLM produces the most precise responses that have high overlaps with the ground-truth responses. In some cases, especially on Vicuna, S-NI, and UnNI, student models reach even higher Rouge-L scores than the teacher models, which matches the observation in [FLT + 18]. We conjecture that the standard teacher-forcing fine-tuning on D brings training-inference discrepancy to the teacher model, also known as exposure bias [BVJS15]. On the contrary, MINILLM is optimized with policy optimization methods, which samples responses from student models during training and thus alleviates exposure bias [PH21]. We include further analysis on exposure bias in Section 3.3.\n\nThird , comparing the results across model sizes and model families, we can see that the improvement of MINILLM is consistent when the base model sizes vary from 120M to 13B across three model families. This tendency is also illustrated in Figure 1, which demonstrates the excellent scalability and generalization of our method in the era of LLMs.\n\nThe human evaluation results on the SelfInst dataset based on the LLaMA family are shown in Figure 4. MINILLM obtains better human preference than all the baselines, performing comparably to the teacher model.\n\nFigure 4: Human evaluation results. We use LLaMA-7B as the student and LLaMA-13B as the teacher.\n\n<!-- image -->\n\n5 We use the ChatGPT's generation [Ope22] for VicunaEval's ground truth responses.\n\nTable 1: Evaluation results. GPT4 and R-L stand for the average GPT-4 feedback scores and RougeL scores across 5 random seeds, respectively. The best scores of each model size are boldfaced , and the scores where the student model outperforms the teacher are marked with *.\n\n| Model   | #Params   | Method        | DollyEval   | DollyEval   | SelfInst   | SelfInst   | VicunaEval GPT4   | VicunaEval GPT4   | S-NI R-L   | UnNI       |\n|---------|-----------|---------------|-------------|-------------|------------|------------|-------------------|-------------------|------------|------------|\n|         |           |               | GPT4        | R-L         | GPT4       | R-L        |                   | R-L               |            | R-L        |\n|         | 1.5B      | Teacher       | 58.4        | 27.6        | 42.9       | 14.3       | 48.6              | 16.3              | 27.6       | 31.8       |\n|         |           | SFT w/o KD    | 38.6        | 23.3        | 26.3       | 10.0       | 32.8              | 14.7              | 16.3       | 18.5       |\n|         |           | KD            | 40.3        | 22.8        | 27.8       | 10.8       | 31.9              | 13.4              | 19.7       | 22.0       |\n|         | 120M      | SeqKD         | 41.2        | 22.7        | 26.2       | 10.1       | 31.0              | 14.3              | 16.4       | 18.8       |\n|         |           | MINILLM       | 44.7        | 24.6        | 29.2       | 13.2       | 34.1              | 16.9 *            | 25.3       | 26.6       |\n| GPT-2   |           | SFT w/o KD    | 51.9        | 25.5        | 39.6       | 13.0       | 42.3              | 16.0              | 25.1       | 32.0       |\n|         |           | KD            | 51.6        | 25.0        | 39.2       | 12.0       | 42.8              | 15.4              | 23.7       | 31.0       |\n|         | 340M      | SeqKD         | 50.5        | 25.3        | 39.0       | 12.6       | 43.0              | 16.9*             | 22.9       | 30.2       |\n|         |           | MINILLM       | 52.2        | 25.4        | 40.5       | 15.6       | 42.6              | 17.7 *            | 27.4       | 34.5       |\n|         |           | SFT w/o KD    | 50.7        | 25.4        | 38.3       | 12.4       | 43.1              |                   | 21.5       | 27.1       |\n|         |           | KD            | 53.4        | 25.9        | 40.4       | 13.4       |                   | 16.1 16.9*        | 25.3       | 31.7       |\n|         | 760M      | SeqKD         | 52.0        | 25.6        | 38.9       | 14.0       | 43.4              | 15.9              | 26.1       | 32.9       |\n|         |           | MINILLM       | 54.7        | 26.4        | 44.6*      | 15.9       | 42.4 45.7         | 18.3*             | 29.3*      | 37.7*      |\n|         |           | Teacher       | 70.3        | 29.2        | 56.1       | 18.4       | 58.0              | 17.8              | 30.4       | 36.1       |\n|         | 13B       | SFT w/o KD    | 52.6        | 26.0        | 37.7       | 11.4       | 40.5              | 15.6              |            | 28.4       |\n|         |           | KD            | 52.7        | 25.4        | 36.0       | 12.2       | 40.8              | 14.9              | 23.1 21.9  | 27.0       |\n|         | 1.3B      | SeqKD         | 51.0        | 26.1        | 36.6       | 12.7       | 42.6              | 16.6              | 21.4       | 28.2       |\n|         |           |               |             |             | 47.0       | 14.8       |                   | 17.9*             | 28.6       | 33.4       |\n|         |           | MINILLM       | 60.7        | 26.7        | 38.9       | 13.9       | 50.6              | 16.6              | 24.9       | 32.3       |\n| OPT     |           | SFT w/o KD    | 55.4        | 27.1        | 48.6       | 13.8       | 44.8              | 16.7              | 26.3       | 30.2       |\n|         |           | KD            | 60.5        | 25.9        | 40.5       | 13.3       | 51.3              |                   |            |            |\n|         | 2.7B      | SeqKD         | 57.6        | 27.5 27.4   | 52.7       | 17.2       | 44.5 55.9         | 16.5 19.1*        | 25.3       | 32.3       |\n|         |           | MINILLM       | 63.2        | 27.6        | 56.4       | 16.4       | 57.3              | 17.8              | 30.7* 30.3 | 35.1 28.6  |\n|         |           | SFT w/o KD    | 67.9        |             |            |            |                   |                   |            |            |\n|         |           | KD            | 68.6        | 28.3        | 58.0       | 17.0       | 57.0              | 17.5              | 30.7*      |            |\n|         | 6.7B      | SeqKD         | 69.6        | 28.5        | 54.0       | 17.0       | 57.6              | 17.9*             | 30.4       | 26.7 28.2  |\n|         |           | MINILLM       | 70.8*       | 29.0        | 58.5*      | 17.5       | 60.1*             | 18.7*             | 32.5*      | 36.7*      |\n|         | 13B       | Teacher       | 79.0        | 29.7        | 75.5       | 23.4       | 65.1              | 19.4              | 35.8       | 38.5       |\n| LLaMA   |           | SFT w/o KD    | 73.0        | 26.3        | 69.2       | 20.8       | 61.6              | 17.5              | 32.4       | 35.8       |\n|         |           | SeqKD MINILLM | 73.6 76.4   | 27.5 29.0   | 71.5 73.1  | 20.8 23.2  | 62.6 64.1         | 18.1 20.7*        | 33.7 35.5  | 37.6 40.2* |\n\n## 3.3 Analysis\n\nScaling Law of Teacher Although it is intuitive that we can distill better student models from larger teacher models, [MFL + 20] has shown that increasing the teacher models' sizes does not guarantee the improvement of student models, sometimes even harming the distillation performance. It is not clear how MINILLM works when we scale up the teacher models' sizes. Therefore, we compare MINILLM and SeqKD using teacher models with different sizes and fix the size of the student model. We present the results based on the GPT-2 family in Figure 5 and that based on the OPT family in Appendix C.2. We can see that MINILLM constantly outperforms SeqKD, and the student model performance is positively correlated with the teacher model sizes. This shows the potential of our method to compress models with massive parameters.\n\nFigure 5: The scaling law of teacher based on the GPT-2 family models. We compare MINILLM and SeqKD with GPT-2-125M as the student and GPT-2 340M, 760M, and 1.5B as teachers.\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 6: The excess error caused by the trainingdecoding discrepancy (ExAccErr) accumulated with the generation length. Lower ExAccErr means the method introduces less exposure bias.\n\n<!-- image -->\n\nFigure 7: The Rouge-L scores of the distilled models against SFT on the different subsets of SNI split by the golden responses' length.\n\nTable 2: The ECE and accuracy scores on SST2 and BoolQ datasets. The best scores among student models are boldfaced .\n\n|         | SST2   | SST2   | BoolQ   | BoolQ   |\n|---------|--------|--------|---------|---------|\n|         | ECE    | Acc.   | ECE     | Acc.    |\n| Teacher | 0.025  | 93.0   | 0.356   | 74.5    |\n| KD      | 0.191  | 84.7   | 0.682   | 63.5    |\n| SeqKD   | 0.243  | 66.5   | 0.681   | 62.8    |\n| MINILLM | 0.099  | 89.7   | 0.502   | 67.8    |\n\nTable 3: The distinct 4-grams (Dist-4) and language modeling loss (Loss) on the test sets based on the LLaMA family. MINILLM preserves generation diversity.\n\n|         | DollyEval   | DollyEval   | SelfInst   | SelfInst   |\n|---------|-------------|-------------|------------|------------|\n|         | Dist-4      | Loss        | Dist-4     | Loss       |\n| Teacher | 99.3        | 3.55        | 99.1       | 4.44       |\n| SFT     | 99.5        | 3.89        | 99.0       | 5.28       |\n| MINILLM | 99.0        | 3.95        | 98.6       | 5.33       |\n\nExposure Bias Language generation models trained to minimize forward KLD suffer from exposure bias [BVJS15] caused by the discrepancy between teacher-forcing training and free-run generation. When training MINILLM, the student model sees samples generated by itself, alleviating the training-inference mismatch [PH21]. In Figure 6, we use the ExAccErr metric [AEABC22] defined in Appendix B.5 to measure the excess accumulated error due to exposure bias. The experiment is based on GPT-2-125M, with GPT-2-1.5B as the teacher, using Dolly as the test set. For each prompt, we sample 10 responses to reduce the variance. We can see that the ExAccErrs of the baselines continuously grow during generation, while MINILLM has a much lower ExAccErr, and the error stops accumulating in long-text generation ( &gt; 150 tokens).\n\nCalibration [Ope23] has shown that models trained with policy optimization are likely to be poorly calibrated. We test the calibration of MINILLM and the KD baselines on two widely-used text classification datasets: SST2 [SPW + 13] and BoolQ [CLC + 19], based on LLaMA-7B. We design zero-shot classification instructions (see Appendix B.2) and take the probability of the label words to compute the ECE scores [NDZ + 19]. From Table 2, we observe that KD and SeqKD models are worse calibrated than the teacher model, which potentially explains their low performance on canonical benchmarks [GWS + 23]. We suspect that minimizing forward KLD causes the models to push high probabilities to void regions of the target distribution, which leads to significant distribution differences between the student and the teacher (see the example in Figure 2). In contrast, MINILLM focuses on accurately learning the major parts of the target distribution and narrows the ECE scores gap between the student and the teacher.\n\nPerformance on Different Response Length Westudy the models' performance when the golden response lengths belong to different ranges. In Figure 7, we illustrate the Rouge-L scores of different KD models against the SFT models on three S-NI subsets split by the length of the ground truth responses. We can see that all methods achieve low scores on prompts that expect short responses ( ≤ 5 tokens), probably because most responses in our training set are long sentences, introducing a distribution shift between training and evaluation [PLH + 23]. Furthermore, the output spaces of these prompts are relatively small, allowing the student model to cover most modes of the teacher, and thus reverse KLD and forward KLD have similar performance. For prompts with longer responses ( ≥ 6 tokens), the teacher distribution contains more modes than the students due to the complex out-\n\nTable 4: The performance on the validation and test set when different combinations of MINILLM optimization strategies are applied.\n\n|                   |   Valid. R-L |   Dolly R-L |\n|-------------------|--------------|-------------|\n| MINILLM           |         27.4 |        24.6 |\n| w/o Length Norm.  |         17.4 |        14.7 |\n| w/o Teacher-Mixed |         22.3 |        20.4 |\n| w/o Single-Step   |         27   |        23.7 |\n\nFigure 8: The reverse KLD between the teacher and the students during MINILLM training when different optimization strategies are applied.\n\n<!-- image -->\n\nput spaces, which shows the advantage of MINILLM against standard KD models. Similar results on UnNI are shown in Appendix C.3.\n\nGeneration Diversity [CCF + 20] has found that the model optimized by minimizing reverse KLD is likely to lose modes, which affects the generation diversity. We follow [PH21] to discuss generation diversity from three aspects: (i) generating multiple distinct responses given a prompt. (ii) generating linguistically complex responses. (iii) the ability to generate contents that have high coverage of the real data distribution. For (i), we argue that for many NLP applications, generating one correct response is sufficient, especially for those scenarios demanding high truthfulness and reliability [JLF + 23]. For (ii) and (iii), we report the responses' distinct 4-gram proportion and the language modeling loss on the test sets in Table 3, using the base models from the LLaMA family (see Appendix B.4 for more details) . We can see that MINILLM preserves the distinct 4-gram proportion in the generated responses and language modeling loss on the test set.\n\n## 3.4 Ablation Studies on Optimization Strategies\n\nWeevaluate the effectiveness of the three strategies proposed to stabilize and accelerate optimization in Section 2.2 by distilling a GPT-2-125M model from the GPT-2-1.5B model. More ablation studies can be found in Appendix C.4. In Table 4, we report the best Rouge-L scores on the validation set of each run and the evaluation results of the corresponding checkpoints. We also plot the reverse KLD between the student and the teacher during training in Figure 8, where the curves are smoothed by 32 steps. We can see that Teacher-Mixed Sampling and Length Normalization works for stabilizing training. Although the reverse KLDs also decrease without these strategies, we find that the models quickly learn to generate repeated, short, or meaningless strings that have high probabilities in the teacher distribution (see examples in Appendix D), which is known as reward hacking [SHKK22]. This also leads to the low generation performance in Table 4. From Figure 8, we also observe that the Single-Step Decomposition effectively reduces the variance of the training process, which also results in higher scores on the validation and test sets.\n\n## 4 Related Work\n\nLarge Language Models Large language models (LLMs; BMR + 20, TDFH + 22, CND + 22, Ope23, ADF + 23) have shown superior performance by solving various NLP tasks in a generative manner. Recent works apply instruction tuning [WBZ + 22, SWR + 22, CHL + 22] or learning from human feedback [OWJ + 22, BJN + 22] to improve the alignment of LLMs with humans further and create general AI assistants [Ope22, Goo23]. There are also efforts to build open-source LLMs [ZRG + 22, TLI + 23, BSA + 23] to facilitate research and industry development. Although appealing, the broad capacities of LLMs usually emerge with large model sizes [KMH + 20, WTB + 22] that require massive computational resources. Therefore, model compression is critical for the practical deployment and further research of LLMs.\n\nKnowledge Distillation Knowledge distillation (KD; HVD15), as a widely used model compression technique, aims at training a student model with the guidance of a teacher model [RCG + 15, SDCW19, JBMD21]. In the NLP community, many works apply KD to text classification tasks by mimicking the teacher model's output distribution [SST + 20, LHS + 21, ZSL + 23], hid-\n\nden states [JYS + 20, SCGL19], or attention scores [WWD + 20, WBH + 21]. For text generation, the standard KD method is to approximately minimize the forward KLD between the student's and the teacher's generation distribution by using the teacher's output at each time step as supervision [SDCW19] or direct training on the teacher's generated texts [KR16, TGZ + 23, CLL + 23, PLH + 23]. In this paper, we minimize the reverse KLD, which is more suitable for LLMs when the teacher distribution is available. Concurrent works [A VS + 23, WLDM23] also explore more the distribution discrepancy metrics in KD.\n\nDistribution Discrepancy Metrics in Text Generation The distribution discrepancy metrics play a significant role in training text generation models. The forward KLD is widely used due to its simplicity when derived as the Maximum Likelihood Estimate (MLE) objective [ZZ19]. However, previous works show that minimizing forward KLDleads to zero-forcing behavior where models try to cover all modes of the target distribution and sacrifice the accuracy of major modes [Hus15]. Some works resort to using other metrics to remedy this problem, such as reverse KLD [JHC + 20], Total Variation Distance [JKH + 23], and Optimal Transport [LLW + 20]. Our paper tackles this problem under the scenario of knowledge distillation for LLMs.\n\n## 5 Conclusion\n\nIn this work, we investigate the problem of distilling the knowledge of LLMs into small language models. We find that the standard distillation methods minimizing the forward KLD is sub-optimal in language generation scenarios because the teacher's output distribution contains more modes than the student's, and forward KLD forces the student distribution to overestimate the low-probability regions of the teacher distribution. Therefore, we propose MINILLM that minimizes the reverse KLD between the teacher and student distribution and design an algorithm to optimize this objective. Extensive experiments show that MINILLM produce more precise responses that have higher overall quality than standard KD models. We also find that MINILLM has lower exposure bias, better calibration, and higher performance in long-text generation with good generation diversity.\n\n## Acknowledgements\n\nThis work was supported by the National Key Research and Development Program of China (No. 2021ZD0113304), the National Science Foundation for Distinguished Young Scholars (with No. 62125604), and the NSFC projects (Key project with No. 61936010).\n\n## References\n\n- [ADF + 23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 , 2023.\n- [AEABC22] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. In Findings of ACL , 2022.\n- [AVS + 23] Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, and Olivier Bachem. GKD: Generalized knowledge distillation for auto-regressive sequence models. arXiv preprint arXiv:2306.13649 , 2023.\n- [BHA + 21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.\n- [BJN + 22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 , 2022.\n\n- [BLW + 21] Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021.\n- [BMR + 20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models are few-shot learners. In Proceedings of NeurIPS , 2020.\n- [BSA + 23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\n- [BVJS15] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Proceedings of NeurIPS , 2015.\n- [CCF + 20] Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Charlin. Language gans falling short. In ICLR , 2020.\n- [CDP + 18] Liqun Chen, Shuyang Dai, Yunchen Pu, Erjin Zhou, Chunyuan Li, Qinliang Su, Changyou Chen, and Lawrence Carin. Symmetric variational autoencoder and connections to adversarial learning. In Proceedings of AISTATS , 2018.\n- [CHL + 22] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\n- [Cio21] Kamil Ciosek. Imitation learning by reinforcement learning. In ICLR , 2021.\n- [CLC + 19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL-HLT , 2019.\n- [CLL + 23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n- [CND + 22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n- [CPO + 19] Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In Proceedings of AISTATS , 2019.\n- [CvdS21] Alex James Chan and Mihaela van der Schaar. Scalable bayesian inverse reinforcement learning. In ICLR , 2021.\n- [FLT + 18] Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In Proceedings of ICML , 2018.\n- [GCPT19] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus, 2019.\n- [Goo23] Google. Bard, 2023.\n- [GWS + 23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717 , 2023.\n- [HBD + 20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In Proceedings of ICLR , 2020.\n\n- [HLM22] Yongchang Hao, Yuxin Liu, and Lili Mou. Teacher forcing recovers reward functions for text generation. In Proceeings of NeurIPS , 2022.\n- [HSLS23] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of ACL , 2023.\n- [HTAL17] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of ICML , 2017.\n- [Hus15] Ferenc Huszár. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101 , 2015.\n- [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015.\n- [HZD + 21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past, present and future. AI Open , 2021.\n- [JBMD21] Gou Jianping, Yu Baosheng, Stephen J Maybank, and Tao Dacheng. Knowledge distillation: A survey. IJCV , 2021.\n- [JHC + 20] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Proceedings ACL , 2020.\n- [JKH + 23] Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, and Minlie Huang. Tailoring language generation models under total variation distance. In Proceedings of ICLR , 2023.\n- [JLF + 23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys , 2023.\n- [JYS + 20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of EMNLP , 2020.\n- [KMH + 20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- [KR16] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Proceedings of EMNLP , 2016.\n- [LGB + 16] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversitypromoting objective function for neural conversation models. In Kevin Knight, Ani Nenkova, and Owen Rambow, editors, Proceedings of NAACL , 2016.\n- [LHS + 21] Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. Mix{kd}: Towards efficient distillation of large-scale language models. In Proceedings of ICLR , 2021.\n- [Lin04] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Proceedings of Text Summarization Branches Out (ACL 2004) , 2004.\n- [LKTF20] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.\n- [LLW + 20] Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuhchen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, et al. Improving text generation with student-forcing optimal transport. In Proceedings of EMNLP , 2020.\n\n- [LOG + 19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.\n- [LPSK23] Hyoje Lee, Yeachan Park, Hyun Seo, and Myungjoo Kang. Self-knowledge distillation via dropout. Computer Vision and Image Understanding , 2023.\n- [M + 05] Tom Minka et al. Divergence measures and message passing. Technical report, Citeseer, 2005.\n- [MFL + 20] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of AAAI , 2020.\n- [MG19] Andrey Malinin and Mark Gales. Reverse KL-divergence training of prior networks: Improved uncertainty and adversarial robustness. In Proceedings of NeurIPS , 2019.\n- [NCT16] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Proceedings of NeurIPS , 2016.\n- [NDZ + 19] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops , 2019.\n- [Ope22] OpenAI. OpenAI: Introducing ChatGPT, 2022.\n- [Ope23] OpenAI. GPT-4 technical report, 2023.\n- [OWJ + 22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS , 2022.\n- [PH21] Richard Yuanzhe Pang and He He. Text generation by learning from demonstrations. In Proceedings of ICLR , 2021.\n- [PLH + 23] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277 , 2023.\n- [PSS00] Doina Precup, Richard S Sutton, and Satinder P Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of ICML , 2000.\n- [RCG + 15] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv preprint arXiv:1511.06295 , 2015.\n- [RWC + 19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Technical report , 2019.\n- [SCGL19] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In Proceedings EMNLP , 2019.\n- [SDCW19] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n- [SHKK22] Joar Max Viktor Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Proceedings of NeurIPS , 2022.\n- [SMSM99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Proceedings of NeurIPS , 1999.\n\n- [SPW + 13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP , October 2013.\n- [SSG + 17] Iulian V Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, et al. A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349 , 2017.\n- [SST + 20] Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. LightPAFF: A two-stage distillation framework for pre-training and fine-tuning. arXiv preprint arXiv:2004.12817 , 2020.\n- [SWD + 17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\n- [SWR + 22] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, et al. Multitask prompted training enables zero-shot task generalization. In Proceedings of ICLR , 2022.\n- [TDFH + 22] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022.\n- [TGZ + 23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instructionfollowing LLaMA model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.\n- [TLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- [TWS18] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of IJCAI , 2018.\n- [WBH + 21] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of ACL , 2021.\n- [WBZ + 22] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings of ICLR , 2022.\n- [Wil92] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 1992.\n- [WK21] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, 2021.\n- [WKM + 23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of ACL , 2023.\n- [WLDM23] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation. In Proceedings of ACL , 2023.\n- [WMA + 22] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. In Proceedings of EMNLP , 2022.\n\n- [WTB + 22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. Transactions on Machine Learning Research , 2022.\n- [WWD + 20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In Proceedings of NeurIPS , 2020.\n- [WWZ + 23] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402 , 2023.\n- [ZCS + 23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of NeurIPS , 2023.\n- [ZLX + 23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. In Proceedings of NeurIPS , 2023.\n- [ZMB + 08] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In Proceedings of AAAI , 2008.\n- [ZRG + 22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022.\n- [ZSL + 23] Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation. arXiv preprint arXiv:2305.05010 , 2023.\n- [ZZ19] Huan Zhang and Hai Zhao. Minimum divergence vs. maximum margin: an empirical comparison on seq2seq models. In International Conference on Learning Representations , 2019.\n\n## A Derivations\n\n## A.1 A Perspective of MINILLM from Inverse Reinforcement Learning\n\nIn Section 2.1, we formulate KD as an optimization problem of minimizing the discrepancy between the teacher distribution and the student distribution and finally reach the objective of minimizing reverse KLD. Alternatively, we can also regard KD as training the student model with the teacher model's guidance, which resembles an agent learning from the feedback from an environment. Following [PH21], we treat token generation as a Markov Decision Process. At each time step t , the student model chooses an action (token) y t from the action space (vocabulary) V conditioning on the state (prefix) ( y &lt;t , x ) based on the policy (generation probability) q θ ( y t | y &lt;t , x ) .\n\nFrom this perspective, standard KD corresponds to behavior cloning (BC; TWS18) in imitation learning [Cio21]. However, BC is known to under-perform Inverse Reinforcement Learning (IRL; ZMB + 08), another imitation learning method that first recovers a reward model from the environment demonstrations and then trains the policy with the reward using policy optimization algorithms [SMSM99, SWD + 17]. Therefore, in the KD scenario, we seek to first induce a reward r ( y t , ( y &lt;t , x )) from the environment (the teacher model) and then train the student model to maximize the reward as the objective. We take the maximum-entropy inverse reinforcement learning framework [ZMB + 08, CvdS21] and thus the Q-function Q ( y t , ( y &lt;t , x )) in the environment satisfies the soft Bellman Equation:\n\n<!-- formula-not-decoded -->\n\nWe follow [HLM22] to parameterize the Q-function as Q ( y t , ( y &lt;t , x )) = f ( y t , ( y &lt;t , x )) and assume γ = 1 , where f ( y t , ( y &lt;t , x )) is the output logits of the teacher model 6 . Then, the reward is given by:\n\n<!-- formula-not-decoded -->\n\nTo maximize the reward, we apply maximum-entropy reinforcement learning [HTAL17], whose learning objective is\n\n<!-- formula-not-decoded -->\n\nwhere H[ q θ ( ·| y &lt;t , x )] = -E y t ∼ q θ ( ·| y &lt;t , x ) log q θ ( ·| y &lt;t , x ) is the entropy of the student model distribution at the time step t .\n\nEquivalence Between Objectives We prove an approximate equivalence between Eq. 10 and Eq. 1. We first rewrite the summation of the reward ∑ | y | t =1 r ( y t , ( y &lt;t , x )) by the associative law:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n6 The teacher model's distribution satisifies p ( y t | y &lt;t , x ) = exp( f ( y t , ( y &lt;t , x ))) ∑ y ′ ∈ V exp( f ( y ′ , ( y &lt;t , x ))) .\n\nThen, J ( θ ) can be approximately rewritten as:\n\n<!-- formula-not-decoded -->\n\nTherefore, maximizing J ( θ ) is approximately equivalent to minimizing L ( θ ) .\n\n## A.2 Derivation of Equation 2\n\nWe compute the gradient of L ( θ ) = KL[ q θ || p ] with respect to θ using the Policy Gradient Theorem [SMSM99]:\n\n<!-- formula-not-decoded -->\n\nwhere Eq. 21 is based on the fact that log q θ ( y t | y &lt;t , x ) can only affect tokens at ≥ t positions in y . By setting R t = ∑ T t ′ = t log p ( y t ′ | y &lt;t ′ , x ) q θ ( y t ′ | y &lt;t ′ , x ) , we obtain Eq. 2.\n\n## A.3 Derivation of Equation 3\n\nTo derive Eq. 3, we first denote:\n\n<!-- formula-not-decoded -->\n\nThen, we re-write ∇L ( θ ) as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere Eq. 27 uses the product rule of the gradient and r t = log p ( y t | y &lt;t , x ) q θ ( y t | y &lt;t , x ) .\n\n## B Experimental Details\n\n## B.1 Training Details\n\nBaselines Our baselines include SFT w/o KD , KD , and SeqKD . For models with less than 1.3B parameters, we search for the learning rates in [5e-4, 1e-4, 5e-5], the batch sizes in [32, 64], and train these models for 20 epochs. For other models, we search for the learning rate in [5e-5, 1e5, 5e-6], the batch sizes in [32, 64], and train these models for 10 epochs. For KD , we follow [SST + 20] to mix the distillation loss with the language modeling loss on the ground truth responses by a mixture rate of 0.5. The checkpoints of each baseline are selected by the Rouge-L [Lin04] scores on the validation set because, as stated in previous works [WMA + 22, OWJ + 22], we also find that Rouge-L is better correlated with human judgments.\n\nMINILLM As stated in Section 2.3, training of MINILLM has two phases which is similar to Reinforcement Learning from Human Feedback (RLHF;OWJ + 22).\n\n- Phase 1 : We fine-tune the student model on the instruction-response training set D to get a starting point for the subsequent MINILLM training. We fine-tune the model for 3 epochs using the best learning rate and batch size of the corresponding SFT w/o KD baselines. Note that different from the SFT w/o KD baseline, we select the checkpoint with the lowest validation loss , not the Rouge-L score in this phase.\n- Phase 2 : We continuously train the model from Phase 1 as described in Algorithm 2.3 using a learning rate 5e-6, a mini-batch size 64 in all cases. The training and validation set are same as in Phase 1 . Similar to [OWJ + 22], we collect 256 sentences at once and adopt 4 inner epochs when doing the policy optimization. The clipping rate ϵ is set to 0.2, and the max length of the model is 512. We use temperature = 1 when sampling from q θ . We train the model for 5000 steps and select the final checkpoint using the Rouge-L score on the validation set. Our experiments are based on the NVIDIA V100 32G GPUs. Distilling LLaMA-7B from LLaMA-13B takes less than 10 ours on 16 GPUs.\n\n## B.2 Automatic Evaluation Details\n\nDuring the evaluation, we sample the responses from each model using temperature = 1, a maxlength limit of 512, and random seeds [10, 20, 30, 40, 50]. Similar to [TGZ + 23], we adopt a prompt wrapper shown in Figure 9 to convert each instruction-response pair to a sentence. For the GPT-4\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Input: {input} ### Response:\n```\n\nFigure 9: The prompt wrapper for training and evaluation.\n\nWe would like to request your feedback on the performance of two AI assistants in response to the user instruction and input displayed above.\n\nPlease rate the helpfulness, relevance, accuracy, and level of detail of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\n\nPlease first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space.\n\nIn the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.\n\nFigure 10: GPT-4 evaluation prompt.\n\nfeedback, we apply the prompt in Figure 10 and set the temperature = 0.7. For the classification tasks in the 'Calibration' paragraph of Section 3.3, we prompt the model to do zero-shot text classification with the templates in Figure 11 and 12.\n\n## B.3 Human Evaluation Details\n\nFollowing [PLH + 23], we use SelfInst [WKM + 23] to perform human evaluation. We randomly sampled 50 prompts because we found that more prompts do not affect the results much. We ask the annotators to compare the responses generated by the baseline models with MINILLM and decide which response is preferred or neither of them is significantly better. Note that which model the responses come from is invisible to the annotators. The interface presented to annotators is shown in Figure 13.\n\nFigure 11: Zero-shot text classification prompt for SST2.\n\n<!-- image -->\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Read the input passage and answer the question: {question}? Your answer should be 'Yes' or 'No'. ### Input: {passage} ### Response:\n```\n\nFigure 12: Zero-shot text classification prompt for BoolQ.\n\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n```\n### Instruction: Desk jobs require writing a lot of emails, so it isn't surprising we get tired of repeating ourselves. Come up with several synonyms for the given word. ### Input: Sincerely ### Response: ##### Answer #1 ##### Fondly, affectionately, lovingly, tenderly, honestly, truly, faithfully, devotedly, passionately ##### Answer #2 ##### Faithfully, Gullibly, Humbly, Piously, Strangely, Weirdly, Yours truly 1: Answer #1 is better 2: Answer #2 is better 3: Tie Your choice:\n```\n\nFigure 13: The prompt wrapper for training and evaluation.\n\n## B.4 Details About Generation Diversity Metrics\n\nIn Table 3, we report the distinct 4-grams (Dist-4) and the language modeling loss (Loss) on the test sets. More details about these two metrics are as follows:\n\n- 'Dist-4' is a fraction: N/C , where N is the number of the distinct 4-grams in the generated responses and C is the total number of 4-grams. It is a widely used metric to measure the generation diversity of a language model [LGB + 16]. The ( N/C ) s on the Dolly test set across 5 random seeds are shown in Table 5. Table 3 reports the average values across the 5 random seeds.\n- 'Loss' is the negative log-likelihood loss on the test set D Test : -∑ x , y ∼D Test log q θ ( y | x ) . It measures the mode coverage of the real data distribution because it is essentially the forward KLDbetween the real data distribution and the model output distribution. This relates to diversity as in the ability to generate different generations given one context with different random seeds.\n\nTable 5: The ( N/C )s, where N is the number of the distinct 4-grams in the generated responses and C is the total number of 4-grams. We report the numbers computed on the Dolly test set when evaluated with 5 random seeds: [10, 20, 30, 40, 50].\n\n| Model   | 10            | 20            | 30            | 40            | 50            |\n|---------|---------------|---------------|---------------|---------------|---------------|\n| Teacher | 23562 / 23696 | 23653 / 23834 | 24306 / 24488 | 24207 / 24381 | 23803 / 23967 |\n| KD      | 25889 / 26064 | 24024 / 24197 | 25663 / 25843 | 25611 / 25763 | 26178 / 26339 |\n| SeqKD   | 25358 / 25519 | 25631 / 25822 | 26190 / 26370 | 25574 / 25748 | 26295 / 26522 |\n| MINILLM | 24187 / 24458 | 25011 / 25272 | 25100 / 25436 | 24067 / 24312 | 25205 / 25519 |\n\nFigure 14: The scaling law of teacher model based on the OPT family models. We compare MINILLM and SeqKD with OPT-1.3M as the student and OPT 2.7B, 6.7B, and 13B as teachers.\n\n<!-- image -->\n\n## B.5 Exposure Bias Analysis\n\nFollowing [AEABC22], we compute the ExAccErr with the following formula:\n\n<!-- formula-not-decoded -->\n\nwhere R ( l ) is the accumulated regret of imitating the teacher distribution p at the time step l during the free-run generation:\n\n<!-- formula-not-decoded -->\n\nand ϵ ( l ) is the average per-step error between q θ and p using the oracle context sampled from p as the prefix:\n\n<!-- formula-not-decoded -->\n\nIntuitively, the regret of q θ during generation is made of two parts: the error to estimate p given the oracle context and the error caused by the low-quality model-generated prefix. The former is calculated by lϵ ( l ) , and the latter reflects the exposure bias. Therefore, ExAccErr measures the relative error caused only by exposure bias.\n\n## C Additional Results\n\n## C.1 GPT-J as the Teacher Model\n\nWe present the evaluation results when using GPT-J as the teacher model and GPT-2-760M, GPT-21.5B, and GPTNeo -2.7B [BLW + 21] as the student models in Table 6. MINILLM outperforms the baselines in most cases.\n\nFigure 15: The Rouge-L scores of the distilled models against the SFT models on the different evaluation subsets of UnNI split by the golden responses' length.\n\n<!-- image -->\n\nTable 6: Evaluation results when GPT-J is the teacher. GPT4 and R-L stand for the average GPT-4 feedback scores and Rouge-L scores across 5 random seeds. The best scores of each model size are boldfaced , and the scores where the student model outperforms the teacher are marked with *.\n\n| Model          | Method        | DollyEval      | DollyEval   | SelfInst   | SelfInst       | VicunaEval   | VicunaEval   | S-NI      | UnNI R-L   |\n|----------------|---------------|----------------|-------------|------------|----------------|--------------|--------------|-----------|------------|\n| Model          | Method        | GPT4           | R-L         | GPT4       | R-L            | GPT4         | R-L          | R-L       |            |\n| GPT-J-6B       | Teacher       | 65.8           | 27.3        | 57.4       | 17.3           | 55.8         | 17.4         | 28.0      | 33.6       |\n| GPT-2-760M     | SFT w/o KD KD | 50.7 51.6 51.4 | 25.4 26.7   | 38.3 38.9  | 12.4 13.4 14.0 | 43.1 43.4    | 16.1 16.4    | 21.5 25.9 | 27.1       |\n| GPT-2-760M     |               |                |             |            |                |              |              |           | 33.2       |\n| GPT-2-760M     | SeqKD         |                | 26.0        | 39.2       |                | 42.0         | 15.3         | 25.5      | 32.5       |\n| GPT-2-760M     | MINILLM       | 54.0           | 25.8        | 43.7       | 16.3           | 44.3         | 19.1 *       | 27.1      | 35.5 *     |\n| GPT-2-1.5B     | SFT w/o KD    | 58.4           | 27.6 *      | 42.9       | 14.3           | 48.6         | 16.3         | 27.6      | 34.6*      |\n| GPT-2-1.5B     | KD            | 56.5           | 26.6        | 46.0       | 14.5           | 47.2         | 16.5         | 27.6      | 34.9*      |\n| GPT-2-1.5B     | SeqKD         | 58.5           | 27.0        | 43.2       | 13.6           | 46.6         | 16.9         | 28.0      | 34.2*      |\n| GPT-2-1.5B     | MINILLM       | 59.6           | 25.9        | 48.5       | 16.6           | 48.9         | 19.4 *       | 28.5 *    | 35.9 *     |\n| GPT- Neo -2.7B | SFT w/o KD    | 60.7           | 26.8        | 45.4       | 15.8           | 51.5         | 17.0         | 26.5      | 31.6       |\n| GPT- Neo -2.7B | KD            | 61.5           | 26.7        | 47.0       | 16.0           | 52.1         | 16.9         | 27.2      | 32.7       |\n| GPT- Neo -2.7B | SeqKD         | 60.8           | 25.6        | 47.2       | 16.2           | 53.0         | 16.9         | 26.1      | 32.9       |\n| GPT- Neo -2.7B | MINILLM       | 63.4           | 28.5*       | 52.5       | 17.1           | 54.1         | 18.6*        | 29.8*     | 35.4*      |\n\nFigure 16: Effect of the α value in the teacher mix-in exploration on the validation Rouge-L score. Larger models to more robust to α .\n\n<!-- image -->\n\n|      |                  | CLS       | Inst.     |\n|------|------------------|-----------|-----------|\n| 1.3B | MINILLM w/o L PT | 70.2 65.7 | 52.8 53.2 |\n| 7B   | MINILLM w/o L PT | 78.8 74.3 | 71.2 71.1 |\n\nTable 7: The effect of adding the pre-training loss. 'CLS' is the average accuracy scores on SST2 and BoolQ. 'Inst.' is the average RougeL score on Dolly, SelfInst, and Vicuna.\n\n## C.2 Scaling Law of Teacher based on the OPT family\n\nWe present the performance of MINILLM and SeqKD when we scale up the sizes of teacher models in Figure 14. Similar to the observations in Section 3.3, MINILLM constantly performs better and distills better student models from larger teacher models.\n\n## C.3 Performance of Response Length on U-NI\n\nThe performance on different U-NI subsets split by the length of the ground truth response is shown in Figure 15. We have the same observation as in Section 3.3 that on short responses, all KD methods perform similarly, and on long responses, MINILLM outperforms other methods.\n\n## C.4 More Ablation Studies\n\nEffect of Teacher-Mix-in Strength α In Figure 16, we plot the best Rouge-L scores on the validation set of GPT-2-125M, OPT-1.3B, and LLaMA-7B using GPT-2-1.5B, OPT-13B, and LLAMA13B as the teachers, with different teacher-mix-in strength α in MINILLM. α = 0 . 0 means we only sample from the student distribution, and when α = 1 . 0 , we sample entirely from the teacher distribution. We find that α = 0 . 2 is generally suitable across different model families and sizes, and larger models are more robust to the choice of α .\n\nEffect of Adding Pre-Training Loss In Table 7, we study the effect of adding the pre-training loss in Algorithm 2.3 by comparing MINILLM with its variant where the language modeling loss\n\n| Case #1                   | Case #1                                                                                                                                                                                                                                |\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Instruction               | During writing, we added an asterisk for the word that did not come to mind. You will need to provide several examples to demonstrate all the words that can be used in the sentence instead of the asterisk.                          |\n| Input                     | we * to know                                                                                                                                                                                                                           |\n| SFT w/o KD                | We can use 'know' in the sentence instead of 'we * to know'                                                                                                                                                                            |\n| KD                        | we went to know \\ n we will go to know \\ n we can go to know \\ n we may go to know \\ n we should go to know \\ n we must go to know                                                                                                     |\n| SeqKD                     | we do to know \\ n we can do to know \\ n we should do to know \\ n we would do to know \\ n we ought to do to know                                                                                                                        |\n| MINILLM                   | We can use the following words in place of the asterisk: \\ n - want \\ n - have \\ n - do \\ n - used \\ n - ought \\ n - desire \\ n - need \\ n - must                                                                                      |\n| MINILLM w/o teacher-mixed | We to know                                                                                                                                                                                                                             |\n| Case #2                   | Case #2                                                                                                                                                                                                                                |\n| Instruction               | Take the title of the product and extract its attributes. The attributes in this case refer to the characteristics of the products, such as their brand and color. Your answer must a list following the format of 'attribute: value'. |\n| Input                     | SAMSUNG980 PRO SSD 2TB PCIe NVMeGen 4 Gaming M.2 Internal Solid State Hard Drive Memory Card, MaximumSpeed, Thermal Control, MZ-V8P2T0B                                                                                                |\n| SFT w/o KD                | Attribute: Maximum Speed \\ n Value: 3500 MB/s                                                                                                                                                                                          |\n| KD                        | attribute: brand, attribute: color, attribute: memory card, attribute: maxi- mum speed, attribute: thermal control                                                                                                                     |\n| SeqKD                     | brand: Samsung, color: black, size: 2TB, interface: PCIe NVME Gen 4, design: M.2 Internal                                                                                                                                              |\n| MINILLM                   | brand: SAMSUNG \\ n color: 980 PRO \\ n capacity: 2TB \\ n interface: PCIe NVME Gen 4 \\ n speed: Maximum Speed \\ n threshold: Thermal Control \\ n form factor: M.2 Internal                                                               |\n| MINILLM w/o teacher-mixed | SAMSUNGC 980 Pro SSD 2TB PCIe NVME Gen 4 Gaming M.II Inter- nal Solid State Hard Drive memory Card, Max Speed, Thermal Control, AP1500K                                                                                                |\n\nTable 8: Instruction-following cases from the SelfInst dataset. MINILLM better follows the instructions and provides more detailed and accurate responses. Without the teacher-mixed sampling strategy in Section 2.2, the distilled model outputs short responses (Case #1) or simply repeats the input (Cases #2).\n\non the pre-training corpus is removed (w/o L PT). We have a similar observation as [OWJ + 22] that adding the pre-training loss helps to preserve the abilities on canonical NLP tasks while keeping the performance on instruction-following tasks nearly unchanged.\n\n## D Cases\n\nWe provide some cases generated by the models distilled by different methods based on the LLaMA model family in Table 8. The prompts are sampled from the SelfInst dataset. We find that MINILLM generates more detailed and accurate responses compared with the baselines.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model   | #Params   | Method        | DollyEval   | DollyEval   | SelfInst   | SelfInst   | VicunaEval GPT4   | VicunaEval GPT4   | S-NI R-L   | UnNI       |\n|---------|-----------|---------------|-------------|-------------|------------|------------|-------------------|-------------------|------------|------------|\n|         |           |               | GPT4        | R-L         | GPT4       | R-L        |                   | R-L               |            | R-L        |\n|         | 1.5B      | Teacher       | 58.4        | 27.6        | 42.9       | 14.3       | 48.6              | 16.3              | 27.6       | 31.8       |\n|         |           | SFT w/o KD    | 38.6        | 23.3        | 26.3       | 10.0       | 32.8              | 14.7              | 16.3       | 18.5       |\n|         |           | KD            | 40.3        | 22.8        | 27.8       | 10.8       | 31.9              | 13.4              | 19.7       | 22.0       |\n|         | 120M      | SeqKD         | 41.2        | 22.7        | 26.2       | 10.1       | 31.0              | 14.3              | 16.4       | 18.8       |\n|         |           | MINILLM       | 44.7        | 24.6        | 29.2       | 13.2       | 34.1              | 16.9 *            | 25.3       | 26.6       |\n| GPT-2   |           | SFT w/o KD    | 51.9        | 25.5        | 39.6       | 13.0       | 42.3              | 16.0              | 25.1       | 32.0       |\n|         |           | KD            | 51.6        | 25.0        | 39.2       | 12.0       | 42.8              | 15.4              | 23.7       | 31.0       |\n|         | 340M      | SeqKD         | 50.5        | 25.3        | 39.0       | 12.6       | 43.0              | 16.9*             | 22.9       | 30.2       |\n|         |           | MINILLM       | 52.2        | 25.4        | 40.5       | 15.6       | 42.6              | 17.7 *            | 27.4       | 34.5       |\n|         |           | SFT w/o KD    | 50.7        | 25.4        | 38.3       | 12.4       | 43.1              |                   | 21.5       | 27.1       |\n|         |           | KD            | 53.4        | 25.9        | 40.4       | 13.4       |                   | 16.1 16.9*        | 25.3       | 31.7       |\n|         | 760M      | SeqKD         | 52.0        | 25.6        | 38.9       | 14.0       | 43.4              | 15.9              | 26.1       | 32.9       |\n|         |           | MINILLM       | 54.7        | 26.4        | 44.6*      | 15.9       | 42.4 45.7         | 18.3*             | 29.3*      | 37.7*      |\n|         |           | Teacher       | 70.3        | 29.2        | 56.1       | 18.4       | 58.0              | 17.8              | 30.4       | 36.1       |\n|         | 13B       | SFT w/o KD    | 52.6        | 26.0        | 37.7       | 11.4       | 40.5              | 15.6              |            | 28.4       |\n|         |           | KD            | 52.7        | 25.4        | 36.0       | 12.2       | 40.8              | 14.9              | 23.1 21.9  | 27.0       |\n|         | 1.3B      | SeqKD         | 51.0        | 26.1        | 36.6       | 12.7       | 42.6              | 16.6              | 21.4       | 28.2       |\n|         |           |               |             |             | 47.0       | 14.8       |                   | 17.9*             | 28.6       | 33.4       |\n|         |           | MINILLM       | 60.7        | 26.7        | 38.9       | 13.9       | 50.6              | 16.6              | 24.9       | 32.3       |\n| OPT     |           | SFT w/o KD    | 55.4        | 27.1        | 48.6       | 13.8       | 44.8              | 16.7              | 26.3       | 30.2       |\n|         |           | KD            | 60.5        | 25.9        | 40.5       | 13.3       | 51.3              |                   |            |            |\n|         | 2.7B      | SeqKD         | 57.6        | 27.5 27.4   | 52.7       | 17.2       | 44.5 55.9         | 16.5 19.1*        | 25.3       | 32.3       |\n|         |           | MINILLM       | 63.2        | 27.6        | 56.4       | 16.4       | 57.3              | 17.8              | 30.7* 30.3 | 35.1 28.6  |\n|         |           | SFT w/o KD    | 67.9        |             |            |            |                   |                   |            |            |\n|         |           | KD            | 68.6        | 28.3        | 58.0       | 17.0       | 57.0              | 17.5              | 30.7*      |            |\n|         | 6.7B      | SeqKD         | 69.6        | 28.5        | 54.0       | 17.0       | 57.6              | 17.9*             | 30.4       | 26.7 28.2  |\n|         |           | MINILLM       | 70.8*       | 29.0        | 58.5*      | 17.5       | 60.1*             | 18.7*             | 32.5*      | 36.7*      |\n|         | 13B       | Teacher       | 79.0        | 29.7        | 75.5       | 23.4       | 65.1              | 19.4              | 35.8       | 38.5       |\n| LLaMA   |           | SFT w/o KD    | 73.0        | 26.3        | 69.2       | 20.8       | 61.6              | 17.5              | 32.4       | 35.8       |\n|         |           | SeqKD MINILLM | 73.6 76.4   | 27.5 29.0   | 71.5 73.1  | 20.8 23.2  | 62.6 64.1         | 18.1 20.7*        | 33.7 35.5  | 37.6 40.2* |"
    },
    {
      "index": 1,
      "markdown": "|         | SST2   | SST2   | BoolQ   | BoolQ   |\n|---------|--------|--------|---------|---------|\n|         | ECE    | Acc.   | ECE     | Acc.    |\n| Teacher | 0.025  | 93.0   | 0.356   | 74.5    |\n| KD      | 0.191  | 84.7   | 0.682   | 63.5    |\n| SeqKD   | 0.243  | 66.5   | 0.681   | 62.8    |\n| MINILLM | 0.099  | 89.7   | 0.502   | 67.8    |"
    },
    {
      "index": 2,
      "markdown": "|         | DollyEval   | DollyEval   | SelfInst   | SelfInst   |\n|---------|-------------|-------------|------------|------------|\n|         | Dist-4      | Loss        | Dist-4     | Loss       |\n| Teacher | 99.3        | 3.55        | 99.1       | 4.44       |\n| SFT     | 99.5        | 3.89        | 99.0       | 5.28       |\n| MINILLM | 99.0        | 3.95        | 98.6       | 5.33       |"
    },
    {
      "index": 3,
      "markdown": "|                   |   Valid. R-L |   Dolly R-L |\n|-------------------|--------------|-------------|\n| MINILLM           |         27.4 |        24.6 |\n| w/o Length Norm.  |         17.4 |        14.7 |\n| w/o Teacher-Mixed |         22.3 |        20.4 |\n| w/o Single-Step   |         27   |        23.7 |"
    },
    {
      "index": 4,
      "markdown": "| Model   | 10            | 20            | 30            | 40            | 50            |\n|---------|---------------|---------------|---------------|---------------|---------------|\n| Teacher | 23562 / 23696 | 23653 / 23834 | 24306 / 24488 | 24207 / 24381 | 23803 / 23967 |\n| KD      | 25889 / 26064 | 24024 / 24197 | 25663 / 25843 | 25611 / 25763 | 26178 / 26339 |\n| SeqKD   | 25358 / 25519 | 25631 / 25822 | 26190 / 26370 | 25574 / 25748 | 26295 / 26522 |\n| MINILLM | 24187 / 24458 | 25011 / 25272 | 25100 / 25436 | 24067 / 24312 | 25205 / 25519 |"
    },
    {
      "index": 5,
      "markdown": "| Model          | Method        | DollyEval      | DollyEval   | SelfInst   | SelfInst       | VicunaEval   | VicunaEval   | S-NI      | UnNI R-L   |\n|----------------|---------------|----------------|-------------|------------|----------------|--------------|--------------|-----------|------------|\n| Model          | Method        | GPT4           | R-L         | GPT4       | R-L            | GPT4         | R-L          | R-L       |            |\n| GPT-J-6B       | Teacher       | 65.8           | 27.3        | 57.4       | 17.3           | 55.8         | 17.4         | 28.0      | 33.6       |\n| GPT-2-760M     | SFT w/o KD KD | 50.7 51.6 51.4 | 25.4 26.7   | 38.3 38.9  | 12.4 13.4 14.0 | 43.1 43.4    | 16.1 16.4    | 21.5 25.9 | 27.1       |\n| GPT-2-760M     |               |                |             |            |                |              |              |           | 33.2       |\n| GPT-2-760M     | SeqKD         |                | 26.0        | 39.2       |                | 42.0         | 15.3         | 25.5      | 32.5       |\n| GPT-2-760M     | MINILLM       | 54.0           | 25.8        | 43.7       | 16.3           | 44.3         | 19.1 *       | 27.1      | 35.5 *     |\n| GPT-2-1.5B     | SFT w/o KD    | 58.4           | 27.6 *      | 42.9       | 14.3           | 48.6         | 16.3         | 27.6      | 34.6*      |\n| GPT-2-1.5B     | KD            | 56.5           | 26.6        | 46.0       | 14.5           | 47.2         | 16.5         | 27.6      | 34.9*      |\n| GPT-2-1.5B     | SeqKD         | 58.5           | 27.0        | 43.2       | 13.6           | 46.6         | 16.9         | 28.0      | 34.2*      |\n| GPT-2-1.5B     | MINILLM       | 59.6           | 25.9        | 48.5       | 16.6           | 48.9         | 19.4 *       | 28.5 *    | 35.9 *     |\n| GPT- Neo -2.7B | SFT w/o KD    | 60.7           | 26.8        | 45.4       | 15.8           | 51.5         | 17.0         | 26.5      | 31.6       |\n| GPT- Neo -2.7B | KD            | 61.5           | 26.7        | 47.0       | 16.0           | 52.1         | 16.9         | 27.2      | 32.7       |\n| GPT- Neo -2.7B | SeqKD         | 60.8           | 25.6        | 47.2       | 16.2           | 53.0         | 16.9         | 26.1      | 32.9       |\n| GPT- Neo -2.7B | MINILLM       | 63.4           | 28.5*       | 52.5       | 17.1           | 54.1         | 18.6*        | 29.8*     | 35.4*      |"
    },
    {
      "index": 6,
      "markdown": "|      |                  | CLS       | Inst.     |\n|------|------------------|-----------|-----------|\n| 1.3B | MINILLM w/o L PT | 70.2 65.7 | 52.8 53.2 |\n| 7B   | MINILLM w/o L PT | 78.8 74.3 | 71.2 71.1 |"
    },
    {
      "index": 7,
      "markdown": "| Case #1                   | Case #1                                                                                                                                                                                                                                |\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Instruction               | During writing, we added an asterisk for the word that did not come to mind. You will need to provide several examples to demonstrate all the words that can be used in the sentence instead of the asterisk.                          |\n| Input                     | we * to know                                                                                                                                                                                                                           |\n| SFT w/o KD                | We can use 'know' in the sentence instead of 'we * to know'                                                                                                                                                                            |\n| KD                        | we went to know \\ n we will go to know \\ n we can go to know \\ n we may go to know \\ n we should go to know \\ n we must go to know                                                                                                     |\n| SeqKD                     | we do to know \\ n we can do to know \\ n we should do to know \\ n we would do to know \\ n we ought to do to know                                                                                                                        |\n| MINILLM                   | We can use the following words in place of the asterisk: \\ n - want \\ n - have \\ n - do \\ n - used \\ n - ought \\ n - desire \\ n - need \\ n - must                                                                                      |\n| MINILLM w/o teacher-mixed | We to know                                                                                                                                                                                                                             |\n| Case #2                   | Case #2                                                                                                                                                                                                                                |\n| Instruction               | Take the title of the product and extract its attributes. The attributes in this case refer to the characteristics of the products, such as their brand and color. Your answer must a list following the format of 'attribute: value'. |\n| Input                     | SAMSUNG980 PRO SSD 2TB PCIe NVMeGen 4 Gaming M.2 Internal Solid State Hard Drive Memory Card, MaximumSpeed, Thermal Control, MZ-V8P2T0B                                                                                                |\n| SFT w/o KD                | Attribute: Maximum Speed \\ n Value: 3500 MB/s                                                                                                                                                                                          |\n| KD                        | attribute: brand, attribute: color, attribute: memory card, attribute: maxi- mum speed, attribute: thermal control                                                                                                                     |\n| SeqKD                     | brand: Samsung, color: black, size: 2TB, interface: PCIe NVME Gen 4, design: M.2 Internal                                                                                                                                              |\n| MINILLM                   | brand: SAMSUNG \\ n color: 980 PRO \\ n capacity: 2TB \\ n interface: PCIe NVME Gen 4 \\ n speed: Maximum Speed \\ n threshold: Thermal Control \\ n form factor: M.2 Internal                                                               |\n| MINILLM w/o teacher-mixed | SAMSUNGC 980 Pro SSD 2TB PCIe NVME Gen 4 Gaming M.II Inter- nal Solid State Hard Drive memory Card, Max Speed, Thermal Control, AP1500K                                                                                                |"
    }
  ],
  "stats": {
    "pages": 23,
    "chunksCreated": 114,
    "totalCharacters": 77462,
    "totalWords": 12320,
    "numTables": 8,
    "processingTimeMs": 38345
  }
}