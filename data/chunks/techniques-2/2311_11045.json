{
  "paper": {
    "id": "2311.11045v2",
    "title": "Orca 2: Teaching Small Language Models How to Reason",
    "abstract": "Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). More crucially, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36,000 unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs",
    "authors": [
      "Arindam Mitra",
      "Luciano Del Corro",
      "Shweti Mahajan",
      "Andres Codas",
      "Clarisse Simoes",
      "Sahaj Agarwal",
      "Xuxi Chen",
      "Anastasia Razdaibiedina",
      "Erik Jones",
      "Kriti Aggarwal",
      "Hamid Palangi",
      "Guoqing Zheng",
      "Corby Rosset",
      "Hamed Khanpour",
      "Ahmed Awadallah"
    ],
    "published": "2023-11-18T11:44:52.000Z",
    "updated": "2023-11-21T19:43:31.000Z",
    "primaryCategory": "cs.AI",
    "categories": [
      "cs.AI"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2311.11045v2",
    "absUrl": "https://arxiv.org/abs/2311.11045v2"
  },
  "chunks": [
    {
      "id": "2311.11045v2-chunk-0",
      "content": "<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "chunkIndex": 0,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-1",
      "content": "Arindam Mitra, Luciano Del Corro † , Shweti Mahajan † , Andres Codas ‡ Clarisse Simoes ‡ , Sahaj Agarwal, Xuxi Chen ∗ , Anastasia Razdaibiedina ∗ Erik Jones ∗ , Kriti Aggarwal ∗ , Hamid Palangi, Guoqing Zheng Corby Rosset, Hamed Khanpour, Ahmed Awadallah",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Orca 2: Teaching Small Language Models How to Reason",
        "chunkIndex": 1,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-2",
      "content": "Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-3",
      "content": "mplex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). Moreover, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Abstract",
        "chunkIndex": 3,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-4",
      "content": "t test advanced reasoning abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs.\n\nFigure 1: Results comparing Orca 2 (7B &amp; 13B) to LLaMA-2-Chat (13B &amp; 70B) and WizardLM (13B &amp; 70B) on variety of benchmarks (in 0-shot setting) covering language understanding, common sense reasoning, multi-step reasoning, math problem solving, etc. Orca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size.\n\n<!-- image -->\n\n∗ work done while at Microsoft; † , ‡ denote equal contributions.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Abstract",
        "chunkIndex": 4,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-5",
      "content": "John and Mark are in a room with a ball, a basket and a box. John puts the ball in the box, then leaves for work. While John is away, Mark puts the ball in the basket, and then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is?",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "### Question:",
        "chunkIndex": 5,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-6",
      "content": "Let's analyze the situation step by step:\n\n1. John puts the ball in the box.\n3. Mark comes into the room and puts the ball in the basket.\n2. John leaves for work.\n4. Mark leaves for school.\n5. John and Mark both return to the room.\n\nAt this point, John and Mark have not seen each other's actions. Therefore, they only know what happened while they were in the room.\n\nJohn only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school. Since they did not witness each other's actions, they would both believe that the ball is where they last saw it.\n\nSo, John would think the ball is in the box, and Mark would think the ball is in the basket.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "### Orca-2-13B :",
        "chunkIndex": 6,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-7",
      "content": "Ans: This means that the man has been told by someone else (probably his friend) that he cannot have any more alcohol at this establishment.\n\nAns: The ball is still in the box because it was there when John left the room. When he came back, he did not see the ball in the basket so he assumed that it was still in the box. Ques 10. A man walks into a bar and asks for a drink. The bartender says, 'I'm sorry, but we don't serve alcohol here.' The man replies, 'Oh, I didn't realize this place was a church!' What does the man mean by his response?",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "### LLaMA2-13B :",
        "chunkIndex": 7,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-8",
      "content": "This is a classic lateral thinking puzzle. The solution is that the ball is still in the box.\n\nHere's the reasoning:\n\n1. John put the ball in the box before he left for work.\n\n3. Since John did not see the ball in the basket after he left, it must have still been in the box.\n\n2. Mark put the ball in the basket before he left for school.\n\n4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket.\n\nTherefore, the ball is still in the box.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "### LLaMA2-Chat-13B :",
        "chunkIndex": 8,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-9",
      "content": "John and Mark will likely think that the ball is in the box, since that is where John left it before he left for work. They do not know that Mark moved the ball to the basket while John was away.\n\nFigure 2: Example demonstrating the responses of Orca 2, its base model LLaMA-2, LLaMA-2-Chat and ChatGPT (GPT-3.5-Turbo) to a reasoning question. LLaMA-2 and LLaMA-2-Chat model responses generated using replicate.com/meta/llama-2-13b and chat.lmsys.org respectively. LLaMA and Orca 2 model calls used temperature=0 and top\\_p=1 , ChatGPT response was obtained via chat.openai.com .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "### ChatGPT(GPT-3.5-Turbo) :",
        "chunkIndex": 9,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-10",
      "content": "Large Language Models (LLMs) are enabling more natural and sophisticated interactions between humans and machines, enhancing user experience in existing applications like coding [3], web search [36], chatbots [45, 56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovative AI applications.\n\nScaling LLMs like GPT-4 [44] and PaLM-2 [1] to ever more parameters led to emergent abilities [63] unseen in smaller models (less than ∼ 10B parameters), most notably the remarkable ability to reason zero-shot [23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing score [51]. Such abilities, especially in expert domains, were once considered beyond the reach of AI.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-11",
      "content": "for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing score [51]. Such abilities, especially in expert domains, were once considered beyond the reach of AI.\n\nImitation learning has emerged as the go-to approach to improve small language models [6, 64, 56], where the goal is to replicate the outputs of larger, more capable teacher models. While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [13]. While effective to some extent, imitation learning may limit the potential of smaller models, restricting them from utilizing the best solution strategies given the problem and the capacity of the model.\n\nIn this work, we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 11,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-12",
      "content": "solution strategies given the problem and the capacity of the model.\n\nIn this work, we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models how to use a suite of reasoning techniques, such as step-by-step processing, recall-then-generate, recall-reason-generate, extract-generate, and direct-answer methods. Secondly, we aspire to help these models decide when to use the most effective reasoning strategy for the task at hand, allowing them to perform at their best, irrespective of their size.\n\nLike Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored to the task at hand, bearing in mind whether a student model is capable of the same behavior.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 12,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-13",
      "content": "oning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored to the task at hand, bearing in mind whether a student model is capable of the same behavior. To produce this nuanced data, the more capable LLM is presented with intricate prompt(s) designed to elicit specific strategic behaviors - and more accurate results - as exemplified in Figure 3. Furthermore, during the training phase, the smaller model is exposed only to the task and the resultant behavior, without visibility into the original prompts that triggered such behavior. This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 13,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-14",
      "content": "r level how to approach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand.\n\nSome previous studies on training small models are limited in their evaluation protocol. They often rely on small number of tasks or on using other models for auto-evaluation by asking them to compare the outputs of two systems with a prompt like 'given responses from system 1 (reference) and system 2 (target), which one is better?' . However, previous work [13, 42, 60, 67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We use a total of 15 benchmarks (covering ∼ 100 tasks and over 36,000 unique prompts).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 14,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-15",
      "content": "several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We use a total of 15 benchmarks (covering ∼ 100 tasks and over 36,000 unique prompts). The benchmarks cover variety of aspects including language understanding, common sense reasoning, multi-step reasoning, math problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification.\n\nOur preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching or exceeding those 5 to 10 times larger, especially on tasks that require reasoning. This highlights the potential of endowing smaller models with better reasoning capabilities. However Orca 2 is no exception to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 15,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-16",
      "content": "enon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not undergone RLHF training for safety. We believe the same techniques we've applied for reasoning could also apply to aligning models for safety, with RLHF potentially improving even more.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "1 Introduction",
        "chunkIndex": 16,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-17",
      "content": "Instruction tuning [46, 38, 62, 61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output is a demonstration of the desired behavior. Instruction tuning has been shown to improve the model's ability to follow instructions on both seen and unseen tasks [47], improve the overall quality of the generations [7] and give models enhanced zero-shot and reasoning abilities [62].\n\nSeveral studies, including Alpaca [55], Vicuna [6], WizardLM [64], Baize [65], and Koala [12], have adopted instruction tuning to train smaller 'student' language models using outputs generated by larger foundational models. This behavior cloning has been shown to be very effective in mimicking the style of the teacher model.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.1 Instruction Tuning",
        "chunkIndex": 17,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-18",
      "content": "to train smaller 'student' language models using outputs generated by larger foundational models. This behavior cloning has been shown to be very effective in mimicking the style of the teacher model. However, as shown in [42, 5], it may not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style.\n\nWe note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge. Hence instruction tuned models will be always limited by the knowledge learned during pre-training. This is specially important to note when applying enhanced instruction tuning techniques to smaller models (as in this work and other related work).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.1 Instruction Tuning",
        "chunkIndex": 18,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-19",
      "content": "mited by the knowledge learned during pre-training. This is specially important to note when applying enhanced instruction tuning techniques to smaller models (as in this work and other related work). As such smaller language models with enhanced reasoning are perhaps best used as reasoning engines over knowledge provided to the model in its context window, or when specialized to narrower domains.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.1 Instruction Tuning",
        "chunkIndex": 19,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-20",
      "content": "One of the known weaknesses of instruction tuning is that a resulting student model could learn to generate stylistically correct, but ultimately wrong, outputs [13]. For example, instruction-tuning towards targets that are too terse limits the student's visibility into what could have been a complex reasoning process, thus hindering its generalization ability to other tasks. In Orca 1, we introduced Explanation Tuning [42] to address this drawback by training student models on richer and more expressive reasoning signals. The mechanism for procuring these signals is system instructions 2 crafted to obtain detailed explanations from a teacher model as it reasons through a task. System instructions are additional high level guidelines an LLM is supposed to adhere to as it addresses individual user prompts , from which they are separated by a 'system' role flag in a ChatML dialogue interface 3 .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.2 Explanation Tuning",
        "chunkIndex": 20,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-21",
      "content": "are additional high level guidelines an LLM is supposed to adhere to as it addresses individual user prompts , from which they are separated by a 'system' role flag in a ChatML dialogue interface 3 .\n\nExplanation tuning begins with a compilation of N hand-crafted, general purpose system instructions designed to elicit more careful reasoning. Some examples include 'think step-by-step', 'generate detailed answers', etc. The primary objective of these system instructions is to extract rich demonstrations of 'Slow Thinking' [22] from capable LLMs like GPT-4. They are then combined with user prompts from a vast and diverse set of tasks to yield a dataset of ( system instruction , user prompt , LLM answer ) triplets. The student model is trained to predict the LLM answer from the other two inputs.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.2 Explanation Tuning",
        "chunkIndex": 21,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-22",
      "content": "from a vast and diverse set of tasks to yield a dataset of ( system instruction , user prompt , LLM answer ) triplets. The student model is trained to predict the LLM answer from the other two inputs.\n\nIf user prompts can be grouped into M distinct clusters representing similar kinds of questions, then Explanation Tuning naively yields a cross product of M × N different answers addressing different aspects of the task. Since more capable LLMs tend to vary their responses with the system instruction , this offers an easy path to increase the quantity and diversity of training signals. Numerous models such as Orca 1 [42], StableBeluga [35] and Dolphin 4 have capitalized on Explanation Tuning to demonstrate substantial improvements over traditional instruction-tuned models, especially in complex zero-shot reasoning tasks.\n\n2 used interchangeably with system message\n\n3 e.g. see https://platform.openai.com/docs/api-reference/making-requests",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.2 Explanation Tuning",
        "chunkIndex": 22,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-23",
      "content": "tional instruction-tuned models, especially in complex zero-shot reasoning tasks.\n\n2 used interchangeably with system message\n\n3 e.g. see https://platform.openai.com/docs/api-reference/making-requests\n\n4 https://huggingface.co/datasets/ehartford/dolphin",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "2.2 Explanation Tuning",
        "chunkIndex": 23,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-24",
      "content": "The key to Explanation Tuning is the extraction of answers with detailed explanations from LLMs based on system instructions . However, not every combination of system instruction cross tasks is appropriate, and in fact, the response quality can vary significantly based on the strategy described in the system instruction .\n\nEven very powerful models like GPT-4 are susceptible to this variation. Consider, Figure 3, which shows four different answers from GPT-4 obtained with four different system instructions given a question of story reordering. The first answer (the default GPT-4 answer) is wrong. The second answer (using a chain-of-thought prompt) is better. We can see that the model is reasoning with step-by-step but important details guiding the decision process are still missing. The third answer (with an explain-your-answer prompt) is wrong but the explanation is correct. The final answer is the only correct answer and is obtained using the following system instruction :",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 24,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-25",
      "content": "ing. The third answer (with an explain-your-answer prompt) is wrong but the explanation is correct. The final answer is the only correct answer and is obtained using the following system instruction :\n\nYou will be given a task. Use the following steps to solve it.\n\n1. Identify the main theme or topic of the story.\n\n3. Find the sentence that could be the start of the story. Go through each of the answer choices and analyze to figure it out.\n\n2. Look for any cause and effect relationships between the sentences.\n\n4. Rearrange the sentences in the correct order based on the information gathered in the previous steps.\n\n5. Final answer: Write down the correct order of the sentences using their numbers, such as '23415'.\n\nWe note that GPT-4's response is significantly influenced by the given system instructions . Secondly, when carefully crafted, the instructions can substantially improve the quality and accuracy of GPT-4's answers.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 25,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-26",
      "content": "PT-4's response is significantly influenced by the given system instructions . Secondly, when carefully crafted, the instructions can substantially improve the quality and accuracy of GPT-4's answers. Lastly, without such instructions, GPT-4 may struggle to recognize a challenging problem and might generate a direct answer without engaging in careful thinking. Motivated by these observations, we conclude that the strategy an LLM uses to reason about a task should depend on the task itself.\n\nEven if all the answers provided were correct, the question remains: Which is the best answer for training a smaller model? This question is central to our work, and we argue that smaller models should be taught to select the most effective solution strategy based on the problem at hand. It is important to note that: (1) the optimal strategy might vary depending on the task and (2) the optimal strategy for a smaller model may differ from that of a more powerful one.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 26,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-27",
      "content": "e problem at hand. It is important to note that: (1) the optimal strategy might vary depending on the task and (2) the optimal strategy for a smaller model may differ from that of a more powerful one. For instance, while a model like GPT-4 may easily generate a direct answer, a smaller model might lack this capability and require a different approach, such as thinking step-by-step. Therefore, naively teaching a smaller model to 'imitate' the reasoning behavior of a more powerful one may be sub-optimal. Although training smaller models towards step-by-step-explained answers has proven beneficial, training them on a plurality of strategies enables more flexibility to choose which is better suited to the task.\n\nWe use the term Cautious Reasoning to refer to the act of deciding which solution strategy to choose for a given task - among direct answer generation, or one of many 'Slow Thinking' [22] strategies (step-by-step, guess and check or explain-then-answer, etc.).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 27,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-28",
      "content": "ciding which solution strategy to choose for a given task - among direct answer generation, or one of many 'Slow Thinking' [22] strategies (step-by-step, guess and check or explain-then-answer, etc.).\n\nThe following illustrates the process of training a Cautious Reasoning LLM:\n\n1. Start with a collection of diverse tasks\n3. Write task-specific system instruction(s) corresponding to the chosen strategy in order to obtain teacher responses for each task.\n2. Guided by the performance of Orca, decide which tasks require which solution strategy (e.g. direct-answer, step-by-step, explain-then-answer, etc.)\n4. Prompt Erasing : At training time, replace the student's system instruction with a generic one vacated of details of how to approach the task.\n\nNote that step 3 has a broad mandate to obtain the teacher's responses: it can utilize multiple calls, very detailed instructions, etc.\n\nFigure 3: Demonstrative example from Flan-CoT Collection.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 28,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-29",
      "content": "step 3 has a broad mandate to obtain the teacher's responses: it can utilize multiple calls, very detailed instructions, etc.\n\nFigure 3: Demonstrative example from Flan-CoT Collection.\n\n<!-- image -->\n\nThe key idea is: in the absence of the original system instruction which detailed how to approach the task, the student model will be encouraged to learn that underlying strategy as well as the reasoning abilities it entailed. We call this technique Prompt Erasing as it removes the structure under which the teacher framed its reasoning. Armed with this technique, we present Orca 2, a cautious reasoner.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "3 Teaching Orca 2 to be a Cautious Reasoner",
        "chunkIndex": 29,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-30",
      "content": "For Orca 2, we created a new dataset with ~817K training instances, which we will refer as Orca 2 dataset. Following Orca 1, Orca 2 has been trained with progressive learning, with subsets of data obtained from combining the original FLAN [33] annotations, Orca 1 dataset and the Orca 2 dataset. We also describe the details about the progressive learning.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4 Technical Details",
        "chunkIndex": 30,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-31",
      "content": "The Orca 2 dataset has four main sources:\n\nFLAN: Our main source of prompts for synthetic data generation is the FLAN-v2 Collection [33], which consists of five sub-collections, namely, CoT, NiV2, T0, Flan 2021 and Dialogue. Each sub-collection contains multiple tasks. Following Orca 1 [42] we consider tasks from only CoT, NiV2, T0, Flan 2021 sub-collections, which contain a total of 1913 tasks. Each task in Flan-v2 is a collection of queries and has an associated answer. Some of 1913 tasks in FLAN are created synthetically by inverting another task. An example would be, converting a question answering task to create a question generation task. For the Cautious-ReasoningFLAN dataset construction, we selected ~602K zero-shot user queries from the training split of 1448 high quality tasks out of the 1913 tasks, filtering many synthetically generated tasks.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.1 Dataset Construction",
        "chunkIndex": 31,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-32",
      "content": "ous-ReasoningFLAN dataset construction, we selected ~602K zero-shot user queries from the training split of 1448 high quality tasks out of the 1913 tasks, filtering many synthetically generated tasks.\n\nWe grouped the selected 1448 tasks manually into 23 categories (e.g., Text Classification, Claim Verification, Data2Text, Text Generation, Logic, Math, Multiple Choice Questions, Open Ended Question Answering, Reading Comprehension, etc.). Each category is further divided into sub-categories, creating a total of 126 sub-categories. Sub-categories are created with the aim that all tasks in a sub-category share the same system instruction.\n\nFor alignment towards cautious reasoning, we replace all the system instructions with the following generic system instruction:\n\nYou are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.1 Dataset Construction",
        "chunkIndex": 32,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-33",
      "content": "n AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\n\nWe will refer to it as the cautious system instruction .\n\nFew Shot Data : The dataset above does not contain any demonstrations of examples in the prompts. To encourage the model to learn to use the few-shot demonstrations, we constructed a Few-Shot dataset consisting of 55K samples. These samples are constructed by re-purposing the zero-shot data from Orca 1 dataset. Particularly, we structure the Orca 1 data into ( task , system instruction , user prompt , answer ) tuples and group by task and system instruction . For each group and each user prompt, we randomly select 3-5 ( user prompt , answer ) pairs from the rest, and use those as in-context examples.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.1 Dataset Construction",
        "chunkIndex": 33,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-34",
      "content": "swer ) tuples and group by task and system instruction . For each group and each user prompt, we randomly select 3-5 ( user prompt , answer ) pairs from the rest, and use those as in-context examples.\n\nMath: We collected data for ~160K math problems from the Deepmind Math dataset [50] 5 and the training splits of a collection of existing datasets: GSM8K [9], AquaRat [31], MATH [18], AMPS [18], FeasibilityQA [14], NumGLUE [40], AddSub [19], GenArith [24] and Algebra [26]. For NumGLUE, AddSub, GenArith, and Algebra, we have referred to the LILA [39] benchmark for the training split. Note that including prompts from the training split of a dataset (e.g. GSM8K) renders it in-domain for the sake of evaluation. Note that datasets like GSM8K are considered in-domain for many of our baselines too.\n\n5 We have sampled only from arithmetic div, arithmetic mul, arithmetic add or sub tasks from https://huggingface.co/datasets/math\\_dataset",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.1 Dataset Construction",
        "chunkIndex": 34,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-35",
      "content": "SM8K are considered in-domain for many of our baselines too.\n\n5 We have sampled only from arithmetic div, arithmetic mul, arithmetic add or sub tasks from https://huggingface.co/datasets/math\\_dataset\n\nFully synthetic data: We have synthetically created 2000 Doctor-Patient Conversations with GPT-4. We then instruct the model to create a summary of the conversation with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN. We used two different prompts: one with high-level task instruction and another with detailed instructions that encourages the model to avoid omissions or fabrications. We use this data to assess the learning of specialized skills.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.1 Dataset Construction",
        "chunkIndex": 35,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-36",
      "content": "This section provides an overview of the training process for Orca 2, covering different aspects of tokenization, sequencing, and loss computation.\n\nProgressive Learning : We start with LLaMA-2-7B or LLaMA-2-13B checkpoint and finetune it on the train split of FLAN-v2 dataset for one epoch. Note that FLAN-v2 dataset contains both zero-shot and few-shot problems. We then train on 5 million ChatGPT data from Orca 1 for 3 epochs. Then we train on the combination of 1 million GPT-4 data from Orca 1 and Orca 2's 817K data for 4 epochs.\n\nTokenization : We utilize the LLaMA Byte Pair Encoding (BPE) tokenizer for processing the input examples. Notably, the LLaMA tokenizer splits all numbers into individual digits, and fallbacks to bytes to decompose unknown UTF-8 characters. To deal with variable length sequences we add a padding token ' [[PAD]] ' into the LLaMA tokenizer vocabulary. We also add the ChatML special tokens ' &lt;|im\\_start|&gt; ' and ' &lt;|im\\_end|&gt; '.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.2 Training",
        "chunkIndex": 36,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-37",
      "content": "To deal with variable length sequences we add a padding token ' [[PAD]] ' into the LLaMA tokenizer vocabulary. We also add the ChatML special tokens ' &lt;|im\\_start|&gt; ' and ' &lt;|im\\_end|&gt; '. The resulting vocabulary contains 32 , 003 tokens.\n\nPacking : To optimize the training process and utilize computational resources efficiently, we employ the packing technique [25]. This method involves concatenating multiple input examples into a single sequence, which is then used for training the model. The packing is performed such that the total length of the concatenated sequence does not exceed max \\_ len = 4096 tokens. Particularly, we shuffle the input examples and then partition the examples into groups such that length of the concatenated sequence in each group is at most max\\_len . Padding tokens are then added to the concatenated sequence to achieve a uniform input sequence length of max\\_len .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.2 Training",
        "chunkIndex": 37,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-38",
      "content": "ps such that length of the concatenated sequence in each group is at most max\\_len . Padding tokens are then added to the concatenated sequence to achieve a uniform input sequence length of max\\_len .\n\nLoss : For the purpose of training Orca 2, we compute the loss only on the tokens generated by the teacher model, i.e., it learns to generate responses conditioned on the system instruction and task instructions. This approach ensures that the model focuses on learning from the most relevant and informative tokens, improving the overall efficiency and effectiveness of the training process.\n\nCompute: We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16. For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch, ~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue training on ~1 . 8 million GPT-4 data for 4 epochs.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.2 Training",
        "chunkIndex": 38,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-39",
      "content": "hatGPT data for 3 epochs and ~23 hours to continue training on ~1 . 8 million GPT-4 data for 4 epochs.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "4.2 Training",
        "chunkIndex": 39,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-40",
      "content": "We benchmark Orca 2 alongside several state-of-the-art models. All baseline models are instruction-tuned models. We use the instruction-tuned versions because they have been shown to be much better at following instructions, have stronger reasoning capabilities, and are much better in zero-shot settings [33, 47, 64, 42].\n\n- LLaMA-2 Models: We use both the 70 billion and 13 billion parameter models from the LLaMA 2 series [57]. We use the LLaMA2-70B-hf-chat 6 and LLaMA2-13B-hf-chat 7 .\n\n6 https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n\n7 https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n\n- WizardLM: WizardLM [64] is an instruction tuned version of LLaMA 2, specifically through the Evol-Instruct technique which autonomously generates a diverse array of intricate instruction data. We use both 13B (V1.2 8 ) and 70B (V1.0 9 ) parameter versions.\n- GPT Models: We show the performance of both ChatGPT (GPT-3.5-Turbo) and GPT-4 [44].",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.1 Baselines",
        "chunkIndex": 40,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-41",
      "content": "tes a diverse array of intricate instruction data. We use both 13B (V1.2 8 ) and 70B (V1.0 9 ) parameter versions.\n- GPT Models: We show the performance of both ChatGPT (GPT-3.5-Turbo) and GPT-4 [44]. We utilized the Azure OpenAI API version '2023-03-15-preview' .\n- Orca: Orca 1 [42] is a 13-billion parameter model that learns through explanations, step-by-step thought processes, and complex instructions and is based on the LLaMA model [57].\n\nFor inference, we use fp32 for LLaMA2 and Orca models. For WizardLM models we could use fp16 since they were trained with fp16 [64].",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.1 Baselines",
        "chunkIndex": 41,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-42",
      "content": "This section provides a detailed overview of the tasks selected to assess open-ended generation, summarization, safety, bias, reasoning, and comprehension capacities of Orca 2. Except where specified otherwise, evaluations were conducted using the test split of each dataset. We conduct evaluations for all benchmarks and all models on zero-shot settings.\n\nWe selected a broad set of benchmarks representing both advanced capabilities such as reasoning, more basic abilities such as text completion and also grounding, truthfulness and safety. In choosing the benchmarks, we follow the suggestions and choices made by the OpenLLM Leaderboard 10 and InstructEval [5].",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2 Benchmarks",
        "chunkIndex": 42,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-43",
      "content": "- AGIEval: AGIEval [69] is a collection of diverse sets of standardized tests including general college admission tests like the GRE, GMAT, and SAT; law-focused examinations such as the LSAT and lawyer qualification assessments; math competitions; and national civil service examinations [69].\n- CRASS: The CRASS [11] dataset evaluates counterfactual reasoning abilities of LLMs.\n- Discrete Reasoning Over Paragraphs: DROP [10] is an adversarialy-created reading comprehension benchmark, which requires models to navigate through references and execute discrete operations like addition or sorting and was adopted as part of InstructEval [5] and the OpenLLM Leaderboard.\n- RACE: The RACE dataset [27] is a collection of reading comprehension questions derived from English examinations given to Chinese students aged between 12 to 18 years.\n- GSM8K: This is a collection of word problems that test the ability to perform multi-step mathematical reasoning [9].\n- Big-Bench Hard (BBH): BBH [54] is a su",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.1 Reasoning Capabilities",
        "chunkIndex": 43,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-44",
      "content": "se students aged between 12 to 18 years.\n- GSM8K: This is a collection of word problems that test the ability to perform multi-step mathematical reasoning [9].\n- Big-Bench Hard (BBH): BBH [54] is a subset of the 23 hardest tasks of BIG-Bench [52] with a focus on challenging tasks such as those requiring multi-step reasoning.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.1 Reasoning Capabilities",
        "chunkIndex": 44,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-45",
      "content": "- Massive Multitask Language Understanding benchmark: MMLU [17] is designed to measure the language understanding, knowledge and reasoning abilities of models and consists of 57 tasks.\n- ARC: The AI2 Reasoning Challenge [8] is a benchmark that tests the ability of text models to answer multiple-choice questions from science exams spanning Grade 3 to Grade 9 with two subsets: Easy and Challenge.\n\n8 https://huggingface.co/WizardLM/WizardLM-13B-V1.2\n\n9 https://huggingface.co/WizardLM/WizardLM-70B-V1.0\n\n10 https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.2 Knowledge and Language Understanding",
        "chunkIndex": 45,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-46",
      "content": "- HellaSwag: A dataset [66] for evaluating commonsense natural language inference. It tests the ability of natural language models to complete text with what might happen next in the scene about physical situations.\n- LAMBADA: This dataset [48] is a collection of 10,022 passages from 2,663 novels that tests the ability of natural language models to perform long-range contextual understanding.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.3 Text Completion",
        "chunkIndex": 46,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-47",
      "content": "- MT-bench: is a benchmark tailored for evaluating the proficiency of chat assistants in multi-turn conversations [67] using GPT-4 as the judge.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.4 Multi Turn Open Ended Conversations",
        "chunkIndex": 47,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-48",
      "content": "- ACI-BENCH: It contains full doctor-patient conversations and associated clinical notes from various medical domains. The task is to generate a clinical note from the dialogue [59].\n- QMSum: A benchmark [68] for query-based multi-domain meeting summarization, where models have to select and summarize relevant spans of meetings in response to a query.\n- MS-MARCO: This dataset [2] is a large-scale collection of natural language questions and answers derived from real web queries and documents.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.5 Grounding and Abstractive Summarization",
        "chunkIndex": 48,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-49",
      "content": "- ToxiGen : This is a large-scale, machine-generated dataset [16] of 274,186 toxic and benign statements about 13 minority groups with a focus on implicit hate speech that does not contain slurs or profanity. We use the dataset to test a model's ability to both identify and generate toxic content.\n- TruthfulQA: A benchmark [30] for evaluating the truthfulness of LLMs in generating answers to questions constructed in a way that humans tend to answer the curated questions falsely due to false believes, biases and misconceptions. The evaluation benchmark contains 817 questions spanning 38 categories (e.g., health, law, finance and politics). We evaluate the models on a multiple-choice variant of the dataset.\n- HHH: This dataset [53] is benchmark for evaluating the alignment of language models with respect to helpfulness, honesty and harmlessness, where a language model is asked to choose the best response among two options.\n- Automated RAI Measurement Framework: We also use a recently pro",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.6 Safety and Truthfulness",
        "chunkIndex": 49,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-50",
      "content": "with respect to helpfulness, honesty and harmlessness, where a language model is asked to choose the best response among two options.\n- Automated RAI Measurement Framework: We also use a recently proposed framework [34] for evaluating the safety of a given chat-optimized model in conversational setting. Particularly, one LLM poses as a user and engages in a conversation with the LLM under test to evaluate potential harmful content, IP leakage and jailbreaks.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.2.6 Safety and Truthfulness",
        "chunkIndex": 50,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-51",
      "content": "We evaluate models' capabilities on all tasks under zero-shot setting and without any exemplars or CoT prompting. Note that we observe, in preliminary experiments, that larger models benefit more from few-shot settings than smaller models like Orca 2. We conduct evaluation only based on the zero-shot settings, we leave a detailed analysis of the few-shot capabilities to future work. In all experiments, we utilize a greedy decoding approach without sampling.\n\nPrompts : We use empty system messages and simple prompts for all models to avoid variations in quality due to prompt engineering, except for general guidelines around answer formats for some task. To minimize diversity and establish a reliable evaluation process, we often include formatting guidelines in system messages to enhance the accuracy of answer extraction. For instance, we might use a system message like ' At the end, output ###Final answer: {answer choice} ' and ' select the answer from the provided options.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 51,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-52",
      "content": "o enhance the accuracy of answer extraction. For instance, we might use a system message like ' At the end, output ###Final answer: {answer choice} ' and ' select the answer from the provided options. ' Table F shows the prompts used for each dataset. For Orca 2, we report performance with both an\n\n'empty' system message and a 'cautious' system message. The latter is a generic system message that was described in Section 4.\n\nAnswer parsing : Parsing answers from free-form responses from generative models is a difficult task. Therefore, we divided the evaluation tasks into 3 categories based on the type of task and the extraction required, namely:\n\n- MCQ (Multiple-Choice Questions): These tasks require extraction of the option selected as the final answer by the model. We also formatted any classification tasks into this category as well where the classes represent the options for the model to choose from.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 52,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-53",
      "content": "ion of the option selected as the final answer by the model. We also formatted any classification tasks into this category as well where the classes represent the options for the model to choose from. The prompt for these tasks included the question, followed by the answer choices.\n- No extraction required: This category is for tasks that did not require extraction. Open-ended question answering falls into this category.\n- Exact Match/Span Extraction: These tasks require extraction of the exact final answer in the response or a span from the context provided.\n\nIn the categories requiring extraction (MCQ and Exact Match/Span Extraction), we compile an extensive set of patterns and delimiters like 'Final answer', 'So, the answer is', 'Final option:', etc. to extract the text from the response that might contain the answer. We then use regular expressions to extract the right option IDs or the exact text of the option selected by the model as the answer.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 53,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-54",
      "content": "o extract the text from the response that might contain the answer. We then use regular expressions to extract the right option IDs or the exact text of the option selected by the model as the answer. Answer parsing for exact matches/span extraction varies depending on the task. Responses are matched for consistency with the gold answers. Along with evaluation metrics, we also calculate a format-OK metric which is the percentage of samples from which our parsing logic was able to extract an answer. We employ the same parsing logic to all the models' responses for consistency and we acknowledge that performance of all models could be improved with a better parsing logic.\n\nHowever, models may not always adhere to these formatting guidelines. The extraction coverage and models' sensitivity to system instructions and prompts may lead to different results for some baselines compared to those reported in other studies.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 54,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-55",
      "content": "formatting guidelines. The extraction coverage and models' sensitivity to system instructions and prompts may lead to different results for some baselines compared to those reported in other studies. Nonetheless, all models in this study undergo the same evaluation pipeline.\n\nIn addition to the tasks from FLANv2, we include tasks from the training portions of the following datasets (hence they should be considered in-domain, even with a zero-shot evaluation): DROP, ARC, RACE, Hellaswag, Lambada, MS Marco and GSM8K. The rest of the benchmarks should be considered as out-of-domain to the best of our knowledge. Note that we do not have detailed information about the data used for training the base model (LLAMA-2) and hence we cannot completely rule out further data leakage. However, we report the performance of several instruction-tuned versions of LLAMA-2 for reference.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 55,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-56",
      "content": "d for training the base model (LLAMA-2) and hence we cannot completely rule out further data leakage. However, we report the performance of several instruction-tuned versions of LLAMA-2 for reference.\n\nIn the following sections, we discuss the performance of Orca 2 and other baseline models on the benchmarks described above in zero-shot setting.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "5.3 Evaluation Settings",
        "chunkIndex": 56,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-57",
      "content": "Reasoning capabilities are pivotal in ascertaining the efficacy of LLMs. Here we assess the reasoning prowess of Orca 2 models by testing them against a wide range of benchmarks, such as AGI Eval, BigBench-Hard (BBH), DROP, RACE, GSM8K, and CRASS. The average performance across these benchmarks is depicted in Figure 4. When comparing Orca 2, we observe the following phenomenon:\n\n- Surpassing models of the same size - Orca-2-13B significantly outperforms models of the same size on zero-shot reasoning tasks. Orca-2-13B provides a relative improvement of 47 . 54% over LLaMA-2-Chat-13B and 28 . 15% over WizardLM-13B. Notably, all three models - Orca-2-13B, LLaMA-2-Chat-13B, and WizardLM-13B - share the same base model, highlighting the efficacy of the training process employed by Orca 2.\n- Competitive with models 5-10x larger - Furthermore, Orca-2-13B exceeds the performance of LLaMA-2-Chat-70B and performs comparably to WizardLM-70B and\n\nAverage Performance on Reasoning Benchmarks",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.1 Reasoning",
        "chunkIndex": 57,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-58",
      "content": "2.\n- Competitive with models 5-10x larger - Furthermore, Orca-2-13B exceeds the performance of LLaMA-2-Chat-70B and performs comparably to WizardLM-70B and\n\nAverage Performance on Reasoning Benchmarks\n\nFigure 4: Macro-average Performance of different models on reasoning benchmarks. Detailed task specific performance breakdown is reported in Table 1\n\n<!-- image -->\n\nTable 1: Zero-Shot performance comparison of different models on reasoning benchmarks.\n\n| Model            |   AGI |   BBH |   DROP |   CRASS |   RACE |   GSM8K |\n|------------------|-------|-------|--------|---------|--------|---------|\n| Orca 2-7B        | 45.1  | 45.93 |  60.26 |   84.31 |  80.79 |   47.23 |\n| w/ cautious sm   | 43.97 | 42.8  |  69.09 |   88.32 |  75.82 |   55.72 |\n| Orca 2-13B       | 49.93 | 50.18 |  57.97 |   86.86 |  82.87 |   59.14 |\n| w/ cautious sm   | 48.18 | 50.01 |  70.88 |   87.59 |  79.16 |   65.73 |\n| Orca-1-13B       | 45.69 | 47.84 |  53.63 |   90.15 |  81.76 |   26.46 |\n| LLaMA-2-Chat-13B",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.1 Reasoning",
        "chunkIndex": 58,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-59",
      "content": "|   86.86 |  82.87 |   59.14 |\n| w/ cautious sm   | 48.18 | 50.01 |  70.88 |   87.59 |  79.16 |   65.73 |\n| Orca-1-13B       | 45.69 | 47.84 |  53.63 |   90.15 |  81.76 |   26.46 |\n| LLaMA-2-Chat-13B | 38.85 | 33.6  |  40.73 |   61.31 |  62.69 |   25.09 |\n| WizardLM-13B     | 38.25 | 38.47 |  45.97 |   67.88 |  62.77 |   48.6  |\n| LLaMA-2-Chat-70B | 46.7  | 44.68 |  54.11 |   74.82 |  68.79 |   52.01 |\n| WizardLM-70B     | 48.73 | 51.08 |  59.62 |   86.13 |  78.96 |   73.24 |\n| ChatGPT          | 53.13 | 55.38 |  64.39 |   85.77 |  67.87 |   79.38 |\n| GPT-4            | 70.4  | 69.04 |  71.59 |   94.53 |  83.08 |   85.52 |\n\nChatGPT. Orca-2-7B is better or comparable to LLaMA-2-Chat-70B on all reasoning tasks.\n\n- Cautious system message adds a small boost - Using the cautious system message with both the 7B and 13B models provides small gains over the empty system message.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.1 Reasoning",
        "chunkIndex": 59,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-60",
      "content": "2-Chat-70B on all reasoning tasks.\n\n- Cautious system message adds a small boost - Using the cautious system message with both the 7B and 13B models provides small gains over the empty system message.\n\nNote that for baseline evaluations, results obtained from our runs are comparable to other public results with zero-shot setting and within a reasonable difference compared to few-shot results. Our numbers are sometimes better than publicly reported (e.g., our ChatGPT and GPT-4 runs on AGIEval compared to those reported in [69], our WizardLM-13B and WizardLM-70B runs on DROP in contrast to those reported in the Open LLM Leaderboard). However, some of them are worse, for example on RACE, our ChatGPT run is 9 pts lower than reported in [28]. This could be attributed to different ChatGPT endpoints and versions, or to different prompts used for evaluation.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.1 Reasoning",
        "chunkIndex": 60,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-61",
      "content": "are worse, for example on RACE, our ChatGPT run is 9 pts lower than reported in [28]. This could be attributed to different ChatGPT endpoints and versions, or to different prompts used for evaluation.\n\nPerformance breakdown across different tasks of AGIEval and BBH is provided in Appendix A. Examples from each dataset with the response from Orca 2 is presented in Appendix F.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.1 Reasoning",
        "chunkIndex": 61,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-62",
      "content": "MMLU, ARC-Easy and ARC-Challenge assess the language understanding, knowledge and reasoning of LLMS. As with other benchmarks, we compare only to instruction-tuned models and conduct a zero-shot evaluation. Table 2 displays the results for knowledge and language\n\n| Model            |   MMLU |   ARC Easy |   ARC Challenge |\n|------------------|--------|------------|-----------------|\n| Orca-2-7B        |  53.7  |      87.79 |           78.41 |\n| w/ cautious sm   |  53.91 |      85.1  |           74.83 |\n| Orca-2-13B       |  57.73 |      92.85 |           83.36 |\n| w/ cautious sm   |  59.32 |      85.31 |           79.95 |\n| LLAMA-2-Chat-13B |  49.14 |      76.26 |           61.18 |\n| WizardLM-13B     |  42.81 |      68.98 |           50.43 |\n| Orca-1-13B       |  53.8  |      86.24 |           74.74 |\n| LLAMA-2-Chat-70B |  58.54 |      82.2  |           67.66 |\n| WizardLM-70B     |  55    |      80.68 |           71.93 |\n| ChatGPT          |  68.92 |      93.73 |           84.73 |\n| GP",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.2 Knowledge and Language Understanding",
        "chunkIndex": 62,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-63",
      "content": "74.74 |\n| LLAMA-2-Chat-70B |  58.54 |      82.2  |           67.66 |\n| WizardLM-70B     |  55    |      80.68 |           71.93 |\n| ChatGPT          |  68.92 |      93.73 |           84.73 |\n| GPT-4            |  80.61 |      96.63 |           93.26 |\n\nTable 2: Zero-Shot performance comparison of different models on MMLU, ARC Easy and ARC Challenge. System Message indicates if the system message was 'empty' or 'cautious'.\n\ncomprehension benchmarks. Overall, we observe similar trends as with the reasoning tasks:\n\n- Surpassing models of the same size - Orca-2-13B surpasses LLaMA-2-Chat-13B and WizardLM-13B (both using the same base model as Orca-2) in performance on each individual benchmarks. On average, Orca-2-13B achieves a relative improvement of 25 . 38% over LLaMA2-Chat-13B and 44 . 22% over WizardLM-13B.\n- Competitive with models 5-10x larger - Orca-2-13B also outperforms both 70B baseline models. In the MMLU benchmark, Orca-2-13B (57 .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.2 Knowledge and Language Understanding",
        "chunkIndex": 63,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-64",
      "content": "of 25 . 38% over LLaMA2-Chat-13B and 44 . 22% over WizardLM-13B.\n- Competitive with models 5-10x larger - Orca-2-13B also outperforms both 70B baseline models. In the MMLU benchmark, Orca-2-13B (57 . 73%) achieves a score similar to LLaMA2-Chat-70B (58 . 54%) and WizardLM-70 (55 . 00%), both of which are approximately 5 times larger than Orca-2-13B. Additionally, Orca-2-7B surpasses both 70B baselines on the ARC test set.\n\nWe further note our baseline runs for this set of evaluations align with publicly reported results under zero-shot settings, considering the differences in prompts and possible variations in API endpoints for GPT models. We also point out that publicly reported results with LLaMA-2 models on MMLU are higher (54.8 and 68.9 for 13B and 70B variants, respectively [58]). However, these numbers are in few-shot settings, compared to the zero-shot settings reported in this paper.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.2 Knowledge and Language Understanding",
        "chunkIndex": 64,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-65",
      "content": "aMA-2 models on MMLU are higher (54.8 and 68.9 for 13B and 70B variants, respectively [58]). However, these numbers are in few-shot settings, compared to the zero-shot settings reported in this paper.\n\nWhile we did not perform a comprehensive few-shot evaluation of Orca 2, preliminary results on one task point to smaller gains (over zero-shot settings) for Orca 2 compared to LLaMA-2 models, especially when compared to the 70B base models. We discuss this in Section 7 and aim to study this further moving forward.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.2 Knowledge and Language Understanding",
        "chunkIndex": 65,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-66",
      "content": "In addition to benchmarks measuring advanced reasoning capabilities, we also use HellaSwag and LAMBADA to measure text completion abilities. HellaSwag measures text completion skills in a multiple-choice question format, while LAMBADA is a single-word completion task.\n\nFigure 5 shows the performance of different models on text completion benchmarks. Both Orca-2-7B and Orca-2-13B exhibit strong performance on HellaSwag outperforming the 13B and 70B baselines. Orca-2-13B achieves a relative improvement of 33 . 13% over LLaMA-2Chat-13B and 61 . 94% over WizardLM-13B.\n\nFigure 5: Performance of different models on text completion test sets in zero-shot setting.\n\n<!-- image -->\n\nWe compare baseline results from our runs with publicly reported results and identify that on HellaSwag, LLaMA-2-13B has much higher performance than LLaMA-2-Chat-13B.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.3 Text Completion",
        "chunkIndex": 66,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-67",
      "content": "o-shot setting.\n\n<!-- image -->\n\nWe compare baseline results from our runs with publicly reported results and identify that on HellaSwag, LLaMA-2-13B has much higher performance than LLaMA-2-Chat-13B. We randomly sampled from LLaMA-2-Chat-13B and LLaMA-2-Chat-70B responses and manually reviewed them to find that indeed many of the answers were wrong, with several cases where the models refuse to answer citing safety concerns, sometimes incorrectly. We conjecture that chat models might not be best suited for text completion tasks like HellaSwag.\n\nWe also investigate the subpar performance of GPT-4 in the LAMBADA task. Our preliminary analysis shows that GPT-4 often claims that the context does not provide sufficient information to accurately identify the missing word or proposes a word that does not match the gold label. For example:",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.3 Text Completion",
        "chunkIndex": 67,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-68",
      "content": "lysis shows that GPT-4 often claims that the context does not provide sufficient information to accurately identify the missing word or proposes a word that does not match the gold label. For example:\n\ni glanced up to hunter who was at his dresser spraying on some cologne . ' mom , hang on . ' i covered the phone . ' mom said not to worry about ryder and go out with the boys and then we can do sunday dinner there . is that ok with you ? ' i missed having family dinners too . ' yeah , sounds good , i 'll call mom and tell her about \\_\\_.' What is the word in the blank space (\\_\\_)? The answer is\n\nThe gold answer is Dinner but GPT-4 responds with\n\nIt is not possible for me to determine the exact word that should be in the blank space without more context. However, based on the provided text, a possible word could be 'it.' The sentence would then read: 'yeah, sounds good, I'll call mom and tell her about it.'",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.3 Text Completion",
        "chunkIndex": 68,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-69",
      "content": "be in the blank space without more context. However, based on the provided text, a possible word could be 'it.' The sentence would then read: 'yeah, sounds good, I'll call mom and tell her about it.'\n\nAlthough GPT-4's performance could be enhanced through prompt engineering, it appears that LAMBADA might need additional prompt engineering and may not be suitable for evaluating chat-optimized models.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.3 Text Completion",
        "chunkIndex": 69,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-70",
      "content": "We evaluate the capabilities of Large Language Models (LLMs) in multi-turn conversational settings, utilizing the MT Bench dataset [67]. MT-Bench initiates conversations with LLMs through predetermined inquiries. Each dialogue consists of an initial query (Turn 1) and a follow-up query (Turn 2). Notably, the follow-up query remains unaltered, irrespective of the LLM's response to the opening query.\n\nMT-Bench employs GPT-4 for evaluation purposes. For each turn, MT-Bench calculates a score ranging from 1 to 10 using GPT-4. The per-turn score and the average score on MTBench can be found in Table 3. We have examined different GPT-4 endpoints and discovered\n\nTable 3: MT-Bench scores per turn and average\n\n| Model            |   Turn 1 |   Turn 2 |   Average |\n|------------------|----------|----------|-----------|\n| Orca-2-7B        |     6.14 |     5.15 |      5.65 |\n| w/ cautious sm   |     5.96 |     3.99 |      4.97 |\n| Orca-2-13B       |     6.69 |     5.6  |      6.15 |\n| w/ cautious",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.4 Multi-Turn Open Ended Conversations",
        "chunkIndex": 70,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-71",
      "content": "-------|-----------|\n| Orca-2-7B        |     6.14 |     5.15 |      5.65 |\n| w/ cautious sm   |     5.96 |     3.99 |      4.97 |\n| Orca-2-13B       |     6.69 |     5.6  |      6.15 |\n| w/ cautious sm   |     6.12 |     5.31 |      5.72 |\n| LLaMA-2-Chat-13B |     7.17 |     6.11 |      6.64 |\n| WizardLM-13B     |     7.14 |     5.58 |      6.36 |\n| Orca-1-13B       |     6.66 |     5.19 |      5.92 |\n| LLaMA-2-Chat-70B |     7.05 |     6.59 |      6.82 |\n| WizardLM-70B     |     8.07 |     7.45 |      7.76 |\n| ChatGPT          |     8.19 |     7.84 |      8.01 |\n| GPT-4            |     9.01 |     9.06 |      9.04 |\n\nthat they yield different assessments. This raises a question about the comparability of the results produced by different GPT-4 versions. To minimize potential issues, we have employed the same GPT-4 endpoint and version for conducting evaluations.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.4 Multi-Turn Open Ended Conversations",
        "chunkIndex": 71,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-72",
      "content": "s a question about the comparability of the results produced by different GPT-4 versions. To minimize potential issues, we have employed the same GPT-4 endpoint and version for conducting evaluations.\n\nOrca-2-13B performs comparably with other 13B models. The average second turn score of Orca-2-13B is lower than the first turn score, which can be attributed to the absence of conversations in its training data. However, Orca 2 is still capable of engaging in conversations, and this ability can be enhanced by packing multiple zero-shot examples into the same input sequence. It is part of our future work to improve Orca 2's multi-turn conversational ability.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.4 Multi-Turn Open Ended Conversations",
        "chunkIndex": 72,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-73",
      "content": "Generating responses that are grounded in specific context is a desired property for many LLM applications. We use three different tasks for this evaluation covering query-based meeting summarization, web question answering where answers are generated and have long format and doctor-patient conversation summarization. Abstractive summarization and grounded questions answering are frequently used as test beds to evaluate groundedness.\n\nWe use the grounding evaluation framework proposed in [34]. The framework uses GPT-4 as a judge to measure in-context groundedness. Note that using any model as a proxy for evaluation (including GPT-4) has limitations depending on the model, for example, if the model has tendency to favour samples with specific characteristics like its own generations, long text or specific order of samples [67, 60, 37]. Working on increasing consistency between human evaluation and LLM based evaluation is an open area of research [32, 15, 43, 34, 67].",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.5 Grounding",
        "chunkIndex": 73,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-74",
      "content": "generations, long text or specific order of samples [67, 60, 37]. Working on increasing consistency between human evaluation and LLM based evaluation is an open area of research [32, 15, 43, 34, 67].\n\nFigure 6 presents hallucination rate results for different models averaged over three benchmarks we have conducted experiments on.\n\nWe note that Orca-2-13B exhibits the lowest rate of hallucination among all Orca 2 variants and other 13B and 70B LLMs. When compared with the LLaMA-2-13B and WizardLM-13B models, Orca-2-13B demonstrates a relative reduction of 76 . 92% and 61 . 71% in hallucination rate. Though cautious system message consistently increases the Hallucination Rate across the three tasks studied in this work. Through manual analysis, we found evidence that during the reasoning process led by cautious system message, Orca 2 might extrapolate the information available in the context, and uses the extrapolated content to create the summary.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.5 Grounding",
        "chunkIndex": 74,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-75",
      "content": "d evidence that during the reasoning process led by cautious system message, Orca 2 might extrapolate the information available in the context, and uses the extrapolated content to create the summary. The ungrounded generated contents are often factually accurate, but they are not supported by the context. Examples of this situation for each of the datasets are presented in Appendix F.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.5 Grounding",
        "chunkIndex": 75,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-76",
      "content": "Acomprehensive evaluation of LLMs for safety is quite complex and is an open area of research. In this study, we have conducted experiments on the publicly available datasets described in\n\nAverage Fault on Grounding in Abstractive Summarization Benchmarks\n\nFigure 6: The hallucination rate evaluated by GPT-4 as discriminator averaged over three abstractive summarization benchmarks described in section 5 (the lower the better). Task specific performance breakdown is reported in Table 11.\n\n<!-- image -->\n\nsection 5.2.6 and a recent measurement framework proposed in [34]. They represent tasks with a focus domain of implicit and explicit toxicity, truthfulness, three categories of content harms, three domains for IP and three areas of jailbreaks. Our experiments include two evaluation regimes for each model, a discriminative regime where the model is used as a classifier to identify the type of given content, and a generative regime where the model is prompted to produce output that does no",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 76,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-77",
      "content": "or each model, a discriminative regime where the model is used as a classifier to identify the type of given content, and a generative regime where the model is prompted to produce output that does not follow safety guidelines and is then evaluated on how many times it follows the prompt intent. Please note that in both regimes, models might behave differently both in terms of not filtering the content correctly or filtering the content more aggressively which might result in erasure for different social groups.\n\nFor discriminative evaluation we have used ToxiGen, HHH and TruthfulQA:\n\nToxiGen: In the human annotated portion of this dataset each of the statements has a toxicity score provided by three human annotators. For the purpose of this experiment, we formulate the task as a classification task and assign each statement with a score lower than 2 . 5 as benign and greater than or equals to 2 . 5 as being toxic. The model is then prompted to score each of the statements.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 77,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-78",
      "content": "a classification task and assign each statement with a score lower than 2 . 5 as benign and greater than or equals to 2 . 5 as being toxic. The model is then prompted to score each of the statements. The model's accuracy is calculated as the percentage of statements it classifies correctly as toxic or benign. Results are presented in Figure 7 and Figure 8.\n\nFrom the experiments we observe that Orca-2-7B, WizardLM-13B, LLaMA-2-70B and Orca 1 models perform better at classifying toxic statements than classifying neutral statements. This is an important note as it might cause erasure (filtering out the content related to specific groups of people even if it is not problematic) for target identity groups in ToxiGen. Orca-2-13B, WizardLM-70B and LLaMA-2-13B do not have this problem for this experiment.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 78,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-79",
      "content": "ontent related to specific groups of people even if it is not problematic) for target identity groups in ToxiGen. Orca-2-13B, WizardLM-70B and LLaMA-2-13B do not have this problem for this experiment.\n\nNote that we also conducted an experiment to ensure instruction following of various models for this experiment, i.e., making sure the model outputs the requested format. All models in Orca 2 family, LLaMa-2 family and WizardLM family had rates above 96%. They were relatively lower for Orca 1 model, 79%, which does not follow task instruction as well.\n\nPerformance breakdown for each of the categories in ToxiGen are presented in Appendix D.1.\n\nTruthfulQA: For evaluation on this dataset we have used the multiple-choice variant of the dataset, TruthfulQA MC from EleutherAI, which includes questions from TruthfulQA in multiple choice format. Multiple choice style evaluation for TruthfulQA has also been used",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 79,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-80",
      "content": "ple-choice variant of the dataset, TruthfulQA MC from EleutherAI, which includes questions from TruthfulQA in multiple choice format. Multiple choice style evaluation for TruthfulQA has also been used\n\nFigure 7: ToxiGen evaluation results for toxic statement classification averaged over all the 13 categories.\n\n<!-- image -->\n\nFigure 8: ToxiGen evaluation results for neutral statement classification averaged over all the 13 categories.\n\n<!-- image -->\n\nin [44]. There are related works that have used generative style evaluation for this dataset (e.g., [57]) using another model as judge which we have not used in this experiment.\n\nThe results are presented in Figure 9, where we observe that Orca-2-13B performs better in answering the questions compared to other models of similar size and comparable to models with much larger size.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 80,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-81",
      "content": "results are presented in Figure 9, where we observe that Orca-2-13B performs better in answering the questions compared to other models of similar size and comparable to models with much larger size.\n\nFigure 9: Performance of different models on TruthfulQA benchmark. We report the accuracy as the percentage of times the model generated the correct answer to the given multiple choice questions.\n\n<!-- image -->\n\nPlease note that the reason for the performance difference for both LLaMA-2-Chat-13B and LLaMA-2-Chat-70B from the ones reported in LLaMA-2 report [57] for TruthfulQA is that the evaluation schemes are different. In LLaMA-2, they report a generative style evaluation where GPT-3 has been used as annotator while we have used multiple choice version of the dataset to avoid limitations of using GPT-3 (or any LLM) as a judge (order bias, length bias, etc) when the gold labels are available.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 81,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-82",
      "content": "ed as annotator while we have used multiple choice version of the dataset to avoid limitations of using GPT-3 (or any LLM) as a judge (order bias, length bias, etc) when the gold labels are available.\n\nHHH: The results for the HHH task are presented in Figure 10 where both Orca-2-13B and Orca-2-7B outperform models of similar sizes. We also observed that LLaMA-2-Chat-13B and LLaMA-2-Chat-70B models occasionally avoid answering HHH questions, responding with answers like: 'I cannot fulfill your request.' Additionally, both LLaMA-2-Chat and Orca 2 models (w/ cautious systems message) tend to produce open-ended responses to some questions (especially longer ones) instead of choosing a specific answer. We believe that additional prompt engineering could help address some of these cases.\n\nFor generative style evaluation we have used the framework proposed in [34] and ToxiGen.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 82,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-83",
      "content": "osing a specific answer. We believe that additional prompt engineering could help address some of these cases.\n\nFor generative style evaluation we have used the framework proposed in [34] and ToxiGen. It is important to note that any model used as annotator (including the ones that we have selected) is a proxy and may come with its own weaknesses and biases depending on the data it has been trained on. We recommend to take these experiments as an evaluation methodology and repeat them for the given domain of interest or application using the models and annotation approaches for the given domain:\n\nAutomated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI metrics for LLMs [34]. In this framework, one LLM poses as a user and engages in a synthetic conversation with the LLM under test which is Orca 2 in the experiments we are presenting in this section.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 83,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-84",
      "content": "metrics for LLMs [34]. In this framework, one LLM poses as a user and engages in a synthetic conversation with the LLM under test which is Orca 2 in the experiments we are presenting in this section. Another LLM then evaluates the responses to measure its tendency to violate Responsible AI guidelines from the following aspects: (1) How many times the user succeeded in Jailbreaking the model under the test? (2) How many times the model under the test generates Potentially Harmful Content? (3) How many times the model leaks Intellectual Property (IP)? (For example while responding to 'The Great Gatsby book full text'). These categories are divided into multiple sub-categories described in [34]. Results of our experiments using this framework are presented in Table 4 and 5. Please note that the automated framework sets its own\n\nFigure 10: Evaluation results for HHH dataset.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 84,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-85",
      "content": "]. Results of our experiments using this framework are presented in Table 4 and 5. Please note that the automated framework sets its own\n\nFigure 10: Evaluation results for HHH dataset.\n\n<!-- image -->\n\nsystem message while assessing chat model's safety. Please note that Orca 2 has not gone through RLHF safety training similar to LLaMA-2-Chat-13B and the tables 4 and 5 reflect some aspects of that. We hypothesize that since the teacher models (ChatGPT, GPT-4) have gone through various steps of safety tuning, and the use of content filtering in the data generation process, Orca 2 might have learned some aspects of those through instruction tuning. We probe some of the categories where we see a larger difference (e.g., violent) and observe that Orca 2 tends to counter the harmful positions more often (which is penalized by the metric), while models that have gone through RLHF safety training tend to decline to respond more often (which is rewarded by the metric).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 85,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-86",
      "content": "r the harmful positions more often (which is penalized by the metric), while models that have gone through RLHF safety training tend to decline to respond more often (which is rewarded by the metric).\n\n|                  | Potentially Harmful Content   | Potentially Harmful Content   | Potentially Harmful Content   | IP      | IP      | IP     |\n|------------------|-------------------------------|-------------------------------|-------------------------------|---------|---------|--------|\n| Model            | Sexual ↓                      | Violent ↓                     | Hate ↓                        | Songs ↓ | Books ↓ | News ↓ |\n| Orca-2-13B       | 0.60%                         | 13.47%                        | 2.26%                         | 18.47%  | 21.85%  | 21.48% |\n| LLaMA-2-Chat-13B | 0.00%                         | 0.17%                         | 0.83%                         | 16.84%  | 15.49%  | 18.18% |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 86,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-87",
      "content": "| 18.47%  | 21.85%  | 21.48% |\n| LLaMA-2-Chat-13B | 0.00%                         | 0.17%                         | 0.83%                         | 16.84%  | 15.49%  | 18.18% |\n\nTable 4: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Harmful Content and IP .\n\n| Model            | Adult Content ↓   | Illegal Persuasion ↓   | Leaking Guidelines ↓   |\n|------------------|-------------------|------------------------|------------------------|\n| Orca-2-13B       | 4.55% 1.00%       | 7.58% 6.00%            | 24.24% 70.00%          |\n| LLaMA-2-Chat-13B |                   |                        |                        |\n\nTable 5: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Jailbreak .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 87,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-88",
      "content": "|                        |                        |\n\nTable 5: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Jailbreak .\n\nToxiGen: In this experiment, we prompt the model with the test set of ToxiGen [16] dataset containing toxic and benign statements. The task for the model is to continue the text further. To compute the toxicity of the content generated, we use HateBERT [4], an off-the-shelf hate speech detector to generate toxicity probability with higher values signifying more toxicity in the content. We represent them as a number between 0 to 100 for better visibility. Results for generation using toxic and neutral prompts are presented in Figure 11\n\nFigure 11: Comparison between different models on their tendency to generate toxic and neutral content over different categories when prompted with a text completion task for ToxiGen dataset using HateBERT as proxy for toxicity detection (lower is better).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 88,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-89",
      "content": "tendency to generate toxic and neutral content over different categories when prompted with a text completion task for ToxiGen dataset using HateBERT as proxy for toxicity detection (lower is better).\n\n<!-- image -->\n\nFigure 12: Zero-Shot exact match performance on story reordering task of various models. GPT4 refers to default GPT-4 answer (Answer 1 in Figure 3), GPT4 w/ step-by-step corresponds to default step-by-step answer (Answer 3 in Figure 3), GPT4 w/ explain refers to answer then explain type of answer (Answer 2 in Figure 3). GPT4 w/ special step-by-step (Answer 4 in Figure 3) is used to generate data for Orca 2.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.6 Safety",
        "chunkIndex": 89,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-90",
      "content": "We create 5,000 training samples for story reordering using the prompt in Figure 3. We do not use the complex prompt during Orca 2 training (i.e. applying prompt erasing ). We mix the task-specific data with the rest of the training dataset and evaluate Orca 2 on a distinct set of the ROCStories corpus [41]. While sampling the test instances, we remove any instances from ROCStories that are in FLAN training split to avoid contamination. Figure 12 compares the performance of Orca 2 with different system messages for GPT-4. It also captures the performance of ChatGPT, Orca 1, LLaMA and WizardLM models. This experiment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "6.7 Effect of Task-Specific Data with Story Reordering",
        "chunkIndex": 90,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-91",
      "content": "Orca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well as the common limitations of other large language models and limitations originating from Orca 2's training process, including:\n\nData Biases: Large language models, trained on extensive data, can inadvertently carry biases present in the source data. Consequently, the models may generate outputs that could be potentially biased or unfair.\n\nLack of Transparency: Due to the complexity and size, large language models can act as 'black boxes', making it difficult to comprehend the rationale behind specific outputs or decisions. We recommend reviewing transparency notes from Azure for more information 11 .\n\nContent Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 91,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-92",
      "content": "on 11 .\n\nContent Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them. It is recommended to leverage various content moderation services provided by different companies and institutions. On an important note, we hope for better regulations and standards from government and technology leaders around content harms for AI technologies in future. We value and acknowledge the important role that research and open source community can play in this direction.\n\nHallucination: It is important to be aware and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models from fabricating content.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 92,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-93",
      "content": "and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models from fabricating content. Moreover, it is not clear whether small models may be more susceptible to hallucination in ungrounded generation use cases due to their smaller sizes and hence reduced memorization capacities. This is an active research topic and we hope there will be more rigorous measurement, understanding and mitigations around this topic.\n\nPotential for Misuse: Without suitable safeguards, there is a risk that these models could be maliciously used for generating disinformation or harmful content.\n\nData Distribution: Orca 2's performance is likely to correlate strongly with the distribution of the tuning data. This correlation might limit its accuracy in areas underrepresented in the training dataset such as math and coding.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 93,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-94",
      "content": "rformance is likely to correlate strongly with the distribution of the tuning data. This correlation might limit its accuracy in areas underrepresented in the training dataset such as math and coding.\n\nSystem messages: Orca 2 demonstrates variance in performance depending on the system instructions. Additionally, the stochasticity introduced by the model size may lead to generation of non-deterministic responses to different system instructions.\n\nZero-Shot Settings: Orca 2 was trained on data that mostly simulate zero-shot settings. While the model demonstrates very strong performance in zero-shot setting, it does not show the same gains of using few-shot learning compared to other, specially larger, models.\n\nSynthetic data: As Orca 2 is trained on synthetic data, it could inherit both the advantages and shortcomings of the models and methods used for data generation.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 94,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-95",
      "content": "d to other, specially larger, models.\n\nSynthetic data: As Orca 2 is trained on synthetic data, it could inherit both the advantages and shortcomings of the models and methods used for data generation. We posit that Orca 2 benefits from the safety measures incorporated during training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks.\n\nSmall Model Capacity: We note that post-training, while significantly beneficial in teaching the model how to solve a task, it does not necessarily teach the model new knowledge. Hence post-trained models will be mostly limited by the knowledge learned during pre-training. While this process can enhance the small model ability to reason, it does not expand its ability as a knowledge store.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 95,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-96",
      "content": "ained models will be mostly limited by the knowledge learned during pre-training. While this process can enhance the small model ability to reason, it does not expand its ability as a knowledge store. As such Orca 2is perhaps more suitable as reasoning engine over knowledge provided to the model in its context window, or when fine-tuned to specialize into narrower domains.\n\nThis model is solely designed for research settings, and its testing has only been carried out in such environments. It should not be used in downstream applications, as additional analysis is needed to assess potential harm or bias in the proposed application.\n\n11 https://learn.microsoft.com/en-us/legal/cognitive-services/openai/ transparency-note",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "7 Limitations",
        "chunkIndex": 96,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-97",
      "content": "Our study has demonstrated that improving the reasoning capabilities of smaller language models is not only possible, but also attainable through training on tailored synthetic data. Orca 2 models, by implementing a variety of reasoning techniques and recognizing the most effective solution strategy for each task, achieve performance levels comparable to, and often exceeding, models that are much larger, especially on zero-shot reasoning tasks. Though these models still exhibit limitations and constraints inherent to their base models, they show a promising potential for future improvement, especially in terms of better reasoning capabilities, control and safety, through the use of synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide another opportunity for improving the overall safety of the models.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "8 Conclusions",
        "chunkIndex": 97,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-98",
      "content": "we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide another opportunity for improving the overall safety of the models. While the journey towards fully realizing the potential of small language models is ongoing, our work represents a step forward, especially highlighting the value of teaching smaller models to reason. It also highlights the potential of using tailored and high-quality synthetic data, created by a more powerful model, for training language models using complex prompts and potentially multiple model calls. While frontier models will continue to demonstrate superior capabilities, we believe that research toward building more capable smaller models will help pave the way for new applications that require different deployment scenarios and trade offs between efficiency and capability.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "8 Conclusions",
        "chunkIndex": 98,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-99",
      "content": "- [1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Kri",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 99,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-100",
      "content": "alez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 100,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-101",
      "content": "an, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\n- [2] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension dataset, 2018.\n- [3] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. Taking flight with copilot: Early insights and opportunities of ai-powered pair-programming tools. Queue , 20(6):35-57, jan 2023. ISSN 1542-7730. doi: 10.1145/3582083. URL https://doi.org/10.1145/3582083 .\n- [4] Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and M. Granitzer.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 101,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-102",
      "content": "-programming tools. Queue , 20(6):35-57, jan 2023. ISSN 1542-7730. doi: 10.1145/3582083. URL https://doi.org/10.1145/3582083 .\n- [4] Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and M. Granitzer. Hatebert: Retraining bert for abusive language detection in english. ArXiv , abs/2010.12472, 2021.\n- [5] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757 , 2023.\n\n- [6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://vicuna.lmsys.org .\n- [7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Ch",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 102,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-103",
      "content": "n Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.\n- [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457, 2018. URL https://api.semanticscholar.org/CorpusID: 3922816 .\n- [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 103,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-104",
      "content": "anticscholar.org/CorpusID: 3922816 .\n- [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- [10] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/ N19-1246 .\n- [11] Jörg Frohberg and Frank Binder.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 104,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-105",
      "content": "ages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/ N19-1246 .\n- [11] Jörg Frohberg and Frank Binder. Crass: A novel data set and benchmark to test counterfactual reasoning of large language models, 2022.\n- [12] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/ .\n- [13] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms, 2023.\n- [14] Himanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, and Chitta Baral. \" john is 50 years old, can his son be 65?\" evaluating nlp models' understanding of feasibility.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 105,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-106",
      "content": "waroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, and Chitta Baral. \" john is 50 years old, can his son be 65?\" evaluating nlp models' understanding of feasibility. arXiv preprint arXiv:2210.07471 , 2022.\n- [15] Veronika Hackl, Alexandra Elena Müller, Michael Granitzer, and Maximilian Sailer. Is gpt-4 a reliable rater? evaluating consistency in gpt-4 text ratings, 2023.\n- [16] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3309-3326. Association for Computational Linguistics, 2022.\n- [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 106,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-107",
      "content": "n for Computational Linguistics, 2022.\n- [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n- [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021.\n- [19] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 523-533, 2014.\n- [20] Frederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet M. Baker. Perplexity-a measure of the difficulty of speech recognition tasks. Journal of the Acoustical Society of America , 62, 1977.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 107,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-108",
      "content": "20] Frederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet M. Baker. Perplexity-a measure of the difficulty of speech recognition tasks. Journal of the Acoustical Society of America , 62, 1977. URL https://api.semanticscholar.org/CorpusID:121680873 .\n\n- [21] Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. Teaching language models to hallucinate less with synthetic tasks, 2023.\n- [22] Daniel Kahneman. Thinking, fast and slow . Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL https://www.amazon.de/ Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl\\_it\\_dp\\_o\\_pdT1\\_nS\\_nC? ie=UTF8&amp;colid=151193SNGKJT9&amp;coliid=I3OCESLZCVDFL7 .\n- [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 108,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-109",
      "content": "man/dp/0374275637/ref=wl\\_it\\_dp\\_o\\_pdT1\\_nS\\_nC? ie=UTF8&amp;colid=151193SNGKJT9&amp;coliid=I3OCESLZCVDFL7 .\n- [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023.\n- [24] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics , 3:585-597, 2015.\n- [25] Mario Michael Krell, Matej Kosec, Sergio P. Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance, 2022.\n- [26] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 109,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-110",
      "content": ": Accelerating large language models without impacting performance, 2022.\n- [26] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 271-281, 2014.\n- [27] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082 .\n- [28] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 110,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-111",
      "content": "- [28] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 431-469, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.29. URL https://aclanthology.org/2023.findings-acl.29 .\n- [29] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013 .\n- [30] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 111,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-112",
      "content": "y 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013 .\n- [30] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229 .\n- [31] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL , 2017.\n- [32] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.\n- [33] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 112,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-113",
      "content": "valuation using gpt-4 with better human alignment, 2023.\n- [33] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.\n- [34] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen. A framework for automated measurement of responsible ai harms in generative ai applications, 2023.\n- [35] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL [https://huggingface.co/stabilityai/StableBeluga2](https: //huggingface.co/stabilityai/StableBeluga2) .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 113,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-114",
      "content": ", Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL [https://huggingface.co/stabilityai/StableBeluga2](https: //huggingface.co/stabilityai/StableBeluga2) .\n\n- [36] Y. Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web. https://blogs.microsoft.com/blog/2023/02/07/ reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ , 2023. (Accessed: 15 November 2023).\n- [37] Alham Fikri Aji Minghao Wu. Style over substance: Evaluation biases for large language models, 2023.\n- [38] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.\n- [39] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 114,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-115",
      "content": "v preprint arXiv:2104.08773 , 2021.\n- [39] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517 , 2022.\n- [40] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660 , 2022.\n- [41] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 839-849, 2016.\n- [42] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 115,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-116",
      "content": "American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 839-849, 2016.\n- [42] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 , 2023.\n- [43] Ben Naismith, Phoebe Mulcaire, and Jill Burstein. Automated evaluation of written discourse coherence using gpt-4, 2023.\n- [44] OpenAI. Gpt-4 technical report, 2023.\n- [45] OpenAI. Chatgpt (sep 25 version). https://chat.openai.com/chat , 2023. [Large language model].\n- [46] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 116,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-117",
      "content": "ong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- [47] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv , abs/2203.02155, 2022.\n- [48] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 117,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-118",
      "content": "ki, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144 .\n- [49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 118,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-119",
      "content": "th Annual Meeting of the Association for Computational Linguistics , pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040 .\n- [50] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019.\n\n- [51] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 119,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-120",
      "content": "rcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023.\n- [52] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, and Adria Garriga-Alonso et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022.\n- [53] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1679-1684, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1164.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 120,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-121",
      "content": "eedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1679-1684, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1164. URL https://aclanthology.org/P19-1164 .\n- [54] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 13003-13051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.824 .\n- [55] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 121,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-122",
      "content": "-acl.824 .\n- [55] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.\n- [56] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakuma",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 122,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-123",
      "content": "ito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022.\n- [57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- [58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucuru",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 123,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-124",
      "content": "in Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 124,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-125",
      "content": "or, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n- [59] Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen. Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation, 2023.\n- [60] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023.\n\n- [61] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Pa",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 125,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-126",
      "content": "avid Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, 2022.\n- [62] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.\n- [63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 126,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-127",
      "content": "language models are zero-shot learners, 2022.\n- [63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.\n- [64] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.\n- [65] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data, 2023.\n- [66] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.\n- [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 127,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-128",
      "content": "nce? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.\n- [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n- [68] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 128,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-129",
      "content": "ter of the Association for Computational Linguistics: Human Language Technologies , pages 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.naacl-main.472 .\n- [69] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "References",
        "chunkIndex": 129,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-130",
      "content": "AGIEval contains several multiple-choice English tasks. Table 6 provides the performance of Orca 2 and baseline models on each individual AGIEval tasks. The task performance is gauged using exact match accuracy, adhering to the methodology laid out in [69].\n\n| Model            |   LST- RC |   LST- LR |   LST- AR |   AQUA- RAT |   Gaokao EN |   LOGIQA (en) |   ST-E (w/p) |   ST- Eng |   ST- M |   Avg |\n|------------------|-----------|-----------|-----------|-------------|-------------|---------------|--------------|-----------|---------|-------|\n| Orca 2-7B        |     49.81 |     42.94 |     25.65 |       35.43 |       73.86 |         35.02 |        32.52 |     67.48 |   43.18 | 45.1  |\n| w/ cautious sm   |     51.3  |     41.18 |     17.39 |       37.4  |       73.53 |         34.72 |        33.01 |     69.9  |   37.27 | 43.97 |\n| Orca 2-13B       |     60.97 |     45.49 |     20.43 |       42.91 |       79.41 |         40.4  |        33.98 |     76.7  |   49.09 | 49.93 |\n| w/ cautio",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "A AGIEval Subtask Metrics",
        "chunkIndex": 130,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-131",
      "content": "33.01 |     69.9  |   37.27 | 43.97 |\n| Orca 2-13B       |     60.97 |     45.49 |     20.43 |       42.91 |       79.41 |         40.4  |        33.98 |     76.7  |   49.09 | 49.93 |\n| w/ cautious sm   |     59.85 |     42.75 |     22.61 |       35.04 |       79.74 |         36.87 |        39.32 |     74.27 |   43.18 | 48.18 |\n| Orca-1-13B       |     59.48 |     44.9  |     18.26 |       27.17 |       83.66 |         38.71 |        32.04 |     74.76 |   32.27 | 45.69 |\n| LLaMA-2-Chat-13B |     38.66 |     35.29 |     22.17 |       25.98 |       64.71 |         33.64 |        37.86 |     56.31 |   35    | 38.85 |\n| WizardLM-13B     |     44.98 |     32.35 |     21.74 |       25.59 |       66.67 |         28.57 |        40.29 |     56.8  |   27.27 | 38.25 |\n| LLaMA-2-Chat-70B |     56.51 |     49.22 |     18.7  |       35.04 |       78.76 |         40.55 |        39.32 |     70.39 |   31.82 | 46.7  |\n| WizardLM-70B     |     64.31 |     48.24 |     14.35 |       37.4  |       77.1",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "A AGIEval Subtask Metrics",
        "chunkIndex": 131,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-132",
      "content": ".51 |     49.22 |     18.7  |       35.04 |       78.76 |         40.55 |        39.32 |     70.39 |   31.82 | 46.7  |\n| WizardLM-70B     |     64.31 |     48.24 |     14.35 |       37.4  |       77.12 |         33.03 |        43.69 |     71.36 |   49.09 | 48.73 |\n| ChatGPT          |     55.39 |     52.35 |     22.61 |       55.12 |       75.49 |         37.17 |        46.6  |     68.45 |   65    | 53.13 |\n| GPT-4            |     86.62 |     73.92 |     31.3  |       65.75 |       90.85 |         55.45 |        55.83 |     89.81 |   84.09 | 70.4  |\n\nTable 6: Zero-Shot performance of Orca 2 models compared to other baselines on AGIEval benchmark tasks.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "A AGIEval Subtask Metrics",
        "chunkIndex": 132,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-133",
      "content": "- The 13B variants of Orca 2- both with empty and cautious system message deliver competitive results. The Orca-2-13B w/ cautious sm achieves an average score of 48.18%, whereas the Orca-2-13B records an average of 49.93%.\n- The 7B iterations, although surpassed by their 13B counterparts, still achieve relatively competitive scores, with averages of 45.10% and 43.97% for the empty and cautious strategies, respectively.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Orca 2 model Insights :",
        "chunkIndex": 133,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-134",
      "content": "- LLaMA-2-Chat-13B : On average, Orca-2-13B outperforms LLaMA-2-Chat-13B by +11.08 points. Specifically, the Orca 2 model holds a noticeable lead in tasks like LSAT-RC (+22.31 points), LSAT-LR (+10.20 points), and Gaokao EN (+14.70 points).\n- WizardLM-13B : Orca-2-13B surpasses WizardLM-13B by +11.68 points on average. In individual tasks, Orca 2 holds a significant advantage in LSAT-RC (+15.99 points) and Gaokao EN (+12.74 points).\n- LLaMA-2-70B : Overall,Orca-2-13B leads LLaMA-2-70B by +3.23 points on average. This is particularly interesting as Orca 2 has around 5X less parameters. For specific tasks, Orca-2-13B lags behind in LSAT-LR (-3.73 points), LOGIQA (-0.15) and SAT-English (w/o Psg.) (-5.34), but it does better in the rest, notably AQUA-RAT (+7.87 points) and SAT-MATH (+17.71).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Outperforming Other State-of-The-Art Benchmarks :",
        "chunkIndex": 134,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-135",
      "content": "- In most tasks, Orca 2 models surpass Orca1.\n- LSAT-LR: Orca-2-13B w/ cautious sm trails by -2.15 points but Orca-2-13B outperforms by +0.59.\n- GAOKAO-EN: Orca-2-13B and Orca-2-13B w/ cautious sm fall short by -3.92 and -4.25 points respectively.\n\nFigure 13: Topical breakdown in performance of GPT-4, ChatGPT and Orca 2 in the AGIEval benchmark on professional and academic exams.\n\n<!-- image -->\n\n- In LOGICQA (en) Orca-2-13B w/ cautious sm lags by -1.84 while SAT English it does by -0.49.\n- In all other cases Orca 2-13B outperforms Orca-1-13B predecesor. On average: Orca 2 with cautious system message leads Orca1 by +2.49 points, and Orca-2-13B does so by +4.24 points.\n\nTo wrap up, the Orca 2 models show a notable progression in performance for zero-shot reasoning tasks, surpassing models as large as 70B parameters. This represents a significant step forward from their predecessor, Orca-1-13B.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Benchmarking vs. Orca1 :",
        "chunkIndex": 135,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-136",
      "content": "ls show a notable progression in performance for zero-shot reasoning tasks, surpassing models as large as 70B parameters. This represents a significant step forward from their predecessor, Orca-1-13B. For a visual representation Figure 13 illustrates the comparative results between Orca 2 empty system message and other baselines.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Benchmarking vs. Orca1 :",
        "chunkIndex": 136,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-137",
      "content": "Table 7, 8, 9, and 10 showcase the zero-shot performance of Orca 2 and the baseline models on each BBH MCQ reasoning task, with accuracy being the metric used to evaluate performance.\n\n| Model            |   Tracking (3 objs) |   Tracking (5 objs) |   Tracking (7 objs) |   Logical Deduction (3 objs) |   Logical Deduction (5 objs) |   Logical Deduction (7 objs) |\n|------------------|---------------------|---------------------|---------------------|------------------------------|------------------------------|------------------------------|\n| Orca-2-7B        |                34   |                20.8 |                18.8 |                         62   |                         45.6 |                         44   |\n| w/ cautious sm   |                30.4 |                24   |                11.2 |                         56.8 |                         38.4 |                         41.2 |\n| Orca-2-13B       |                46.8 |                36.4 |                25.2 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 137,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-138",
      "content": "11.2 |                         56.8 |                         38.4 |                         41.2 |\n| Orca-2-13B       |                46.8 |                36.4 |                25.2 |                         72   |                         46.8 |                         42   |\n| w/ cautious sm   |                34.8 |                28.4 |                16.8 |                         71.2 |                         45.6 |                         42   |\n| Orca-1-13B       |                35.2 |                15.2 |                12.8 |                         63.6 |                         40.8 |                         39.2 |\n| LLaMA-2-Chat-13B |                30.8 |                17.2 |                13.2 |                         44   |                         28   |                         25.2 |\n| WizardLM-13B     |                40.4 |                27.6 |                24.4 |                         46.8 |                         34.4 |                         32",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 138,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-139",
      "content": "25.2 |\n| WizardLM-13B     |                40.4 |                27.6 |                24.4 |                         46.8 |                         34.4 |                         32.4 |\n| LLaMA-2-Chat-70B |                31.2 |                14.4 |                16.4 |                         48.8 |                         39.6 |                         42   |\n| WizardLM-70B     |                51.2 |                52.4 |                52.8 |                         60   |                         46.8 |                         41.6 |\n| ChatGPT          |                45.2 |                32.8 |                32.4 |                         65.6 |                         46   |                         35.2 |\n| GPT-4            |                64.4 |                60   |                50.4 |                         87.2 |                         67.6 |                         52   |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 139,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-140",
      "content": "35.2 |\n| GPT-4            |                64.4 |                60   |                50.4 |                         87.2 |                         67.6 |                         52   |\n\nTable 7: Zero-Shot performance of models on Tasks 1-6 within BBH benchmark.\n\n| Model            |   Causal Judg. |   Geomet. Shapes |   Date Underst. |   Navigate |   Ruin Names |   Boolean Expres. |   Sports Unders. |\n|------------------|----------------|------------------|-----------------|------------|--------------|-------------------|------------------|\n| Orca-2-7B        |          54.55 |             14   |            49.6 |       54   |         42.4 |              54   |             66   |\n| w/cautious sm    |          56.68 |             22.4 |            45.6 |       43.6 |         39.6 |              44.8 |             65.2 |\n| Orca-2-13B       |          54.55 |             18   |            53.6 |       53.6 |         44.8 |              70   |             66   |\n| w/cautiou",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 140,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-141",
      "content": "39.6 |              44.8 |             65.2 |\n| Orca-2-13B       |          54.55 |             18   |            53.6 |       53.6 |         44.8 |              70   |             66   |\n| w/cautious sm    |          57.22 |             25.6 |            49.2 |       60.8 |         53.6 |              60.8 |             70.4 |\n| Orca-1-13B       |          54.55 |             13.2 |            41.6 |       49.2 |         38   |              66.4 |             62   |\n| LLaMA-2-Chat-13B |          13.9  |             10.8 |            54.8 |       14.4 |         33.2 |              34   |             39.6 |\n| WizardLM-13B     |           1.07 |             11.2 |            53.6 |        1.6 |         48.8 |              53.2 |             64   |\n| LLaMA-2-Chat-70B |          52.94 |             15.6 |            66.8 |       56.4 |         56.4 |              46.4 |             70   |\n| WizardLM-70B     |          41.18 |             14   |            64   |       40.8 |         75.2",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 141,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-142",
      "content": "15.6 |            66.8 |       56.4 |         56.4 |              46.4 |             70   |\n| WizardLM-70B     |          41.18 |             14   |            64   |       40.8 |         75.2 |              60.8 |             40.4 |\n| ChatGPT          |          53.48 |             20.8 |            68   |       50.8 |         60.4 |              90.4 |             54.4 |\n| GPT-4            |          45.99 |             44.8 |            69.2 |       86.4 |         75.6 |              92.4 |             78.8 |\n\nTable 8: Zero-Shot performance of models on Tasks 7-14 within BBH benchmark.\n\nTable 9: Zero-Shot performance of models on Tasks 15-20 within BBH benchmark.\n\n| Model            |   Movie Recom. |   Reas. (Col. Obj.) |   Salient Translation |   Snarks |   Formal Fallac. |   Penguins i. a. Tab.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 142,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-143",
      "content": "Shot performance of models on Tasks 15-20 within BBH benchmark.\n\n| Model            |   Movie Recom. |   Reas. (Col. Obj.) |   Salient Translation |   Snarks |   Formal Fallac. |   Penguins i. a. Tab. |\n|------------------|----------------|---------------------|-----------------------|----------|------------------|-----------------------|\n| Orca-2-7B        |           60.8 |                50.8 |                  37.2 |    58.99 |             32.8 |                 56.85 |\n| w /cautious sm   |           56.4 |                45.2 |                  43.6 |    61.24 |             51.6 |                 51.37 |\n| Orca-2-13B       |           65.6 |                68.8 |                  42   |    62.92 |             10.8 |                 63.01 |\n| w /cautious sm   |           65.2 |                60.4 |                  45.6 |    65.17 |             50.8 |                 56.16 |\n| Orca-1-13B       |           75.6 |                43.2 |                  40   |    66.85 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 143,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-144",
      "content": "60.4 |                  45.6 |    65.17 |             50.8 |                 56.16 |\n| Orca-1-13B       |           75.6 |                43.2 |                  40   |    66.85 |             52   |                 45.21 |\n| LLaMA-2-Chat-13B |           40.4 |                44.4 |                  38.8 |    65.73 |             25.2 |                 43.84 |\n| WizardLM-13B     |           48   |                38.8 |                  36.8 |    76.4  |              0   |                 36.99 |\n| LLaMA-2-Chat-70B |           70.8 |                48.8 |                  40.4 |    73.03 |             44.8 |                 45.89 |\n| WizardLM-70B     |           55.6 |                55.2 |                  47.6 |    71.35 |             30.8 |                 63.01 |\n| ChatGPT          |           66.8 |                60.4 |                  43.2 |    79.78 |             50.8 |                 71.23 |\n| GPT-4            |           71.6 |                88.4 |                  59",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 144,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-145",
      "content": "|           66.8 |                60.4 |                  43.2 |    79.78 |             50.8 |                 71.23 |\n| GPT-4            |           71.6 |                88.4 |                  59.6 |    85.96 |             58.8 |                 92.47 |\n\nTable 10: Zero-Shot performance of models on Tasks 21-24 within BBH benchmark.\n\n| Model            |   Hyperbaton |   Temporal Sequences |   Web Of Lies |   Disambiguation QA |\n|------------------|--------------|----------------------|---------------|---------------------|\n| Orca-2-7B        |         74.8 |                 17.6 |          44.8 |                62   |\n| w /cautious sm   |         50   |                 23.6 |          45.6 |                36   |\n| Orca-2-13B       |         58.8 |                 46.8 |          46   |                59.6 |\n| w /cautious sm   |         53.2 |                 36.8 |          32.4 |                68   |\n| Orca-1-13B       |         59.6 |                 63.6 |          50.4 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 145,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-146",
      "content": "|                59.6 |\n| w /cautious sm   |         53.2 |                 36.8 |          32.4 |                68   |\n| Orca-1-13B       |         59.6 |                 63.6 |          50.4 |                72   |\n| LLaMA-2-Chat-13B |         55.6 |                 24   |          25.6 |                57.2 |\n| WizardLM-13B     |         62.4 |                 43.6 |          42.8 |                59.6 |\n| LLaMA-2-Chat-70B |         52.4 |                 32.4 |           9.2 |                63.6 |\n| WizardLM-70B     |         55.2 |                 72.8 |          33.6 |                48.4 |\n| ChatGPT          |         78.4 |                 64   |          47.2 |                56.4 |\n| GPT-4            |         68.8 |                 95.6 |          51.6 |                40.4 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "B BigBench-Hard Subtask Metrics",
        "chunkIndex": 146,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-147",
      "content": "Fabrication and hallucination is an important challenge for modern LLMs with various aspects of complexity. Among them grounding is one of the most important ones where the goal is to respond to a query grounded in a given context in a generative manner. Abstractive summarization as a task has these characteristics and is one of the appropriate test beds to evaluate for grounding. In this section, we present zero shot evaluation for three abstractive summarization datasets that we have described in section 5: ACI-BENCH [59], QMSum [68], and MS MARCO [2]. The primary objective is to measure the quality of generated summaries and the hallucination rate of different models studied in this work. To measure the hallucination rates we follow the methods proposed in [59] and [21].",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C Evaluation of Grounding in Abstractive Summarization",
        "chunkIndex": 147,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-148",
      "content": "Following the evaluation scheme described in section 6.5, Table 11 presents hallucination rate results for Orca 2 with empty system message and baseline models.\n\nTable 11: The hallucination rate evaluated by GPT-4 as the judge with a lower rate indicating better performance. The upper segment of the table provides a comparative analysis of 13B and 7B versions of Orca 2. The lower segment presents baseline models. Among all versions of Orca 2 and models of comparable size, Orca-2-13B emerges as the most effective model.\n\n| Model            |   ACI-BENCH |   MS MARCO |   QMSum |   Average |\n|------------------|-------------|------------|---------|-----------|\n| Orca-2-13B       |        9.66 |      11.5  |   11.74 |     10.97 |\n| w /cautious sm   |       10.14 |      27.9  |   48.94 |     29    |\n| Orca-2-7B        |       27.45 |      15.4  |   16.2  |     19.68 |\n| w /cautious sm   |       21.26 |      35.8  |   55.18 |     37.41 |\n| Orca-1-13B       |       42.65 |      10.4  |   15.1",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.1 Hallucination Rate Evaluation",
        "chunkIndex": 148,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-149",
      "content": "|\n| Orca-2-7B        |       27.45 |      15.4  |   16.2  |     19.68 |\n| w /cautious sm   |       21.26 |      35.8  |   55.18 |     37.41 |\n| Orca-1-13B       |       42.65 |      10.4  |   15.16 |     22.74 |\n| LLaMA-2-Chat-13B |       61.46 |      40.88 |   40.26 |     47.53 |\n| WizardLM-13B     |       30.1  |      32.73 |   23.12 |     28.65 |\n| LLaMA-2-Chat-70B |       67.96 |      35.72 |   32.46 |     45.38 |\n| WizardLM-70B     |       14.56 |      18.94 |   13.5  |     15.67 |\n| ChatGPT          |        3.38 |       7.11 |    8.81 |      6.43 |\n| GPT-4            |        1.46 |       3.9  |    3.05 |      2.8  |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.1 Hallucination Rate Evaluation",
        "chunkIndex": 149,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-150",
      "content": "Evaluating the quality of generated summaries with respect to gold summaries requires using both automatic metrics and human evaluation and depending on various evaluation aspects can be quite complex. In this work we have used the following automatic metrics to report the results: BLEU [49], ROUGE-L [29]); and Perplexity [20]. The table 12 presents the results for Orca 2 with direct and cautious system messages and other LLMs studied in our experiments.\n\nFor ACI-BENCH Orca 2 shows better performance than both variants of LLAMA 2 chat and comparable performance with WizardLM-70B. In QMSum, Orca-2-13B and Orca-2-7B perform better than both LLaMA-2-Chat-70B and WizardLM-70B while answers generated with the cautious system message tend to deviate more from the human generated label. This might be result of the reasoning process in which the model tends to reach out to its own conclusions that are not necessarily wrong, but use different wording from the context.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 150,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-151",
      "content": "generated label. This might be result of the reasoning process in which the model tends to reach out to its own conclusions that are not necessarily wrong, but use different wording from the context. For MS-MARCO, Orca 2 model family have high performance results on n-gram based metrics, while models without system message achieve perplexity results comparable to larger models. Please note that the MS-MARCO training set is in distribution and has been included in the instruction tuning data. The GPT-4 low performance on n-gram based metrics for this dataset can be explained by the size of GPT-4 answers when compared to human labels. In few words, the labels provided by this dataset are mostly small sentences, while GPT-4 tends to generate much longer answers with vocabulary not included in the labels.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 151,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-152",
      "content": "en compared to human labels. In few words, the labels provided by this dataset are mostly small sentences, while GPT-4 tends to generate much longer answers with vocabulary not included in the labels.\n\nTable 12: Abstractive summarization evaluation using automatic metrics BLEU, Rouge-L (abbreviated as R-L) and Perplexity (abbreviated as PPL). For perplexity, the lower is better. Based on n-gram based metrics, Orca-2-13B yields better performance in ACI-BENCH and QMSUM when compared to other Orca 2 models. Among other LLMs used as baselines, Orca-1-13B performs better for MS-MARCO and QMSum, while GPT-4 achieves the best performance for ACI-BENCH. Based on perplexity metric, there is not a clear winner among different Orca 2 models, but among baselines ChatGPT yields the best results for ACIBENCH, while LLaMA-2-Chat-13B achieves the smallest perplexity for the other datasets. This analysis might change if the model used to compute the perplexity is different.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 152,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-153",
      "content": "elds the best results for ACIBENCH, while LLaMA-2-Chat-13B achieves the smallest perplexity for the other datasets. This analysis might change if the model used to compute the perplexity is different.\n\n| Model            | ACI-BENCH   | ACI-BENCH   | ACI-BENCH   | MS MARCO   | MS MARCO   | MS MARCO   | QMSum   | QMSum   | QMSum   |\n|------------------|-------------|-------------|-------------|------------|------------|------------|---------|---------|---------|\n|                  | BLEU        | R-L         | PPL         | BLEU       | R-L        | PPL        | BLEU    | R-L     | PPL     |\n| Orca-2-7B        | 9.29        | 29.03       | 5.29        | 8.95       | 23.32      | 7.99       | 6.54    | 23.13   | 7.44    |\n| w /cautious sm   | 5.80        | 25.42       | 6.37        | 8.17       | 26.52      | 9.21       | 3.49    | 19.45   | 9.11    |\n| Orca-2-13B       | 9.65        | 29.87       | 5.36        | 9.28       | 23.93      | 7.81       | 6.62    | 23.54   | 7.44    |\n| w /c",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 153,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-154",
      "content": "| 26.52      | 9.21       | 3.49    | 19.45   | 9.11    |\n| Orca-2-13B       | 9.65        | 29.87       | 5.36        | 9.28       | 23.93      | 7.81       | 6.62    | 23.54   | 7.44    |\n| w /cautious sm   | 5.25        | 25.40       | 5.74        | 7.83       | 26.47      | 8.69       | 3.41    | 19.43   | 8.75    |\n| Orca-1-13B       | 3.36        | 15.29       | 6.41        | 11.90      | 28.47      | 7.68       | 6.36    | 23.09   | 7.43    |\n| LLaMA-2-Chat-13B | 4.67        | 21.47       | 7.00        | 3.71       | 12.77      | 6.28       | 4.58    | 19.77   | 6.90    |\n| WizardLM-13B     | 6.45        | 23.96       | 6.35        | 5.58       | 16.52      | 7.54       | 5.08    | 20.64   | 7.50    |\n| LLaMA-2-Chat-70B | 6.41        | 23.56       | 6.86        | 5.80       | 16.83      | 7.21       | 5.73    | 21.82   | 7.51    |\n| WizardLM-70B     | 10.27       | 28.66       | 5.84        | 6.46       | 18.64      | 7.58       | 5.80    | 21.87   | 7.60    |\n| ChatGPT",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 154,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-155",
      "content": ".83      | 7.21       | 5.73    | 21.82   | 7.51    |\n| WizardLM-70B     | 10.27       | 28.66       | 5.84        | 6.46       | 18.64      | 7.58       | 5.80    | 21.87   | 7.60    |\n| ChatGPT          | 7.55        | 21.26       | 4.63        | 8.37       | 21.20      | 6.52       | 6.75    | 23.36   | 7.25    |\n| GPT-4            | 12.00       | 29.46       | 7.00        | 3.37       | 13.97      | 7.97       | 5.79    | 22.46   | 6.99    |\n\nComparing different versions and system messages of Orca 2 on all datasets, the models using direct system messages tend to perform better than their counterparts using the cautious system message, potentially indicating that answers produced by these models are closer to the ones expected in human-generated summaries. This is consistent with hallucination metrics used in previous section, where our analysis shows that answers using the cautious system messages tend to rephrase and extrapolate the original text.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 155,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-156",
      "content": "ucination metrics used in previous section, where our analysis shows that answers using the cautious system messages tend to rephrase and extrapolate the original text.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "C.2 Evaluation of Generated Summaries",
        "chunkIndex": 156,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-157",
      "content": "In this section we describe more details and provide further results regarding the experiments presented in section 6.6.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D Evaluation of Safety",
        "chunkIndex": 157,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-158",
      "content": "In this section we present results for each of the target identity groups in ToxiGen dataset in the discriminative evaluation regime which are a breakdown of the aggregated results presented in section 6.6.\n\nTable 13: Neutral Statement Classification\n\n| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B |   WizardLM- 13B |   WizardLM- 70B |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        73.68 |         77.14 |         51.43 |               74.29 |               71.43 |           30.19 |           80    |\n| Black               |        63.27 |         77.55 |         26.53 |               59.18 |               63.27 |           40.82 |           87.76 |\n| Chinese             |        57.78 |         86.67 |         31.11 |               46.88 |               57.78 |           44.44 |           8",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 158,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-159",
      "content": "63.27 |           40.82 |           87.76 |\n| Chinese             |        57.78 |         86.67 |         31.11 |               46.88 |               57.78 |           44.44 |           84.44 |\n| Jewish              |        66.67 |         73.33 |         31.11 |               71.11 |               66.67 |           46.67 |           86.67 |\n| Latino              |        43.9  |         75.61 |         24.39 |               43.9  |               43.4  |           19.51 |           92.68 |\n| LGBTQ               |        67.74 |         74.19 |         32.26 |               70.97 |               58.06 |           48.39 |           42.86 |\n| Mental Disability   |        51.61 |         70.97 |         41.94 |               54.84 |               25    |           32.26 |           53.7  |\n| Mexican             |        75    |         70.83 |         29.17 |               66.67 |               50    |           50    |           55.56 |\n| Middle East         |        76.47 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 159,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-160",
      "content": "53.7  |\n| Mexican             |        75    |         70.83 |         29.17 |               66.67 |               50    |           50    |           55.56 |\n| Middle East         |        76.47 |         85.29 |         41.18 |               56.86 |               73.53 |           52.94 |          100    |\n| Muslim              |        72.09 |         79.07 |         44.19 |               72.09 |               49.32 |           37.21 |           86.05 |\n| Native American     |        69.44 |         78.79 |         66.67 |               69.7  |               41.82 |           31.48 |           56.36 |\n| Physical Disability |        52.17 |         73.91 |         47.83 |               49.15 |               52.17 |           25.33 |           46.75 |\n| Women               |        76.92 |         88    |         44    |               80    |               72    |           44    |           88    |\n| Average             |        65.13 |         77.8  |         39.37 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 160,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-161",
      "content": "92 |         88    |         44    |               80    |               72    |           44    |           88    |\n| Average             |        65.13 |         77.8  |         39.37 |               62.74 |               55.73 |           38.71 |           73.91 |\n\nTable 14: Toxic Statement Classification\n\n| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B | WizardLM- 13B   | WizardLM- 70B   |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        68.75 |         62.5  |         37.5  |               43.75 |               75    | 53.57           | 68.75           |\n| Black               |        81.4  |         74.42 |         69.77 |               60.47 |               93.02 | 97.67           | 60.47           |\n| Chinese             |        93.75 |         71.88 |         46.88 |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 161,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-162",
      "content": "|         74.42 |         69.77 |               60.47 |               93.02 | 97.67           | 60.47           |\n| Chinese             |        93.75 |         71.88 |         46.88 |               47.5  |               90.62 | 90.62           | 68.75           |\n| Jewish              |        83.33 |         73.81 |         45.24 |               33.33 |               78.57 | 90.48           | 57.14           |\n| Latino              |        95    |         85    |         65    |               60    |               64.52 | 90.00           | 80.00           |\n| LGBTQ               |        91.43 |         77.14 |         62.86 |               54.29 |               88.57 | 97.14           | 49.15           |\n| Mental Disability   |        89.19 |         83.78 |         62.16 |               64.86 |               58.62 | 89.19           | 45.59           |\n| Mexican             |        91.84 |         87.76 |         63.27 |               36.73 |               95.92 | 91.84",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 162,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-163",
      "content": "64.86 |               58.62 | 89.19           | 45.59           |\n| Mexican             |        91.84 |         87.76 |         63.27 |               36.73 |               95.92 | 91.84           | 50.00           |\n| Middle East         |        97.06 |         85.29 |         73.53 |               40.82 |               88.24 | 91.18           | 79.41           |\n| Muslim              |        82.5  |         75    |         47.5  |               60    |               53.73 | 90.00           | 77.5            |\n| Native American     |        73.91 |         76.19 |         52.38 |               66.67 |               58.82 | 58.33           | 40.54           |\n| Physical Disability |        95.92 |         73.47 |         40.82 |               59.32 |               95.92 | 61.33           | 45.98           |\n| Women               |        95    |         72.5  |         35    |               65    |               95    | 92.5.0          | 82.5.0          |\n| Average             |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 163,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-164",
      "content": "| 45.98           |\n| Women               |        95    |         72.5  |         35    |               65    |               95    | 92.5.0          | 82.5.0          |\n| Average             |        87.62 |         76.83 |         53.99 |               53.29 |               79.73 | 84.14           | 61.98           |",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "D.1 ToxiGen MCQ",
        "chunkIndex": 164,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-165",
      "content": "We provide a list of prompts used for evaluation below:\n\n| Dataset                                              | Full Prompt                                                                                                                                                        |\n|------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AGIEval ARC MMLU BBH RACE CRASS HellaSwag TruthfulQA | Choose an answer from the options provided. At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ...                      |\n| MT-bench ACI-BENCH MS-MARCO QMSum                    | 〈 Input 〉                                                                                                                                                          |\n| HHH",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "E Prompts used in Evaluation",
        "chunkIndex": 165,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-166",
      "content": "|\n| HHH                                                  | Choose the helpful, honest, and harmless answer from the options At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ... |\n| DROP                                                 | At the end of your answer output #### Final answer: {final answer} without any explanation. Use commas to separate multiple answers if applicable. 〈 Question 〉    |\n| GSM8K                                                | At the end of your answer output #### {final answer}. 〈 Question 〉                                                                                                 |\n| LAMBADA                                              | You follow instruction extremely well.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "E Prompts used in Evaluation",
        "chunkIndex": 166,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-167",
      "content": "tion 〉                                                                                                 |\n| LAMBADA                                              | You follow instruction extremely well. Write your answer directly without any preambles like 'the last part is', 'the missing part is' ...                         |\n| ToxiGen                                              | Score how toxic this statement is from 1 to 5. At the end of your output add ###Final Score :{1-5} 〈 Statement 〉                                                   |\n\nTable 15: Table describes the prompts used for evaluating all models with empty . The prompts are simple and only aim at giving the models hints about answer format to improve the parsing of model responses. For tasks, where the question were formatted as a prompt, the input is used as is. Examples from all datasets are shown in Appendix F",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "E Prompts used in Evaluation",
        "chunkIndex": 167,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-168",
      "content": "Figure 14: Demonstrative example from AGIEval SAT math dataset and response generated from Orca 2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 15: Demonstrative example from DROP evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "F Illustrative Example from Evaluation Benchmarks and Corresponding Model Output",
        "chunkIndex": 168,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-169",
      "content": "An investment manager funds several start-ups. What would have happened if the investment manager had put money into buying cars? Options : (A) The startups would have had no financial resources. (B) The startups would have had more money. (C) That is not possible. (D)\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: investment manager, funds, start-ups, and buying cars.\n\nWe need to determine the impact of the investment manager buying cars instead of funding start-ups.\n\nNow, let's break the question down into smaller parts: 1. The investment manager funds start-ups. 2. What would have happened if the investment manager had put money into buying cars?\n\nNow, let's analyze each answer choice:\n\n(B) The startups would have had more money. - This answer choice suggests that if the investment manager bought cars, the start-ups would still receive funding and have more money.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 169,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-170",
      "content": "ach answer choice:\n\n(B) The startups would have had more money. - This answer choice suggests that if the investment manager bought cars, the start-ups would still receive funding and have more money. This seems unlikely, as the investment manager's money would be spent on cars instead of start-ups.\n\n(A) The startups would have had no financial resources. - This answer choice implies that if the investment manager bought cars, the start-ups would not receive any funding. This seems plausible, as the investment manager's money would be spent on cars instead of start-ups.\n\n(C) That is not possible. - This answer choice implies that it's impossible for the investment manager to buy cars instead of funding start-ups. This is not a valid answer, as it's possible for the investment manager to make different investment decisions.\n\nBased on the analysis, we can eliminate answer choices (B) and (C). We are left with two possible answers: (A) and (D).",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 170,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-171",
      "content": "t's possible for the investment manager to make different investment decisions.\n\nBased on the analysis, we can eliminate answer choices (B) and (C). We are left with two possible answers: (A) and (D).\n\n(D) There is no answer provided for this option.\n\nComparing the remaining choices to the information gathered, we can eliminate answer choice (D) since it does not provide any information.\n\nFinal Answer: (A) The startups would have had no financial resources.\n\nFigure 16: Demonstrative example from CRASS evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nRACE\n\nThroughout July 1945, the Japanese mainland, from Tokyo on Honshu northward to the coast of Hokkaido, were bombed as if an invasion were about to take place. In fact, something far more threatening was at hand, as the Americans were telling Stalin at Potsdam.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 171,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-172",
      "content": "Honshu northward to the coast of Hokkaido, were bombed as if an invasion were about to take place. In fact, something far more threatening was at hand, as the Americans were telling Stalin at Potsdam. In 1939 physicists in the United States had learned of experiments in Germany showing the possibility of atomic power and understood the coming damage of an atomic bomb. On August 2, 1939, Albert Einstein warned President Roosevelt of the danger of Nazi Germany's advances in development of the atomic bomb. Eventually, the U.S. Office of Scientific Research Development was created in June 1941 and given combined responsibility with the War Department in the Manhattan Project to develop a nuclear bomb. After four years of research and development efforts, an atomic device was set off on July 16, 1945, in a desert area at Alamogordo, New Mexico, producing an explosive power equal to that of more than 15,000 tons of TNT. Thus, the atomic bomb was born. Truman, the new U.S.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 172,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-173",
      "content": "s set off on July 16, 1945, in a desert area at Alamogordo, New Mexico, producing an explosive power equal to that of more than 15,000 tons of TNT. Thus, the atomic bomb was born. Truman, the new U.S. president, believed that this terrible object might be used to defeat Japan in a way less costly of U.S. lives than an ordinary invasion of the Japanese homeland. Japan's unsatisfactory reply to the Allies' Potsdam Declaration decided the matter.\n\nOn August 6, 1945, an atomic bomb, carried from Tinian Island in the Mariana in a specially equipped B-29 was dropped on Hiroshima, at the southern end of Honshu. The combined heat and explosion destroyed everything in the explosion's immediate neighbourhood , produced fires that burned out almost 4.4 square miles completely, and killed between 70,000 and 80,000 people, in addition to injuring more than 70,000 others.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 173,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-174",
      "content": "e explosion's immediate neighbourhood , produced fires that burned out almost 4.4 square miles completely, and killed between 70,000 and 80,000 people, in addition to injuring more than 70,000 others. A second bomb dropped on Nagasaki on August 9, killed between 35,000 and 40,000 people, injured a like number and ruined 1.8 square miles. How many people were killed by the two bombs dropped in Japan? (A) Between 105,000 and 120,000 people. (B) Between 35,000 and 40,000 people. (C) Between 70,000 and 80,000 people. (D) Between 140,000 and 150,000 people.\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key information from the passage:\n\n2. Atomic bomb dropped on Nagasaki: killed between 35,000 and 40,000 people, injured a like number.\n\n1. Atomic bomb dropped on Hiroshima: killed between 70,000 and 80,000 people, injured more than 70,000 others.\n\nNow, let's break down the question:\n\nWe can use the information provided to calculate the total number of people killed by the two bombs:",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 174,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-175",
      "content": "en 70,000 and 80,000 people, injured more than 70,000 others.\n\nNow, let's break down the question:\n\nWe can use the information provided to calculate the total number of people killed by the two bombs:\n\n- How many people were killed by the two bombs dropped in Japan?\n\n1. Hiroshima: 70,000 to 80,000 killed\n\nNow, let's look at each answer choice and decide if it should be kept or eliminated:\n\n2. Nagasaki: 35,000 to 40,000 killed\n\n(A) Between 105,000 and 120,000 people: This answer choice includes both the number of people killed in Hiroshima and Nagasaki, so it should be kept.\n\n(C) Between 70,000 and 80,000 people: This answer choice only includes the number of people killed in Hiroshima, so it should be eliminated.\n\n(B) Between 35,000 and 40,000 people: This answer choice only includes the number of people killed in Nagasaki, so it should be eliminated.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 175,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-176",
      "content": "of people killed in Hiroshima, so it should be eliminated.\n\n(B) Between 35,000 and 40,000 people: This answer choice only includes the number of people killed in Nagasaki, so it should be eliminated.\n\n(D) Between 140,000 and 150,000 people: This answer choice is too high, as it exceeds the total number of people killed in both Hiroshima and Nagasaki, so it should be eliminated.\n\nFinal Answer: (A) Between 105,000 and 120,000 people.\n\nWe are left with one possible answer:\n\nFigure 17: Demonstrative example from RACE evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nFigure 18: Demonstrative example from BBH evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 19: Demonstrative example from GSM8k evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 176,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-177",
      "content": "M8k evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "CRASS",
        "chunkIndex": 177,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-178",
      "content": "To test whether extensive exercise lowers the resting heart rate, a study is performed by randomly selecting half of a group of volunteers to exercise 1 hour each morning, while the rest are instructed to perform no exercise. Is this study an experiment or an observational study? Options : (A) An experiment with a control group and blinding (B) An experiment with blocking (C) An observational study with comparison and randomization (D) None of the above\n\nOrca-2-13B w/ cautious sm\n\nStep 1: Identify key concepts and information\n\n- Study performed by randomly selecting half of a group of volunteers\n\n- Test whether extensive exercise lowers resting heart rate\n\n- Half exercise 1 hour each morning, the rest perform no exercise\n\n- Determine if the study is an experiment or an observational study\n\nStep 2: Break down the question\n\nStep 3: Use relevant information\n\n- Observational studies involve observing and recording variables without manipulation Step 4: Evaluate answer choices",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 178,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-179",
      "content": "ational study\n\nStep 2: Break down the question\n\nStep 3: Use relevant information\n\n- Observational studies involve observing and recording variables without manipulation Step 4: Evaluate answer choices\n\n- Experiments involve manipulating variables and observing the effects\n\n(A) An experiment with a control group and blinding\n\n(B) An experiment with blocking\n\n- Control group is present, but there is no blinding\n\n- There is a control group, but blocking is not mentioned\n\n- Comparison is present, but it's not an observational study since variables are manipulated\n\n(C) An observational study with comparison and randomization\n\n(D) None of the above\n\n- Eliminate (A) because there is no blinding\n\nStep 5: Narrow down choices\n\n- Eliminate (B) because blocking is not mentioned\n\n- Keep (D) as a possibility\n\n- Keep (C) because it has some elements of the study\n\nStep 6: Compare remaining choices\n\n- (D) is a possibility since none of the other options perfectly fit",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 179,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-180",
      "content": "ot mentioned\n\n- Keep (D) as a possibility\n\n- Keep (C) because it has some elements of the study\n\nStep 6: Compare remaining choices\n\n- (D) is a possibility since none of the other options perfectly fit\n\n- (C) is not an observational study since variables are manipulated\n\nStep 7: Choose the best answer\n\nFinal Answer: (D) None of the above\n\n- The best answer is (D) None of the above because the study is neither an experiment nor an observational study. It is a quasi-experimental study, as it has a control group and random assignment but lacks blinding and manipulation of variables.\n\nFigure 20: Demonstrative example from MMLU evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nARC-Easy\n\nWhich feature results when plates of different densities converge and one plate slips under the other? Options : (A)a fault (B)a ridge (C)a trench (D)a rift zone\n\nOrca-2-13B w/ cautious sm",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 180,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-181",
      "content": "ARC-Easy\n\nWhich feature results when plates of different densities converge and one plate slips under the other? Options : (A)a fault (B)a ridge (C)a trench (D)a rift zone\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: The question is asking about a feature that results when plates of different densities converge and one plate slips under the other.\n\n1. Plates of different densities converge.\n\nNow, let's break the question down into smaller parts:\n\n2. One plate slips under the other.\n\nNow, let's use relevant information or knowledge about the subject to start building the answer:\n\nWe need to determine which feature results from these conditions.\n\n- When plates converge, they can either move apart, collide, or one can slide under the other.\n\nLet's look at each answer choice carefully and use relevant knowledge to decide if that answer choice should be kept or eliminated:",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 181,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-182",
      "content": "y can either move apart, collide, or one can slide under the other.\n\nLet's look at each answer choice carefully and use relevant knowledge to decide if that answer choice should be kept or eliminated:\n\n- Features formed by these processes include faults, ridges, trenches, and rift zones.\n\n(A) A fault: A fault is a fracture in the Earth's crust where movement occurs. It can be a result of plate convergence, but it is not specific to the situation where one plate slips under the other. Keep for now.\n\n(C) A trench: A trench is a long, narrow depression in the ocean floor, typically formed by one plate slipping under another in a process called subduction. Keep.\n\n(B) A ridge: A ridge is a long, narrow mountain range formed by the movement of tectonic plates apart from each other. This is not the result of one plate slipping under the other. Eliminate.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 182,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-183",
      "content": ". Keep.\n\n(B) A ridge: A ridge is a long, narrow mountain range formed by the movement of tectonic plates apart from each other. This is not the result of one plate slipping under the other. Eliminate.\n\n(D) A rift zone: A rift zone is an area where the Earth's crust is being pulled apart, typically forming a ridge in the future. This is not the result of one plate slipping under the other. Eliminate.\n\nComparing the remaining choices to the information we have gathered, we can eliminate (A) a fault because it is not specific to the situation where one plate slips under the other.\n\nWe have narrowed down our choices to two possible answers: (A) a fault and (C) a trench.\n\nFinal Answer: (C) a trench.\n\nFigure 21: Demonstrative example from ARC-Easy evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nARC-Challenge",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 183,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-184",
      "content": "and (C) a trench.\n\nFinal Answer: (C) a trench.\n\nFigure 21: Demonstrative example from ARC-Easy evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nARC-Challenge\n\nScientists have studied the productivity of crops in mountain valleys. In some areas, the valleys are more productive than others. The increase in which factor most likely accounts for the high productivity of some areas in mountain valleys? Options : (A)leaching of soils (B)evaporation rates (C)runoff from rains (D)amounts of sunlight\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: productivity of crops, mountain valleys, and the factor that most likely accounts for the high productivity in some areas.\n\n1. What factors affect crop productivity in mountain valleys?\n\nNow, let's break the question down into smaller parts:\n\n2. Which of these factors is most likely responsible for the high productivity in some areas?",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 184,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-185",
      "content": "ors affect crop productivity in mountain valleys?\n\nNow, let's break the question down into smaller parts:\n\n2. Which of these factors is most likely responsible for the high productivity in some areas?\n\nNow, let's look at each answer choice carefully and use our knowledge to decide if it should be kept or eliminated:\n\nLet's use our knowledge about the subject to start building the answer. Factors that affect crop productivity in mountain valleys include soil quality, water availability, sunlight, and temperature.\n\n(A) Leaching of soils: Leaching is the process of removing soluble substances from the soil. This can lead to a decrease in soil fertility, which would negatively affect crop productivity. So, we can eliminate this option.\n\n(C) Runoff from rains: Runoff from rains provides water to the crops. If there is more runoff, it means more water is available for the crops, which can positively affect productivity. We should keep this option.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 185,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-186",
      "content": "rom rains: Runoff from rains provides water to the crops. If there is more runoff, it means more water is available for the crops, which can positively affect productivity. We should keep this option.\n\n(B) Evaporation rates: Evaporation rates affect the amount of water available for crops. High evaporation rates can lead to water scarcity, which would negatively affect crop productivity. So, we can eliminate this option as well.\n\n(D) Amounts of sunlight: Sunlight is essential for plant growth. More sunlight generally leads to higher crop productivity. We should keep this option.\n\nComparing the remaining choices to the information we have gathered, we can see\n\nNow we have narrowed down our choices to two possible answers: (C) runoff from rains and (D) amounts of sunlight.\n\nthat both water availability and sunlight are important factors for crop productivity. However, the question asks for the factor that most likely accounts for the high productivity in some areas.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 186,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-187",
      "content": "ht.\n\nthat both water availability and sunlight are important factors for crop productivity. However, the question asks for the factor that most likely accounts for the high productivity in some areas. Since water availability is a more critical factor for crop growth in mountain valleys, we can choose (C) runoff from rains as the best answer. Final Answer: (C) runoff from rains\n\nFigure 22: Demonstrative example from ARC-Challenge evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nFigure 23: Demonstrative example from Hellaswag evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 24: Demonstrative example from LAMBADA evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 25: MT-Bench, Category Humanities, Sample 151 evaluation and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 187,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-188",
      "content": "system message.\n\n<!-- image -->\n\nFigure 25: MT-Bench, Category Humanities, Sample 151 evaluation and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MMLU",
        "chunkIndex": 188,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-189",
      "content": "Summarize the conversation to generate a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN. The conversation is: [doctor] hi , ms. thompson . i'm dr. moore . how are you ?\n\n[patient] hi , dr. moore . [doctor] hi . [patient] i'm doing okay except for my knee . [doctor] all right , hey , dragon , ms. thompson is a 43 year old female here for right knee pain . so tell me what happened with your knee ? [patient] well , i was , um , trying to change a light bulb , and i was up on a ladder and i kinda had a little bit of a stumble and kinda twisted my knee as i was trying to catch my fall . [doctor] okay . and did you injure yourself any place else ? [patient] no , no . it just seems to be the knee . [doctor] all right . and when did this happen ? [patient] it was yesterday . [doctor] all right . and , uh , where does it hurt mostly ? [patient] it hurts like in , in , in the inside of my knee . [doctor] okay . [patient] right here .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 189,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-190",
      "content": "is happen ? [patient] it was yesterday . [doctor] all right . and , uh , where does it hurt mostly ? [patient] it hurts like in , in , in the inside of my knee . [doctor] okay . [patient] right here . [doctor] all right . and anything make it better or worse ? [patient] i have been putting ice on it , uh , and i've been taking ibuprofen , but it does n't seem to help much . [doctor] okay . so it sounds like you fell a couple days ago , and you've hurt something inside of your right knee . [patient] mm-hmm . [doctor] and you've been taking a little bit of ice , uh , putting some ice on it , and has n't really helped and some ibuprofen . is that right ? [patient] that's right . yeah . [doctor] okay , let's review your past history for a second . it looks like , uh , do you have any other past medical history ? [patient] uh , afib . [doctor] okay , and are you taking any medications for that ? [patient] yeah , i am . um , begins with a d. [doctor] uh , digoxin ? [patient] that's it .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 190,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-191",
      "content": "r past medical history ? [patient] uh , afib . [doctor] okay , and are you taking any medications for that ? [patient] yeah , i am . um , begins with a d. [doctor] uh , digoxin ? [patient] that's it . yeah , that's it . [doctor] okay , all right . how about any surgeries in the past ? [patient] i have had a nose job . [doctor] all right . um , let's do your exam , okay ? so is it tender ... where is it mostly tender right now ? [patient] right on the inside of my knee . right here . [doctor] all right , so if i bend your knee forward , does that seem to hurt ? [patient] yes , that hurts . [doctor] all right , how about if i twist it a little bit that way . [patient] that hurts a lot . [doctor] okay , okay . and how about down here ? do you feel me touch you down here ? [patient] yes . [doctor] all right . any other pain down here in your calves ? [patient] no . [doctor] no , okay .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 191,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-192",
      "content": "[doctor] okay , okay . and how about down here ? do you feel me touch you down here ? [patient] yes . [doctor] all right . any other pain down here in your calves ? [patient] no . [doctor] no , okay . so on exam you do have some tenderness over the medial portion of your knee over the medial meniscus area . uh , there is no , uh , there is a little bit of tenderness when i flex your , uh , when i , uh , uh , do some valgus stressing on your , on your leg . um , you have normal sensation . so let's take a look at your x-rays . [patient] okay . [doctor] okay . hey dragon , show me the x-rays . so looking at the x-ray , um , of your left knee , uh , it appears to be there's no fractures there right now . i do n't see any , uh , there's a little bit of , uh , fluid , uh , but there is no , uh , there's no , um , fracture or there's no dislocation . everything else seems to be lined up properly , okay ? [patient] okay .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 192,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-193",
      "content": ", uh , there's a little bit of , uh , fluid , uh , but there is no , uh , there's no , um , fracture or there's no dislocation . everything else seems to be lined up properly , okay ? [patient] okay . [doctor] so in summary after my exam , uh , looking at your knee , uh , on the x-ray and your exam , you have some tenderness over the medial meniscus , so i think you have probably an acute medial meniscus sprain right now or strain . uh , at this point , my recommendation would be to put you in a knee brace , uh , and we'll go ahead and have you use some crutches temporarily for the next couple days . we'll have you come back in about a week and see how you're doing , and if it's not better , we'll get an mri at that time . [patient] okay . [doctor] i'm going to recommend we give you some motrin , 800 milligrams . uh , you can take it about every six hours , uh , with food . uh , and we'll give you about a two week supply . [patient] okay . [doctor] okay .",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 193,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-194",
      "content": "to recommend we give you some motrin , 800 milligrams . uh , you can take it about every six hours , uh , with food . uh , and we'll give you about a two week supply . [patient] okay . [doctor] okay . uh , do you have any questions ? [patient] no , i think i'm good . [doctor] all right . hey , dragon , order the medications and procedures discussed , and finalize the report . okay , come with me and we'll get you checked out .\n\nFigure 26: Demonstrative example from ACI-BENCH evaluation. This is the prompt that we use to summarize a conversation between a doctor and a patient. We highlight the part of the context to which one of the models include an hallucination while creating the summary.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ACI-BENCH",
        "chunkIndex": 194,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-195",
      "content": "Ms. Thompson, a 43-year-old female, presented with right knee pain. She reported an incident of falling from a ladder while changing a light bulb, resulting in a twist to her knee. The pain is localized to the inside of her knee. She has been using ice packs and taking ibuprofen, but the pain persists.\n\nUpon examination, the patient exhibited tenderness over the medial portion of her knee, specifically over the medial meniscus area. There was also tenderness when the leg was subjected to valgus stress. Sensation in the area was normal.\n\nX-rays of the patient's knee revealed no fractures, dislocations, or misalignments. There was a small amount of fluid present.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "PHYSICAL EXAM:",
        "chunkIndex": 195,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-196",
      "content": "The patient is diagnosed with an acute medial meniscus sprain or strain. The recommended treatment plan includes a knee brace, temporary use of crutches for the next couple of days, and a follow-up appointment in one week. If the condition does not improve, an MRI will be conducted. The patient was prescribed 800 milligrams of Motrin, to be taken every six hours with food, for a two-week supply.\n\nFigure 27: Model output summary for the ACI-BENCH example of Figure 26 generated by Orca-2-13B. No hallucination is detected in this output.\n\nOrca-2-13B w/ cautious sm",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "ASSESSMENT AND PLAN:",
        "chunkIndex": 196,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-197",
      "content": "- Patient is a 43-year-old female with right knee pain.\n\nRelevant segments:\n\n- Injury occurred yesterday while changing a light bulb and twisting her knee on a ladder.\n\n- Patient has been using ice and taking ibuprofen, but it hasn't helped much.\n\n- Pain is located inside the knee.\n\n- Past medical history includes atrial fibrillation (afib) and a nose job.\n\nSummary:\n\n- Patient is currently taking digoxin for afib.\n\nThe patient is a 43-year-old female with a history of right knee injury sustained while changing a light bulb. She has been experiencing pain inside the knee, which has not been relieved by ice and ibuprofen. She has a past medical history of afib and a nose job and is currently taking digoxin.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "HISTORY OF PRESENT ILLNESS:",
        "chunkIndex": 197,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-198",
      "content": "- Tenderness over the medial portion of the knee.\n\nRelevant segments:\n\n- No fractures, dislocations, or abnormal sensations in the calves.\n\n- Tenderness increases when the leg is flexed or subjected to valgus stress.\n\n- Normal sensation in the knee area.\n\nSummary:\n\nPhysical examination reveals tenderness over the medial portion of the patient's knee, with no fractures, dislocations, or abnormal sensations in the calves. The patient has normal sensation in the knee area, and tenderness increases when the leg is flexed or subjected to valgus stress.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "PHYSICAL EXAM:",
        "chunkIndex": 198,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-199",
      "content": "- X-ray of the left knee shows no fractures or dislocations.\n\nRelevant segments:\n\n- Slight fluid present in the knee joint .\n\nThe X-ray of the patient's left knee reveals no fractures or dislocations, but there is a slight amount of fluid present in the knee joint . ASSESSMENT AND PLAN:\n\nSummary:\n\nRelevant segments: - Doctor suspects an acute medial meniscus sprain or strain. - Recommends a knee brace and temporary use of crutches for the next couple of days.\n\n- If not better, an MRI will be conducted.\n\n- Follow-up appointment in one week to assess progress.\n\n- Prescribed Motrin 800 mg, to be taken every six hours with food, for a two-week supply.\n\nThe doctor suspects an acute medial meniscus sprain or strain and recommends a knee brace and temporary use of crutches for the next couple of days. The patient is advised to have a follow-up appointment in one week to assess progress. If there is no improvement, an MRI will be conducted.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "RESULTS:",
        "chunkIndex": 199,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-200",
      "content": "and temporary use of crutches for the next couple of days. The patient is advised to have a follow-up appointment in one week to assess progress. If there is no improvement, an MRI will be conducted. The patient is prescribed Motrin 800 mg, to be taken every six hours with food, for a two-week supply.\n\nSummary:\n\nFigure 28: Model output summary for the ACI-BENCH example of Figure 26 generated by Orca-2-13B w/ cautious sm. We highlight the hallucination pointed by GPT-4 judge: the term 'knee joint' is incorrect as only 'knee' is mentioned in the context.\n\nFigure 29: Demonstrative example from QMSum evaluation. This is the prompt that we use to summarize a discussion of a team during a meeting.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "RESULTS:",
        "chunkIndex": 200,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-201",
      "content": "The team building meeting began with the project manager suggesting that everyone draw their favorite animal and share its characteristics. The participants drew their animals and shared their reasons for liking those animals. The discussion included topics such as the animals' appearance, behavior, and unique features. The team members also shared their personal experiences with the animals they chose. The meeting served as a fun and engaging way for the team to bond and get to know each other better.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Orca-2-13B",
        "chunkIndex": 201,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-202",
      "content": "Key points:\n\n- The project manager is the last one to speak and draws a giraffe\n\n- The team is doing a team building activity where they draw their favourite animals and share their characteristics\n\n- The team members have different opinions and preferences about animals and some of them are allergic to cats\n\n- The team members also share some references and jokes about animals from books and movies\n\nSummary:\n\nThe team building activity involves drawing and talking about animals. The project manager goes first and draws a giraffe , admitting that it looks more like a dinosaur. The other team members follow and draw animals like fish, kitty, badger, and liver . They explain why they like their animals and what makes them unique. They also mention some books and movies that feature animals, like Wind in the Willows and Brian, a Liverpudlian writer. The team has some fun and learns more about each other's tastes and humour.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Orca-2-13B w/ cautious sm",
        "chunkIndex": 202,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-203",
      "content": "ue. They also mention some books and movies that feature animals, like Wind in the Willows and Brian, a Liverpudlian writer. The team has some fun and learns more about each other's tastes and humour.\n\nFigure 30: Model output summary for the QMSum example of Figure 29 generated by Orca-2-13Band Orca-2-13B w/ cautious sm. No hallucination is detected on the output generated by Orca-2-13B. While Orca-2-13B w/ cautious smis able to correct extract the facts, the summary mentions two incorrect facts that the project manager 'goes first' and that other team members follow and draw animals like 'liver'. We highlight the hallucinations pointed by GPT-4 judge.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "Orca-2-13B w/ cautious sm",
        "chunkIndex": 203,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-204",
      "content": "The following is a list of passages:\n\n-duracell procell aa batteries. Follow duracell procell aa batteries to get e-mail alerts and updates on your eBay Feed. Unfollow duracell procell aa batteries to stop getting updates on your eBay Feed. Yay! You're now following duracell procell aa batteries in your eBay Feed.You will receive email alerts for new listings. Dont send me e-mail alerts.\n\n-We're building you a better Duracell Professional website. Whether you buy or sell batteries, our new site will be a resource for you to quickly find the right battery solution from our complete line of Duracell Professional products so that you can make educated decision when it comes to your power needs. In the meantime, click on over to Duracell.com for:\n\n-With unparalleled performance that matches the Duracell Coppertop batteries, but with lower costs because of bulk packaging and lower advertising costs the Duracell Procell batteries are an easy choice. AA, C, D &amp; 9V made in the USA.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 204,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-205",
      "content": "tches the Duracell Coppertop batteries, but with lower costs because of bulk packaging and lower advertising costs the Duracell Procell batteries are an easy choice. AA, C, D &amp; 9V made in the USA. AAA made in Thailand. We also carry Panasonic, Sony, and Rayovac bulk batteries.\n\n-LR6 Duracell ProCell Alkaline Batteries. Alkaline batteries are the typical household batteries that you use every day to power a wide variety of devices. Batteries Plus Bulbs has the widest selection of standard and specialty alkaline battery sizes available. Plus, all of our LR6 Duracell ProCell alkaline batteries are guaranteed fresh for superior performance. Trust the Plus for all of your alkaline battery needs. No filters available.\n\n-Follow procell batteries to get e-mail alerts and updates on your eBay Feed. Unfollow procell batteries to stop getting updates on your eBay Feed. Yay! You're now following procell batteries in your eBay Feed.You will receive email alerts for new listings.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 205,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-206",
      "content": "s on your eBay Feed. Unfollow procell batteries to stop getting updates on your eBay Feed. Yay! You're now following procell batteries in your eBay Feed.You will receive email alerts for new listings. Dont send me e-mail alerts.\n\n-Duracell recognizes that people using batteries in the workplace have special requirements - that's why they offer the Duracell Procell battery. The Duracell Procell battery is made to rigorous specifications so that they provide dependable and longlasting performance.\n\n-Duracell Coppertop are the trusted consumer battery for powering personal electronics. Duracell Procell batteries are Duracell's line of batteries designed for industrial and professional use. Duracell Procell batteries are packaged for the volume user, and are\n\n-Request a Volume Quotation. Duracell Procell AA Alkaline Batteries - 24 Pack. Duracell Procell batteries are reliable and long lasting power sources for your important work equipment.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 206,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-207",
      "content": "user, and are\n\n-Request a Volume Quotation. Duracell Procell AA Alkaline Batteries - 24 Pack. Duracell Procell batteries are reliable and long lasting power sources for your important work equipment. Each Procell AA battery provides high quality power with 2850mAh of capacity. This convenient pack of 24 batteries is great for immediate use or long term storage thanks to a 7 Year Shelf Life, one of the longest among primary alkaline batteries.\n\nnot sold in blister packs. A fresh Duracell Procell battery is charged higher (a fresh 9V Procell battery is typically at 9.5 volts or higher) than the Duracell Coppertop .\n\n-Related: duracell procell aaa batteries duracell procell aaa duracell procell d batteries aa batteries duracell procell aa batteries 72 duracell aa batteries.\n\n-Duracell Procell Battery Purchase Requirements: The Duracell Procell line was designed to be sold and distributed solely in the business to business (B2B) market.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 207,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-208",
      "content": "batteries 72 duracell aa batteries.\n\n-Duracell Procell Battery Purchase Requirements: The Duracell Procell line was designed to be sold and distributed solely in the business to business (B2B) market. It is a requirement of the manufacturer that the Duracell Procell line of batteries is not purchased for personal, consumer use.\n\nUsing the passages, respond to the following query:\n\nQuery: what are procell batteries\n\nFigure 31: Demonstrative example from MS-MARCO evaluation. This is the prompt that we use to answer a question based on a list of retrieved facts. We highlight the excerpts that leads to a possible hallucination later.\n\nFigure 32: Model output summary for the ACI-BENCH example of Figure 31 generated by Orca-2-13B. No hallucination is detected in this output.\n\n<!-- image -->\n\nFigure 33: Model output summary for the MS-MARCO example of Figure 31 generated by Orca-2-13B w/ cautious sm.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 208,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-209",
      "content": "31 generated by Orca-2-13B. No hallucination is detected in this output.\n\n<!-- image -->\n\nFigure 33: Model output summary for the MS-MARCO example of Figure 31 generated by Orca-2-13B w/ cautious sm. We highlight the hallucination pointed by GPT-4 judge: the 'capacity' is only specified for Procell battery, not for Coppertop. Therefore this comparison can be considered an hallucination.\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 34: Examples from ToxiGen dataset for the MCQ and content generation tasks from 'Latino', 'Women', 'Asian' and 'LGBTQ' categories. The MCQ task examples have a score, for the purpose of our experiments anything that has been annotated to have a score equals to or higher than 2.5 is categorized as 'Toxic' and 'Neutral' otherwise. For the content generation task Example 1 prompts the model with hateful sentences to continue toxic content generation while Example 2 prompts with neutral sentences.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 209,
        "totalChunks": 211
      }
    },
    {
      "id": "2311.11045v2-chunk-210",
      "content": "e 1 prompts the model with hateful sentences to continue toxic content generation while Example 2 prompts with neutral sentences.",
      "metadata": {
        "source": "arxiv:2311.11045v2",
        "title": "Orca 2: Teaching Small Language Models How to Reason",
        "authors": [
          "Arindam Mitra",
          "Luciano Del Corro",
          "Shweti Mahajan",
          "Andres Codas",
          "Clarisse Simoes",
          "Sahaj Agarwal",
          "Xuxi Chen",
          "Anastasia Razdaibiedina",
          "Erik Jones",
          "Kriti Aggarwal",
          "Hamid Palangi",
          "Guoqing Zheng",
          "Corby Rosset",
          "Hamed Khanpour",
          "Ahmed Awadallah"
        ],
        "section": "MS-MARCO",
        "chunkIndex": 210,
        "totalChunks": 211
      }
    }
  ],
  "fullText": "<!-- image -->\n\n## Orca 2: Teaching Small Language Models How to Reason\n\nArindam Mitra, Luciano Del Corro † , Shweti Mahajan † , Andres Codas ‡ Clarisse Simoes ‡ , Sahaj Agarwal, Xuxi Chen ∗ , Anastasia Razdaibiedina ∗ Erik Jones ∗ , Kriti Aggarwal ∗ , Hamid Palangi, Guoqing Zheng Corby Rosset, Hamed Khanpour, Ahmed Awadallah\n\n## Microsoft Research\n\n## Abstract\n\nOrca 1 learns from rich signals, such as explanation traces, allowing it to outperform conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. In Orca 2, we continue exploring how improved training signals can enhance smaller LMs' reasoning abilities. Research on training small LMs has often relied on imitation learning to replicate the output of more capable models. We contend that excessive emphasis on imitation may restrict the potential of smaller models. We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model. For example, while larger models might provide a direct answer to a complex task, smaller models may not have the same capacity. In Orca 2, we teach the model various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct answer, etc.). Moreover, we aim to help the model learn to determine the most effective solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca 2 significantly surpasses models of similar size and attains performance levels similar or better to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm to support research on the development, evaluation, and alignment of smaller LMs.\n\nFigure 1: Results comparing Orca 2 (7B &amp; 13B) to LLaMA-2-Chat (13B &amp; 70B) and WizardLM (13B &amp; 70B) on variety of benchmarks (in 0-shot setting) covering language understanding, common sense reasoning, multi-step reasoning, math problem solving, etc. Orca 2 models match or surpass all other models including models 5-10x larger. Note that all models are using the same LLaMA-2 base models of the respective size.\n\n<!-- image -->\n\n∗ work done while at Microsoft; † , ‡ denote equal contributions.\n\n## ### Question:\n\nJohn and Mark are in a room with a ball, a basket and a box. John puts the ball in the box, then leaves for work. While John is away, Mark puts the ball in the basket, and then leaves for school. They both come back together later in the day, and they do not know what happened in the room after each of them left the room. Where do they think the ball is?\n\n## ### Orca-2-13B :\n\nLet's analyze the situation step by step:\n\n1. John puts the ball in the box.\n3. Mark comes into the room and puts the ball in the basket.\n2. John leaves for work.\n4. Mark leaves for school.\n5. John and Mark both return to the room.\n\nAt this point, John and Mark have not seen each other's actions. Therefore, they only know what happened while they were in the room.\n\nJohn only saw the ball in the box before he left for work. Mark only saw the ball in the basket before he left for school. Since they did not witness each other's actions, they would both believe that the ball is where they last saw it.\n\nSo, John would think the ball is in the box, and Mark would think the ball is in the basket.\n\n## ### LLaMA2-13B :\n\nAns: This means that the man has been told by someone else (probably his friend) that he cannot have any more alcohol at this establishment.\n\nAns: The ball is still in the box because it was there when John left the room. When he came back, he did not see the ball in the basket so he assumed that it was still in the box. Ques 10. A man walks into a bar and asks for a drink. The bartender says, 'I'm sorry, but we don't serve alcohol here.' The man replies, 'Oh, I didn't realize this place was a church!' What does the man mean by his response?\n\n## ### LLaMA2-Chat-13B :\n\nThis is a classic lateral thinking puzzle. The solution is that the ball is still in the box.\n\nHere's the reasoning:\n\n1. John put the ball in the box before he left for work.\n\n3. Since John did not see the ball in the basket after he left, it must have still been in the box.\n\n2. Mark put the ball in the basket before he left for school.\n\n4. Similarly, since Mark did not see the ball in the box after he left, it must have still been in the basket.\n\nTherefore, the ball is still in the box.\n\n## ### ChatGPT(GPT-3.5-Turbo) :\n\nJohn and Mark will likely think that the ball is in the box, since that is where John left it before he left for work. They do not know that Mark moved the ball to the basket while John was away.\n\nFigure 2: Example demonstrating the responses of Orca 2, its base model LLaMA-2, LLaMA-2-Chat and ChatGPT (GPT-3.5-Turbo) to a reasoning question. LLaMA-2 and LLaMA-2-Chat model responses generated using replicate.com/meta/llama-2-13b and chat.lmsys.org respectively. LLaMA and Orca 2 model calls used temperature=0 and top\\_p=1 , ChatGPT response was obtained via chat.openai.com .\n\n## 1 Introduction\n\nLarge Language Models (LLMs) are enabling more natural and sophisticated interactions between humans and machines, enhancing user experience in existing applications like coding [3], web search [36], chatbots [45, 56], customer service and content creation. This transformation brought by LLMs is also paving the way for new innovative AI applications.\n\nScaling LLMs like GPT-4 [44] and PaLM-2 [1] to ever more parameters led to emergent abilities [63] unseen in smaller models (less than ∼ 10B parameters), most notably the remarkable ability to reason zero-shot [23]. These abilities include answering complex questions, generating explanations, and solving multi-step problems, for instance, such as those on the US Medical Licensing exam, on which LLMs now achieve a passing score [51]. Such abilities, especially in expert domains, were once considered beyond the reach of AI.\n\nImitation learning has emerged as the go-to approach to improve small language models [6, 64, 56], where the goal is to replicate the outputs of larger, more capable teacher models. While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [13]. While effective to some extent, imitation learning may limit the potential of smaller models, restricting them from utilizing the best solution strategies given the problem and the capacity of the model.\n\nIn this work, we continue to pursue the question of how we can teach smaller LMs to reason. The objectives of Orca 2 are two-fold. Firstly, we aim to teach smaller models how to use a suite of reasoning techniques, such as step-by-step processing, recall-then-generate, recall-reason-generate, extract-generate, and direct-answer methods. Secondly, we aspire to help these models decide when to use the most effective reasoning strategy for the task at hand, allowing them to perform at their best, irrespective of their size.\n\nLike Orca 1, we utilize more capable LLMs to demonstrate various reasoning strategies across various tasks. However, in Orca 2, the reasoning strategies are carefully tailored to the task at hand, bearing in mind whether a student model is capable of the same behavior. To produce this nuanced data, the more capable LLM is presented with intricate prompt(s) designed to elicit specific strategic behaviors - and more accurate results - as exemplified in Figure 3. Furthermore, during the training phase, the smaller model is exposed only to the task and the resultant behavior, without visibility into the original prompts that triggered such behavior. This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task. Rather than naively imitating powerful LLMs, we treat them as a reservoir of behaviors from which we carefully select those best suited for the task at hand.\n\nSome previous studies on training small models are limited in their evaluation protocol. They often rely on small number of tasks or on using other models for auto-evaluation by asking them to compare the outputs of two systems with a prompt like 'given responses from system 1 (reference) and system 2 (target), which one is better?' . However, previous work [13, 42, 60, 67] has demonstrated that this approach has several drawbacks. In this work, we provide a comprehensive evaluation comparing Orca 2 to several other models. We use a total of 15 benchmarks (covering ∼ 100 tasks and over 36,000 unique prompts). The benchmarks cover variety of aspects including language understanding, common sense reasoning, multi-step reasoning, math problem solving, reading comprehension, summarization, groundedness, truthfulness and toxic content generation and identification.\n\nOur preliminary results indicate that Orca 2 significantly surpasses models of a similar size, even matching or exceeding those 5 to 10 times larger, especially on tasks that require reasoning. This highlights the potential of endowing smaller models with better reasoning capabilities. However Orca 2 is no exception to the phenomenon that all models are to some extent constrained by their underlying pre-trained model (while Orca 2 training could be applied any base LLM, we report results on LLaMA-2 7B and 13B in this report). Orca 2 models have not undergone RLHF training for safety. We believe the same techniques we've applied for reasoning could also apply to aligning models for safety, with RLHF potentially improving even more.\n\n## 2 Preliminaries\n\n## 2.1 Instruction Tuning\n\nInstruction tuning [46, 38, 62, 61] has emerged as a crucial step in training language models. Instruction tuning involves learning from input-output pairs where the input is natural language task description,and the output is a demonstration of the desired behavior. Instruction tuning has been shown to improve the model's ability to follow instructions on both seen and unseen tasks [47], improve the overall quality of the generations [7] and give models enhanced zero-shot and reasoning abilities [62].\n\nSeveral studies, including Alpaca [55], Vicuna [6], WizardLM [64], Baize [65], and Koala [12], have adopted instruction tuning to train smaller 'student' language models using outputs generated by larger foundational models. This behavior cloning has been shown to be very effective in mimicking the style of the teacher model. However, as shown in [42, 5], it may not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style.\n\nWe note that instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge. Hence instruction tuned models will be always limited by the knowledge learned during pre-training. This is specially important to note when applying enhanced instruction tuning techniques to smaller models (as in this work and other related work). As such smaller language models with enhanced reasoning are perhaps best used as reasoning engines over knowledge provided to the model in its context window, or when specialized to narrower domains.\n\n## 2.2 Explanation Tuning\n\nOne of the known weaknesses of instruction tuning is that a resulting student model could learn to generate stylistically correct, but ultimately wrong, outputs [13]. For example, instruction-tuning towards targets that are too terse limits the student's visibility into what could have been a complex reasoning process, thus hindering its generalization ability to other tasks. In Orca 1, we introduced Explanation Tuning [42] to address this drawback by training student models on richer and more expressive reasoning signals. The mechanism for procuring these signals is system instructions 2 crafted to obtain detailed explanations from a teacher model as it reasons through a task. System instructions are additional high level guidelines an LLM is supposed to adhere to as it addresses individual user prompts , from which they are separated by a 'system' role flag in a ChatML dialogue interface 3 .\n\nExplanation tuning begins with a compilation of N hand-crafted, general purpose system instructions designed to elicit more careful reasoning. Some examples include 'think step-by-step', 'generate detailed answers', etc. The primary objective of these system instructions is to extract rich demonstrations of 'Slow Thinking' [22] from capable LLMs like GPT-4. They are then combined with user prompts from a vast and diverse set of tasks to yield a dataset of ( system instruction , user prompt , LLM answer ) triplets. The student model is trained to predict the LLM answer from the other two inputs.\n\nIf user prompts can be grouped into M distinct clusters representing similar kinds of questions, then Explanation Tuning naively yields a cross product of M × N different answers addressing different aspects of the task. Since more capable LLMs tend to vary their responses with the system instruction , this offers an easy path to increase the quantity and diversity of training signals. Numerous models such as Orca 1 [42], StableBeluga [35] and Dolphin 4 have capitalized on Explanation Tuning to demonstrate substantial improvements over traditional instruction-tuned models, especially in complex zero-shot reasoning tasks.\n\n2 used interchangeably with system message\n\n3 e.g. see https://platform.openai.com/docs/api-reference/making-requests\n\n4 https://huggingface.co/datasets/ehartford/dolphin\n\n## 3 Teaching Orca 2 to be a Cautious Reasoner\n\nThe key to Explanation Tuning is the extraction of answers with detailed explanations from LLMs based on system instructions . However, not every combination of system instruction cross tasks is appropriate, and in fact, the response quality can vary significantly based on the strategy described in the system instruction .\n\nEven very powerful models like GPT-4 are susceptible to this variation. Consider, Figure 3, which shows four different answers from GPT-4 obtained with four different system instructions given a question of story reordering. The first answer (the default GPT-4 answer) is wrong. The second answer (using a chain-of-thought prompt) is better. We can see that the model is reasoning with step-by-step but important details guiding the decision process are still missing. The third answer (with an explain-your-answer prompt) is wrong but the explanation is correct. The final answer is the only correct answer and is obtained using the following system instruction :\n\nYou will be given a task. Use the following steps to solve it.\n\n1. Identify the main theme or topic of the story.\n\n3. Find the sentence that could be the start of the story. Go through each of the answer choices and analyze to figure it out.\n\n2. Look for any cause and effect relationships between the sentences.\n\n4. Rearrange the sentences in the correct order based on the information gathered in the previous steps.\n\n5. Final answer: Write down the correct order of the sentences using their numbers, such as '23415'.\n\nWe note that GPT-4's response is significantly influenced by the given system instructions . Secondly, when carefully crafted, the instructions can substantially improve the quality and accuracy of GPT-4's answers. Lastly, without such instructions, GPT-4 may struggle to recognize a challenging problem and might generate a direct answer without engaging in careful thinking. Motivated by these observations, we conclude that the strategy an LLM uses to reason about a task should depend on the task itself.\n\nEven if all the answers provided were correct, the question remains: Which is the best answer for training a smaller model? This question is central to our work, and we argue that smaller models should be taught to select the most effective solution strategy based on the problem at hand. It is important to note that: (1) the optimal strategy might vary depending on the task and (2) the optimal strategy for a smaller model may differ from that of a more powerful one. For instance, while a model like GPT-4 may easily generate a direct answer, a smaller model might lack this capability and require a different approach, such as thinking step-by-step. Therefore, naively teaching a smaller model to 'imitate' the reasoning behavior of a more powerful one may be sub-optimal. Although training smaller models towards step-by-step-explained answers has proven beneficial, training them on a plurality of strategies enables more flexibility to choose which is better suited to the task.\n\nWe use the term Cautious Reasoning to refer to the act of deciding which solution strategy to choose for a given task - among direct answer generation, or one of many 'Slow Thinking' [22] strategies (step-by-step, guess and check or explain-then-answer, etc.).\n\nThe following illustrates the process of training a Cautious Reasoning LLM:\n\n1. Start with a collection of diverse tasks\n3. Write task-specific system instruction(s) corresponding to the chosen strategy in order to obtain teacher responses for each task.\n2. Guided by the performance of Orca, decide which tasks require which solution strategy (e.g. direct-answer, step-by-step, explain-then-answer, etc.)\n4. Prompt Erasing : At training time, replace the student's system instruction with a generic one vacated of details of how to approach the task.\n\nNote that step 3 has a broad mandate to obtain the teacher's responses: it can utilize multiple calls, very detailed instructions, etc.\n\nFigure 3: Demonstrative example from Flan-CoT Collection.\n\n<!-- image -->\n\nThe key idea is: in the absence of the original system instruction which detailed how to approach the task, the student model will be encouraged to learn that underlying strategy as well as the reasoning abilities it entailed. We call this technique Prompt Erasing as it removes the structure under which the teacher framed its reasoning. Armed with this technique, we present Orca 2, a cautious reasoner.\n\n## 4 Technical Details\n\nFor Orca 2, we created a new dataset with ~817K training instances, which we will refer as Orca 2 dataset. Following Orca 1, Orca 2 has been trained with progressive learning, with subsets of data obtained from combining the original FLAN [33] annotations, Orca 1 dataset and the Orca 2 dataset. We also describe the details about the progressive learning.\n\n## 4.1 Dataset Construction\n\nThe Orca 2 dataset has four main sources:\n\nFLAN: Our main source of prompts for synthetic data generation is the FLAN-v2 Collection [33], which consists of five sub-collections, namely, CoT, NiV2, T0, Flan 2021 and Dialogue. Each sub-collection contains multiple tasks. Following Orca 1 [42] we consider tasks from only CoT, NiV2, T0, Flan 2021 sub-collections, which contain a total of 1913 tasks. Each task in Flan-v2 is a collection of queries and has an associated answer. Some of 1913 tasks in FLAN are created synthetically by inverting another task. An example would be, converting a question answering task to create a question generation task. For the Cautious-ReasoningFLAN dataset construction, we selected ~602K zero-shot user queries from the training split of 1448 high quality tasks out of the 1913 tasks, filtering many synthetically generated tasks.\n\nWe grouped the selected 1448 tasks manually into 23 categories (e.g., Text Classification, Claim Verification, Data2Text, Text Generation, Logic, Math, Multiple Choice Questions, Open Ended Question Answering, Reading Comprehension, etc.). Each category is further divided into sub-categories, creating a total of 126 sub-categories. Sub-categories are created with the aim that all tasks in a sub-category share the same system instruction.\n\nFor alignment towards cautious reasoning, we replace all the system instructions with the following generic system instruction:\n\nYou are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\n\nWe will refer to it as the cautious system instruction .\n\nFew Shot Data : The dataset above does not contain any demonstrations of examples in the prompts. To encourage the model to learn to use the few-shot demonstrations, we constructed a Few-Shot dataset consisting of 55K samples. These samples are constructed by re-purposing the zero-shot data from Orca 1 dataset. Particularly, we structure the Orca 1 data into ( task , system instruction , user prompt , answer ) tuples and group by task and system instruction . For each group and each user prompt, we randomly select 3-5 ( user prompt , answer ) pairs from the rest, and use those as in-context examples.\n\nMath: We collected data for ~160K math problems from the Deepmind Math dataset [50] 5 and the training splits of a collection of existing datasets: GSM8K [9], AquaRat [31], MATH [18], AMPS [18], FeasibilityQA [14], NumGLUE [40], AddSub [19], GenArith [24] and Algebra [26]. For NumGLUE, AddSub, GenArith, and Algebra, we have referred to the LILA [39] benchmark for the training split. Note that including prompts from the training split of a dataset (e.g. GSM8K) renders it in-domain for the sake of evaluation. Note that datasets like GSM8K are considered in-domain for many of our baselines too.\n\n5 We have sampled only from arithmetic div, arithmetic mul, arithmetic add or sub tasks from https://huggingface.co/datasets/math\\_dataset\n\nFully synthetic data: We have synthetically created 2000 Doctor-Patient Conversations with GPT-4. We then instruct the model to create a summary of the conversation with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN. We used two different prompts: one with high-level task instruction and another with detailed instructions that encourages the model to avoid omissions or fabrications. We use this data to assess the learning of specialized skills.\n\n## 4.2 Training\n\nThis section provides an overview of the training process for Orca 2, covering different aspects of tokenization, sequencing, and loss computation.\n\nProgressive Learning : We start with LLaMA-2-7B or LLaMA-2-13B checkpoint and finetune it on the train split of FLAN-v2 dataset for one epoch. Note that FLAN-v2 dataset contains both zero-shot and few-shot problems. We then train on 5 million ChatGPT data from Orca 1 for 3 epochs. Then we train on the combination of 1 million GPT-4 data from Orca 1 and Orca 2's 817K data for 4 epochs.\n\nTokenization : We utilize the LLaMA Byte Pair Encoding (BPE) tokenizer for processing the input examples. Notably, the LLaMA tokenizer splits all numbers into individual digits, and fallbacks to bytes to decompose unknown UTF-8 characters. To deal with variable length sequences we add a padding token ' [[PAD]] ' into the LLaMA tokenizer vocabulary. We also add the ChatML special tokens ' &lt;|im\\_start|&gt; ' and ' &lt;|im\\_end|&gt; '. The resulting vocabulary contains 32 , 003 tokens.\n\nPacking : To optimize the training process and utilize computational resources efficiently, we employ the packing technique [25]. This method involves concatenating multiple input examples into a single sequence, which is then used for training the model. The packing is performed such that the total length of the concatenated sequence does not exceed max \\_ len = 4096 tokens. Particularly, we shuffle the input examples and then partition the examples into groups such that length of the concatenated sequence in each group is at most max\\_len . Padding tokens are then added to the concatenated sequence to achieve a uniform input sequence length of max\\_len .\n\nLoss : For the purpose of training Orca 2, we compute the loss only on the tokens generated by the teacher model, i.e., it learns to generate responses conditioned on the system instruction and task instructions. This approach ensures that the model focuses on learning from the most relevant and informative tokens, improving the overall efficiency and effectiveness of the training process.\n\nCompute: We trained Orca 2 on 32 NVIDIA A100 GPUs with 80GB memory with bfloat16. For the 13B checkpoint, it took ~17 hours to train Orca 2 on FLAN dataset for one epoch, ~40 hours to train on 5 million ChatGPT data for 3 epochs and ~23 hours to continue training on ~1 . 8 million GPT-4 data for 4 epochs.\n\n## 5 Experimental Setup\n\n## 5.1 Baselines\n\nWe benchmark Orca 2 alongside several state-of-the-art models. All baseline models are instruction-tuned models. We use the instruction-tuned versions because they have been shown to be much better at following instructions, have stronger reasoning capabilities, and are much better in zero-shot settings [33, 47, 64, 42].\n\n- LLaMA-2 Models: We use both the 70 billion and 13 billion parameter models from the LLaMA 2 series [57]. We use the LLaMA2-70B-hf-chat 6 and LLaMA2-13B-hf-chat 7 .\n\n6 https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n\n7 https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n\n- WizardLM: WizardLM [64] is an instruction tuned version of LLaMA 2, specifically through the Evol-Instruct technique which autonomously generates a diverse array of intricate instruction data. We use both 13B (V1.2 8 ) and 70B (V1.0 9 ) parameter versions.\n- GPT Models: We show the performance of both ChatGPT (GPT-3.5-Turbo) and GPT-4 [44]. We utilized the Azure OpenAI API version '2023-03-15-preview' .\n- Orca: Orca 1 [42] is a 13-billion parameter model that learns through explanations, step-by-step thought processes, and complex instructions and is based on the LLaMA model [57].\n\nFor inference, we use fp32 for LLaMA2 and Orca models. For WizardLM models we could use fp16 since they were trained with fp16 [64].\n\n## 5.2 Benchmarks\n\nThis section provides a detailed overview of the tasks selected to assess open-ended generation, summarization, safety, bias, reasoning, and comprehension capacities of Orca 2. Except where specified otherwise, evaluations were conducted using the test split of each dataset. We conduct evaluations for all benchmarks and all models on zero-shot settings.\n\nWe selected a broad set of benchmarks representing both advanced capabilities such as reasoning, more basic abilities such as text completion and also grounding, truthfulness and safety. In choosing the benchmarks, we follow the suggestions and choices made by the OpenLLM Leaderboard 10 and InstructEval [5].\n\n## 5.2.1 Reasoning Capabilities\n\n- AGIEval: AGIEval [69] is a collection of diverse sets of standardized tests including general college admission tests like the GRE, GMAT, and SAT; law-focused examinations such as the LSAT and lawyer qualification assessments; math competitions; and national civil service examinations [69].\n- CRASS: The CRASS [11] dataset evaluates counterfactual reasoning abilities of LLMs.\n- Discrete Reasoning Over Paragraphs: DROP [10] is an adversarialy-created reading comprehension benchmark, which requires models to navigate through references and execute discrete operations like addition or sorting and was adopted as part of InstructEval [5] and the OpenLLM Leaderboard.\n- RACE: The RACE dataset [27] is a collection of reading comprehension questions derived from English examinations given to Chinese students aged between 12 to 18 years.\n- GSM8K: This is a collection of word problems that test the ability to perform multi-step mathematical reasoning [9].\n- Big-Bench Hard (BBH): BBH [54] is a subset of the 23 hardest tasks of BIG-Bench [52] with a focus on challenging tasks such as those requiring multi-step reasoning.\n\n## 5.2.2 Knowledge and Language Understanding\n\n- Massive Multitask Language Understanding benchmark: MMLU [17] is designed to measure the language understanding, knowledge and reasoning abilities of models and consists of 57 tasks.\n- ARC: The AI2 Reasoning Challenge [8] is a benchmark that tests the ability of text models to answer multiple-choice questions from science exams spanning Grade 3 to Grade 9 with two subsets: Easy and Challenge.\n\n8 https://huggingface.co/WizardLM/WizardLM-13B-V1.2\n\n9 https://huggingface.co/WizardLM/WizardLM-70B-V1.0\n\n10 https://huggingface.co/spaces/HuggingFaceH4/open\\_llm\\_leaderboard\n\n## 5.2.3 Text Completion\n\n- HellaSwag: A dataset [66] for evaluating commonsense natural language inference. It tests the ability of natural language models to complete text with what might happen next in the scene about physical situations.\n- LAMBADA: This dataset [48] is a collection of 10,022 passages from 2,663 novels that tests the ability of natural language models to perform long-range contextual understanding.\n\n## 5.2.4 Multi Turn Open Ended Conversations\n\n- MT-bench: is a benchmark tailored for evaluating the proficiency of chat assistants in multi-turn conversations [67] using GPT-4 as the judge.\n\n## 5.2.5 Grounding and Abstractive Summarization\n\n- ACI-BENCH: It contains full doctor-patient conversations and associated clinical notes from various medical domains. The task is to generate a clinical note from the dialogue [59].\n- QMSum: A benchmark [68] for query-based multi-domain meeting summarization, where models have to select and summarize relevant spans of meetings in response to a query.\n- MS-MARCO: This dataset [2] is a large-scale collection of natural language questions and answers derived from real web queries and documents.\n\n## 5.2.6 Safety and Truthfulness\n\n- ToxiGen : This is a large-scale, machine-generated dataset [16] of 274,186 toxic and benign statements about 13 minority groups with a focus on implicit hate speech that does not contain slurs or profanity. We use the dataset to test a model's ability to both identify and generate toxic content.\n- TruthfulQA: A benchmark [30] for evaluating the truthfulness of LLMs in generating answers to questions constructed in a way that humans tend to answer the curated questions falsely due to false believes, biases and misconceptions. The evaluation benchmark contains 817 questions spanning 38 categories (e.g., health, law, finance and politics). We evaluate the models on a multiple-choice variant of the dataset.\n- HHH: This dataset [53] is benchmark for evaluating the alignment of language models with respect to helpfulness, honesty and harmlessness, where a language model is asked to choose the best response among two options.\n- Automated RAI Measurement Framework: We also use a recently proposed framework [34] for evaluating the safety of a given chat-optimized model in conversational setting. Particularly, one LLM poses as a user and engages in a conversation with the LLM under test to evaluate potential harmful content, IP leakage and jailbreaks.\n\n## 5.3 Evaluation Settings\n\nWe evaluate models' capabilities on all tasks under zero-shot setting and without any exemplars or CoT prompting. Note that we observe, in preliminary experiments, that larger models benefit more from few-shot settings than smaller models like Orca 2. We conduct evaluation only based on the zero-shot settings, we leave a detailed analysis of the few-shot capabilities to future work. In all experiments, we utilize a greedy decoding approach without sampling.\n\nPrompts : We use empty system messages and simple prompts for all models to avoid variations in quality due to prompt engineering, except for general guidelines around answer formats for some task. To minimize diversity and establish a reliable evaluation process, we often include formatting guidelines in system messages to enhance the accuracy of answer extraction. For instance, we might use a system message like ' At the end, output ###Final answer: {answer choice} ' and ' select the answer from the provided options. ' Table F shows the prompts used for each dataset. For Orca 2, we report performance with both an\n\n'empty' system message and a 'cautious' system message. The latter is a generic system message that was described in Section 4.\n\nAnswer parsing : Parsing answers from free-form responses from generative models is a difficult task. Therefore, we divided the evaluation tasks into 3 categories based on the type of task and the extraction required, namely:\n\n- MCQ (Multiple-Choice Questions): These tasks require extraction of the option selected as the final answer by the model. We also formatted any classification tasks into this category as well where the classes represent the options for the model to choose from. The prompt for these tasks included the question, followed by the answer choices.\n- No extraction required: This category is for tasks that did not require extraction. Open-ended question answering falls into this category.\n- Exact Match/Span Extraction: These tasks require extraction of the exact final answer in the response or a span from the context provided.\n\nIn the categories requiring extraction (MCQ and Exact Match/Span Extraction), we compile an extensive set of patterns and delimiters like 'Final answer', 'So, the answer is', 'Final option:', etc. to extract the text from the response that might contain the answer. We then use regular expressions to extract the right option IDs or the exact text of the option selected by the model as the answer. Answer parsing for exact matches/span extraction varies depending on the task. Responses are matched for consistency with the gold answers. Along with evaluation metrics, we also calculate a format-OK metric which is the percentage of samples from which our parsing logic was able to extract an answer. We employ the same parsing logic to all the models' responses for consistency and we acknowledge that performance of all models could be improved with a better parsing logic.\n\nHowever, models may not always adhere to these formatting guidelines. The extraction coverage and models' sensitivity to system instructions and prompts may lead to different results for some baselines compared to those reported in other studies. Nonetheless, all models in this study undergo the same evaluation pipeline.\n\nIn addition to the tasks from FLANv2, we include tasks from the training portions of the following datasets (hence they should be considered in-domain, even with a zero-shot evaluation): DROP, ARC, RACE, Hellaswag, Lambada, MS Marco and GSM8K. The rest of the benchmarks should be considered as out-of-domain to the best of our knowledge. Note that we do not have detailed information about the data used for training the base model (LLAMA-2) and hence we cannot completely rule out further data leakage. However, we report the performance of several instruction-tuned versions of LLAMA-2 for reference.\n\nIn the following sections, we discuss the performance of Orca 2 and other baseline models on the benchmarks described above in zero-shot setting.\n\n## 6 Evaluation Results\n\n## 6.1 Reasoning\n\nReasoning capabilities are pivotal in ascertaining the efficacy of LLMs. Here we assess the reasoning prowess of Orca 2 models by testing them against a wide range of benchmarks, such as AGI Eval, BigBench-Hard (BBH), DROP, RACE, GSM8K, and CRASS. The average performance across these benchmarks is depicted in Figure 4. When comparing Orca 2, we observe the following phenomenon:\n\n- Surpassing models of the same size - Orca-2-13B significantly outperforms models of the same size on zero-shot reasoning tasks. Orca-2-13B provides a relative improvement of 47 . 54% over LLaMA-2-Chat-13B and 28 . 15% over WizardLM-13B. Notably, all three models - Orca-2-13B, LLaMA-2-Chat-13B, and WizardLM-13B - share the same base model, highlighting the efficacy of the training process employed by Orca 2.\n- Competitive with models 5-10x larger - Furthermore, Orca-2-13B exceeds the performance of LLaMA-2-Chat-70B and performs comparably to WizardLM-70B and\n\nAverage Performance on Reasoning Benchmarks\n\nFigure 4: Macro-average Performance of different models on reasoning benchmarks. Detailed task specific performance breakdown is reported in Table 1\n\n<!-- image -->\n\nTable 1: Zero-Shot performance comparison of different models on reasoning benchmarks.\n\n| Model            |   AGI |   BBH |   DROP |   CRASS |   RACE |   GSM8K |\n|------------------|-------|-------|--------|---------|--------|---------|\n| Orca 2-7B        | 45.1  | 45.93 |  60.26 |   84.31 |  80.79 |   47.23 |\n| w/ cautious sm   | 43.97 | 42.8  |  69.09 |   88.32 |  75.82 |   55.72 |\n| Orca 2-13B       | 49.93 | 50.18 |  57.97 |   86.86 |  82.87 |   59.14 |\n| w/ cautious sm   | 48.18 | 50.01 |  70.88 |   87.59 |  79.16 |   65.73 |\n| Orca-1-13B       | 45.69 | 47.84 |  53.63 |   90.15 |  81.76 |   26.46 |\n| LLaMA-2-Chat-13B | 38.85 | 33.6  |  40.73 |   61.31 |  62.69 |   25.09 |\n| WizardLM-13B     | 38.25 | 38.47 |  45.97 |   67.88 |  62.77 |   48.6  |\n| LLaMA-2-Chat-70B | 46.7  | 44.68 |  54.11 |   74.82 |  68.79 |   52.01 |\n| WizardLM-70B     | 48.73 | 51.08 |  59.62 |   86.13 |  78.96 |   73.24 |\n| ChatGPT          | 53.13 | 55.38 |  64.39 |   85.77 |  67.87 |   79.38 |\n| GPT-4            | 70.4  | 69.04 |  71.59 |   94.53 |  83.08 |   85.52 |\n\nChatGPT. Orca-2-7B is better or comparable to LLaMA-2-Chat-70B on all reasoning tasks.\n\n- Cautious system message adds a small boost - Using the cautious system message with both the 7B and 13B models provides small gains over the empty system message.\n\nNote that for baseline evaluations, results obtained from our runs are comparable to other public results with zero-shot setting and within a reasonable difference compared to few-shot results. Our numbers are sometimes better than publicly reported (e.g., our ChatGPT and GPT-4 runs on AGIEval compared to those reported in [69], our WizardLM-13B and WizardLM-70B runs on DROP in contrast to those reported in the Open LLM Leaderboard). However, some of them are worse, for example on RACE, our ChatGPT run is 9 pts lower than reported in [28]. This could be attributed to different ChatGPT endpoints and versions, or to different prompts used for evaluation.\n\nPerformance breakdown across different tasks of AGIEval and BBH is provided in Appendix A. Examples from each dataset with the response from Orca 2 is presented in Appendix F.\n\n## 6.2 Knowledge and Language Understanding\n\nMMLU, ARC-Easy and ARC-Challenge assess the language understanding, knowledge and reasoning of LLMS. As with other benchmarks, we compare only to instruction-tuned models and conduct a zero-shot evaluation. Table 2 displays the results for knowledge and language\n\n| Model            |   MMLU |   ARC Easy |   ARC Challenge |\n|------------------|--------|------------|-----------------|\n| Orca-2-7B        |  53.7  |      87.79 |           78.41 |\n| w/ cautious sm   |  53.91 |      85.1  |           74.83 |\n| Orca-2-13B       |  57.73 |      92.85 |           83.36 |\n| w/ cautious sm   |  59.32 |      85.31 |           79.95 |\n| LLAMA-2-Chat-13B |  49.14 |      76.26 |           61.18 |\n| WizardLM-13B     |  42.81 |      68.98 |           50.43 |\n| Orca-1-13B       |  53.8  |      86.24 |           74.74 |\n| LLAMA-2-Chat-70B |  58.54 |      82.2  |           67.66 |\n| WizardLM-70B     |  55    |      80.68 |           71.93 |\n| ChatGPT          |  68.92 |      93.73 |           84.73 |\n| GPT-4            |  80.61 |      96.63 |           93.26 |\n\nTable 2: Zero-Shot performance comparison of different models on MMLU, ARC Easy and ARC Challenge. System Message indicates if the system message was 'empty' or 'cautious'.\n\ncomprehension benchmarks. Overall, we observe similar trends as with the reasoning tasks:\n\n- Surpassing models of the same size - Orca-2-13B surpasses LLaMA-2-Chat-13B and WizardLM-13B (both using the same base model as Orca-2) in performance on each individual benchmarks. On average, Orca-2-13B achieves a relative improvement of 25 . 38% over LLaMA2-Chat-13B and 44 . 22% over WizardLM-13B.\n- Competitive with models 5-10x larger - Orca-2-13B also outperforms both 70B baseline models. In the MMLU benchmark, Orca-2-13B (57 . 73%) achieves a score similar to LLaMA2-Chat-70B (58 . 54%) and WizardLM-70 (55 . 00%), both of which are approximately 5 times larger than Orca-2-13B. Additionally, Orca-2-7B surpasses both 70B baselines on the ARC test set.\n\nWe further note our baseline runs for this set of evaluations align with publicly reported results under zero-shot settings, considering the differences in prompts and possible variations in API endpoints for GPT models. We also point out that publicly reported results with LLaMA-2 models on MMLU are higher (54.8 and 68.9 for 13B and 70B variants, respectively [58]). However, these numbers are in few-shot settings, compared to the zero-shot settings reported in this paper.\n\nWhile we did not perform a comprehensive few-shot evaluation of Orca 2, preliminary results on one task point to smaller gains (over zero-shot settings) for Orca 2 compared to LLaMA-2 models, especially when compared to the 70B base models. We discuss this in Section 7 and aim to study this further moving forward.\n\n## 6.3 Text Completion\n\nIn addition to benchmarks measuring advanced reasoning capabilities, we also use HellaSwag and LAMBADA to measure text completion abilities. HellaSwag measures text completion skills in a multiple-choice question format, while LAMBADA is a single-word completion task.\n\nFigure 5 shows the performance of different models on text completion benchmarks. Both Orca-2-7B and Orca-2-13B exhibit strong performance on HellaSwag outperforming the 13B and 70B baselines. Orca-2-13B achieves a relative improvement of 33 . 13% over LLaMA-2Chat-13B and 61 . 94% over WizardLM-13B.\n\nFigure 5: Performance of different models on text completion test sets in zero-shot setting.\n\n<!-- image -->\n\nWe compare baseline results from our runs with publicly reported results and identify that on HellaSwag, LLaMA-2-13B has much higher performance than LLaMA-2-Chat-13B. We randomly sampled from LLaMA-2-Chat-13B and LLaMA-2-Chat-70B responses and manually reviewed them to find that indeed many of the answers were wrong, with several cases where the models refuse to answer citing safety concerns, sometimes incorrectly. We conjecture that chat models might not be best suited for text completion tasks like HellaSwag.\n\nWe also investigate the subpar performance of GPT-4 in the LAMBADA task. Our preliminary analysis shows that GPT-4 often claims that the context does not provide sufficient information to accurately identify the missing word or proposes a word that does not match the gold label. For example:\n\ni glanced up to hunter who was at his dresser spraying on some cologne . ' mom , hang on . ' i covered the phone . ' mom said not to worry about ryder and go out with the boys and then we can do sunday dinner there . is that ok with you ? ' i missed having family dinners too . ' yeah , sounds good , i 'll call mom and tell her about \\_\\_.' What is the word in the blank space (\\_\\_)? The answer is\n\nThe gold answer is Dinner but GPT-4 responds with\n\nIt is not possible for me to determine the exact word that should be in the blank space without more context. However, based on the provided text, a possible word could be 'it.' The sentence would then read: 'yeah, sounds good, I'll call mom and tell her about it.'\n\nAlthough GPT-4's performance could be enhanced through prompt engineering, it appears that LAMBADA might need additional prompt engineering and may not be suitable for evaluating chat-optimized models.\n\n## 6.4 Multi-Turn Open Ended Conversations\n\nWe evaluate the capabilities of Large Language Models (LLMs) in multi-turn conversational settings, utilizing the MT Bench dataset [67]. MT-Bench initiates conversations with LLMs through predetermined inquiries. Each dialogue consists of an initial query (Turn 1) and a follow-up query (Turn 2). Notably, the follow-up query remains unaltered, irrespective of the LLM's response to the opening query.\n\nMT-Bench employs GPT-4 for evaluation purposes. For each turn, MT-Bench calculates a score ranging from 1 to 10 using GPT-4. The per-turn score and the average score on MTBench can be found in Table 3. We have examined different GPT-4 endpoints and discovered\n\nTable 3: MT-Bench scores per turn and average\n\n| Model            |   Turn 1 |   Turn 2 |   Average |\n|------------------|----------|----------|-----------|\n| Orca-2-7B        |     6.14 |     5.15 |      5.65 |\n| w/ cautious sm   |     5.96 |     3.99 |      4.97 |\n| Orca-2-13B       |     6.69 |     5.6  |      6.15 |\n| w/ cautious sm   |     6.12 |     5.31 |      5.72 |\n| LLaMA-2-Chat-13B |     7.17 |     6.11 |      6.64 |\n| WizardLM-13B     |     7.14 |     5.58 |      6.36 |\n| Orca-1-13B       |     6.66 |     5.19 |      5.92 |\n| LLaMA-2-Chat-70B |     7.05 |     6.59 |      6.82 |\n| WizardLM-70B     |     8.07 |     7.45 |      7.76 |\n| ChatGPT          |     8.19 |     7.84 |      8.01 |\n| GPT-4            |     9.01 |     9.06 |      9.04 |\n\nthat they yield different assessments. This raises a question about the comparability of the results produced by different GPT-4 versions. To minimize potential issues, we have employed the same GPT-4 endpoint and version for conducting evaluations.\n\nOrca-2-13B performs comparably with other 13B models. The average second turn score of Orca-2-13B is lower than the first turn score, which can be attributed to the absence of conversations in its training data. However, Orca 2 is still capable of engaging in conversations, and this ability can be enhanced by packing multiple zero-shot examples into the same input sequence. It is part of our future work to improve Orca 2's multi-turn conversational ability.\n\n## 6.5 Grounding\n\nGenerating responses that are grounded in specific context is a desired property for many LLM applications. We use three different tasks for this evaluation covering query-based meeting summarization, web question answering where answers are generated and have long format and doctor-patient conversation summarization. Abstractive summarization and grounded questions answering are frequently used as test beds to evaluate groundedness.\n\nWe use the grounding evaluation framework proposed in [34]. The framework uses GPT-4 as a judge to measure in-context groundedness. Note that using any model as a proxy for evaluation (including GPT-4) has limitations depending on the model, for example, if the model has tendency to favour samples with specific characteristics like its own generations, long text or specific order of samples [67, 60, 37]. Working on increasing consistency between human evaluation and LLM based evaluation is an open area of research [32, 15, 43, 34, 67].\n\nFigure 6 presents hallucination rate results for different models averaged over three benchmarks we have conducted experiments on.\n\nWe note that Orca-2-13B exhibits the lowest rate of hallucination among all Orca 2 variants and other 13B and 70B LLMs. When compared with the LLaMA-2-13B and WizardLM-13B models, Orca-2-13B demonstrates a relative reduction of 76 . 92% and 61 . 71% in hallucination rate. Though cautious system message consistently increases the Hallucination Rate across the three tasks studied in this work. Through manual analysis, we found evidence that during the reasoning process led by cautious system message, Orca 2 might extrapolate the information available in the context, and uses the extrapolated content to create the summary. The ungrounded generated contents are often factually accurate, but they are not supported by the context. Examples of this situation for each of the datasets are presented in Appendix F.\n\n## 6.6 Safety\n\nAcomprehensive evaluation of LLMs for safety is quite complex and is an open area of research. In this study, we have conducted experiments on the publicly available datasets described in\n\nAverage Fault on Grounding in Abstractive Summarization Benchmarks\n\nFigure 6: The hallucination rate evaluated by GPT-4 as discriminator averaged over three abstractive summarization benchmarks described in section 5 (the lower the better). Task specific performance breakdown is reported in Table 11.\n\n<!-- image -->\n\nsection 5.2.6 and a recent measurement framework proposed in [34]. They represent tasks with a focus domain of implicit and explicit toxicity, truthfulness, three categories of content harms, three domains for IP and three areas of jailbreaks. Our experiments include two evaluation regimes for each model, a discriminative regime where the model is used as a classifier to identify the type of given content, and a generative regime where the model is prompted to produce output that does not follow safety guidelines and is then evaluated on how many times it follows the prompt intent. Please note that in both regimes, models might behave differently both in terms of not filtering the content correctly or filtering the content more aggressively which might result in erasure for different social groups.\n\nFor discriminative evaluation we have used ToxiGen, HHH and TruthfulQA:\n\nToxiGen: In the human annotated portion of this dataset each of the statements has a toxicity score provided by three human annotators. For the purpose of this experiment, we formulate the task as a classification task and assign each statement with a score lower than 2 . 5 as benign and greater than or equals to 2 . 5 as being toxic. The model is then prompted to score each of the statements. The model's accuracy is calculated as the percentage of statements it classifies correctly as toxic or benign. Results are presented in Figure 7 and Figure 8.\n\nFrom the experiments we observe that Orca-2-7B, WizardLM-13B, LLaMA-2-70B and Orca 1 models perform better at classifying toxic statements than classifying neutral statements. This is an important note as it might cause erasure (filtering out the content related to specific groups of people even if it is not problematic) for target identity groups in ToxiGen. Orca-2-13B, WizardLM-70B and LLaMA-2-13B do not have this problem for this experiment.\n\nNote that we also conducted an experiment to ensure instruction following of various models for this experiment, i.e., making sure the model outputs the requested format. All models in Orca 2 family, LLaMa-2 family and WizardLM family had rates above 96%. They were relatively lower for Orca 1 model, 79%, which does not follow task instruction as well.\n\nPerformance breakdown for each of the categories in ToxiGen are presented in Appendix D.1.\n\nTruthfulQA: For evaluation on this dataset we have used the multiple-choice variant of the dataset, TruthfulQA MC from EleutherAI, which includes questions from TruthfulQA in multiple choice format. Multiple choice style evaluation for TruthfulQA has also been used\n\nFigure 7: ToxiGen evaluation results for toxic statement classification averaged over all the 13 categories.\n\n<!-- image -->\n\nFigure 8: ToxiGen evaluation results for neutral statement classification averaged over all the 13 categories.\n\n<!-- image -->\n\nin [44]. There are related works that have used generative style evaluation for this dataset (e.g., [57]) using another model as judge which we have not used in this experiment.\n\nThe results are presented in Figure 9, where we observe that Orca-2-13B performs better in answering the questions compared to other models of similar size and comparable to models with much larger size.\n\nFigure 9: Performance of different models on TruthfulQA benchmark. We report the accuracy as the percentage of times the model generated the correct answer to the given multiple choice questions.\n\n<!-- image -->\n\nPlease note that the reason for the performance difference for both LLaMA-2-Chat-13B and LLaMA-2-Chat-70B from the ones reported in LLaMA-2 report [57] for TruthfulQA is that the evaluation schemes are different. In LLaMA-2, they report a generative style evaluation where GPT-3 has been used as annotator while we have used multiple choice version of the dataset to avoid limitations of using GPT-3 (or any LLM) as a judge (order bias, length bias, etc) when the gold labels are available.\n\nHHH: The results for the HHH task are presented in Figure 10 where both Orca-2-13B and Orca-2-7B outperform models of similar sizes. We also observed that LLaMA-2-Chat-13B and LLaMA-2-Chat-70B models occasionally avoid answering HHH questions, responding with answers like: 'I cannot fulfill your request.' Additionally, both LLaMA-2-Chat and Orca 2 models (w/ cautious systems message) tend to produce open-ended responses to some questions (especially longer ones) instead of choosing a specific answer. We believe that additional prompt engineering could help address some of these cases.\n\nFor generative style evaluation we have used the framework proposed in [34] and ToxiGen. It is important to note that any model used as annotator (including the ones that we have selected) is a proxy and may come with its own weaknesses and biases depending on the data it has been trained on. We recommend to take these experiments as an evaluation methodology and repeat them for the given domain of interest or application using the models and annotation approaches for the given domain:\n\nAutomated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI metrics for LLMs [34]. In this framework, one LLM poses as a user and engages in a synthetic conversation with the LLM under test which is Orca 2 in the experiments we are presenting in this section. Another LLM then evaluates the responses to measure its tendency to violate Responsible AI guidelines from the following aspects: (1) How many times the user succeeded in Jailbreaking the model under the test? (2) How many times the model under the test generates Potentially Harmful Content? (3) How many times the model leaks Intellectual Property (IP)? (For example while responding to 'The Great Gatsby book full text'). These categories are divided into multiple sub-categories described in [34]. Results of our experiments using this framework are presented in Table 4 and 5. Please note that the automated framework sets its own\n\nFigure 10: Evaluation results for HHH dataset.\n\n<!-- image -->\n\nsystem message while assessing chat model's safety. Please note that Orca 2 has not gone through RLHF safety training similar to LLaMA-2-Chat-13B and the tables 4 and 5 reflect some aspects of that. We hypothesize that since the teacher models (ChatGPT, GPT-4) have gone through various steps of safety tuning, and the use of content filtering in the data generation process, Orca 2 might have learned some aspects of those through instruction tuning. We probe some of the categories where we see a larger difference (e.g., violent) and observe that Orca 2 tends to counter the harmful positions more often (which is penalized by the metric), while models that have gone through RLHF safety training tend to decline to respond more often (which is rewarded by the metric).\n\n|                  | Potentially Harmful Content   | Potentially Harmful Content   | Potentially Harmful Content   | IP      | IP      | IP     |\n|------------------|-------------------------------|-------------------------------|-------------------------------|---------|---------|--------|\n| Model            | Sexual ↓                      | Violent ↓                     | Hate ↓                        | Songs ↓ | Books ↓ | News ↓ |\n| Orca-2-13B       | 0.60%                         | 13.47%                        | 2.26%                         | 18.47%  | 21.85%  | 21.48% |\n| LLaMA-2-Chat-13B | 0.00%                         | 0.17%                         | 0.83%                         | 16.84%  | 15.49%  | 18.18% |\n\nTable 4: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Harmful Content and IP .\n\n| Model            | Adult Content ↓   | Illegal Persuasion ↓   | Leaking Guidelines ↓   |\n|------------------|-------------------|------------------------|------------------------|\n| Orca-2-13B       | 4.55% 1.00%       | 7.58% 6.00%            | 24.24% 70.00%          |\n| LLaMA-2-Chat-13B |                   |                        |                        |\n\nTable 5: Safety evaluation of Orca 2 with automated Responsible AI measurement framework [34], measured as defect rate for Jailbreak .\n\nToxiGen: In this experiment, we prompt the model with the test set of ToxiGen [16] dataset containing toxic and benign statements. The task for the model is to continue the text further. To compute the toxicity of the content generated, we use HateBERT [4], an off-the-shelf hate speech detector to generate toxicity probability with higher values signifying more toxicity in the content. We represent them as a number between 0 to 100 for better visibility. Results for generation using toxic and neutral prompts are presented in Figure 11\n\nFigure 11: Comparison between different models on their tendency to generate toxic and neutral content over different categories when prompted with a text completion task for ToxiGen dataset using HateBERT as proxy for toxicity detection (lower is better).\n\n<!-- image -->\n\nFigure 12: Zero-Shot exact match performance on story reordering task of various models. GPT4 refers to default GPT-4 answer (Answer 1 in Figure 3), GPT4 w/ step-by-step corresponds to default step-by-step answer (Answer 3 in Figure 3), GPT4 w/ explain refers to answer then explain type of answer (Answer 2 in Figure 3). GPT4 w/ special step-by-step (Answer 4 in Figure 3) is used to generate data for Orca 2.\n\n<!-- image -->\n\n## 6.7 Effect of Task-Specific Data with Story Reordering\n\nWe create 5,000 training samples for story reordering using the prompt in Figure 3. We do not use the complex prompt during Orca 2 training (i.e. applying prompt erasing ). We mix the task-specific data with the rest of the training dataset and evaluate Orca 2 on a distinct set of the ROCStories corpus [41]. While sampling the test instances, we remove any instances from ROCStories that are in FLAN training split to avoid contamination. Figure 12 compares the performance of Orca 2 with different system messages for GPT-4. It also captures the performance of ChatGPT, Orca 1, LLaMA and WizardLM models. This experiment highlights the potential of specializing Orca 2 models for specific tasks using synthetic data generated with prompt erasing .\n\n## 7 Limitations\n\nOrca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well as the common limitations of other large language models and limitations originating from Orca 2's training process, including:\n\nData Biases: Large language models, trained on extensive data, can inadvertently carry biases present in the source data. Consequently, the models may generate outputs that could be potentially biased or unfair.\n\nLack of Transparency: Due to the complexity and size, large language models can act as 'black boxes', making it difficult to comprehend the rationale behind specific outputs or decisions. We recommend reviewing transparency notes from Azure for more information 11 .\n\nContent Harms: There are various types of content harms that large language models can cause. It is important to be aware of them when using these models, and to take actions to prevent them. It is recommended to leverage various content moderation services provided by different companies and institutions. On an important note, we hope for better regulations and standards from government and technology leaders around content harms for AI technologies in future. We value and acknowledge the important role that research and open source community can play in this direction.\n\nHallucination: It is important to be aware and cautious not to entirely rely on a given language model for critical decisions or information that might have deep impact as it is not obvious how to prevent these models from fabricating content. Moreover, it is not clear whether small models may be more susceptible to hallucination in ungrounded generation use cases due to their smaller sizes and hence reduced memorization capacities. This is an active research topic and we hope there will be more rigorous measurement, understanding and mitigations around this topic.\n\nPotential for Misuse: Without suitable safeguards, there is a risk that these models could be maliciously used for generating disinformation or harmful content.\n\nData Distribution: Orca 2's performance is likely to correlate strongly with the distribution of the tuning data. This correlation might limit its accuracy in areas underrepresented in the training dataset such as math and coding.\n\nSystem messages: Orca 2 demonstrates variance in performance depending on the system instructions. Additionally, the stochasticity introduced by the model size may lead to generation of non-deterministic responses to different system instructions.\n\nZero-Shot Settings: Orca 2 was trained on data that mostly simulate zero-shot settings. While the model demonstrates very strong performance in zero-shot setting, it does not show the same gains of using few-shot learning compared to other, specially larger, models.\n\nSynthetic data: As Orca 2 is trained on synthetic data, it could inherit both the advantages and shortcomings of the models and methods used for data generation. We posit that Orca 2 benefits from the safety measures incorporated during training and safety guardrails (e.g., content filter) within the Azure OpenAI API. However, detailed studies are required for better quantification of such risks.\n\nSmall Model Capacity: We note that post-training, while significantly beneficial in teaching the model how to solve a task, it does not necessarily teach the model new knowledge. Hence post-trained models will be mostly limited by the knowledge learned during pre-training. While this process can enhance the small model ability to reason, it does not expand its ability as a knowledge store. As such Orca 2is perhaps more suitable as reasoning engine over knowledge provided to the model in its context window, or when fine-tuned to specialize into narrower domains.\n\nThis model is solely designed for research settings, and its testing has only been carried out in such environments. It should not be used in downstream applications, as additional analysis is needed to assess potential harm or bias in the proposed application.\n\n11 https://learn.microsoft.com/en-us/legal/cognitive-services/openai/ transparency-note\n\n## 8 Conclusions\n\nOur study has demonstrated that improving the reasoning capabilities of smaller language models is not only possible, but also attainable through training on tailored synthetic data. Orca 2 models, by implementing a variety of reasoning techniques and recognizing the most effective solution strategy for each task, achieve performance levels comparable to, and often exceeding, models that are much larger, especially on zero-shot reasoning tasks. Though these models still exhibit limitations and constraints inherent to their base models, they show a promising potential for future improvement, especially in terms of better reasoning capabilities, control and safety, through the use of synthetic data for post-training. While Orca 2 models have not gone through RLHF training for safety, we believe that the use of synthetic data for post-training that has been filtered with various content safety filters could provide another opportunity for improving the overall safety of the models. While the journey towards fully realizing the potential of small language models is ongoing, our work represents a step forward, especially highlighting the value of teaching smaller models to reason. It also highlights the potential of using tailored and high-quality synthetic data, created by a more powerful model, for training language models using complex prompts and potentially multiple model calls. While frontier models will continue to demonstrate superior capabilities, we believe that research toward building more capable smaller models will help pave the way for new applications that require different deployment scenarios and trade offs between efficiency and capability.\n\n## References\n\n- [1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\n- [2] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension dataset, 2018.\n- [3] Christian Bird, Denae Ford, Thomas Zimmermann, Nicole Forsgren, Eirini Kalliamvakou, Travis Lowdermilk, and Idan Gazit. Taking flight with copilot: Early insights and opportunities of ai-powered pair-programming tools. Queue , 20(6):35-57, jan 2023. ISSN 1542-7730. doi: 10.1145/3582083. URL https://doi.org/10.1145/3582083 .\n- [4] Tommaso Caselli, Valerio Basile, Jelena Mitrovic, and M. Granitzer. Hatebert: Retraining bert for abusive language detection in english. ArXiv , abs/2010.12472, 2021.\n- [5] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757 , 2023.\n\n- [6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://vicuna.lmsys.org .\n- [7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.\n- [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv , abs/1803.05457, 2018. URL https://api.semanticscholar.org/CorpusID: 3922816 .\n- [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n- [10] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/ N19-1246 .\n- [11] Jörg Frohberg and Frank Binder. Crass: A novel data set and benchmark to test counterfactual reasoning of large language models, 2022.\n- [12] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/ .\n- [13] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms, 2023.\n- [14] Himanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Saurabh Arjun Sawant, Kevin Scaria, Siddharth Goyal, and Chitta Baral. \" john is 50 years old, can his son be 65?\" evaluating nlp models' understanding of feasibility. arXiv preprint arXiv:2210.07471 , 2022.\n- [15] Veronika Hackl, Alexandra Elena Müller, Michael Granitzer, and Maximilian Sailer. Is gpt-4 a reliable rater? evaluating consistency in gpt-4 text ratings, 2023.\n- [16] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3309-3326. Association for Computational Linguistics, 2022.\n- [17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR) , 2021.\n- [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 , 2021.\n- [19] Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 523-533, 2014.\n- [20] Frederick Jelinek, Robert L. Mercer, Lalit R. Bahl, and Janet M. Baker. Perplexity-a measure of the difficulty of speech recognition tasks. Journal of the Acoustical Society of America , 62, 1977. URL https://api.semanticscholar.org/CorpusID:121680873 .\n\n- [21] Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Awadallah, and Ece Kamar. Teaching language models to hallucinate less with synthetic tasks, 2023.\n- [22] Daniel Kahneman. Thinking, fast and slow . Farrar, Straus and Giroux, New York, 2011. ISBN 9780374275631 0374275637. URL https://www.amazon.de/ Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl\\_it\\_dp\\_o\\_pdT1\\_nS\\_nC? ie=UTF8&amp;colid=151193SNGKJT9&amp;coliid=I3OCESLZCVDFL7 .\n- [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2023.\n- [24] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics , 3:585-597, 2015.\n- [25] Mario Michael Krell, Matej Kosec, Sergio P. Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance, 2022.\n- [26] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 271-281, 2014.\n- [27] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082 .\n- [28] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023 , pages 431-469, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.29. URL https://aclanthology.org/2023.findings-acl.29 .\n- [29] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013 .\n- [30] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229 .\n- [31] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL , 2017.\n- [32] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023.\n- [33] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning, 2023.\n- [34] Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, and Mei Chen. A framework for automated measurement of responsible ai harms in generative ai applications, 2023.\n- [35] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. Stable beluga models. URL [https://huggingface.co/stabilityai/StableBeluga2](https: //huggingface.co/stabilityai/StableBeluga2) .\n\n- [36] Y. Mehdi. Reinventing search with a new ai-powered microsoft bing and edge, your copilot for the web. https://blogs.microsoft.com/blog/2023/02/07/ reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ , 2023. (Accessed: 15 November 2023).\n- [37] Alham Fikri Aji Minghao Wu. Style over substance: Evaluation biases for large language models, 2023.\n- [38] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773 , 2021.\n- [39] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning. arXiv preprint arXiv:2210.17517 , 2022.\n- [40] Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: A suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660 , 2022.\n- [41] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 839-849, 2016.\n- [42] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 , 2023.\n- [43] Ben Naismith, Phoebe Mulcaire, and Jill Burstein. Automated evaluation of written discourse coherence using gpt-4, 2023.\n- [44] OpenAI. Gpt-4 technical report, 2023.\n- [45] OpenAI. Chatgpt (sep 25 version). https://chat.openai.com/chat , 2023. [Large language model].\n- [46] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.\n- [47] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv , abs/2203.02155, 2022.\n- [48] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144 .\n- [49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040 .\n- [50] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557 , 2019.\n\n- [51] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023.\n- [52] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, and Adria Garriga-Alonso et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022.\n- [53] Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1679-1684, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1164. URL https://aclanthology.org/P19-1164 .\n- [54] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 13003-13051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.824 .\n- [55] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford\\_alpaca , 2023.\n- [56] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022.\n- [57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- [58] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n- [59] Wen wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliha Yetisgen. Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation, 2023.\n- [60] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023.\n\n- [61] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks, 2022.\n- [62] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022.\n- [63] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.\n- [64] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023.\n- [65] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data, 2023.\n- [66] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019.\n- [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n- [68] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 5905-5921, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.naacl-main.472 .\n- [69] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023.\n\n## A AGIEval Subtask Metrics\n\nAGIEval contains several multiple-choice English tasks. Table 6 provides the performance of Orca 2 and baseline models on each individual AGIEval tasks. The task performance is gauged using exact match accuracy, adhering to the methodology laid out in [69].\n\n| Model            |   LST- RC |   LST- LR |   LST- AR |   AQUA- RAT |   Gaokao EN |   LOGIQA (en) |   ST-E (w/p) |   ST- Eng |   ST- M |   Avg |\n|------------------|-----------|-----------|-----------|-------------|-------------|---------------|--------------|-----------|---------|-------|\n| Orca 2-7B        |     49.81 |     42.94 |     25.65 |       35.43 |       73.86 |         35.02 |        32.52 |     67.48 |   43.18 | 45.1  |\n| w/ cautious sm   |     51.3  |     41.18 |     17.39 |       37.4  |       73.53 |         34.72 |        33.01 |     69.9  |   37.27 | 43.97 |\n| Orca 2-13B       |     60.97 |     45.49 |     20.43 |       42.91 |       79.41 |         40.4  |        33.98 |     76.7  |   49.09 | 49.93 |\n| w/ cautious sm   |     59.85 |     42.75 |     22.61 |       35.04 |       79.74 |         36.87 |        39.32 |     74.27 |   43.18 | 48.18 |\n| Orca-1-13B       |     59.48 |     44.9  |     18.26 |       27.17 |       83.66 |         38.71 |        32.04 |     74.76 |   32.27 | 45.69 |\n| LLaMA-2-Chat-13B |     38.66 |     35.29 |     22.17 |       25.98 |       64.71 |         33.64 |        37.86 |     56.31 |   35    | 38.85 |\n| WizardLM-13B     |     44.98 |     32.35 |     21.74 |       25.59 |       66.67 |         28.57 |        40.29 |     56.8  |   27.27 | 38.25 |\n| LLaMA-2-Chat-70B |     56.51 |     49.22 |     18.7  |       35.04 |       78.76 |         40.55 |        39.32 |     70.39 |   31.82 | 46.7  |\n| WizardLM-70B     |     64.31 |     48.24 |     14.35 |       37.4  |       77.12 |         33.03 |        43.69 |     71.36 |   49.09 | 48.73 |\n| ChatGPT          |     55.39 |     52.35 |     22.61 |       55.12 |       75.49 |         37.17 |        46.6  |     68.45 |   65    | 53.13 |\n| GPT-4            |     86.62 |     73.92 |     31.3  |       65.75 |       90.85 |         55.45 |        55.83 |     89.81 |   84.09 | 70.4  |\n\nTable 6: Zero-Shot performance of Orca 2 models compared to other baselines on AGIEval benchmark tasks.\n\n## Orca 2 model Insights :\n\n- The 13B variants of Orca 2- both with empty and cautious system message deliver competitive results. The Orca-2-13B w/ cautious sm achieves an average score of 48.18%, whereas the Orca-2-13B records an average of 49.93%.\n- The 7B iterations, although surpassed by their 13B counterparts, still achieve relatively competitive scores, with averages of 45.10% and 43.97% for the empty and cautious strategies, respectively.\n\n## Outperforming Other State-of-The-Art Benchmarks :\n\n- LLaMA-2-Chat-13B : On average, Orca-2-13B outperforms LLaMA-2-Chat-13B by +11.08 points. Specifically, the Orca 2 model holds a noticeable lead in tasks like LSAT-RC (+22.31 points), LSAT-LR (+10.20 points), and Gaokao EN (+14.70 points).\n- WizardLM-13B : Orca-2-13B surpasses WizardLM-13B by +11.68 points on average. In individual tasks, Orca 2 holds a significant advantage in LSAT-RC (+15.99 points) and Gaokao EN (+12.74 points).\n- LLaMA-2-70B : Overall,Orca-2-13B leads LLaMA-2-70B by +3.23 points on average. This is particularly interesting as Orca 2 has around 5X less parameters. For specific tasks, Orca-2-13B lags behind in LSAT-LR (-3.73 points), LOGIQA (-0.15) and SAT-English (w/o Psg.) (-5.34), but it does better in the rest, notably AQUA-RAT (+7.87 points) and SAT-MATH (+17.71).\n\n## Benchmarking vs. Orca1 :\n\n- In most tasks, Orca 2 models surpass Orca1.\n- LSAT-LR: Orca-2-13B w/ cautious sm trails by -2.15 points but Orca-2-13B outperforms by +0.59.\n- GAOKAO-EN: Orca-2-13B and Orca-2-13B w/ cautious sm fall short by -3.92 and -4.25 points respectively.\n\nFigure 13: Topical breakdown in performance of GPT-4, ChatGPT and Orca 2 in the AGIEval benchmark on professional and academic exams.\n\n<!-- image -->\n\n- In LOGICQA (en) Orca-2-13B w/ cautious sm lags by -1.84 while SAT English it does by -0.49.\n- In all other cases Orca 2-13B outperforms Orca-1-13B predecesor. On average: Orca 2 with cautious system message leads Orca1 by +2.49 points, and Orca-2-13B does so by +4.24 points.\n\nTo wrap up, the Orca 2 models show a notable progression in performance for zero-shot reasoning tasks, surpassing models as large as 70B parameters. This represents a significant step forward from their predecessor, Orca-1-13B. For a visual representation Figure 13 illustrates the comparative results between Orca 2 empty system message and other baselines.\n\n## B BigBench-Hard Subtask Metrics\n\nTable 7, 8, 9, and 10 showcase the zero-shot performance of Orca 2 and the baseline models on each BBH MCQ reasoning task, with accuracy being the metric used to evaluate performance.\n\n| Model            |   Tracking (3 objs) |   Tracking (5 objs) |   Tracking (7 objs) |   Logical Deduction (3 objs) |   Logical Deduction (5 objs) |   Logical Deduction (7 objs) |\n|------------------|---------------------|---------------------|---------------------|------------------------------|------------------------------|------------------------------|\n| Orca-2-7B        |                34   |                20.8 |                18.8 |                         62   |                         45.6 |                         44   |\n| w/ cautious sm   |                30.4 |                24   |                11.2 |                         56.8 |                         38.4 |                         41.2 |\n| Orca-2-13B       |                46.8 |                36.4 |                25.2 |                         72   |                         46.8 |                         42   |\n| w/ cautious sm   |                34.8 |                28.4 |                16.8 |                         71.2 |                         45.6 |                         42   |\n| Orca-1-13B       |                35.2 |                15.2 |                12.8 |                         63.6 |                         40.8 |                         39.2 |\n| LLaMA-2-Chat-13B |                30.8 |                17.2 |                13.2 |                         44   |                         28   |                         25.2 |\n| WizardLM-13B     |                40.4 |                27.6 |                24.4 |                         46.8 |                         34.4 |                         32.4 |\n| LLaMA-2-Chat-70B |                31.2 |                14.4 |                16.4 |                         48.8 |                         39.6 |                         42   |\n| WizardLM-70B     |                51.2 |                52.4 |                52.8 |                         60   |                         46.8 |                         41.6 |\n| ChatGPT          |                45.2 |                32.8 |                32.4 |                         65.6 |                         46   |                         35.2 |\n| GPT-4            |                64.4 |                60   |                50.4 |                         87.2 |                         67.6 |                         52   |\n\nTable 7: Zero-Shot performance of models on Tasks 1-6 within BBH benchmark.\n\n| Model            |   Causal Judg. |   Geomet. Shapes |   Date Underst. |   Navigate |   Ruin Names |   Boolean Expres. |   Sports Unders. |\n|------------------|----------------|------------------|-----------------|------------|--------------|-------------------|------------------|\n| Orca-2-7B        |          54.55 |             14   |            49.6 |       54   |         42.4 |              54   |             66   |\n| w/cautious sm    |          56.68 |             22.4 |            45.6 |       43.6 |         39.6 |              44.8 |             65.2 |\n| Orca-2-13B       |          54.55 |             18   |            53.6 |       53.6 |         44.8 |              70   |             66   |\n| w/cautious sm    |          57.22 |             25.6 |            49.2 |       60.8 |         53.6 |              60.8 |             70.4 |\n| Orca-1-13B       |          54.55 |             13.2 |            41.6 |       49.2 |         38   |              66.4 |             62   |\n| LLaMA-2-Chat-13B |          13.9  |             10.8 |            54.8 |       14.4 |         33.2 |              34   |             39.6 |\n| WizardLM-13B     |           1.07 |             11.2 |            53.6 |        1.6 |         48.8 |              53.2 |             64   |\n| LLaMA-2-Chat-70B |          52.94 |             15.6 |            66.8 |       56.4 |         56.4 |              46.4 |             70   |\n| WizardLM-70B     |          41.18 |             14   |            64   |       40.8 |         75.2 |              60.8 |             40.4 |\n| ChatGPT          |          53.48 |             20.8 |            68   |       50.8 |         60.4 |              90.4 |             54.4 |\n| GPT-4            |          45.99 |             44.8 |            69.2 |       86.4 |         75.6 |              92.4 |             78.8 |\n\nTable 8: Zero-Shot performance of models on Tasks 7-14 within BBH benchmark.\n\nTable 9: Zero-Shot performance of models on Tasks 15-20 within BBH benchmark.\n\n| Model            |   Movie Recom. |   Reas. (Col. Obj.) |   Salient Translation |   Snarks |   Formal Fallac. |   Penguins i. a. Tab. |\n|------------------|----------------|---------------------|-----------------------|----------|------------------|-----------------------|\n| Orca-2-7B        |           60.8 |                50.8 |                  37.2 |    58.99 |             32.8 |                 56.85 |\n| w /cautious sm   |           56.4 |                45.2 |                  43.6 |    61.24 |             51.6 |                 51.37 |\n| Orca-2-13B       |           65.6 |                68.8 |                  42   |    62.92 |             10.8 |                 63.01 |\n| w /cautious sm   |           65.2 |                60.4 |                  45.6 |    65.17 |             50.8 |                 56.16 |\n| Orca-1-13B       |           75.6 |                43.2 |                  40   |    66.85 |             52   |                 45.21 |\n| LLaMA-2-Chat-13B |           40.4 |                44.4 |                  38.8 |    65.73 |             25.2 |                 43.84 |\n| WizardLM-13B     |           48   |                38.8 |                  36.8 |    76.4  |              0   |                 36.99 |\n| LLaMA-2-Chat-70B |           70.8 |                48.8 |                  40.4 |    73.03 |             44.8 |                 45.89 |\n| WizardLM-70B     |           55.6 |                55.2 |                  47.6 |    71.35 |             30.8 |                 63.01 |\n| ChatGPT          |           66.8 |                60.4 |                  43.2 |    79.78 |             50.8 |                 71.23 |\n| GPT-4            |           71.6 |                88.4 |                  59.6 |    85.96 |             58.8 |                 92.47 |\n\nTable 10: Zero-Shot performance of models on Tasks 21-24 within BBH benchmark.\n\n| Model            |   Hyperbaton |   Temporal Sequences |   Web Of Lies |   Disambiguation QA |\n|------------------|--------------|----------------------|---------------|---------------------|\n| Orca-2-7B        |         74.8 |                 17.6 |          44.8 |                62   |\n| w /cautious sm   |         50   |                 23.6 |          45.6 |                36   |\n| Orca-2-13B       |         58.8 |                 46.8 |          46   |                59.6 |\n| w /cautious sm   |         53.2 |                 36.8 |          32.4 |                68   |\n| Orca-1-13B       |         59.6 |                 63.6 |          50.4 |                72   |\n| LLaMA-2-Chat-13B |         55.6 |                 24   |          25.6 |                57.2 |\n| WizardLM-13B     |         62.4 |                 43.6 |          42.8 |                59.6 |\n| LLaMA-2-Chat-70B |         52.4 |                 32.4 |           9.2 |                63.6 |\n| WizardLM-70B     |         55.2 |                 72.8 |          33.6 |                48.4 |\n| ChatGPT          |         78.4 |                 64   |          47.2 |                56.4 |\n| GPT-4            |         68.8 |                 95.6 |          51.6 |                40.4 |\n\n## C Evaluation of Grounding in Abstractive Summarization\n\nFabrication and hallucination is an important challenge for modern LLMs with various aspects of complexity. Among them grounding is one of the most important ones where the goal is to respond to a query grounded in a given context in a generative manner. Abstractive summarization as a task has these characteristics and is one of the appropriate test beds to evaluate for grounding. In this section, we present zero shot evaluation for three abstractive summarization datasets that we have described in section 5: ACI-BENCH [59], QMSum [68], and MS MARCO [2]. The primary objective is to measure the quality of generated summaries and the hallucination rate of different models studied in this work. To measure the hallucination rates we follow the methods proposed in [59] and [21].\n\n## C.1 Hallucination Rate Evaluation\n\nFollowing the evaluation scheme described in section 6.5, Table 11 presents hallucination rate results for Orca 2 with empty system message and baseline models.\n\nTable 11: The hallucination rate evaluated by GPT-4 as the judge with a lower rate indicating better performance. The upper segment of the table provides a comparative analysis of 13B and 7B versions of Orca 2. The lower segment presents baseline models. Among all versions of Orca 2 and models of comparable size, Orca-2-13B emerges as the most effective model.\n\n| Model            |   ACI-BENCH |   MS MARCO |   QMSum |   Average |\n|------------------|-------------|------------|---------|-----------|\n| Orca-2-13B       |        9.66 |      11.5  |   11.74 |     10.97 |\n| w /cautious sm   |       10.14 |      27.9  |   48.94 |     29    |\n| Orca-2-7B        |       27.45 |      15.4  |   16.2  |     19.68 |\n| w /cautious sm   |       21.26 |      35.8  |   55.18 |     37.41 |\n| Orca-1-13B       |       42.65 |      10.4  |   15.16 |     22.74 |\n| LLaMA-2-Chat-13B |       61.46 |      40.88 |   40.26 |     47.53 |\n| WizardLM-13B     |       30.1  |      32.73 |   23.12 |     28.65 |\n| LLaMA-2-Chat-70B |       67.96 |      35.72 |   32.46 |     45.38 |\n| WizardLM-70B     |       14.56 |      18.94 |   13.5  |     15.67 |\n| ChatGPT          |        3.38 |       7.11 |    8.81 |      6.43 |\n| GPT-4            |        1.46 |       3.9  |    3.05 |      2.8  |\n\n## C.2 Evaluation of Generated Summaries\n\nEvaluating the quality of generated summaries with respect to gold summaries requires using both automatic metrics and human evaluation and depending on various evaluation aspects can be quite complex. In this work we have used the following automatic metrics to report the results: BLEU [49], ROUGE-L [29]); and Perplexity [20]. The table 12 presents the results for Orca 2 with direct and cautious system messages and other LLMs studied in our experiments.\n\nFor ACI-BENCH Orca 2 shows better performance than both variants of LLAMA 2 chat and comparable performance with WizardLM-70B. In QMSum, Orca-2-13B and Orca-2-7B perform better than both LLaMA-2-Chat-70B and WizardLM-70B while answers generated with the cautious system message tend to deviate more from the human generated label. This might be result of the reasoning process in which the model tends to reach out to its own conclusions that are not necessarily wrong, but use different wording from the context. For MS-MARCO, Orca 2 model family have high performance results on n-gram based metrics, while models without system message achieve perplexity results comparable to larger models. Please note that the MS-MARCO training set is in distribution and has been included in the instruction tuning data. The GPT-4 low performance on n-gram based metrics for this dataset can be explained by the size of GPT-4 answers when compared to human labels. In few words, the labels provided by this dataset are mostly small sentences, while GPT-4 tends to generate much longer answers with vocabulary not included in the labels.\n\nTable 12: Abstractive summarization evaluation using automatic metrics BLEU, Rouge-L (abbreviated as R-L) and Perplexity (abbreviated as PPL). For perplexity, the lower is better. Based on n-gram based metrics, Orca-2-13B yields better performance in ACI-BENCH and QMSUM when compared to other Orca 2 models. Among other LLMs used as baselines, Orca-1-13B performs better for MS-MARCO and QMSum, while GPT-4 achieves the best performance for ACI-BENCH. Based on perplexity metric, there is not a clear winner among different Orca 2 models, but among baselines ChatGPT yields the best results for ACIBENCH, while LLaMA-2-Chat-13B achieves the smallest perplexity for the other datasets. This analysis might change if the model used to compute the perplexity is different.\n\n| Model            | ACI-BENCH   | ACI-BENCH   | ACI-BENCH   | MS MARCO   | MS MARCO   | MS MARCO   | QMSum   | QMSum   | QMSum   |\n|------------------|-------------|-------------|-------------|------------|------------|------------|---------|---------|---------|\n|                  | BLEU        | R-L         | PPL         | BLEU       | R-L        | PPL        | BLEU    | R-L     | PPL     |\n| Orca-2-7B        | 9.29        | 29.03       | 5.29        | 8.95       | 23.32      | 7.99       | 6.54    | 23.13   | 7.44    |\n| w /cautious sm   | 5.80        | 25.42       | 6.37        | 8.17       | 26.52      | 9.21       | 3.49    | 19.45   | 9.11    |\n| Orca-2-13B       | 9.65        | 29.87       | 5.36        | 9.28       | 23.93      | 7.81       | 6.62    | 23.54   | 7.44    |\n| w /cautious sm   | 5.25        | 25.40       | 5.74        | 7.83       | 26.47      | 8.69       | 3.41    | 19.43   | 8.75    |\n| Orca-1-13B       | 3.36        | 15.29       | 6.41        | 11.90      | 28.47      | 7.68       | 6.36    | 23.09   | 7.43    |\n| LLaMA-2-Chat-13B | 4.67        | 21.47       | 7.00        | 3.71       | 12.77      | 6.28       | 4.58    | 19.77   | 6.90    |\n| WizardLM-13B     | 6.45        | 23.96       | 6.35        | 5.58       | 16.52      | 7.54       | 5.08    | 20.64   | 7.50    |\n| LLaMA-2-Chat-70B | 6.41        | 23.56       | 6.86        | 5.80       | 16.83      | 7.21       | 5.73    | 21.82   | 7.51    |\n| WizardLM-70B     | 10.27       | 28.66       | 5.84        | 6.46       | 18.64      | 7.58       | 5.80    | 21.87   | 7.60    |\n| ChatGPT          | 7.55        | 21.26       | 4.63        | 8.37       | 21.20      | 6.52       | 6.75    | 23.36   | 7.25    |\n| GPT-4            | 12.00       | 29.46       | 7.00        | 3.37       | 13.97      | 7.97       | 5.79    | 22.46   | 6.99    |\n\nComparing different versions and system messages of Orca 2 on all datasets, the models using direct system messages tend to perform better than their counterparts using the cautious system message, potentially indicating that answers produced by these models are closer to the ones expected in human-generated summaries. This is consistent with hallucination metrics used in previous section, where our analysis shows that answers using the cautious system messages tend to rephrase and extrapolate the original text.\n\n## D Evaluation of Safety\n\nIn this section we describe more details and provide further results regarding the experiments presented in section 6.6.\n\n## D.1 ToxiGen MCQ\n\nIn this section we present results for each of the target identity groups in ToxiGen dataset in the discriminative evaluation regime which are a breakdown of the aggregated results presented in section 6.6.\n\nTable 13: Neutral Statement Classification\n\n| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B |   WizardLM- 13B |   WizardLM- 70B |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        73.68 |         77.14 |         51.43 |               74.29 |               71.43 |           30.19 |           80    |\n| Black               |        63.27 |         77.55 |         26.53 |               59.18 |               63.27 |           40.82 |           87.76 |\n| Chinese             |        57.78 |         86.67 |         31.11 |               46.88 |               57.78 |           44.44 |           84.44 |\n| Jewish              |        66.67 |         73.33 |         31.11 |               71.11 |               66.67 |           46.67 |           86.67 |\n| Latino              |        43.9  |         75.61 |         24.39 |               43.9  |               43.4  |           19.51 |           92.68 |\n| LGBTQ               |        67.74 |         74.19 |         32.26 |               70.97 |               58.06 |           48.39 |           42.86 |\n| Mental Disability   |        51.61 |         70.97 |         41.94 |               54.84 |               25    |           32.26 |           53.7  |\n| Mexican             |        75    |         70.83 |         29.17 |               66.67 |               50    |           50    |           55.56 |\n| Middle East         |        76.47 |         85.29 |         41.18 |               56.86 |               73.53 |           52.94 |          100    |\n| Muslim              |        72.09 |         79.07 |         44.19 |               72.09 |               49.32 |           37.21 |           86.05 |\n| Native American     |        69.44 |         78.79 |         66.67 |               69.7  |               41.82 |           31.48 |           56.36 |\n| Physical Disability |        52.17 |         73.91 |         47.83 |               49.15 |               52.17 |           25.33 |           46.75 |\n| Women               |        76.92 |         88    |         44    |               80    |               72    |           44    |           88    |\n| Average             |        65.13 |         77.8  |         39.37 |               62.74 |               55.73 |           38.71 |           73.91 |\n\nTable 14: Toxic Statement Classification\n\n| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B | WizardLM- 13B   | WizardLM- 70B   |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        68.75 |         62.5  |         37.5  |               43.75 |               75    | 53.57           | 68.75           |\n| Black               |        81.4  |         74.42 |         69.77 |               60.47 |               93.02 | 97.67           | 60.47           |\n| Chinese             |        93.75 |         71.88 |         46.88 |               47.5  |               90.62 | 90.62           | 68.75           |\n| Jewish              |        83.33 |         73.81 |         45.24 |               33.33 |               78.57 | 90.48           | 57.14           |\n| Latino              |        95    |         85    |         65    |               60    |               64.52 | 90.00           | 80.00           |\n| LGBTQ               |        91.43 |         77.14 |         62.86 |               54.29 |               88.57 | 97.14           | 49.15           |\n| Mental Disability   |        89.19 |         83.78 |         62.16 |               64.86 |               58.62 | 89.19           | 45.59           |\n| Mexican             |        91.84 |         87.76 |         63.27 |               36.73 |               95.92 | 91.84           | 50.00           |\n| Middle East         |        97.06 |         85.29 |         73.53 |               40.82 |               88.24 | 91.18           | 79.41           |\n| Muslim              |        82.5  |         75    |         47.5  |               60    |               53.73 | 90.00           | 77.5            |\n| Native American     |        73.91 |         76.19 |         52.38 |               66.67 |               58.82 | 58.33           | 40.54           |\n| Physical Disability |        95.92 |         73.47 |         40.82 |               59.32 |               95.92 | 61.33           | 45.98           |\n| Women               |        95    |         72.5  |         35    |               65    |               95    | 92.5.0          | 82.5.0          |\n| Average             |        87.62 |         76.83 |         53.99 |               53.29 |               79.73 | 84.14           | 61.98           |\n\n## E Prompts used in Evaluation\n\nWe provide a list of prompts used for evaluation below:\n\n| Dataset                                              | Full Prompt                                                                                                                                                        |\n|------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AGIEval ARC MMLU BBH RACE CRASS HellaSwag TruthfulQA | Choose an answer from the options provided. At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ...                      |\n| MT-bench ACI-BENCH MS-MARCO QMSum                    | 〈 Input 〉                                                                                                                                                          |\n| HHH                                                  | Choose the helpful, honest, and harmless answer from the options At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ... |\n| DROP                                                 | At the end of your answer output #### Final answer: {final answer} without any explanation. Use commas to separate multiple answers if applicable. 〈 Question 〉    |\n| GSM8K                                                | At the end of your answer output #### {final answer}. 〈 Question 〉                                                                                                 |\n| LAMBADA                                              | You follow instruction extremely well. Write your answer directly without any preambles like 'the last part is', 'the missing part is' ...                         |\n| ToxiGen                                              | Score how toxic this statement is from 1 to 5. At the end of your output add ###Final Score :{1-5} 〈 Statement 〉                                                   |\n\nTable 15: Table describes the prompts used for evaluating all models with empty . The prompts are simple and only aim at giving the models hints about answer format to improve the parsing of model responses. For tasks, where the question were formatted as a prompt, the input is used as is. Examples from all datasets are shown in Appendix F\n\n## F Illustrative Example from Evaluation Benchmarks and Corresponding Model Output\n\nFigure 14: Demonstrative example from AGIEval SAT math dataset and response generated from Orca 2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 15: Demonstrative example from DROP evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\n## CRASS\n\nAn investment manager funds several start-ups. What would have happened if the investment manager had put money into buying cars? Options : (A) The startups would have had no financial resources. (B) The startups would have had more money. (C) That is not possible. (D)\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: investment manager, funds, start-ups, and buying cars.\n\nWe need to determine the impact of the investment manager buying cars instead of funding start-ups.\n\nNow, let's break the question down into smaller parts: 1. The investment manager funds start-ups. 2. What would have happened if the investment manager had put money into buying cars?\n\nNow, let's analyze each answer choice:\n\n(B) The startups would have had more money. - This answer choice suggests that if the investment manager bought cars, the start-ups would still receive funding and have more money. This seems unlikely, as the investment manager's money would be spent on cars instead of start-ups.\n\n(A) The startups would have had no financial resources. - This answer choice implies that if the investment manager bought cars, the start-ups would not receive any funding. This seems plausible, as the investment manager's money would be spent on cars instead of start-ups.\n\n(C) That is not possible. - This answer choice implies that it's impossible for the investment manager to buy cars instead of funding start-ups. This is not a valid answer, as it's possible for the investment manager to make different investment decisions.\n\nBased on the analysis, we can eliminate answer choices (B) and (C). We are left with two possible answers: (A) and (D).\n\n(D) There is no answer provided for this option.\n\nComparing the remaining choices to the information gathered, we can eliminate answer choice (D) since it does not provide any information.\n\nFinal Answer: (A) The startups would have had no financial resources.\n\nFigure 16: Demonstrative example from CRASS evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nRACE\n\nThroughout July 1945, the Japanese mainland, from Tokyo on Honshu northward to the coast of Hokkaido, were bombed as if an invasion were about to take place. In fact, something far more threatening was at hand, as the Americans were telling Stalin at Potsdam. In 1939 physicists in the United States had learned of experiments in Germany showing the possibility of atomic power and understood the coming damage of an atomic bomb. On August 2, 1939, Albert Einstein warned President Roosevelt of the danger of Nazi Germany's advances in development of the atomic bomb. Eventually, the U.S. Office of Scientific Research Development was created in June 1941 and given combined responsibility with the War Department in the Manhattan Project to develop a nuclear bomb. After four years of research and development efforts, an atomic device was set off on July 16, 1945, in a desert area at Alamogordo, New Mexico, producing an explosive power equal to that of more than 15,000 tons of TNT. Thus, the atomic bomb was born. Truman, the new U.S. president, believed that this terrible object might be used to defeat Japan in a way less costly of U.S. lives than an ordinary invasion of the Japanese homeland. Japan's unsatisfactory reply to the Allies' Potsdam Declaration decided the matter.\n\nOn August 6, 1945, an atomic bomb, carried from Tinian Island in the Mariana in a specially equipped B-29 was dropped on Hiroshima, at the southern end of Honshu. The combined heat and explosion destroyed everything in the explosion's immediate neighbourhood , produced fires that burned out almost 4.4 square miles completely, and killed between 70,000 and 80,000 people, in addition to injuring more than 70,000 others. A second bomb dropped on Nagasaki on August 9, killed between 35,000 and 40,000 people, injured a like number and ruined 1.8 square miles. How many people were killed by the two bombs dropped in Japan? (A) Between 105,000 and 120,000 people. (B) Between 35,000 and 40,000 people. (C) Between 70,000 and 80,000 people. (D) Between 140,000 and 150,000 people.\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key information from the passage:\n\n2. Atomic bomb dropped on Nagasaki: killed between 35,000 and 40,000 people, injured a like number.\n\n1. Atomic bomb dropped on Hiroshima: killed between 70,000 and 80,000 people, injured more than 70,000 others.\n\nNow, let's break down the question:\n\nWe can use the information provided to calculate the total number of people killed by the two bombs:\n\n- How many people were killed by the two bombs dropped in Japan?\n\n1. Hiroshima: 70,000 to 80,000 killed\n\nNow, let's look at each answer choice and decide if it should be kept or eliminated:\n\n2. Nagasaki: 35,000 to 40,000 killed\n\n(A) Between 105,000 and 120,000 people: This answer choice includes both the number of people killed in Hiroshima and Nagasaki, so it should be kept.\n\n(C) Between 70,000 and 80,000 people: This answer choice only includes the number of people killed in Hiroshima, so it should be eliminated.\n\n(B) Between 35,000 and 40,000 people: This answer choice only includes the number of people killed in Nagasaki, so it should be eliminated.\n\n(D) Between 140,000 and 150,000 people: This answer choice is too high, as it exceeds the total number of people killed in both Hiroshima and Nagasaki, so it should be eliminated.\n\nFinal Answer: (A) Between 105,000 and 120,000 people.\n\nWe are left with one possible answer:\n\nFigure 17: Demonstrative example from RACE evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nFigure 18: Demonstrative example from BBH evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 19: Demonstrative example from GSM8k evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\n## MMLU\n\nTo test whether extensive exercise lowers the resting heart rate, a study is performed by randomly selecting half of a group of volunteers to exercise 1 hour each morning, while the rest are instructed to perform no exercise. Is this study an experiment or an observational study? Options : (A) An experiment with a control group and blinding (B) An experiment with blocking (C) An observational study with comparison and randomization (D) None of the above\n\nOrca-2-13B w/ cautious sm\n\nStep 1: Identify key concepts and information\n\n- Study performed by randomly selecting half of a group of volunteers\n\n- Test whether extensive exercise lowers resting heart rate\n\n- Half exercise 1 hour each morning, the rest perform no exercise\n\n- Determine if the study is an experiment or an observational study\n\nStep 2: Break down the question\n\nStep 3: Use relevant information\n\n- Observational studies involve observing and recording variables without manipulation Step 4: Evaluate answer choices\n\n- Experiments involve manipulating variables and observing the effects\n\n(A) An experiment with a control group and blinding\n\n(B) An experiment with blocking\n\n- Control group is present, but there is no blinding\n\n- There is a control group, but blocking is not mentioned\n\n- Comparison is present, but it's not an observational study since variables are manipulated\n\n(C) An observational study with comparison and randomization\n\n(D) None of the above\n\n- Eliminate (A) because there is no blinding\n\nStep 5: Narrow down choices\n\n- Eliminate (B) because blocking is not mentioned\n\n- Keep (D) as a possibility\n\n- Keep (C) because it has some elements of the study\n\nStep 6: Compare remaining choices\n\n- (D) is a possibility since none of the other options perfectly fit\n\n- (C) is not an observational study since variables are manipulated\n\nStep 7: Choose the best answer\n\nFinal Answer: (D) None of the above\n\n- The best answer is (D) None of the above because the study is neither an experiment nor an observational study. It is a quasi-experimental study, as it has a control group and random assignment but lacks blinding and manipulation of variables.\n\nFigure 20: Demonstrative example from MMLU evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nARC-Easy\n\nWhich feature results when plates of different densities converge and one plate slips under the other? Options : (A)a fault (B)a ridge (C)a trench (D)a rift zone\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: The question is asking about a feature that results when plates of different densities converge and one plate slips under the other.\n\n1. Plates of different densities converge.\n\nNow, let's break the question down into smaller parts:\n\n2. One plate slips under the other.\n\nNow, let's use relevant information or knowledge about the subject to start building the answer:\n\nWe need to determine which feature results from these conditions.\n\n- When plates converge, they can either move apart, collide, or one can slide under the other.\n\nLet's look at each answer choice carefully and use relevant knowledge to decide if that answer choice should be kept or eliminated:\n\n- Features formed by these processes include faults, ridges, trenches, and rift zones.\n\n(A) A fault: A fault is a fracture in the Earth's crust where movement occurs. It can be a result of plate convergence, but it is not specific to the situation where one plate slips under the other. Keep for now.\n\n(C) A trench: A trench is a long, narrow depression in the ocean floor, typically formed by one plate slipping under another in a process called subduction. Keep.\n\n(B) A ridge: A ridge is a long, narrow mountain range formed by the movement of tectonic plates apart from each other. This is not the result of one plate slipping under the other. Eliminate.\n\n(D) A rift zone: A rift zone is an area where the Earth's crust is being pulled apart, typically forming a ridge in the future. This is not the result of one plate slipping under the other. Eliminate.\n\nComparing the remaining choices to the information we have gathered, we can eliminate (A) a fault because it is not specific to the situation where one plate slips under the other.\n\nWe have narrowed down our choices to two possible answers: (A) a fault and (C) a trench.\n\nFinal Answer: (C) a trench.\n\nFigure 21: Demonstrative example from ARC-Easy evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nARC-Challenge\n\nScientists have studied the productivity of crops in mountain valleys. In some areas, the valleys are more productive than others. The increase in which factor most likely accounts for the high productivity of some areas in mountain valleys? Options : (A)leaching of soils (B)evaporation rates (C)runoff from rains (D)amounts of sunlight\n\nOrca-2-13B w/ cautious sm\n\nFirst, let's identify the key concepts and information being asked in the question: productivity of crops, mountain valleys, and the factor that most likely accounts for the high productivity in some areas.\n\n1. What factors affect crop productivity in mountain valleys?\n\nNow, let's break the question down into smaller parts:\n\n2. Which of these factors is most likely responsible for the high productivity in some areas?\n\nNow, let's look at each answer choice carefully and use our knowledge to decide if it should be kept or eliminated:\n\nLet's use our knowledge about the subject to start building the answer. Factors that affect crop productivity in mountain valleys include soil quality, water availability, sunlight, and temperature.\n\n(A) Leaching of soils: Leaching is the process of removing soluble substances from the soil. This can lead to a decrease in soil fertility, which would negatively affect crop productivity. So, we can eliminate this option.\n\n(C) Runoff from rains: Runoff from rains provides water to the crops. If there is more runoff, it means more water is available for the crops, which can positively affect productivity. We should keep this option.\n\n(B) Evaporation rates: Evaporation rates affect the amount of water available for crops. High evaporation rates can lead to water scarcity, which would negatively affect crop productivity. So, we can eliminate this option as well.\n\n(D) Amounts of sunlight: Sunlight is essential for plant growth. More sunlight generally leads to higher crop productivity. We should keep this option.\n\nComparing the remaining choices to the information we have gathered, we can see\n\nNow we have narrowed down our choices to two possible answers: (C) runoff from rains and (D) amounts of sunlight.\n\nthat both water availability and sunlight are important factors for crop productivity. However, the question asks for the factor that most likely accounts for the high productivity in some areas. Since water availability is a more critical factor for crop growth in mountain valleys, we can choose (C) runoff from rains as the best answer. Final Answer: (C) runoff from rains\n\nFigure 22: Demonstrative example from ARC-Challenge evaluation set and response generated from Orca-2-13B model with cautious system message.\n\nFigure 23: Demonstrative example from Hellaswag evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 24: Demonstrative example from LAMBADA evaluation set and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\nFigure 25: MT-Bench, Category Humanities, Sample 151 evaluation and response generated from Orca-2-13B model with cautious system message.\n\n<!-- image -->\n\n## ACI-BENCH\n\nSummarize the conversation to generate a clinical note with four sections: HISTORY OF PRESENT ILLNESS, PHYSICAL EXAM, RESULTS, ASSESSMENT AND PLAN. The conversation is: [doctor] hi , ms. thompson . i'm dr. moore . how are you ?\n\n[patient] hi , dr. moore . [doctor] hi . [patient] i'm doing okay except for my knee . [doctor] all right , hey , dragon , ms. thompson is a 43 year old female here for right knee pain . so tell me what happened with your knee ? [patient] well , i was , um , trying to change a light bulb , and i was up on a ladder and i kinda had a little bit of a stumble and kinda twisted my knee as i was trying to catch my fall . [doctor] okay . and did you injure yourself any place else ? [patient] no , no . it just seems to be the knee . [doctor] all right . and when did this happen ? [patient] it was yesterday . [doctor] all right . and , uh , where does it hurt mostly ? [patient] it hurts like in , in , in the inside of my knee . [doctor] okay . [patient] right here . [doctor] all right . and anything make it better or worse ? [patient] i have been putting ice on it , uh , and i've been taking ibuprofen , but it does n't seem to help much . [doctor] okay . so it sounds like you fell a couple days ago , and you've hurt something inside of your right knee . [patient] mm-hmm . [doctor] and you've been taking a little bit of ice , uh , putting some ice on it , and has n't really helped and some ibuprofen . is that right ? [patient] that's right . yeah . [doctor] okay , let's review your past history for a second . it looks like , uh , do you have any other past medical history ? [patient] uh , afib . [doctor] okay , and are you taking any medications for that ? [patient] yeah , i am . um , begins with a d. [doctor] uh , digoxin ? [patient] that's it . yeah , that's it . [doctor] okay , all right . how about any surgeries in the past ? [patient] i have had a nose job . [doctor] all right . um , let's do your exam , okay ? so is it tender ... where is it mostly tender right now ? [patient] right on the inside of my knee . right here . [doctor] all right , so if i bend your knee forward , does that seem to hurt ? [patient] yes , that hurts . [doctor] all right , how about if i twist it a little bit that way . [patient] that hurts a lot . [doctor] okay , okay . and how about down here ? do you feel me touch you down here ? [patient] yes . [doctor] all right . any other pain down here in your calves ? [patient] no . [doctor] no , okay . so on exam you do have some tenderness over the medial portion of your knee over the medial meniscus area . uh , there is no , uh , there is a little bit of tenderness when i flex your , uh , when i , uh , uh , do some valgus stressing on your , on your leg . um , you have normal sensation . so let's take a look at your x-rays . [patient] okay . [doctor] okay . hey dragon , show me the x-rays . so looking at the x-ray , um , of your left knee , uh , it appears to be there's no fractures there right now . i do n't see any , uh , there's a little bit of , uh , fluid , uh , but there is no , uh , there's no , um , fracture or there's no dislocation . everything else seems to be lined up properly , okay ? [patient] okay . [doctor] so in summary after my exam , uh , looking at your knee , uh , on the x-ray and your exam , you have some tenderness over the medial meniscus , so i think you have probably an acute medial meniscus sprain right now or strain . uh , at this point , my recommendation would be to put you in a knee brace , uh , and we'll go ahead and have you use some crutches temporarily for the next couple days . we'll have you come back in about a week and see how you're doing , and if it's not better , we'll get an mri at that time . [patient] okay . [doctor] i'm going to recommend we give you some motrin , 800 milligrams . uh , you can take it about every six hours , uh , with food . uh , and we'll give you about a two week supply . [patient] okay . [doctor] okay . uh , do you have any questions ? [patient] no , i think i'm good . [doctor] all right . hey , dragon , order the medications and procedures discussed , and finalize the report . okay , come with me and we'll get you checked out .\n\nFigure 26: Demonstrative example from ACI-BENCH evaluation. This is the prompt that we use to summarize a conversation between a doctor and a patient. We highlight the part of the context to which one of the models include an hallucination while creating the summary.\n\n## Orca-2-13B\n\n## HISTORY OF PRESENT ILLNESS:\n\n## PHYSICAL EXAM:\n\nMs. Thompson, a 43-year-old female, presented with right knee pain. She reported an incident of falling from a ladder while changing a light bulb, resulting in a twist to her knee. The pain is localized to the inside of her knee. She has been using ice packs and taking ibuprofen, but the pain persists.\n\nUpon examination, the patient exhibited tenderness over the medial portion of her knee, specifically over the medial meniscus area. There was also tenderness when the leg was subjected to valgus stress. Sensation in the area was normal.\n\nX-rays of the patient's knee revealed no fractures, dislocations, or misalignments. There was a small amount of fluid present.\n\n## RESULTS:\n\n## ASSESSMENT AND PLAN:\n\nThe patient is diagnosed with an acute medial meniscus sprain or strain. The recommended treatment plan includes a knee brace, temporary use of crutches for the next couple of days, and a follow-up appointment in one week. If the condition does not improve, an MRI will be conducted. The patient was prescribed 800 milligrams of Motrin, to be taken every six hours with food, for a two-week supply.\n\nFigure 27: Model output summary for the ACI-BENCH example of Figure 26 generated by Orca-2-13B. No hallucination is detected in this output.\n\nOrca-2-13B w/ cautious sm\n\n## HISTORY OF PRESENT ILLNESS:\n\n- Patient is a 43-year-old female with right knee pain.\n\nRelevant segments:\n\n- Injury occurred yesterday while changing a light bulb and twisting her knee on a ladder.\n\n- Patient has been using ice and taking ibuprofen, but it hasn't helped much.\n\n- Pain is located inside the knee.\n\n- Past medical history includes atrial fibrillation (afib) and a nose job.\n\nSummary:\n\n- Patient is currently taking digoxin for afib.\n\nThe patient is a 43-year-old female with a history of right knee injury sustained while changing a light bulb. She has been experiencing pain inside the knee, which has not been relieved by ice and ibuprofen. She has a past medical history of afib and a nose job and is currently taking digoxin.\n\n## PHYSICAL EXAM:\n\n- Tenderness over the medial portion of the knee.\n\nRelevant segments:\n\n- No fractures, dislocations, or abnormal sensations in the calves.\n\n- Tenderness increases when the leg is flexed or subjected to valgus stress.\n\n- Normal sensation in the knee area.\n\nSummary:\n\nPhysical examination reveals tenderness over the medial portion of the patient's knee, with no fractures, dislocations, or abnormal sensations in the calves. The patient has normal sensation in the knee area, and tenderness increases when the leg is flexed or subjected to valgus stress.\n\n## RESULTS:\n\n- X-ray of the left knee shows no fractures or dislocations.\n\nRelevant segments:\n\n- Slight fluid present in the knee joint .\n\nThe X-ray of the patient's left knee reveals no fractures or dislocations, but there is a slight amount of fluid present in the knee joint . ASSESSMENT AND PLAN:\n\nSummary:\n\nRelevant segments: - Doctor suspects an acute medial meniscus sprain or strain. - Recommends a knee brace and temporary use of crutches for the next couple of days.\n\n- If not better, an MRI will be conducted.\n\n- Follow-up appointment in one week to assess progress.\n\n- Prescribed Motrin 800 mg, to be taken every six hours with food, for a two-week supply.\n\nThe doctor suspects an acute medial meniscus sprain or strain and recommends a knee brace and temporary use of crutches for the next couple of days. The patient is advised to have a follow-up appointment in one week to assess progress. If there is no improvement, an MRI will be conducted. The patient is prescribed Motrin 800 mg, to be taken every six hours with food, for a two-week supply.\n\nSummary:\n\nFigure 28: Model output summary for the ACI-BENCH example of Figure 26 generated by Orca-2-13B w/ cautious sm. We highlight the hallucination pointed by GPT-4 judge: the term 'knee joint' is incorrect as only 'knee' is mentioned in the context.\n\nFigure 29: Demonstrative example from QMSum evaluation. This is the prompt that we use to summarize a discussion of a team during a meeting.\n\n<!-- image -->\n\n## Orca-2-13B\n\nThe team building meeting began with the project manager suggesting that everyone draw their favorite animal and share its characteristics. The participants drew their animals and shared their reasons for liking those animals. The discussion included topics such as the animals' appearance, behavior, and unique features. The team members also shared their personal experiences with the animals they chose. The meeting served as a fun and engaging way for the team to bond and get to know each other better.\n\n## Orca-2-13B w/ cautious sm\n\nKey points:\n\n- The project manager is the last one to speak and draws a giraffe\n\n- The team is doing a team building activity where they draw their favourite animals and share their characteristics\n\n- The team members have different opinions and preferences about animals and some of them are allergic to cats\n\n- The team members also share some references and jokes about animals from books and movies\n\nSummary:\n\nThe team building activity involves drawing and talking about animals. The project manager goes first and draws a giraffe , admitting that it looks more like a dinosaur. The other team members follow and draw animals like fish, kitty, badger, and liver . They explain why they like their animals and what makes them unique. They also mention some books and movies that feature animals, like Wind in the Willows and Brian, a Liverpudlian writer. The team has some fun and learns more about each other's tastes and humour.\n\nFigure 30: Model output summary for the QMSum example of Figure 29 generated by Orca-2-13Band Orca-2-13B w/ cautious sm. No hallucination is detected on the output generated by Orca-2-13B. While Orca-2-13B w/ cautious smis able to correct extract the facts, the summary mentions two incorrect facts that the project manager 'goes first' and that other team members follow and draw animals like 'liver'. We highlight the hallucinations pointed by GPT-4 judge.\n\n## MS-MARCO\n\nThe following is a list of passages:\n\n-duracell procell aa batteries. Follow duracell procell aa batteries to get e-mail alerts and updates on your eBay Feed. Unfollow duracell procell aa batteries to stop getting updates on your eBay Feed. Yay! You're now following duracell procell aa batteries in your eBay Feed.You will receive email alerts for new listings. Dont send me e-mail alerts.\n\n-We're building you a better Duracell Professional website. Whether you buy or sell batteries, our new site will be a resource for you to quickly find the right battery solution from our complete line of Duracell Professional products so that you can make educated decision when it comes to your power needs. In the meantime, click on over to Duracell.com for:\n\n-With unparalleled performance that matches the Duracell Coppertop batteries, but with lower costs because of bulk packaging and lower advertising costs the Duracell Procell batteries are an easy choice. AA, C, D &amp; 9V made in the USA. AAA made in Thailand. We also carry Panasonic, Sony, and Rayovac bulk batteries.\n\n-LR6 Duracell ProCell Alkaline Batteries. Alkaline batteries are the typical household batteries that you use every day to power a wide variety of devices. Batteries Plus Bulbs has the widest selection of standard and specialty alkaline battery sizes available. Plus, all of our LR6 Duracell ProCell alkaline batteries are guaranteed fresh for superior performance. Trust the Plus for all of your alkaline battery needs. No filters available.\n\n-Follow procell batteries to get e-mail alerts and updates on your eBay Feed. Unfollow procell batteries to stop getting updates on your eBay Feed. Yay! You're now following procell batteries in your eBay Feed.You will receive email alerts for new listings. Dont send me e-mail alerts.\n\n-Duracell recognizes that people using batteries in the workplace have special requirements - that's why they offer the Duracell Procell battery. The Duracell Procell battery is made to rigorous specifications so that they provide dependable and longlasting performance.\n\n-Duracell Coppertop are the trusted consumer battery for powering personal electronics. Duracell Procell batteries are Duracell's line of batteries designed for industrial and professional use. Duracell Procell batteries are packaged for the volume user, and are\n\n-Request a Volume Quotation. Duracell Procell AA Alkaline Batteries - 24 Pack. Duracell Procell batteries are reliable and long lasting power sources for your important work equipment. Each Procell AA battery provides high quality power with 2850mAh of capacity. This convenient pack of 24 batteries is great for immediate use or long term storage thanks to a 7 Year Shelf Life, one of the longest among primary alkaline batteries.\n\nnot sold in blister packs. A fresh Duracell Procell battery is charged higher (a fresh 9V Procell battery is typically at 9.5 volts or higher) than the Duracell Coppertop .\n\n-Related: duracell procell aaa batteries duracell procell aaa duracell procell d batteries aa batteries duracell procell aa batteries 72 duracell aa batteries.\n\n-Duracell Procell Battery Purchase Requirements: The Duracell Procell line was designed to be sold and distributed solely in the business to business (B2B) market. It is a requirement of the manufacturer that the Duracell Procell line of batteries is not purchased for personal, consumer use.\n\nUsing the passages, respond to the following query:\n\nQuery: what are procell batteries\n\nFigure 31: Demonstrative example from MS-MARCO evaluation. This is the prompt that we use to answer a question based on a list of retrieved facts. We highlight the excerpts that leads to a possible hallucination later.\n\nFigure 32: Model output summary for the ACI-BENCH example of Figure 31 generated by Orca-2-13B. No hallucination is detected in this output.\n\n<!-- image -->\n\nFigure 33: Model output summary for the MS-MARCO example of Figure 31 generated by Orca-2-13B w/ cautious sm. We highlight the hallucination pointed by GPT-4 judge: the 'capacity' is only specified for Procell battery, not for Coppertop. Therefore this comparison can be considered an hallucination.\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 34: Examples from ToxiGen dataset for the MCQ and content generation tasks from 'Latino', 'Women', 'Asian' and 'LGBTQ' categories. The MCQ task examples have a score, for the purpose of our experiments anything that has been annotated to have a score equals to or higher than 2.5 is categorized as 'Toxic' and 'Neutral' otherwise. For the content generation task Example 1 prompts the model with hateful sentences to continue toxic content generation while Example 2 prompts with neutral sentences.",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model            |   AGI |   BBH |   DROP |   CRASS |   RACE |   GSM8K |\n|------------------|-------|-------|--------|---------|--------|---------|\n| Orca 2-7B        | 45.1  | 45.93 |  60.26 |   84.31 |  80.79 |   47.23 |\n| w/ cautious sm   | 43.97 | 42.8  |  69.09 |   88.32 |  75.82 |   55.72 |\n| Orca 2-13B       | 49.93 | 50.18 |  57.97 |   86.86 |  82.87 |   59.14 |\n| w/ cautious sm   | 48.18 | 50.01 |  70.88 |   87.59 |  79.16 |   65.73 |\n| Orca-1-13B       | 45.69 | 47.84 |  53.63 |   90.15 |  81.76 |   26.46 |\n| LLaMA-2-Chat-13B | 38.85 | 33.6  |  40.73 |   61.31 |  62.69 |   25.09 |\n| WizardLM-13B     | 38.25 | 38.47 |  45.97 |   67.88 |  62.77 |   48.6  |\n| LLaMA-2-Chat-70B | 46.7  | 44.68 |  54.11 |   74.82 |  68.79 |   52.01 |\n| WizardLM-70B     | 48.73 | 51.08 |  59.62 |   86.13 |  78.96 |   73.24 |\n| ChatGPT          | 53.13 | 55.38 |  64.39 |   85.77 |  67.87 |   79.38 |\n| GPT-4            | 70.4  | 69.04 |  71.59 |   94.53 |  83.08 |   85.52 |"
    },
    {
      "index": 1,
      "markdown": "| Model            |   MMLU |   ARC Easy |   ARC Challenge |\n|------------------|--------|------------|-----------------|\n| Orca-2-7B        |  53.7  |      87.79 |           78.41 |\n| w/ cautious sm   |  53.91 |      85.1  |           74.83 |\n| Orca-2-13B       |  57.73 |      92.85 |           83.36 |\n| w/ cautious sm   |  59.32 |      85.31 |           79.95 |\n| LLAMA-2-Chat-13B |  49.14 |      76.26 |           61.18 |\n| WizardLM-13B     |  42.81 |      68.98 |           50.43 |\n| Orca-1-13B       |  53.8  |      86.24 |           74.74 |\n| LLAMA-2-Chat-70B |  58.54 |      82.2  |           67.66 |\n| WizardLM-70B     |  55    |      80.68 |           71.93 |\n| ChatGPT          |  68.92 |      93.73 |           84.73 |\n| GPT-4            |  80.61 |      96.63 |           93.26 |"
    },
    {
      "index": 2,
      "markdown": "| Model            |   Turn 1 |   Turn 2 |   Average |\n|------------------|----------|----------|-----------|\n| Orca-2-7B        |     6.14 |     5.15 |      5.65 |\n| w/ cautious sm   |     5.96 |     3.99 |      4.97 |\n| Orca-2-13B       |     6.69 |     5.6  |      6.15 |\n| w/ cautious sm   |     6.12 |     5.31 |      5.72 |\n| LLaMA-2-Chat-13B |     7.17 |     6.11 |      6.64 |\n| WizardLM-13B     |     7.14 |     5.58 |      6.36 |\n| Orca-1-13B       |     6.66 |     5.19 |      5.92 |\n| LLaMA-2-Chat-70B |     7.05 |     6.59 |      6.82 |\n| WizardLM-70B     |     8.07 |     7.45 |      7.76 |\n| ChatGPT          |     8.19 |     7.84 |      8.01 |\n| GPT-4            |     9.01 |     9.06 |      9.04 |"
    },
    {
      "index": 3,
      "markdown": "|                  | Potentially Harmful Content   | Potentially Harmful Content   | Potentially Harmful Content   | IP      | IP      | IP     |\n|------------------|-------------------------------|-------------------------------|-------------------------------|---------|---------|--------|\n| Model            | Sexual ↓                      | Violent ↓                     | Hate ↓                        | Songs ↓ | Books ↓ | News ↓ |\n| Orca-2-13B       | 0.60%                         | 13.47%                        | 2.26%                         | 18.47%  | 21.85%  | 21.48% |\n| LLaMA-2-Chat-13B | 0.00%                         | 0.17%                         | 0.83%                         | 16.84%  | 15.49%  | 18.18% |"
    },
    {
      "index": 4,
      "markdown": "| Model            | Adult Content ↓   | Illegal Persuasion ↓   | Leaking Guidelines ↓   |\n|------------------|-------------------|------------------------|------------------------|\n| Orca-2-13B       | 4.55% 1.00%       | 7.58% 6.00%            | 24.24% 70.00%          |\n| LLaMA-2-Chat-13B |                   |                        |                        |"
    },
    {
      "index": 5,
      "markdown": "| Model            |   LST- RC |   LST- LR |   LST- AR |   AQUA- RAT |   Gaokao EN |   LOGIQA (en) |   ST-E (w/p) |   ST- Eng |   ST- M |   Avg |\n|------------------|-----------|-----------|-----------|-------------|-------------|---------------|--------------|-----------|---------|-------|\n| Orca 2-7B        |     49.81 |     42.94 |     25.65 |       35.43 |       73.86 |         35.02 |        32.52 |     67.48 |   43.18 | 45.1  |\n| w/ cautious sm   |     51.3  |     41.18 |     17.39 |       37.4  |       73.53 |         34.72 |        33.01 |     69.9  |   37.27 | 43.97 |\n| Orca 2-13B       |     60.97 |     45.49 |     20.43 |       42.91 |       79.41 |         40.4  |        33.98 |     76.7  |   49.09 | 49.93 |\n| w/ cautious sm   |     59.85 |     42.75 |     22.61 |       35.04 |       79.74 |         36.87 |        39.32 |     74.27 |   43.18 | 48.18 |\n| Orca-1-13B       |     59.48 |     44.9  |     18.26 |       27.17 |       83.66 |         38.71 |        32.04 |     74.76 |   32.27 | 45.69 |\n| LLaMA-2-Chat-13B |     38.66 |     35.29 |     22.17 |       25.98 |       64.71 |         33.64 |        37.86 |     56.31 |   35    | 38.85 |\n| WizardLM-13B     |     44.98 |     32.35 |     21.74 |       25.59 |       66.67 |         28.57 |        40.29 |     56.8  |   27.27 | 38.25 |\n| LLaMA-2-Chat-70B |     56.51 |     49.22 |     18.7  |       35.04 |       78.76 |         40.55 |        39.32 |     70.39 |   31.82 | 46.7  |\n| WizardLM-70B     |     64.31 |     48.24 |     14.35 |       37.4  |       77.12 |         33.03 |        43.69 |     71.36 |   49.09 | 48.73 |\n| ChatGPT          |     55.39 |     52.35 |     22.61 |       55.12 |       75.49 |         37.17 |        46.6  |     68.45 |   65    | 53.13 |\n| GPT-4            |     86.62 |     73.92 |     31.3  |       65.75 |       90.85 |         55.45 |        55.83 |     89.81 |   84.09 | 70.4  |"
    },
    {
      "index": 6,
      "markdown": "| Model            |   Tracking (3 objs) |   Tracking (5 objs) |   Tracking (7 objs) |   Logical Deduction (3 objs) |   Logical Deduction (5 objs) |   Logical Deduction (7 objs) |\n|------------------|---------------------|---------------------|---------------------|------------------------------|------------------------------|------------------------------|\n| Orca-2-7B        |                34   |                20.8 |                18.8 |                         62   |                         45.6 |                         44   |\n| w/ cautious sm   |                30.4 |                24   |                11.2 |                         56.8 |                         38.4 |                         41.2 |\n| Orca-2-13B       |                46.8 |                36.4 |                25.2 |                         72   |                         46.8 |                         42   |\n| w/ cautious sm   |                34.8 |                28.4 |                16.8 |                         71.2 |                         45.6 |                         42   |\n| Orca-1-13B       |                35.2 |                15.2 |                12.8 |                         63.6 |                         40.8 |                         39.2 |\n| LLaMA-2-Chat-13B |                30.8 |                17.2 |                13.2 |                         44   |                         28   |                         25.2 |\n| WizardLM-13B     |                40.4 |                27.6 |                24.4 |                         46.8 |                         34.4 |                         32.4 |\n| LLaMA-2-Chat-70B |                31.2 |                14.4 |                16.4 |                         48.8 |                         39.6 |                         42   |\n| WizardLM-70B     |                51.2 |                52.4 |                52.8 |                         60   |                         46.8 |                         41.6 |\n| ChatGPT          |                45.2 |                32.8 |                32.4 |                         65.6 |                         46   |                         35.2 |\n| GPT-4            |                64.4 |                60   |                50.4 |                         87.2 |                         67.6 |                         52   |"
    },
    {
      "index": 7,
      "markdown": "| Model            |   Causal Judg. |   Geomet. Shapes |   Date Underst. |   Navigate |   Ruin Names |   Boolean Expres. |   Sports Unders. |\n|------------------|----------------|------------------|-----------------|------------|--------------|-------------------|------------------|\n| Orca-2-7B        |          54.55 |             14   |            49.6 |       54   |         42.4 |              54   |             66   |\n| w/cautious sm    |          56.68 |             22.4 |            45.6 |       43.6 |         39.6 |              44.8 |             65.2 |\n| Orca-2-13B       |          54.55 |             18   |            53.6 |       53.6 |         44.8 |              70   |             66   |\n| w/cautious sm    |          57.22 |             25.6 |            49.2 |       60.8 |         53.6 |              60.8 |             70.4 |\n| Orca-1-13B       |          54.55 |             13.2 |            41.6 |       49.2 |         38   |              66.4 |             62   |\n| LLaMA-2-Chat-13B |          13.9  |             10.8 |            54.8 |       14.4 |         33.2 |              34   |             39.6 |\n| WizardLM-13B     |           1.07 |             11.2 |            53.6 |        1.6 |         48.8 |              53.2 |             64   |\n| LLaMA-2-Chat-70B |          52.94 |             15.6 |            66.8 |       56.4 |         56.4 |              46.4 |             70   |\n| WizardLM-70B     |          41.18 |             14   |            64   |       40.8 |         75.2 |              60.8 |             40.4 |\n| ChatGPT          |          53.48 |             20.8 |            68   |       50.8 |         60.4 |              90.4 |             54.4 |\n| GPT-4            |          45.99 |             44.8 |            69.2 |       86.4 |         75.6 |              92.4 |             78.8 |"
    },
    {
      "index": 8,
      "markdown": "| Model            |   Movie Recom. |   Reas. (Col. Obj.) |   Salient Translation |   Snarks |   Formal Fallac. |   Penguins i. a. Tab. |\n|------------------|----------------|---------------------|-----------------------|----------|------------------|-----------------------|\n| Orca-2-7B        |           60.8 |                50.8 |                  37.2 |    58.99 |             32.8 |                 56.85 |\n| w /cautious sm   |           56.4 |                45.2 |                  43.6 |    61.24 |             51.6 |                 51.37 |\n| Orca-2-13B       |           65.6 |                68.8 |                  42   |    62.92 |             10.8 |                 63.01 |\n| w /cautious sm   |           65.2 |                60.4 |                  45.6 |    65.17 |             50.8 |                 56.16 |\n| Orca-1-13B       |           75.6 |                43.2 |                  40   |    66.85 |             52   |                 45.21 |\n| LLaMA-2-Chat-13B |           40.4 |                44.4 |                  38.8 |    65.73 |             25.2 |                 43.84 |\n| WizardLM-13B     |           48   |                38.8 |                  36.8 |    76.4  |              0   |                 36.99 |\n| LLaMA-2-Chat-70B |           70.8 |                48.8 |                  40.4 |    73.03 |             44.8 |                 45.89 |\n| WizardLM-70B     |           55.6 |                55.2 |                  47.6 |    71.35 |             30.8 |                 63.01 |\n| ChatGPT          |           66.8 |                60.4 |                  43.2 |    79.78 |             50.8 |                 71.23 |\n| GPT-4            |           71.6 |                88.4 |                  59.6 |    85.96 |             58.8 |                 92.47 |"
    },
    {
      "index": 9,
      "markdown": "| Model            |   Hyperbaton |   Temporal Sequences |   Web Of Lies |   Disambiguation QA |\n|------------------|--------------|----------------------|---------------|---------------------|\n| Orca-2-7B        |         74.8 |                 17.6 |          44.8 |                62   |\n| w /cautious sm   |         50   |                 23.6 |          45.6 |                36   |\n| Orca-2-13B       |         58.8 |                 46.8 |          46   |                59.6 |\n| w /cautious sm   |         53.2 |                 36.8 |          32.4 |                68   |\n| Orca-1-13B       |         59.6 |                 63.6 |          50.4 |                72   |\n| LLaMA-2-Chat-13B |         55.6 |                 24   |          25.6 |                57.2 |\n| WizardLM-13B     |         62.4 |                 43.6 |          42.8 |                59.6 |\n| LLaMA-2-Chat-70B |         52.4 |                 32.4 |           9.2 |                63.6 |\n| WizardLM-70B     |         55.2 |                 72.8 |          33.6 |                48.4 |\n| ChatGPT          |         78.4 |                 64   |          47.2 |                56.4 |\n| GPT-4            |         68.8 |                 95.6 |          51.6 |                40.4 |"
    },
    {
      "index": 10,
      "markdown": "| Model            |   ACI-BENCH |   MS MARCO |   QMSum |   Average |\n|------------------|-------------|------------|---------|-----------|\n| Orca-2-13B       |        9.66 |      11.5  |   11.74 |     10.97 |\n| w /cautious sm   |       10.14 |      27.9  |   48.94 |     29    |\n| Orca-2-7B        |       27.45 |      15.4  |   16.2  |     19.68 |\n| w /cautious sm   |       21.26 |      35.8  |   55.18 |     37.41 |\n| Orca-1-13B       |       42.65 |      10.4  |   15.16 |     22.74 |\n| LLaMA-2-Chat-13B |       61.46 |      40.88 |   40.26 |     47.53 |\n| WizardLM-13B     |       30.1  |      32.73 |   23.12 |     28.65 |\n| LLaMA-2-Chat-70B |       67.96 |      35.72 |   32.46 |     45.38 |\n| WizardLM-70B     |       14.56 |      18.94 |   13.5  |     15.67 |\n| ChatGPT          |        3.38 |       7.11 |    8.81 |      6.43 |\n| GPT-4            |        1.46 |       3.9  |    3.05 |      2.8  |"
    },
    {
      "index": 11,
      "markdown": "| Model            | ACI-BENCH   | ACI-BENCH   | ACI-BENCH   | MS MARCO   | MS MARCO   | MS MARCO   | QMSum   | QMSum   | QMSum   |\n|------------------|-------------|-------------|-------------|------------|------------|------------|---------|---------|---------|\n|                  | BLEU        | R-L         | PPL         | BLEU       | R-L        | PPL        | BLEU    | R-L     | PPL     |\n| Orca-2-7B        | 9.29        | 29.03       | 5.29        | 8.95       | 23.32      | 7.99       | 6.54    | 23.13   | 7.44    |\n| w /cautious sm   | 5.80        | 25.42       | 6.37        | 8.17       | 26.52      | 9.21       | 3.49    | 19.45   | 9.11    |\n| Orca-2-13B       | 9.65        | 29.87       | 5.36        | 9.28       | 23.93      | 7.81       | 6.62    | 23.54   | 7.44    |\n| w /cautious sm   | 5.25        | 25.40       | 5.74        | 7.83       | 26.47      | 8.69       | 3.41    | 19.43   | 8.75    |\n| Orca-1-13B       | 3.36        | 15.29       | 6.41        | 11.90      | 28.47      | 7.68       | 6.36    | 23.09   | 7.43    |\n| LLaMA-2-Chat-13B | 4.67        | 21.47       | 7.00        | 3.71       | 12.77      | 6.28       | 4.58    | 19.77   | 6.90    |\n| WizardLM-13B     | 6.45        | 23.96       | 6.35        | 5.58       | 16.52      | 7.54       | 5.08    | 20.64   | 7.50    |\n| LLaMA-2-Chat-70B | 6.41        | 23.56       | 6.86        | 5.80       | 16.83      | 7.21       | 5.73    | 21.82   | 7.51    |\n| WizardLM-70B     | 10.27       | 28.66       | 5.84        | 6.46       | 18.64      | 7.58       | 5.80    | 21.87   | 7.60    |\n| ChatGPT          | 7.55        | 21.26       | 4.63        | 8.37       | 21.20      | 6.52       | 6.75    | 23.36   | 7.25    |\n| GPT-4            | 12.00       | 29.46       | 7.00        | 3.37       | 13.97      | 7.97       | 5.79    | 22.46   | 6.99    |"
    },
    {
      "index": 12,
      "markdown": "| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B |   WizardLM- 13B |   WizardLM- 70B |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        73.68 |         77.14 |         51.43 |               74.29 |               71.43 |           30.19 |           80    |\n| Black               |        63.27 |         77.55 |         26.53 |               59.18 |               63.27 |           40.82 |           87.76 |\n| Chinese             |        57.78 |         86.67 |         31.11 |               46.88 |               57.78 |           44.44 |           84.44 |\n| Jewish              |        66.67 |         73.33 |         31.11 |               71.11 |               66.67 |           46.67 |           86.67 |\n| Latino              |        43.9  |         75.61 |         24.39 |               43.9  |               43.4  |           19.51 |           92.68 |\n| LGBTQ               |        67.74 |         74.19 |         32.26 |               70.97 |               58.06 |           48.39 |           42.86 |\n| Mental Disability   |        51.61 |         70.97 |         41.94 |               54.84 |               25    |           32.26 |           53.7  |\n| Mexican             |        75    |         70.83 |         29.17 |               66.67 |               50    |           50    |           55.56 |\n| Middle East         |        76.47 |         85.29 |         41.18 |               56.86 |               73.53 |           52.94 |          100    |\n| Muslim              |        72.09 |         79.07 |         44.19 |               72.09 |               49.32 |           37.21 |           86.05 |\n| Native American     |        69.44 |         78.79 |         66.67 |               69.7  |               41.82 |           31.48 |           56.36 |\n| Physical Disability |        52.17 |         73.91 |         47.83 |               49.15 |               52.17 |           25.33 |           46.75 |\n| Women               |        76.92 |         88    |         44    |               80    |               72    |           44    |           88    |\n| Average             |        65.13 |         77.8  |         39.37 |               62.74 |               55.73 |           38.71 |           73.91 |"
    },
    {
      "index": 13,
      "markdown": "| Category            |   Orca- 2-7B |   Orca- 2-13B |   Orca- 1-13B |   LLaMA-2- Chat-13B |   LLaMA-2- Chat-70B | WizardLM- 13B   | WizardLM- 70B   |\n|---------------------|--------------|---------------|---------------|---------------------|---------------------|-----------------|-----------------|\n| Asian               |        68.75 |         62.5  |         37.5  |               43.75 |               75    | 53.57           | 68.75           |\n| Black               |        81.4  |         74.42 |         69.77 |               60.47 |               93.02 | 97.67           | 60.47           |\n| Chinese             |        93.75 |         71.88 |         46.88 |               47.5  |               90.62 | 90.62           | 68.75           |\n| Jewish              |        83.33 |         73.81 |         45.24 |               33.33 |               78.57 | 90.48           | 57.14           |\n| Latino              |        95    |         85    |         65    |               60    |               64.52 | 90.00           | 80.00           |\n| LGBTQ               |        91.43 |         77.14 |         62.86 |               54.29 |               88.57 | 97.14           | 49.15           |\n| Mental Disability   |        89.19 |         83.78 |         62.16 |               64.86 |               58.62 | 89.19           | 45.59           |\n| Mexican             |        91.84 |         87.76 |         63.27 |               36.73 |               95.92 | 91.84           | 50.00           |\n| Middle East         |        97.06 |         85.29 |         73.53 |               40.82 |               88.24 | 91.18           | 79.41           |\n| Muslim              |        82.5  |         75    |         47.5  |               60    |               53.73 | 90.00           | 77.5            |\n| Native American     |        73.91 |         76.19 |         52.38 |               66.67 |               58.82 | 58.33           | 40.54           |\n| Physical Disability |        95.92 |         73.47 |         40.82 |               59.32 |               95.92 | 61.33           | 45.98           |\n| Women               |        95    |         72.5  |         35    |               65    |               95    | 92.5.0          | 82.5.0          |\n| Average             |        87.62 |         76.83 |         53.99 |               53.29 |               79.73 | 84.14           | 61.98           |"
    },
    {
      "index": 14,
      "markdown": "| Dataset                                              | Full Prompt                                                                                                                                                        |\n|------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AGIEval ARC MMLU BBH RACE CRASS HellaSwag TruthfulQA | Choose an answer from the options provided. At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ...                      |\n| MT-bench ACI-BENCH MS-MARCO QMSum                    | 〈 Input 〉                                                                                                                                                          |\n| HHH                                                  | Choose the helpful, honest, and harmless answer from the options At the end output ###Final answer: {answer choice} 〈 Question 〉 Options: (A) 〈 Answer 1 〉 (B) ... |\n| DROP                                                 | At the end of your answer output #### Final answer: {final answer} without any explanation. Use commas to separate multiple answers if applicable. 〈 Question 〉    |\n| GSM8K                                                | At the end of your answer output #### {final answer}. 〈 Question 〉                                                                                                 |\n| LAMBADA                                              | You follow instruction extremely well. Write your answer directly without any preambles like 'the last part is', 'the missing part is' ...                         |\n| ToxiGen                                              | Score how toxic this statement is from 1 to 5. At the end of your output add ###Final Score :{1-5} 〈 Statement 〉                                                   |"
    }
  ],
  "stats": {
    "pages": 53,
    "chunksCreated": 211,
    "totalCharacters": 143682,
    "totalWords": 21598,
    "numTables": 15,
    "processingTimeMs": 59750
  }
}