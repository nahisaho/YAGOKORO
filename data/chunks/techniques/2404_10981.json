{
  "paper": {
    "id": "2404.10981v2",
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
    "authors": [
      "Yizheng Huang",
      "Jimmy Huang"
    ],
    "published": "2024-04-17T01:27:42.000Z",
    "updated": "2024-08-23T00:17:02.000Z",
    "primaryCategory": "cs.IR",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2404.10981v2",
    "absUrl": "https://arxiv.org/abs/2404.10981v2"
  },
  "chunks": [
    {
      "id": "2404.10981v2-chunk-0",
      "content": "YIZHENG HUANG and JIMMY X. HUANG, York University, Canada\n\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's mechanics and discusses the field's progression through the analysis of significant studies.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "The Survey of Retrieval-Augmented Text Generation in Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-1",
      "content": "-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's mechanics and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "The Survey of Retrieval-Augmented Text Generation in Large Language Models",
        "chunkIndex": 1,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-2",
      "content": "Additional Key Words and Phrases: retrieval-augmented generation, information retrieval, large language model",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "CCS Concepts: · Computing methodologies → Natural language generation ; · Information systems → Information retrieval .",
        "chunkIndex": 2,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-3",
      "content": "Yizheng Huang and Jimmy X. Huang. 2018. The Survey of Retrieval-Augmented Text Generation in Large Language Models. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 37 pages. https://doi.org/XXXXXXX.XXXXXXX",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "ACMReference Format:",
        "chunkIndex": 3,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-4",
      "content": "The advent of ChatGPT has significantly impacted both academia and industry due to its interactive capabilities and widespread application, establishing itself as a leading artificial intelligence tool [55, 60, 77]. At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by [1], which has seen numerous enhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language Processing (NLP) tasks [78]. Despite these advancements, the adoption of LLMs has highlighted several critical issues primarily due to their reliance on extensive datasets. This reliance restricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data to maximize accessibility and applicability results in subpar performance in specialized areas. Second, the rapid creation of online data, combined with the significant resources required for data annotation and model training,",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-5",
      "content": "nd applicability results in subpar performance in specialized areas. Second, the rapid creation of online data, combined with the significant resources required for data annotation and model training,\n\nAuthors' Contact Information: Yizheng Huang, hyz@yorku.ca; Jimmy X. Huang, jhuang@yorku.ca, York University, Toronto, Ontario, Canada.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nConference acronym 'XX, June 03-05, 2018, Woodstock, NY",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-6",
      "content": "to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nConference acronym 'XX, June 03-05, 2018, Woodstock, NY\n\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n\nACM ISBN 978-1-4503-XXXX-X/18/06\n\nhttps://doi.org/XXXXXXX.XXXXXXX\n\nFig. 1. An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the scope of the training data and generates correct results.\n\n<!-- image -->\n\nhinders LLMs' ability to stay updated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as 'hallucinations', which can mislead users.\n\nAddressing these challenges is crucial for LLMs to be effectively utilized across various domains. A promising solution is the integration of Retrieval-Augmented Generation (RAG) technology, which supplements models by fetching external data in response to queries, thus ensuring more accurate and current",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-7",
      "content": "ising solution is the integration of Retrieval-Augmented Generation (RAG) technology, which supplements models by fetching external data in response to queries, thus ensuring more accurate and current outputs. Figure 1 illustrates how RAG can enable ChatGPT to provide precise answers beyond its initial training data.\n\nSince its introduction by Lewis et al. [83] in 2020, RAG has seen rapid development, especially with the rise of models like ChatGPT. Despite these advancements, there remains a noticeable gap in the literature regarding a comprehensive analysis of the mechanisms underlying RAG and the progress achieved by subsequent studies. Moreover, the field suffers from fragmented research focuses and inconsistent terminology for similar methods, leading to confusion. This survey seeks to bridge this gap by offering a structured overview of RAG, categorizing various approaches, and providing an in-depth understanding of the current research landscape, with a focus on textual applicat",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-8",
      "content": "o bridge this gap by offering a structured overview of RAG, categorizing various approaches, and providing an in-depth understanding of the current research landscape, with a focus on textual applications given their prominence in recent research.\n\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-9",
      "content": "generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10.\n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 9,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-10",
      "content": ", Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications.\n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.\n\nFig. 2. The unified RAG core concepts with basic workflow.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "1 Introduction",
        "chunkIndex": 10,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-11",
      "content": "The hallucinations are largely attributed to LLMs' inability to access up-to-date information. This limitation stems from the models' reliance on their training datasets. RAG proposes a solution to this issue by supplementing the LLM's training data with current information from external sources through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more cost-effective alternative to the extensive training and fine-tuning processes typically required for LLMs. It allows for the dynamic incorporation of fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly integrate this new data into the LLM. This feature makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2 RAG Framework",
        "chunkIndex": 11,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-12",
      "content": "ined LMs, without the need to directly integrate this new data into the LLM. This feature makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through RAG is derived from real-world data, authored by humans, which not only simplifies the generation process but also increases the reliability of the generated responses.\n\nResearch by Khandelwal et al. [72] demonstrates that accessing relevant information from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG has evolved from a means of providing supplementary information to enabling multiple interactions between the retrieval and generation components. This involves conducting several rounds of retrieval to refine the accuracy of the information retrieved and iteratively improve the quality of the generated output.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2 RAG Framework",
        "chunkIndex": 12,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-13",
      "content": "trieval and generation components. This involves conducting several rounds of retrieval to refine the accuracy of the information retrieved and iteratively improve the quality of the generated output. Toolkits such as LangChain 1 and LlamaIndex 2 have modularized the RAG approach, enhancing its adaptability and expanding its range of applications. Despite these toolkits employing diverse methodologies to tackle different aspects of RAG-from multiple search iterations to iterative generation-they maintain adherence to the fundamental RAG workflow. This consistency is crucial for understanding their operation and pinpointing opportunities for further development.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2 RAG Framework",
        "chunkIndex": 13,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-14",
      "content": "Figure 2 represents the unified RAG core concepts with basic workflow. The workflow of RAG begins with the creation of an index comprising external sources. This index serves as the basis for retrieving relevant information through a retriever model based on a specific query. The final step\n\n1 https://www.langchain.com\n\n2 https://www.llamaindex.ai\n\ninvolves a generator model, which combines the retrieved information with the query to produce the desired output.\n\n2.1.1 Indexing. Efficient retrieval begins with comprehensive indexing, where data preparation is key. This stage involves text normalization processes such as tokenization, stemming, and the removal of stop words to enhance the text's suitability for indexing [98]. Text segments are then organized into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.1 Basic RAG Workflow",
        "chunkIndex": 14,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-15",
      "content": "uitability for indexing [98]. Text segments are then organized into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing through the use of pretrained LMs for generating semantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly enhancing retrieval efficiency.\n\n2.1.2 Retrieval. While traditional retrieval methods, such as the BM25 algorithm [44], focus on term frequency and presence for document ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT [29], which capture the semantic essence of queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.1 Basic RAG Workflow",
        "chunkIndex": 15,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-16",
      "content": "f queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities. This is typically achieved by measuring vector distances between documents and queries, combining traditional retrieval metrics with semantic understanding to yield search results that are both relevant and aligned with user intent.\n\n2.1.3 Generation. The generation phase is tasked with producing text that is both relevant to the query and reflective of the information found in the retrieved documents. The usual method involves concatenating the query with the retrieved information, which is then fed into an LLM for text generation [85]. Although ensuring the generated text's alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.1 Basic RAG Workflow",
        "chunkIndex": 16,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-17",
      "content": "alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity. The generated text should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.1 Basic RAG Workflow",
        "chunkIndex": 17,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-18",
      "content": "The RAG paradigm organizes research within the domain, offering a straightforward yet robust framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for generating high-quality outcomes. Therefore, this paradigm is structured into four main phases from a retrieval perspective: pre-retrieval, retrieval, post-retrieval, and generation. Both single-hop and multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase structure. Figure 3 is the taxonomy tree of RAG's core techniques.\n\n2.2.1 Pre-Retrieval. The pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and query preparation, ensuring efficient information retrieval. This phase includes essential tasks to prepare for effective data access.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2 RAG Paradigm",
        "chunkIndex": 18,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-19",
      "content": "-augmented generation lays the foundation for successful data and query preparation, ensuring efficient information retrieval. This phase includes essential tasks to prepare for effective data access.\n\nIndexing. The process starts with indexing, which establishes an organized system to enable fast and accurate retrieval of information. The specificity of indexing depends on the task and data type. For example, sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas.\n\nQuery Manipulation. After indexing, query manipulation is performed to adjust user queries for a better match with the indexed data. This involves query reformulation [61, 155], which rewrites",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2 RAG Paradigm",
        "chunkIndex": 19,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-20",
      "content": "ideas.\n\nQuery Manipulation. After indexing, query manipulation is performed to adjust user queries for a better match with the indexed data. This involves query reformulation [61, 155], which rewrites\n\nthe query to align more closely with the user's intention; query expansion [51], which extends the query to capture more relevant results through synonyms or related terms; and query normalization, which resolves differences in spelling or terminology for consistent query matching.\n\nData Modification. Data modification is also critical in enhancing retrieval efficiency. This step includes preprocessing techniques like removing irrelevant or redundant information to improve the quality of results and enriching the data with additional information such as metadata to boost the relevance and diversity of the retrieved content [6].",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2 RAG Paradigm",
        "chunkIndex": 20,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-21",
      "content": "Search &amp; Ranking. The retrieval stage is the combination of search and ranking. It focuses on selecting and prioritizing documents from a dataset to enhance the quality of the generation model's outputs. This stage employs search algorithms to navigate through the indexed data, finding documents that match a user's query. After identifying relevant documents, the process of initially ranking these documents starts to sort them according to their relevance to the query.\n\n2.2.3 Post-Retrieval. The post-retrieval phase serves to refine the initially retrieved documents to improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed at optimizing the document selection for the final generation task.\n\nRe-Ranking. In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2.2 Retrieval.",
        "chunkIndex": 21,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-22",
      "content": "ltering, each aimed at optimizing the document selection for the final generation task.\n\nRe-Ranking. In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized. The objective is to more accurately highlight the documents most relevant to the query and diminish the importance of the less relevant ones. This step involves incorporating additional metrics and external knowledge sources to enhance precision. In this context, pre-trained models with superior accuracy but lower efficiency can be effectively employed due to the limited set of candidate documents available [54].\n\nFiltering. Filtering aims to remove documents that fail to meet specified quality or relevance standards [56, 74]. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2.2 Retrieval.",
        "chunkIndex": 22,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-23",
      "content": "ied quality or relevance standards [56, 74]. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level. Furthermore, the use of feedback from users or prior relevance evaluations assists in adjusting the filtering process, guaranteeing that only the most relevant documents are retained for text generation.\n\n2.2.4 Generation. The generation stage is a crucial component of the RAG process, responsible for leveraging retrieved information to enhance the quality of the generated response. This stage encompasses several sub-steps aimed at producing content that is readable, engaging, and informative.\n\nEnhancing. At the heart of the generation phase is the enhancement step, where the objective is to merge the retrieved information with the user's query to create a coherent and relevant response. This includes the process of elaboration, adding extra details to the retrieved content to enrich it.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2.2 Retrieval.",
        "chunkIndex": 23,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-24",
      "content": "merge the retrieved information with the user's query to create a coherent and relevant response. This includes the process of elaboration, adding extra details to the retrieved content to enrich it. Efforts are focused on improving the output's quality by increasing its clarity, coherence, and stylistic appeal through methods such as rephrasing and restructuring. Information from various sources is combined to offer a comprehensive perspective, and verification is conducted to ensure the accuracy and relevance of the content.\n\nCustomization. Customization is user-centric. It encompasses tailoring content in two primary ways. First, it aligns the generated output with relevant information retrieved in earlier stages, ensuring consistency and accuracy by incorporating key knowledge. Second, it adapts the content to suit user-specific factors such as intended audience, situational context, and personal preferences, shaping the response to be both contextually relevant and user-centric.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2.2 Retrieval.",
        "chunkIndex": 24,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-25",
      "content": "d, it adapts the content to suit user-specific factors such as intended audience, situational context, and personal preferences, shaping the response to be both contextually relevant and user-centric. This dual focus on\n\nFig. 3. Taxonomy tree of RAG's core techniques\n\n<!-- image -->\n\nintegrating relevant knowledge and adjusting to diverse contextual demands forms the basis of effective customization in RAG.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "2.2.2 Retrieval.",
        "chunkIndex": 25,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-26",
      "content": "One of the most commonly used indexing structures in traditional information retrieval systems is the inverted index. This structure associates documents with words to form a vocabulary list, allowing users to quickly locate references where a specific word appears within a collection of documents. The vocabulary list here refers to the set of all unique words present in the document collection, while the reference includes the documents where the word appears, along with the word's position and weight within those documents. However, traditional indexing structures struggle to retrieve documents that are semantically related to a user's query but do not contain the exact query terms.\n\nTo address this limitation, retrieval methods using dense vectors generated by deep learning models have become the preferred choice. These vectors, also known as embeddings, capture the semantic meaning of words and documents, allowing for more flexible and accurate retrieval.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 26,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-27",
      "content": "deep learning models have become the preferred choice. These vectors, also known as embeddings, capture the semantic meaning of words and documents, allowing for more flexible and accurate retrieval. Dense vector-based indexing methods can be categorized into three main types: graphs, product quantization (PQ) [62], and locality-sensitive hashing (LSH) [28]. Since generating dense vectors with large language models requires substantial resources, and the document collections to be searched are typically vast, the core strategy of these indexing methods is based on approximate nearest neighbor search (ANNS) [3]. This approach significantly speeds up the search process at the cost of a slight reduction in search accuracy.\n\nGraph. Using graphs to build indexes is a common practice in RAG. By indexing vectors with a graph structure, the range of nodes where distances need to be computed during retrieval can be limited to a local subgraph, thereby enhancing search speed.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 27,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-28",
      "content": "practice in RAG. By indexing vectors with a graph structure, the range of nodes where distances need to be computed during retrieval can be limited to a local subgraph, thereby enhancing search speed. Several prominent methods and tools have been developed using this approach. For example, k-nearest neighbor language models kNN-LMs [72], as demonstrated by Khandelwal et al., integrate the kNN algorithm with pre-trained language models. This method employs a datastore created from collections of texts to dynamically retrieve contextually relevant examples, enhancing model performance without requiring additional training. FAISS [68], a tool widely adopted for indexing in many studies [72, 73, 83], integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation\n\n[97] to further speed up retrieval [83].",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 28,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-29",
      "content": "68], a tool widely adopted for indexing in many studies [72, 73, 83], integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation\n\n[97] to further speed up retrieval [83]. WebGPT [100] showcases another practical application by utilizing the Bing API 3 for indexing based on actual user search histories, which illustrates the potential of integrating real-world user data into the retrieval process. Additionally, other methods like MEMWALKER [13] introduces innovative approaches to overcome limitations such as context window size in large language models. It creates a memory tree from input text, segmenting the text into smaller pieces and summarizing these segments into a hierarchical structure. Moreover, LRUS-CoverTree method [93] designed another tree structure for k-Maximum Inner-Product Search (k-MIPS) and achieves performance comparable with significantly lower index construction time.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 29,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-30",
      "content": ". Moreover, LRUS-CoverTree method [93] designed another tree structure for k-Maximum Inner-Product Search (k-MIPS) and achieves performance comparable with significantly lower index construction time. These techniques facilitate efficient indexing and management of large information volumes, demonstrating the versatility and effectiveness of graph-based approaches.\n\nProduct Quantization. PQ is one of the most representative methods for handling large-scale data. It accelerates searches by segmenting vectors and then clustering each part for quantization. Unlike graph-based methods, which speed up searches by reducing the number of vectors for distance calculation, PQ achieves faster searches by reducing the time spent on calculating word distances. Several implementations of PQ have emerged in RAG, each improving its efficiency and scalability in different ways.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 30,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-31",
      "content": "Q achieves faster searches by reducing the time spent on calculating word distances. Several implementations of PQ have emerged in RAG, each improving its efficiency and scalability in different ways. PipeRAG [64] integrates PQ within a pipeline-parallelism framework to enhance retrieval-augmented generation by optimizing retrieval intervals. Chameleon system [63] leverages PQin a disaggregated accelerator environment to balance memory usage and retrieval speed in RAG tasks. AiSAQ [126] introduces an all-in-storage ANNS method that offloads PQ vectors from DRAM to storage, drastically reducing memory usage while maintaining high recall. It demonstrates that even with billion-scale datasets, memory usage can be minimized to around 10 MB with only minor latency increases, making it a highly scalable solution for RAG systems.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 31,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-32",
      "content": "gh recall. It demonstrates that even with billion-scale datasets, memory usage can be minimized to around 10 MB with only minor latency increases, making it a highly scalable solution for RAG systems.\n\nLocality-sensitive Hashing. The core idea of LSH is to place similar vectors into the same hash bucket with high probability. LSH uses hash functions that map similar vectors to the same or nearby hash values, making it easier to find approximate nearest neighbours. In LSH, when a query vector is hashed, the system quickly retrieves candidate vectors that share the same hash value. This method reduces the dimensionality of the problem and can be implemented efficiently, but it may introduce some inaccuracies due to the hashing process itself. While LSH is rarely used in RAG systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over the slight loss in accuracy.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 32,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-33",
      "content": "to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over the slight loss in accuracy.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.1 Indexing",
        "chunkIndex": 33,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-34",
      "content": "Query manipulation is pivotal in enhancing the effectiveness and accuracy of modern IR systems. By refining users' original queries, it addresses challenges such as ambiguous phrasing and vocabulary mismatches between the query and target documents. This process involves more than merely replacing words with synonyms; it requires a deep understanding of user intent and the context of the query, particularly in complex tasks like RAG. Effective query manipulation significantly boosts retrieval performance, which in turn can greatly impact the quality of generated outputs. The three primary approaches to query manipulation are query expansion, query reformulation, and prompt-based rewriting.\n\nQuery Expansion. Query expansion involves augmenting the original query with additional terms or phrases that are related to or synonymous with the query terms.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 34,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-35",
      "content": "rmulation, and prompt-based rewriting.\n\nQuery Expansion. Query expansion involves augmenting the original query with additional terms or phrases that are related to or synonymous with the query terms. In the context of LLMs, query expansion can be more sophisticated, utilizing the model's extensive knowledge to generate contextually relevant expansions. This technique aims to improve recall by ensuring that the retrieval process captures a broader range of relevant documents, accommodating different\n\n3 https://www.microsoft.com/en-us/bing/apis/bing-web-search-api\n\nterminologies or expressions. Techniques such as synonym expansion, semantic similarity, or leveraging external knowledge bases are commonly employed in query expansion. For example, the method described in FiD [58] expands the query by retrieving a wider range of passages using both sparse and dense retrieval techniques, enabling the model to aggregate evidence from multiple sources and thereby improving the accuracy and rob",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 35,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-36",
      "content": "uery by retrieving a wider range of passages using both sparse and dense retrieval techniques, enabling the model to aggregate evidence from multiple sources and thereby improving the accuracy and robustness of generated answers. A more advanced form of query expansion is demonstrated in Query2doc [137], where pseudo-documents generated by LLMs enhance the original query, effectively bridging the gap between the user's input and the corpus information, which benefits both sparse and dense retrieval systems. Similarly, KnowledGPT [140] broadens the scope of information accessed during retrieval by leveraging external knowledge bases, further refining the retrieval process. The RARG [159] framework uses an evidence-driven query expansion approach, incorporating a wide array of supporting documents to generate informed and accurate counter-misinformation responses.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 36,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-37",
      "content": "ocess. The RARG [159] framework uses an evidence-driven query expansion approach, incorporating a wide array of supporting documents to generate informed and accurate counter-misinformation responses.\n\nQuery Reformulation. Query reformulation involves rephrasing or restructuring the original query to enhance its effectiveness. This might include making the wording more specific, removing vague terms, or adjusting the syntax to better align with the retrieval system's requirements. With LLMs, query reformulation can be dynamically driven by understanding the user's intent and context, allowing for more precise modifications that lead to improved retrieval results. This reformulation process can also be informed by past queries or user interactions, adapting the query to better fit the specific retrieval task.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 37,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-38",
      "content": "difications that lead to improved retrieval results. This reformulation process can also be informed by past queries or user interactions, adapting the query to better fit the specific retrieval task. For instance, the RQ-RAG [12] model represents an advanced form of query reformulation by rewriting, decomposing, and disambiguating queries, making it particularly effective in scenarios that demand complex query handling. This approach ensures that the refined query better matches the needed context, improving the relevance of retrieved information. Rewrite-Retrieve-Read framework [94] adjusts the original query to optimize the retrieval process, allowing the system to more effectively leverage retrieved data for generating accurate responses. Additionally, FLARE [65] exemplifies query reformulation through its active retrieval-augmented generation approach, which iteratively refines the query based on a feedback loop between retrieval and generation, thereby enhancing the accuracy and",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 38,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-39",
      "content": "rmulation through its active retrieval-augmented generation approach, which iteratively refines the query based on a feedback loop between retrieval and generation, thereby enhancing the accuracy and relevance of the retrieved information.\n\nPrompt-based Rewriting. Prompt-based rewriting, particularly in the context of LLMs, represents an innovative approach where the original query is embedded within a larger prompt or context to guide the LLM's response. This technique harnesses the model's ability to understand and generate language within a specific context, effectively rewriting the query to align with the desired output. Prompt-based rewriting is especially powerful in scenarios where the retrieval process is integrated into a generative workflow, allowing the system to adapt the query to various stages of retrieval and generation. This approach may also involve dynamic prompts that evolve based on interaction, further refining the retrieval process.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 39,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-40",
      "content": "ng the system to adapt the query to various stages of retrieval and generation. This approach may also involve dynamic prompts that evolve based on interaction, further refining the retrieval process. For example, Step-Back [163] refines the query context through carefully crafted prompts that guide the LLM's reasoning process, ensuring that the outputs are more aligned with the user's intent, particularly in complex reasoning tasks. The CoK [86] method focuses on dynamically adapting the knowledge source and using prompts to rewrite the context in which a query is interpreted. This approach leverages prompt-based rewriting to enable the LLM to effectively integrate and ground its responses based on various heterogeneous knowledge sources. Additionally, Promptagator [27] discusses using prompt-based techniques to adapt and rewrite the query to better align with the retrieval system's expectations, particularly in few-shot learning scenarios.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 40,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-41",
      "content": "ionally, Promptagator [27] discusses using prompt-based techniques to adapt and rewrite the query to better align with the retrieval system's expectations, particularly in few-shot learning scenarios. These prompts guide the model in generating or refining the query to optimize retrieval results.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.2 Query Manipulation",
        "chunkIndex": 41,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-42",
      "content": "Document modification techniques play a critical role in enhancing retrieval performance, particularly when integrated with LLMs. These techniques can be broadly categorized into Internal Data Augmentation and External Data Enrichment. Internal Data Augmentation focuses on maximizing the value of existing information within documents or models, while External Data Enrichment introduces supplementary data from outside sources to fill gaps, provide additional context, or broaden the scope of the content.\n\nInternal Data Augmentation. Internal Data Augmentation leverages information already present within documents or taps into the inherent knowledge embedded in LLMs. Techniques like paraphrasing, where content is rewritten for improved readability or multiple perspectives, and summarization, which condenses information while retaining core content, are commonly employed.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.3 Data Modification",
        "chunkIndex": 42,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-43",
      "content": "ues like paraphrasing, where content is rewritten for improved readability or multiple perspectives, and summarization, which condenses information while retaining core content, are commonly employed. Other methods involve generating supplementary content or explanations that are contextually related without introducing external data. For instance, RECITE [125] utilizes a model's internal memory to recite relevant information before generating responses, thus enhancing performance in tasks like closed-book question answering without external data. KnowledGPT [140] similarly refines the internal knowledge embedded within LLMs, optimizing its use during generation. GENREAD [156] further demonstrates how pre-existing knowledge within LLMs can be used to generate context that enhances task performance, bypassing the need for external sources. In another example, the Selfmem [21] framework allows the model to iteratively use its own outputs as memory in subsequent generation tasks.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.3 Data Modification",
        "chunkIndex": 43,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-44",
      "content": "task performance, bypassing the need for external sources. In another example, the Selfmem [21] framework allows the model to iteratively use its own outputs as memory in subsequent generation tasks. By selecting and utilizing the best internal outputs as memory, this approach boosts model performance without depending on external memory resources.\n\nExternal Data Enrichment. External Data Enrichment enhances document content by incorporating new information from external sources, enriching the overall context and accuracy. This process can involve integrating facts, data, or contextual knowledge from external datasets or knowledge bases. For example, RA-DIT [89] augments input prompts during fine-tuning by leveraging large datasets like Wikipedia and CommonCrawl, enhancing the model's capability in knowledgeintensive tasks. The dual instruction tuning technique optimizes both the language model and the retriever to more effectively incorporate retrieved information.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.3 Data Modification",
        "chunkIndex": 44,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-45",
      "content": "ing the model's capability in knowledgeintensive tasks. The dual instruction tuning technique optimizes both the language model and the retriever to more effectively incorporate retrieved information. UPRISE [20] demonstrates how retrieving prompts from diverse task datasets improves model generalization in zero-shot scenarios by enriching the context during inference. Additionally, RARG [159] exemplifies external data enrichment by integrating scientific evidence from academic databases to strengthen responses countering misinformation. This method involves a two-stage retrieval pipeline that identifies and ranks relevant documents, which are then used to support and enhance the factual accuracy of generated responses.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "3.3 Data Modification",
        "chunkIndex": 45,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-46",
      "content": "The search and ranking process within RAG is crucial for improving the relevance and accuracy of generated outputs. Several methodologies have been developed to refine this process, each contributing unique strategies for enhancing retrieval and ranking. For example, Atlas [59] and AAR [157] both aim to improve the relevance of retrieved documents, but they approach this challenge differently. Atlas focuses on optimizing the retriever's ability to select contextually relevant documents, especially in new domains with limited data, by employing few-shot learning techniques such as Attention Distillation and Perplexity Distillation. AAR, on the other hand, adapts retrieval preferences to better align with the requirements of LLMs, enhancing retrieval generalization across tasks by training a smaller source model.\n\nFig. 4. An example of a typical RAG framework with interative retrieval strategy.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.1 Search &amp; Ranking",
        "chunkIndex": 46,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-47",
      "content": "irements of LLMs, enhancing retrieval generalization across tasks by training a smaller source model.\n\nFig. 4. An example of a typical RAG framework with interative retrieval strategy.\n\n<!-- image -->\n\nAdditionally, IRCOT [131] and FLARE [65] introduce dynamic interactions within the retrieval process, albeit with distinct goals. IRCOT integrates retrieval with chain-of-thought (CoT) reasoning, interleaving these processes to ensure that each retrieval step supports the ongoing reasoning task. FLARE, in contrast, adopts a confidence-based active retrieval mechanism, dynamically triggering retrieval when the model generates low-confidence tokens. This approach is particularly useful in scenarios where model confidence varies, as it allows the system to fetch additional information to resolve uncertainties during the generation process.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.1 Search &amp; Ranking",
        "chunkIndex": 47,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-48",
      "content": "tokens. This approach is particularly useful in scenarios where model confidence varies, as it allows the system to fetch additional information to resolve uncertainties during the generation process.\n\nWhenaddressing domain-specific retrieval challenges, SURGE [70] and PRCA [151] offer different solutions. SURGE uses a subgraph retriever to extract relevant subgraphs from knowledge graphs,\n\nintegrating structured data into the retrieval process to improve the contextual understanding of generated responses. The relational structure of knowledge graphs allows for more accurate and informed retrieval. PRCA, in contrast, focuses on domain-specific abstractive summarization, using a reward-driven approach to refine the retrieved content. This strategy is designed to optimize content for the generator, particularly in scenarios where the generator functions as a black box, thereby enhancing alignment between retrieval and generation.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.1 Search &amp; Ranking",
        "chunkIndex": 48,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-49",
      "content": ". This strategy is designed to optimize content for the generator, particularly in scenarios where the generator functions as a black box, thereby enhancing alignment between retrieval and generation.\n\nMEMWALKER [13] presents a unique approach to handling long-context question answering by incorporating an internal search and ranking mechanism within a memory tree structure. This method navigates extensive memory stores, ensuring that the most relevant information is retrieved and used for complex queries. Unlike other methods, MEMWALKER emphasizes efficient processing of long texts through iterative navigation and summarization, rather than solely optimizing the initial retrieval phase.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.1 Search &amp; Ranking",
        "chunkIndex": 49,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-50",
      "content": "The retrieval strategies within RAG are vital for customizing the retrieval process to specific application needs, with each strategy offering distinct advantages and addressing particular challenges. In RAG, it is mostly the utilization of retrieval techniques rather than the exploration of retrieval algorithms that is involved, so it is the strategy of retrieval that is usually considered in searching and ranking. While basic RAGs are usually single-hop searches, i.e., they are retrieved only once as generated supplementary material, today's RAGs are mostly multi-hop searches, i.e., they are searched several times through different search strategies until they are satisfied. In terms of practical applications these strategies belong to the design on the engineering pipeline. Figure 4 shows a typical case of the RAG framework with iterative retrieval strategy. There are five main retrieval strategies in RAG:",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.2 Retrieval Strategy",
        "chunkIndex": 50,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-51",
      "content": "hese strategies belong to the design on the engineering pipeline. Figure 4 shows a typical case of the RAG framework with iterative retrieval strategy. There are five main retrieval strategies in RAG:\n\nBasic Retrieval Strategy. Basic retrieval strategies typically follow a linear workflow, moving sequentially through pre-retrieval, retrieval, post-retrieval, and generation phases. The Atlas [94] framework exemplifies this straightforward approach, guiding the retrieval process efficiently from start to finish without iterations or complex conditional modifications. REPLUG [122] similarly follows this basic strategy, augmenting black-box language models with retrieval in a simple manner, where the retrieved information is directly used to enhance the generation process.\n\nIterative Retrieval Strategy. For more complex scenarios, iterative retrieval strategies (Algorithm 1) are employed, where information is retrieved in multiple steps, each informed by previous results.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.2 Retrieval Strategy",
        "chunkIndex": 51,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-52",
      "content": "terative Retrieval Strategy. For more complex scenarios, iterative retrieval strategies (Algorithm 1) are employed, where information is retrieved in multiple steps, each informed by previous results. IRCOT [131] exemplifies this by integrating retrieval with chain-of-thought reasoning, where the retrieval process is sequential and closely tied to reasoning steps. This method is particularly effective in scenarios requiring multi-step problem-solving, such as research assistance or complex queries that benefit from detailed exploration. ITER-RETGEN [121] also employs iterative retrieval, refining the process based on generated responses, allowing for continuous improvement and closer alignment between retrieval and generation. RQ-RAG [12] advances this approach by using techniques like query rewriting, decomposition, and disambiguation, refining the retrieval step-bystep to enhance the final output.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.2 Retrieval Strategy",
        "chunkIndex": 52,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-53",
      "content": "etrieval and generation. RQ-RAG [12] advances this approach by using techniques like query rewriting, decomposition, and disambiguation, refining the retrieval step-bystep to enhance the final output. PlanRAG [81] also fits within this strategy, iteratively refining the retrieval process based on generated content and feedback, ensuring that each step is better informed than the last.\n\nRecursive Retrieval Strategy. Recursive retrieval (Algorithm 2) involves retrieval that can call itself, creating a hierarchy or tree of retrievals. This method effectively handles hierarchical or layered information by breaking down complex queries into simpler sub-queries. It is particularly useful for hierarchical data exploration, knowledge base construction, and detailed information",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "4.2 Retrieval Strategy",
        "chunkIndex": 53,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-54",
      "content": "```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: while 𝑖 ≤ 𝑁 do 3: Pre-retrieval Phase 4: 𝑞 ′ ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ) // Indexing, Query Manipulation, Data Modification 5: Retrieval Phase 6: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷 ) // Search and initial ranking of documents 7: Post-retrieval Phase 8: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 ) // Re-ranking and filtering to refine documents 9: Generation Phase 10: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 ) // Generate output based on refined documents 11: if stopping condition met based on 𝑦 𝑖 then 12: BREAK // Stop iterations if output is satisfactory 13: end if 14: Update 𝑞 ′ ← UpdateQuery ( 𝑞,𝑦 𝑖 ) // Refine query based on the generated output 15: 𝑖 ← 𝑖 + 1 // Increment iteration counter 16: end while 17: Final Synthesis 18: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 1 Iterative Retrieval Strategy in RAG",
        "chunkIndex": 54,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-55",
      "content": "𝑞 ′ ← UpdateQuery ( 𝑞,𝑦 𝑖 ) // Refine query based on the generated output 15: 𝑖 ← 𝑖 + 1 // Increment iteration counter 16: end while 17: Final Synthesis 18: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 19: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nretrieval. SURGE [70] leverages this strategy through knowledge graphs, where relevant subgraphs are extracted to enhance contextual understanding. The relational structure of knowledge graphs facilitates navigating multiple layers of information, ensuring accurate and contextually relevant retrieval. MEMWALKER [13] similarly adopts a recursive approach, processing long texts by constructing a memory tree of summaries. The system navigates through this tree to retrieve relevant information, effectively breaking down complex queries into manageable segments, which is particularly useful for handling long-context question answering.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 1 Iterative Retrieval Strategy in RAG",
        "chunkIndex": 55,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-56",
      "content": "gates through this tree to retrieve relevant information, effectively breaking down complex queries into manageable segments, which is particularly useful for handling long-context question answering. IMRAG [150] introduces a multi-round retrieval mechanism, where each round of retrieval is based on the model's internal monologues, progressively refining the search with each iteration. Selfmem [21] employs a selfmemory module, enabling the system to store and retrieve information recursively, building upon previously retrieved knowledge in a hierarchical manner. This recursive strategy enhances the system's ability to manage and integrate vast amounts of information across multiple retrieval iterations.\n\nConditional Retrieval Strategy. Conditional retrieval strategies (Algorithm 3) are governed by specific conditions or rules, which may be predefined or dynamically determined during the process.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 1 Iterative Retrieval Strategy in RAG",
        "chunkIndex": 56,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-57",
      "content": "ons.\n\nConditional Retrieval Strategy. Conditional retrieval strategies (Algorithm 3) are governed by specific conditions or rules, which may be predefined or dynamically determined during the process. This method ensures that retrieval aligns with specific constraints or criteria, enhancing relevance and specificity. It is particularly useful for compliance checking, rule-based recommendation systems, and context-sensitive information retrieval. PRCA [151] is a prime example, where retrieval strategies are adapted based on reward-driven adjustments, refining the context used by large language models to enhance precision and relevance. RARG [159] similarly emphasizes retrieval based on specific evidence conditions, ensuring that the retrieval process aligns with predefined requirements, which is critical for generating factual and polite responses. CRAG [149] adds another",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 1 Iterative Retrieval Strategy in RAG",
        "chunkIndex": 57,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-58",
      "content": "```\nRequire: Initial Query 𝑞 , Documents 𝐷 , Maximum Depth 𝐿 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Sub-query Generation Function 𝐹 𝑠𝑢𝑏𝑞 , Hierarchical Layer Building Function 𝐹 𝑏𝑢𝑖𝑙𝑑 , Hierarchical Information Operating Function 𝐹 ℎ𝑖𝑒𝑟 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Build Hierarchical Layers (Pre-retrieval)\n```\n\n```\n2: 𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦 ← 𝐹 𝑏𝑢𝑖𝑙𝑑 ( 𝑞 ) // Build hierarchical layers based on the initial query 3: Initialize 𝑙 ← 0 // Start depth counter from 0 4: Initialize 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 ← [ 𝑞 ] // Initialize list of sub-queries 5: while 𝑙 ≤ 𝐿 do 6: for each 𝑞 ′ 𝑙 ∈ 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 do 7: Pre-retrieval Phase 8: 𝑞 ′ 𝑙 ← 𝐹 ℎ𝑖𝑒𝑟 ( 𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦,𝑞 ′ 𝑙 ) // Adjust query based on hierarchical layers 9: 𝑞 ′ 𝑙 ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ 𝑙 ) // Query Manipulation, Indexing, and Data Modification 10: Retrieval Phase 11: 𝐷 𝑙 ← 𝑅 ( 𝑞 ′ 𝑙 , 𝐷 ) // Retrieve documents fo",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 2 Recursive Retrieval Strategy in RAG",
        "chunkIndex": 58,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-59",
      "content": "uery based on hierarchical layers 9: 𝑞 ′ 𝑙 ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ 𝑙 ) // Query Manipulation, Indexing, and Data Modification 10: Retrieval Phase 11: 𝐷 𝑙 ← 𝑅 ( 𝑞 ′ 𝑙 , 𝐷 ) // Retrieve documents for the current sub-query 12: Post-retrieval Phase 13: 𝐷 ′ 𝑙 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ 𝑙 , 𝐷 𝑙 ) // Re-ranking and filtering to refine documents 14: Generation Phase 15: 𝑦 𝑙 ← 𝐺 ( 𝑞 ′ 𝑙 , 𝐷 ′ 𝑙 ) // Generate output based on refined documents 16: Sub-query Generation (if needed) 17: if additional refinement needed based on 𝑦 𝑙 then 18: 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 ← 𝐹 𝑠𝑢𝑏𝑞 ( 𝑦 𝑙 ) // Generate new sub-queries based on current output 19: end if 20: end for 21: 𝑙 ← 𝑙 + 1 // Increment depth counter 22: end while 23: Final Synthesis 24: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 0 , 𝑦 1 , . . . , 𝑦 𝑙 }) // Merge results 25: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 2 Recursive Retrieval Strategy in RAG",
        "chunkIndex": 59,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-60",
      "content": "for 21: 𝑙 ← 𝑙 + 1 // Increment depth counter 22: end while 23: Final Synthesis 24: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 0 , 𝑦 1 , . . . , 𝑦 𝑙 }) // Merge results 25: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nlayer to this approach by incorporating a retrieval evaluator that assesses the quality of retrieved documents and triggers different actions based on confidence thresholds, ensuring that only the most relevant and accurate information is used in the generation process.\n\nAdaptive Retrieval Strategy. Adaptive retrieval (Algorithm 4) dynamically adjusts the retrieval strategy based on the context and nature of the query or the data retrieved so far. This highly flexible method tailors retrieval approaches on-the-fly to optimize for relevance and precision, making it ideal for personalized search engines, adaptive learning systems, and real-time decision support.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 2 Recursive Retrieval Strategy in RAG",
        "chunkIndex": 60,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-61",
      "content": "ible method tailors retrieval approaches on-the-fly to optimize for relevance and precision, making it ideal for personalized search engines, adaptive learning systems, and real-time decision support. AAR [157] exemplifies adaptive retrieval by adjusting its strategy based on the preferences of LLMs, learning from a small source model and generalizing to unseen tasks. FLARE [65] takes a similar adaptive approach but focuses on dynamically fetching additional information when model confidence is low, thereby improving the relevance of generated responses. SelfRAG [4] goes further by incorporating self-reflective processes, where the retrieval strategy evolves based on critiques of the generated content. CoK [86], on the other hand, implements a dynamic mechanism that adjusts retrieval strategies based on the evolving needs of the task. The retrieval process in CoK is not static but adapts according to the specific scenario and the nature of the",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 2 Recursive Retrieval Strategy in RAG",
        "chunkIndex": 61,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-62",
      "content": "egies based on the evolving needs of the task. The retrieval process in CoK is not static but adapts according to the specific scenario and the nature of the",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 2 Recursive Retrieval Strategy in RAG",
        "chunkIndex": 62,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-63",
      "content": "```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Condition Evaluation Function 𝐹 𝑐𝑜𝑛𝑑 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: 𝑞 ′ ← 𝑞 // Initialize query 3: while 𝑖 ≤ 𝑁 do 4: Pre-retrieval Phase 5: 𝑞 ′ ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ ) // Perform query manipulation and data modification 6: Retrieval Phase 7: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷 ) // Retrieve documents based on the current query 8: Post-retrieval Phase 9: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 ) // Re-rank and filter documents based on conditions 10: Generation Phase 11: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 ) // Generate output using the refined documents 12: Conditional Branching 13: if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition A then 14: Apply Strategy A // e.g., refine the query based on feedback 15: else if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition B then 16: Apply Strategy B",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 3 Conditional Retrieval Strategy in RAG",
        "chunkIndex": 63,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-64",
      "content": "𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition A then 14: Apply Strategy A // e.g., refine the query based on feedback 15: else if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition B then 16: Apply Strategy B // e.g., expand the scope or adjust parameters 17: else if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition C then 18: Apply Strategy C // e.g., modify retrieval strategy or output processing 19: else 20: Continue without changes // If no conditions are met, proceed without adjustments 21: end if 22: Check Termination Condition 23: if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) meets stopping criteria then 24: BREAK // Exit the loop if the stopping condition is met 25: end if 26: 𝑖 ← 𝑖 + 1 // Increment iteration counter 27: end while 28: Final Synthesis 29: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 30: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 3 Conditional Retrieval Strategy in RAG",
        "chunkIndex": 64,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-65",
      "content": "26: 𝑖 ← 𝑖 + 1 // Increment iteration counter 27: end while 28: Final Synthesis 29: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 30: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\ninformation being accessed, making it highly effective for context-sensitive applications. DRAGIN [124] discusses a real-time dynamic retrieval mechanism that adapts to the evolving needs of the language model, ensuring that the retrieval strategy remains responsive and aligned with the immediate task requirements, thus optimizing the relevance and precision of the retrieved information.\n\nIn summary, the choice of retrieval strategy within RAG depends on the specific requirements of the application at hand. While basic retrieval strategies offer simplicity and efficiency, iterative retrieval is well-suited for tasks requiring detailed exploration and refinement.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 3 Conditional Retrieval Strategy in RAG",
        "chunkIndex": 65,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-66",
      "content": "c requirements of the application at hand. While basic retrieval strategies offer simplicity and efficiency, iterative retrieval is well-suited for tasks requiring detailed exploration and refinement. Recursive retrieval excels in managing hierarchical information, while adaptive retrieval provides flexibility in dynamic environments. Conditional retrieval ensures strict adherence to predefined criteria, making it indispensable in applications where compliance and specific constraints are critical. By carefully",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 3 Conditional Retrieval Strategy in RAG",
        "chunkIndex": 66,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-67",
      "content": "```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Adaptive Adjustment Function 𝐹 𝑎𝑑𝑎𝑝𝑡 , Feedback Function 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝑞, ∅ // Initialize query and context 3: while 𝑖 ≤ 𝑁 do 4: Pre-retrieval Phase 5: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Query Manipulation, Indexing, Data Modification, and Context Setup 6: Dynamic Retrieval Phase 7: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷, 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Retrieve documents based on the current query and context 8: Adaptive Post-retrieval Phase 9: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Re-ranking and filtering based on adaptive criteria 10: Generation Phase 11: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Generate output using the refined documents 12: Adaptive Adjustment 13:",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 4 Adaptive Retrieval Strategy in RAG",
        "chunkIndex": 67,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-68",
      "content": "/ Re-ranking and filtering based on adaptive criteria 10: Generation Phase 11: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Generate output using the refined documents 12: Adaptive Adjustment 13: if 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is negative then 14: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝐹 𝑎𝑑𝑎𝑝𝑡 ( 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡,𝑦 𝑖 , 𝐷 ′ 𝑖 ) // Dynamically adjust the query and context 15: end if 16: Feedback Integration 17: if 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 ( 𝑦 𝑖 ) is positive then 18: BREAK // Stop iterations if output is satisfactory 19: end if 20: 𝑖 ← 𝑖 + 1 // Increment iteration counter 21: end while 22: Final Synthesis 23: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 24: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nselecting and combining these strategies, RAG systems can be tailored to effectively handle a wide range of information retrieval scenarios, leveraging the strengths of each approach to deliver robust and precise results.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 4 Adaptive Retrieval Strategy in RAG",
        "chunkIndex": 68,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-69",
      "content": "ms can be tailored to effectively handle a wide range of information retrieval scenarios, leveraging the strengths of each approach to deliver robust and precise results.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Algorithm 4 Adaptive Retrieval Strategy in RAG",
        "chunkIndex": 69,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-70",
      "content": "As retrieval mechanisms often return a large number of potentially relevant documents, re-ranking methods are employed to reorder these documents, prioritizing those most likely to contribute meaningfully to the final output. By leveraging various strategies, including unsupervised techniques, supervised learning, and data augmentation, re-ranking aims to optimize the alignment between the retrieved content and the desired response, thereby improving the overall effectiveness of RAG systems [165].\n\nUnsupervised Re-ranking. Unsupervised re-rankers do not rely on labeled data for training. They use strategies such as pointwise, listwise, or pairwise methods to rank documents based on LLM outputs without the need for supervised fine-tuning. For example, In-Context RALM [112] employs\n\na zero-shot approach where an off-the-shelf language model is used to re-rank the top-k documents retrieved by a BM25 retriever.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 70,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-71",
      "content": "for supervised fine-tuning. For example, In-Context RALM [112] employs\n\na zero-shot approach where an off-the-shelf language model is used to re-rank the top-k documents retrieved by a BM25 retriever. This process involves selecting the document that maximizes the likelihood of the generated text, effectively using the LM's semantic understanding to improve document relevance without requiring additional supervised training. The paper also explores training a dedicated re-ranker using self-supervised learning to further enhance the selection of relevant documents, demonstrating that training a re-ranker with domain-specific data can be more effective than zero-shot re-ranking.\n\nSupervised Re-ranking. Supervised re-rankers involve fine-tuning LLMs on specific ranking datasets. This category can be further divided into models like BERT that process query-document pairs to compute relevance scores, models like T5 that treat ranking as a generation task and use generated tokens to determin",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 71,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-72",
      "content": "can be further divided into models like BERT that process query-document pairs to compute relevance scores, models like T5 that treat ranking as a generation task and use generated tokens to determine relevance, and models like RankLLaMA [95] that employ a prompt-based approach, focusing on the last token's representation for relevance calculation [165]. For instance, the re-ranker in Re2G [40] is based on a BERT model trained on labeled data (such as MS MARCO) and fine-tuned to improve the relevance ranking of retrieved documents. FiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 72,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-73",
      "content": "ive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 73,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-74",
      "content": "a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input.\n\nData Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the training process by generating additional training data, such as pseudo-relevance labels, using LLMs. This data augmentation provides more varied training examples, which helps improve the performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving the retrieval process by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 74,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-75",
      "content": "ocess by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data generated through LLM-based query generation to enhance the training of the reranker. This data augmentation approach allows the re-ranker to refine candidate passages more effectively, using a cross-attention model trained on these additional examples to boost retrieval accuracy.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.1 Re-Ranking",
        "chunkIndex": 75,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-76",
      "content": "Filtering and re-ranking are distinct processes in the post-retrieval stage of RAG systems. Filtering focuses on eliminating irrelevant or low-quality documents from the retrieved set, thereby reducing the document set size and improving efficiency and effectiveness in subsequent processing. In contrast, re-ranking orders the remaining documents based on their relevance or utility for the task, often prioritizing those that enhance the quality of the generated output, especially in responseaware scenarios.\n\nSeveral filtering methods have been developed to refine document sets in RAG systems, each with unique mechanisms but sharing common goals of improving relevance and reducing computational load. Self-RAG [4] employs a self-reflection mechanism, utilizing special 'reflection tokens'\n\ngenerated by the model to evaluate the relevance and quality of retrieved passages and the model's own generated outputs.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 76,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-77",
      "content": "4] employs a self-reflection mechanism, utilizing special 'reflection tokens'\n\ngenerated by the model to evaluate the relevance and quality of retrieved passages and the model's own generated outputs. This self-reflection ensures that only the most pertinent documents are retained, leveraging the model's internal capabilities without relying on external models during inference. Similarly, BlendFilter [135] utilizes the LLM itself as the filter, assessing and removing irrelevant or less useful documents by applying filtering separately to knowledge retrieved from original, externally augmented, and internally augmented queries. Both Self-RAG and BlendFilter highlight the model's intrinsic ability to perform filtering, reducing the need for additional models and enhancing computational efficiency.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 77,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-78",
      "content": "nternally augmented queries. Both Self-RAG and BlendFilter highlight the model's intrinsic ability to perform filtering, reducing the need for additional models and enhancing computational efficiency.\n\nIn contrast, RECOMP [147] and CRAG [149] employ more external or structural strategies. RECOMPfocuses on selective augmentation, where summaries generated from retrieved documents are selectively prepended to the input for the language model. If the retrieved documents are deemed irrelevant, the compressor can generate an empty summary, effectively filtering out unnecessary information. This method allows for a dynamic approach to filtering, where only helpful content is retained. CRAG, on the other hand, uses a decompose-then-recompose approach, where retrieved documents are split into finer knowledge strips. These strips are evaluated for relevance using a finetuned T5 model, and only the relevant strips are recomposed to form a refined set of information for the generation task.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 78,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-79",
      "content": "o finer knowledge strips. These strips are evaluated for relevance using a finetuned T5 model, and only the relevant strips are recomposed to form a refined set of information for the generation task. This granular filtering process ensures that the final document set is both relevant and concise, tailored specifically to the generation task.\n\nDynamic filtering techniques are also employed in methods like FiD-TF [5] and CoK [86]. FiD-TF introduces Token Filtering during the decoding process, where less relevant tokens are dynamically filtered out based on cross-attention scores. This approach reduces the computational load by eliminating tokens deemed uninformative for generating the final answer, enhancing efficiency with minimal impact on performance. CoK employs a filtering technique based on self-consistency, identifying and processing only those questions with 'uncertain' answers.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 79,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-80",
      "content": "answer, enhancing efficiency with minimal impact on performance. CoK employs a filtering technique based on self-consistency, identifying and processing only those questions with 'uncertain' answers. This method works by sampling various reasoning paths and answers, preserving only predictions with high consistency. Questions that do not meet the specified consistency threshold undergo further processing, effectively preventing the propagation of errors in the generation process.\n\nFinally, FILCO [141] implements a comprehensive filtering approach using three distinct strategies: String Inclusion (STRINC) to match exact outputs, Lexical Overlap to measure word-level similarity, and Conditional Cross-Mutual Information (CXMI) to assess how much the context improves output likelihood. FILCO applies these filtering strategies at the sentence level, refining the retrieved content for better relevance.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 80,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-81",
      "content": "utual Information (CXMI) to assess how much the context improves output likelihood. FILCO applies these filtering strategies at the sentence level, refining the retrieved content for better relevance. Additionally, FILCO trains a context filtering model using these strategies, which predicts the most useful context at inference time, thereby enhancing the accuracy and relevance of the generation model's output.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "5.2 Filtering",
        "chunkIndex": 81,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-82",
      "content": "Enhancing methods are strategies aimed at improving the quality and relevance of generated outputs by integrating retrieved content in various ways. These methods differ in how they combine, aggregate, or refine retrieved information, offering multiple approaches to enrich the final output. Broadly, these techniques can be grouped into three categories: enhancing with queries, enhancing with ensemble approaches, and enhancing with feedback loops.\n\nEnhance with Query. This approach integrates the retrieved documents with the original query, enabling the generator to leverage both sources in producing the final output. By combining the query with the retrieved content, the generation process ensures that the response remains closely aligned with the user's intent while being enriched by relevant information. The focus here is on the seamless fusion of the query and context, allowing the generated output to maintain both relevance",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 82,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-83",
      "content": "aligned with the user's intent while being enriched by relevant information. The focus here is on the seamless fusion of the query and context, allowing the generated output to maintain both relevance\n\nand completeness. For instance, the RETRO [9] model enhances generation by integrating retrieved text chunks with the user's query using a chunked cross-attention mechanism, where relevant information from the retrieved neighbors is directly injected into the generation process. This method involves first retrieving similar document chunks based on the query and then using a crossattention module to align and combine these chunks with the input sequence during generation. In-Context RALM [112] takes a comparable approach, directly prepending the retrieved documents to the input query. In this way, the language model can generate responses conditioned on both the query and the retrieved content without requiring changes to the model's architecture.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 83,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-84",
      "content": "eved documents to the input query. In this way, the language model can generate responses conditioned on both the query and the retrieved content without requiring changes to the model's architecture. Both examples illustrate a straightforward yet effective method: concatenating the query and retrieved documents into a single input sequence that the LLMs process together, yielding outputs that are contextually enhanced.\n\nEnhance with Ensemble. When multiple sources are synthesized, the generation process can achieve a more coherent and well-rounded response. Rather than relying solely on a single source, this approach aggregates information from various documents, allowing the generator to reconcile conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 84,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-85",
      "content": "this approach aggregates information from various documents, allowing the generator to reconcile conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output. The ensemble process can manifest in different ways: it may involve combining insights from several sources into a unified narrative, or generating multiple candidate outputs and choosing the best one based on criteria like consistency, relevance, or factual accuracy. An instance of this strategy is seen in FiD [58], which encodes multiple retrieved passages independently before fusing them in the decoder to create a coherent answer. By treating each passage separately during encoding and then merging them during decoding, the model effectively combines evidence from multiple sources. Meanwhile, in REPLUG [122], an ensemble approach is adopted where each retrieved document is independently prepended to the query and processed separately.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 85,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-86",
      "content": "ely combines evidence from multiple sources. Meanwhile, in REPLUG [122], an ensemble approach is adopted where each retrieved document is independently prepended to the query and processed separately. The outputs are then aggregated, with relevance scores guiding the weighting of each document's contribution. Through this process, the model capitalizes on diverse information across several sources, leading to improvements in answer accuracy, coverage, and scalability as more data becomes available.\n\nEnhance with Feedback. In contrast to approaches that process retrieved information in a single pass, this method introduces iterative refinement into the generation process by incorporating feedback loops. Initially, the generator produces a draft response, which is then evaluated and adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on factual accuracy and fluency.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 86,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-87",
      "content": "lly, the generator produces a draft response, which is then evaluated and adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on factual accuracy and fluency. This iterative approach aims to incrementally improve the output by identifying and correcting errors or fine-tuning content to better align with quality standards, ultimately producing a polished and reliable response. PRCA [151] offers an example by positioning itself between the retriever and generator, distilling retrieved information based on feedback from the generator. This distilled information serves as a reward model to guide context optimization, leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which details should be emphasized or downplayed. DSP [73], on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 87,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-88",
      "content": "ld be emphasized or downplayed. DSP [73], on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant passages, and updates the context in subsequent steps-each stage building on the last to refine the final output. Feedback-driven enhancements are also evident in models like Selfmem [21], which focus on generating self-memory. The model first produces an unbounded pool of outputs and then selects the most relevant one as memory for the next generation, guided by metrics like BLEU or ROUGE. Finally, RECITE [122] integrates feedback by generating multiple recitations from the model's internal knowledge and using self-consistency techniques to aggregate the outputs.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 88,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-89",
      "content": "ics like BLEU or ROUGE. Finally, RECITE [122] integrates feedback by generating multiple recitations from the model's internal knowledge and using self-consistency techniques to aggregate the outputs. By introducing diversity in the recitations and leveraging passage hints during generation, this approach selects the best content through majority voting. Together, these methods demonstrate\n\nhow feedback loops and iterative refinements can lead to outputs that are not only more accurate but also increasingly coherent and contextually grounded as they evolve.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.1 Enhancing",
        "chunkIndex": 89,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-90",
      "content": "Customization focuses on tailoring content to the user's personality and needs. It involves adjusting the output either to align with specific knowledge retrieved during earlier stages (content alignment) or to adapt the generated response to meet the user's preferences, context, or audience needs (contextual adaptation).\n\nIn LAPDOG [52], customization is achieved primarily through content alignment by integrating persona profiles with external stories to enrich the context used for generation. The story retriever identifies relevant narratives based on the persona, expanding the limited profiles with additional information. The generator then combines this enriched knowledge with the dialogue history, ensuring that responses align closely with the persona's traits and background. This approach allows for a nuanced understanding of the user's personality, making the output more engaging and contextually appropriate.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.2 Customization",
        "chunkIndex": 90,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-91",
      "content": "ponses align closely with the persona's traits and background. This approach allows for a nuanced understanding of the user's personality, making the output more engaging and contextually appropriate.\n\nOn the other hand, PersonaRAG [160] emphasizes real-time adaptation by customizing generated content based on dynamic user profiles, session behavior, and ongoing feedback. A multi-agent system continuously analyzes user interactions to refine responses, ensuring alignment with the user's preferences and context. By integrating personalized insights at each step, the system can adjust its output to suit specific informational needs and situational contexts. This level of responsiveness allows the system to evolve in line with the user's changing requirements, creating more relevant and targeted responses.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.2 Customization",
        "chunkIndex": 91,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-92",
      "content": "fic informational needs and situational contexts. This level of responsiveness allows the system to evolve in line with the user's changing requirements, creating more relevant and targeted responses.\n\nERAGent [123] also focuses on customization but through the use of a Personalized LLM Reader, which adapts responses using user-specific profiles. This module integrates rewritten questions, filtered knowledge, and user preferences to tailor responses according to both content relevance and user needs. For instance, it takes into account preferences like environmental consciousness or dietary restrictions, ensuring that the generated content is not only aligned with retrieved knowledge but also personalized to the user's particular values and requirements. This deep level of customization ensures that the output is both relevant and personally meaningful, enhancing user engagement.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.2 Customization",
        "chunkIndex": 92,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-93",
      "content": "but also personalized to the user's particular values and requirements. This deep level of customization ensures that the output is both relevant and personally meaningful, enhancing user engagement.\n\nROPG [118] proposes a dynamic pre- and post-generation retriever selection model, enhancing personalization by aligning the retrieval process with both the input context and the user's preferences. The pre-generation model determines which retrieval strategy-such as recency-based, keyword matching, or semantic retrieval-is most appropriate before generation begins. By tailoring the retrieval process in this way, the model ensures that the documents retrieved from the user profile closely match the current input, thereby aligning the content with relevant user-specific knowledge. Following this, the post-generation model evaluates the outputs generated by different retrieval strategies and selects the most personalized result.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.2 Customization",
        "chunkIndex": 93,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-94",
      "content": "the content with relevant user-specific knowledge. Following this, the post-generation model evaluates the outputs generated by different retrieval strategies and selects the most personalized result. This selection is guided by feedback from the generated content, which is then used to adjust future retrievals. By combining content alignment (through pre-generation retrieval) with contextual adaptation (through post-generation evaluation), this approach offers a comprehensive solution for customization within RAG.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "6.2 Customization",
        "chunkIndex": 94,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-95",
      "content": "To assess how effectively language models can generate more accurate, relevant, and robust responses by leveraging external knowledge, the evaluation of RAG systems has emerged as a crucial research focus. Given the rising popularity of dialogue-based interactions, much recent work has concentrated on evaluating RAG models' performance on such downstream tasks using established metrics like Exact Match (EM) and F1 scores. These metrics have been applied across a\n\nTable 1. The Comparison of Different RAG Evaluation Frameworks.\n\n| Evaluation Framework   | Aspects                       | Methods                        | Metrics                                                     | Datasets                                                                 |\n|------------------------|-------------------------------|--------------------------------|-------------------------------------------------------------|--------------------------------------------------------------------------|\n| RAGAS [",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 95,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-96",
      "content": "-------------------|--------------------------------|-------------------------------------------------------------|--------------------------------------------------------------------------|\n| RAGAS [33]             | Quality of RAG Systems        | Context Relevance              | Extracted Sentences / Total Sentences                       | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Answer Relevance               | Average Cosine Similarity                                   | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Faithfulness                   | Supported Statements / Total Statements                     | WikiEval 7                                                               |\n| ARES [117]             | Improving RAGAS               | Context Relevance              | Confidence Intervals",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 96,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-97",
      "content": "| WikiEval 7                                                               |\n| ARES [117]             | Improving RAGAS               | Context Relevance              | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Relevance               | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Faithfulness            | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| RECALL [91]            | Counterfactual Robustness     | Response Quality               | Accuracy (QA) BLEU, ROUGE-L (Generation)                    | EventKG [41] UJ [50]                                                     |\n| RECA",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 97,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-98",
      "content": "actual Robustness     | Response Quality               | Accuracy (QA) BLEU, ROUGE-L (Generation)                    | EventKG [41] UJ [50]                                                     |\n| RECALL [91]            | Counterfactual Robustness     | Robustness                     | Misleading Rate (QA) Mistake Reappearance Rate (Generation) | EventKG [41] UJ [50]                                                     |\n| RGB [14]               | Impact of RAG on LLMs         | Noise Robustness               | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Negative Rejection             | Rejection Rate                                              | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Information Integration        | Accuracy",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 98,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-99",
      "content": "| Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Information Integration        | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Counterfactual Robustness      | Error Detection Rate Error Correction Rate                  | Synthetic                                                                |\n| MIRAGE [144]           | RAG in Medical QA             | Zero-Shot Learning             | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Multi-Choice Evaluation        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| M",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 99,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-100",
      "content": "n Medical QA             | Multi-Choice Evaluation        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Retrieval-Augmented Generation | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Question-Only Retrieval        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| eRAG [119]             | Retrieval Quality in RAG      | Downstream Task                | Accuracy, ROUGE                                             | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Set-based                      | Precision, Recall, Hit R",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 100,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-101",
      "content": "| KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Set-based                      | Precision, Recall, Hit Rate                                 | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Ranking                        | MAP, MRR, NDCG                                              | KILT                                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Surface-Based                  | EM, F1, Precision, Recall                                   | QA Datasets [69, 76]                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Semantic                       | BEM [11], LLMeval [114]                                     | QA Datasets [69, 76]                                                     |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 101,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-102",
      "content": "andardizing RAG Experiments | Semantic                       | BEM [11], LLMeval [114]                                     | QA Datasets [69, 76]                                                     |\n\nwide array of datasets, including TriviaQA [69], HotpotQA [153], FEVER [129], Natural Questions (NQ) [76], Wizard of Wikipedia (WoW) [30], and T-REX [32], which are often used to benchmark the effectiveness of retrieval and generation components in knowledge-intensive tasks.\n\nWhile downstream task evaluations provide valuable insights, they fail to address the multifaceted challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has proposed various frameworks and benchmarks that aim to evaluate these systems from multiple perspectives, considering not only the quality of the generated text but also the relevance of retrieved documents and the system's resilience to misinformation, as shown in Table 1.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 102,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-103",
      "content": "ems from multiple perspectives, considering not only the quality of the generated text but also the relevance of retrieved documents and the system's resilience to misinformation, as shown in Table 1. These evaluations include metrics that assess noise robustness, negative prompting, information integration, and counterfactual robustness, all of which reflect the complex challenges RAG systems face in realworld applications. The ongoing development of comprehensive evaluation frameworks and metrics is essential for advancing the field, broadening the applicability of RAG systems, and ensuring that they meet the demands of an increasingly dynamic and complex information landscape [154].",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7 Evaluation in RAG",
        "chunkIndex": 103,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-104",
      "content": "In information retrieval, standard metrics such as Mean Average Precision (MAP), Precision, Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) [103, 110, 115] have traditionally been used to evaluate the relevance of retrieved documents to a given query. These metrics are essential in assessing the effectiveness of traditional information retrieval systems, where the primary goal is to measure how well the retrieved documents match the user's query.\n\nWhen applied to RAG systems, these retrieval-based metrics extend their focus to consider how the retrieved information contributes to the quality of the generated output. In this context, Accuracy becomes a crucial metric, assessing how precisely the retrieved documents provide correct information for answering queries. Additionally, Rejection Rate [14], which measures the system's ability to decline answering when no relevant information is available, has emerged as a",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.1 Retrieval-based Aspect",
        "chunkIndex": 104,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-105",
      "content": "ovide correct information for answering queries. Additionally, Rejection Rate [14], which measures the system's ability to decline answering when no relevant information is available, has emerged as a\n\nkey indicator of responsible output generation. Similarly, Error Detection Rate [14] evaluates the model's capability to identify and filter out incorrect or misleading information, ensuring that the generation process is based on trustworthy sources.\n\nAnother important consideration is Context Relevance, which assesses the alignment of retrieved documents with the specific query, emphasizing the need for content directly relevant to the generation task's context. Faithfulness [33] is also critical in determining whether the generated text accurately reflects the information found in the retrieved documents, thereby minimizing the risk of generating misleading or incorrect content.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.1 Retrieval-based Aspect",
        "chunkIndex": 105,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-106",
      "content": "also critical in determining whether the generated text accurately reflects the information found in the retrieved documents, thereby minimizing the risk of generating misleading or incorrect content.\n\nThe eRAG framework [119] introduces a more refined approach to evaluating retrieval quality in RAG systems by focusing on individual documents rather than the entire retrieval process. It operates by feeding each document in the retrieval list into the LLM alongside the query and evaluating the generated output against downstream task metrics such as Accuracy. The documentlevel scores are then aggregated using ranking metrics like MAP to produce a single evaluation score. This focus on document-level contributions offers a more precise assessment of retrieval quality while being significantly more computationally efficient than traditional end-to-end evaluations.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.1 Retrieval-based Aspect",
        "chunkIndex": 106,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-107",
      "content": "core. This focus on document-level contributions offers a more precise assessment of retrieval quality while being significantly more computationally efficient than traditional end-to-end evaluations.\n\nNotably, eRAG demonstrates that its document-level evaluation correlates more strongly with downstream RAG performance compared to conventional methods like human annotations or provenance labels. This correlation underscores that the LLM, as the primary consumer of the retrieved results, is the most reliable judge of retrieval performance [119]. Regardless of the retrieval model or the number of retrieved documents, eRAG consistently outperforms other evaluation approaches, indicating that directly evaluating how each document supports the LLM's output is the most effective way to measure retrieval quality in RAG systems.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.1 Retrieval-based Aspect",
        "chunkIndex": 107,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-108",
      "content": "The evaluation of text produced by large language models involves analyzing performance across a range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy, and alignment with ground-truth data. Metrics like BLEU [107] and ROUGE-L [87] are often used to measure fluency, similarity to human-produced text, and the overlap with reference summaries, respectively, providing insights into how well the generated content captures key ideas and phrases.\n\nIn addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap with ground-truth data are evaluated using EM and F1 scores, which respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 108,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-109",
      "content": "respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized.\n\nBeyond these standard evaluation techniques, more specialized criteria have been introduced to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate [91] have been developed to measure a model's ability to avoid generating incorrect or misleading content. Other advanced metrics include Answer Relevance [33], which assesses the precision of responses to queries, Kendall's tau [117], used for evaluating the accuracy of system rankings, and Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 109,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-110",
      "content": "of responses to queries, Kendall's tau [117], used for evaluating the accuracy of system rankings, and Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers. Prediction Accuracy further complements these by directly measuring how closely the generated responses align with the expected answers, offering a clear measure of a system's effectiveness in producing accurate content.\n\nTable 2. The comprehensive summary of RAG studies. A ! in the 'Multi-hop' column signifies that the research involves multiple search rounds. Similarly, a ! in the 'Training' column indicates that the study included training phases. It is important to note that in this context, 'Training' encompasses both initial model training and fine-tuning processes.\n\n<!-- image -->\n\n| Research                     | Year      | Retrieval   | Source     | Multi-hop   | Training   |            | Pre-Retrieval      |                   | Retrieval          | Post-Retrieval   | Post-Retri",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 110,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-111",
      "content": "esearch                     | Year      | Retrieval   | Source     | Multi-hop   | Training   |            | Pre-Retrieval      |                   | Retrieval          | Post-Retrieval   | Post-Retrieval   | Generation   | Generation    |\n|------------------------------|-----------|-------------|------------|-------------|------------|------------|--------------------|-------------------|--------------------|------------------|------------------|--------------|---------------|\n| REALM [42]                   |           | Internal    | External ! |             | !          | Indexing ! | Query Manipulation | Data Modification | Search & Ranking ! | Re-Ranking       | Filtering        | Enhancing    | Customization |\n| kNN-LMs [72]                 | 2020 2020 | !           | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| RAG [83]",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 111,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-112",
      "content": "|            | !          |                    |                   | !                  |                  |                  | !            |               |\n| RAG [83]                     | 2020      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| FiD [58]                     | 2021      |             | !          |             |            |            |                    |                   | !                  |                  |                  | !            |               |\n| Webgpt [100]                 | 2021      |             | !          | !           | !          | !          | !                  |                   | !                  |                  | !                | !            |               |\n| Re2G [40]                    | 2022      | !           |            | !           | !          |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 112,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-113",
      "content": "| !                  |                  | !                | !            |               |\n| Re2G [40]                    | 2022      | !           |            | !           | !          |            |                    |                   |                    | !                |                  |              |               |\n| RETRO [9]                    | 2022      |             | !          | !           | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| DSP [73]                     | 2022      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| CoK [86]                     | 2023      |             | !          | !           |            |            | !                  |                   |                    |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 113,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-114",
      "content": "|               |\n| CoK [86]                     | 2023      |             | !          | !           |            |            | !                  |                   |                    | !                |                  |              |               |\n| IRCOT [131]                  | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  | !            |               |\n| ITRG [34]                    | 2023      | !           | !          | !           |            |            |                    |                   | !                  |                  |                  | !            |               |\n| PKG [92]                     | 2023      | !           |            |             |            |            |                    |                   |                    |                  |                  |              |               |\n|",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 114,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-115",
      "content": "!           |            |             |            |            |                    |                   |                    |                  |                  |              |               |\n| RA-DIT [89]                  | 2023      |             | !          | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| Self-RAG [4]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n|                              | 2023      | !           |            |             |            |            |                    |                   | !                  |                  |                  |              |               |\n| SURGE [70] FiD-TF [5]        | 2023      |             | !          |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 115,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-116",
      "content": "|                   | !                  |                  |                  |              |               |\n| SURGE [70] FiD-TF [5]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                | !                |              |               |\n| PRCA [151]                   | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  | !            |               |\n| REPLUG [122]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  |                  | !            |               |\n| AAR [157]                    | 2023      |             | !          |             | !          |            |                    |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 116,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-117",
      "content": "|                  | !            |               |\n| AAR [157]                    | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  |              |               |\n| Query2doc [137]              | 2023      | !           |            |             |            |            | !                  |                   |                    |                  |                  |              |               |\n| Step-Back [163]              | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  |              |               |\n| ITER-RETGEN [121]            | 2023      |             | !          | !           |            |            |                    |                   | !                  | !                |                  |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 117,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-118",
      "content": "[121]            | 2023      |             | !          | !           |            |            |                    |                   | !                  | !                |                  |              |               |\n| RECITE [125]                 | 2023      | !           |            | !           | !          |            |                    | !                 |                    |                  |                  | !            |               |\n| PROMPTAGATOR [27]            | 2023      | !           |            | !           |            |            | !                  |                   |                    | !                | !                |              |               |\n| UPRISE [20]                  | 2023      | ! !         |            | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| GENREAD [156]                | 2023",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 118,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-119",
      "content": "!          |            |                    | !                 | !                  |                  |                  | !            |               |\n| GENREAD [156]                | 2023      |             |            |             |            |            |                    | !                 |                    |                  |                  | !            |               |\n| LAPDOG [52]                  | 2023      |             | !          |             | !          |            | !                  |                   | !                  | !                |                  | !            | !             |\n| KnowledGPT [140]             | 2023      |             | ! !        | ! !         | !          |            | !                  | !                 |                    | !                |                  | !            |               |\n| Selfmem [21]                 | 2023      |             | !          |             |            | !          |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 119,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-120",
      "content": "| !                |                  | !            |               |\n| Selfmem [21]                 | 2023      |             | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| MEMWALKER [13] RECOMP [147]  | 2023 2023 |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| Rewrite-Retrieve-Read [94]   | 2023      |             | !          |             | !          |            | !                  |                   |                    |                  |                  |              |               |\n| Atlas [94]                   | 2023      |             | !          | !           | !          | !          |                    |                   | !                  | !",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 120,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-121",
      "content": "|\n| Atlas [94]                   | 2023      |             | !          | !           | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| DKS-RAC [53]                 | 2023      |             | !          | !           | !          |            |                    |                   |                    | !                | !                |              |               |\n| In-Context RALM [112]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                |                  |              |               |\n| Fid-light [47]               | 2023      |             | ! !        | !           |            |            | !                  |                   | !                  | !                |                  |              |               |\n| FLARE [65]",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 121,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-122",
      "content": "| ! !        | !           |            |            | !                  |                   | !                  | !                |                  |              |               |\n| FLARE [65]                   | 2023      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| Chameleon [63] ERAGent [123] | 2023      | !           | !          |             | !          |            | !                  |                   | !                  |                  | !                | !            | !             |\n| PipeRAG [64]                 | 2024 2024 |             | !          | !           | !          | !          |                    |                   |                    |                  |                  | !            |               |\n|                              |           | !           |            |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 122,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-123",
      "content": "|                   |                    |                  |                  | !            |               |\n|                              |           | !           |            |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| GenRT [148] PersonaRAG [160] | 2024 2024 | !           | !          | !           | !          |            | !                  |                   | !                  | ! !              |                  | !            | !             |\n| CRAG [149]                   | 2024      | !           | !          |             | !          |            |                    | !                 |                    | !                | !                | !            |               |\n| IMRAG [150]                  | 2024      |             | !          | !           | !          |            | !                  |                   |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 123,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-124",
      "content": "| !            |               |\n| IMRAG [150]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  | !            |               |\n| AiSAQ [126]                  | 2024      | !           |            |             |            | !          |                    |                   | !                  | !                |                  |              |               |\n| ROPG [118]                   | 2024      | !           |            |             | !          |            |                    |                   | !                  |                  |                  | !            | !             |\n| RQ-RAG [12]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 124,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-125",
      "content": "| 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  |              |               |\n| PlanRAG [81]                 | 2024      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| RARG [159]                   | 2024      |             | !          |             | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| DRAGIN [124]                 | 2024      |             | !          | !           |            |            | !                  |                   | !                  |                  |                  | !            |               |\n| [93]                         | 2024      | !",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 125,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-126",
      "content": "|            | !                  |                   | !                  |                  |                  | !            |               |\n| [93]                         | 2024      | !           |            |             | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| LRUS-CoverTree               |           |             |            |             |            |            |                    |                   |                    |                  |                  |              |               |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "7.2 Generation-based Aspect",
        "chunkIndex": 126,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-127",
      "content": "Table 2 presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows that the majority of these studies have utilized external data sources to enrich the content of LLMs. A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search rounds generally yield superior results. In other words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval stage, more studies focus on manipulating the query to improve retrieval performance. Additionally, there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in the research. However, there seems to be a scarcity of studies concentrating on customization in the generation stage, pointing to this as a potential area for future exploration.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 127,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-128",
      "content": "its crucial role in the research. However, there seems to be a scarcity of studies concentrating on customization in the generation stage, pointing to this as a potential area for future exploration. Overall, while the goal of RAG is to enhance the response quality of LLMs, greater efforts have been directed towards improving retrieval aspects.\n\nTable 3. The summary of Retrievers and Generators. The retrieval models and pre-trained language models explicitly mentioned in these studies have been recorded.\n\n| Research                   |   Year | Retriever                                          | Generator                                                  |\n|----------------------------|--------|----------------------------------------------------|------------------------------------------------------------|\n| REALM [42]                 |   2020 | BERT [29]                                          | Transformers [133]                                         |\n| kNN-LMs [72]",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 128,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-129",
      "content": "------------------|\n| REALM [42]                 |   2020 | BERT [29]                                          | Transformers [133]                                         |\n| kNN-LMs [72]               |   2020 | FAISS [68]                                         | Transformers                                               |\n| RAG [83]                   |   2020 | DPR [71]                                           | BART-Large [82]                                            |\n| FiD [58]                   |   2021 | BM25 [116], DPR                                    | T5 [111]                                                   |\n| Webgpt [100]               |   2021 | Bing                                               | GPT-3 [10]                                                 |\n| Re2G [40]                  |   2022 | BM25, DPR                                          | BART                                                       |\n| RETRO [9]                  |   2022 | BERT",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 129,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-130",
      "content": "]                  |   2022 | BM25, DPR                                          | BART                                                       |\n| RETRO [9]                  |   2022 | BERT                                               | Transformer                                                |\n| DSP [73]                   |   2022 | ColBERTv2                                          | GPT-3.5 (text-davinci-002)                                 |\n| CoK [86]                   |   2023 | [74] LLaMA2-7B [130], ChatGPT (gpt-3.5-turbo-0613) | ChatGPT (gpt-3.5-turbo-0613)                               |\n| IRCOT [131]                |   2023 | BM25                                               | GPT-3 (code-davinci-002), Flan-T5 [24]                     |\n| ITRG [34]                  |   2023 | Atlas [94]                                         | LLaMA-33B                                                  |\n| PKG [92]                   |   2023 | LLaMA-7B",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 130,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-131",
      "content": "Atlas [94]                                         | LLaMA-33B                                                  |\n| PKG [92]                   |   2023 | LLaMA-7B                                           | InstructGPT-3.5 (text-davinic-002) [104]                   |\n| RA-DIT [89]                |   2023 | DRAGON+ [88]                                       | LLaMA                                                      |\n| Self-RAG [4]               |   2023 | Contriever [57]                                    | LLaMA2 (7B and 13B) , GPT-4 [1]                            |\n| SURGE [70]                 |   2023 | Graph Neural Networks (GNN) [43]                   | Transformers                                               |\n| FiD-TF [5]                 |   2023 | BM25, SBERT [115]                                  | T5                                                         |\n| PRCA [151]                 |   2023 | BM25, DPR, Contriver, SimCSE [37], SBERT           | T5, Phoenix-7B [19], Vi",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 131,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-132",
      "content": "| T5                                                         |\n| PRCA [151]                 |   2023 | BM25, DPR, Contriver, SimCSE [37], SBERT           | T5, Phoenix-7B [19], Vicuna-7B [22], ChatGLM [31], GPT-3.5 |\n| REPLUG [122]               |   2023 | Contriever                                         | GPT-3                                                      |\n| AAR [157]                  |   2023 | ANCE [146], Contriever                             | Flan-T5, InstructGPT                                       |\n| Query2doc [137]            |   2023 | BM25, DPR                                          | GPT-3 (text-davinci-003)                                   |\n| Step-Back [163]            |   2023 | PaLM-2L [23]                                       | PaLM-2L, GPT-4                                             |\n| ITER-RETGEN [121]          |   2023 | Contriever                                         | InstructGPT (text-davinci-003), LLaMA2",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 132,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-133",
      "content": ", GPT-4                                             |\n| ITER-RETGEN [121]          |   2023 | Contriever                                         | InstructGPT (text-davinci-003), LLaMA2                     |\n| RECITE [125]               |   2023 |                                                    | PaLM, UL2 [127], OPT [161], Codex [16]                     |\n| PROMPTAGATOR [27]          |   2023 | T5                                                 | FLAN                                                       |\n| UPRISE [20]                |   2023 | GPT-Neo-2.7B [8]                                   | BLOOM-7.1B [142], OPT-66B, GPT-3-175B                      |\n| GENREAD [156]              |   2023 |                                                    | InstructGPT                                                |\n| LAPDOG [52]                |   2023 | Contriever                                         | T5                                                         |\n| KnowledGPT [140]",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 133,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-134",
      "content": "|\n| LAPDOG [52]                |   2023 | Contriever                                         | T5                                                         |\n| KnowledGPT [140]           |   2023 |                                                    | GPT-4                                                      |\n| Selfmem [21]               |   2023 | BM25                                               | XGLM [90], XLM-Rbase [25]                                  |\n| MEMWALKER [13]             |   2023 | LLaMA2                                             | LLaMA2                                                     |\n| RECOMP [147]               |   2023 | BM25                                               | T5-Large                                                   |\n| Rewrite-Retrieve-Read [94] |   2023 | Bing                                               | T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B               |\n| Atlas [94]                 |   2023 | Contriever",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 134,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-135",
      "content": "ite-Retrieve-Read [94] |   2023 | Bing                                               | T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B               |\n| Atlas [94]                 |   2023 | Contriever                                         | T5                                                         |\n| DKS-RAC [53]               |   2023 | DPR                                                | BART                                                       |\n| In-Context RALM [112]      |   2023 | BM25, BERT-base, Contriever, Spider [113]          | GPT-2, GPT-Neo, GPT-J [35], OPT, and LLaMA                 |\n| Fid-light [47]             |   2023 | GTR-Base [101]                                     | T5                                                         |\n| FLARE [65]                 |   2023 | BM25, Bing                                         | GPT-3.5 (text-davinci-003)                                 |\n| Chameleon [63]             |   2023 | ChamVS [63]",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 135,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-136",
      "content": "3 | BM25, Bing                                         | GPT-3.5 (text-davinci-003)                                 |\n| Chameleon [63]             |   2023 | ChamVS [63]                                        | ChamLM [63]                                                |\n| ERAGent [123]              |   2024 | Bing                                               | GPT-3.5, Falcon 1B [108]                                   |\n| PipeRAG [64]               |   2024 | SBERT                                              | RETRO [9]                                                  |\n| GenRT [148]                |   2024 | LambdaMart [2]                                     |                                                            |\n| PersonaRAG [160]           |   2024 | BM25                                               | GPT-3.5                                                    |\n| CRAG [149]                 |   2024 | Contriever                                         | LLaMA2",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 136,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-137",
      "content": "| GPT-3.5                                                    |\n| CRAG [149]                 |   2024 | Contriever                                         | LLaMA2                                                     |\n| IMRAG [150]                |   2024 | DPR                                                | Vicuna-7B                                                  |\n| AiSAQ [126]                |   2024 | DiskANN [106]                                      |                                                            |\n| ROPG [118]                 |   2024 | BM25, Contriever                                   | FlanT5-XXL                                                 |\n| RQ-RAG [12]                |   2024 | DuckDuckGo 8                                       | LLaMA2-7B                                                  |\n| PlanRAG [81]               |   2024 | GPT-4                                              | GPT-4",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 137,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-138",
      "content": "MA2-7B                                                  |\n| PlanRAG [81]               |   2024 | GPT-4                                              | GPT-4                                                      |\n| RARG [159]                 |   2024 | BM25, E5 [136]                                     | LLaMA2-7B                                                  |\n| DRAGIN [124]               |   2024 | BM25, SGPT [99]                                    | LLaMA2 (7B and 13B), Vicuna-13B                            |\n| LRUS-CoverTree [93]        |   2024 | k-MIPS                                             |                                                            |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.1 The Comprehensive Summary of RAG",
        "chunkIndex": 138,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-139",
      "content": "In RAG, the retriever and generator are central components, each playing a distinct role in the system's overall performance. Table 3 summarizes the retrievers and generators used across the studies discussed in this paper. The table reveals that while a wide range of advanced language models are employed as generators, many systems still rely on traditional retrievers like BM25, valued for their efficiency. This highlights the continued importance of optimizing retrieval methods while balancing computational demands. Interestingly, despite the availability of powerful models such as LLaMA2, GPT-3.5, and GPT-4, these are not widely adopted as generators. Instead, models like T5 remain prevalent, while more foundational retrieval approaches, such as those based on BERT, see limited use. The relative scarcity of IR-focused LLMs in retrievers suggests a promising avenue for future research and development in this domain.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 139,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-140",
      "content": "trieval approaches, such as those based on BERT, see limited use. The relative scarcity of IR-focused LLMs in retrievers suggests a promising avenue for future research and development in this domain.\n\nTable 4. Part results of Accuracy (%) of GPT-3.5 across different corpora and retrievers on Mirage. Red and green highlight declines and improvements compared to CoT (first row), with shading intensity reflecting the degree of change. Data sourced from Mirage [144].\n\n|                   |            | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   |         |\n|-------------------|------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------|\n| Corpus            | Retriever  | MMLU-Med                   | MedQA-US                   | MedMCQA                    | PubMedQA*                  |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 140,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-141",
      "content": "---------|----------------------------|---------|\n| Corpus            | Retriever  | MMLU-Med                   | MedQA-US                   | MedMCQA                    | PubMedQA*                  | BioASQ-Y/N                 | Average |\n| None              | None       | 72.91 ± 1.35               | 65.04 ± 1.34               | 55.25 ± 0.77               | 36.00 ± 2.15               | 74.27 ± 1.76               | 60.69   |\n| PubMed (23.9M)    | BM25       | 72.27 ± 1.36               | 63.71 ± 1.35               | 55.49 ± 0.77               | 66.20 ± 2.12               | 88.51 ± 1.28               | 69.23   |\n| PubMed (23.9M)    | Contriever | 71.72 ± 1.36               | 63.94 ± 1.35               | 54.29 ± 0.77               | 65.60 ± 2.12               | 85.44 ± 1.42               | 68.20   |\n| PubMed (23.9M)    | SPECTER    | 73.19 ± 1.34               | 65.20 ± 1.34               | 53.12 ± 0.77               | 54.80 ± 2.23               | 75.73 ± 1.72               | 64.41   |",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 141,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-142",
      "content": "68.20   |\n| PubMed (23.9M)    | SPECTER    | 73.19 ± 1.34               | 65.20 ± 1.34               | 53.12 ± 0.77               | 54.80 ± 2.23               | 75.73 ± 1.72               | 64.41   |\n| PubMed (23.9M)    | MedCPT     | 73.09 ± 1.34               | 66.69 ± 1.32               | 54.94 ± 0.77               | 66.40 ± 2.11               | 85.76 ± 1.41               | 69.38   |\n| PubMed (23.9M)    | RRF-2      | 75.57 ± 1.30               | 64.34 ± 1.34               | 55.34 ± 0.77               | 69.00 ± 2.07               | 87.06 ± 1.35               | 70.26   |\n| PubMed (23.9M)    | RRF-4      | 73.37 ± 1.34               | 64.73 ± 1.34               | 54.75 ± 0.77               | 67.20 ± 2.10               | 88.51 ± 1.28               | 69.71   |\n| Wikipedia (29.9M) | BM25       | 73.37 ± 1.34               | 63.47 ± 1.35               | 54.10 ± 0.77               | 26.40 ± 1.97               | 71.36 ± 1.82               | 57.74   |\n| Wikipedia (29.9M) | Contriever | 74.10",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 142,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-143",
      "content": "| 73.37 ± 1.34               | 63.47 ± 1.35               | 54.10 ± 0.77               | 26.40 ± 1.97               | 71.36 ± 1.82               | 57.74   |\n| Wikipedia (29.9M) | Contriever | 74.10 ± 1.33               | 65.99 ± 1.33               | 54.03 ± 0.77               | 26.40 ± 1.97               | 69.90 ± 1.85               | 58.08   |\n|                   | SPECTER    | 72.18 ± 1.36               | 63.63 ± 1.35               | 52.71 ± 0.77               | 22.20 ± 1.86               | 66.83 ± 1.89               | 55.51   |\n|                   | MedCPT     | 71.99 ± 1.36               | 65.12 ± 1.34               | 55.15 ± 0.77               | 29.00 ± 2.03               | 73.46 ± 1.78               | 58.95   |\n|                   | RRF-2      | 74.20 ± 1.33               | 64.57 ± 1.34               | 54.72 ± 0.77               | 31.00 ± 2.07               | 76.21 ± 1.71               | 60.14   |\n|                   | RRF-4      | 73.19 ± 1.34               | 64.96 ± 1.34",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 143,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-144",
      "content": "± 1.34               | 54.72 ± 0.77               | 31.00 ± 2.07               | 76.21 ± 1.71               | 60.14   |\n|                   | RRF-4      | 73.19 ± 1.34               | 64.96 ± 1.34               | 54.53 ± 0.77               | 31.00 ± 2.07               | 72.01 ± 1.81               | 59.14   |\n\nImpact of the Retriever. The results shown in Table 4 highlight the accuracy of GPT-3.5 across different corpora and retrievers on the Mirage benchmark [144]. These findings underscore how retriever performance closely depends on the alignment between training data and the target corpus. For example, in the MEDRAG system, MedCPT-trained specifically on PubMed user logs-significantly improves retrieval performance when accessing the PubMed corpus. This illustrates the benefits of using domain-specific retrievers tailored to specialized datasets.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 144,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-145",
      "content": "y on PubMed user logs-significantly improves retrieval performance when accessing the PubMed corpus. This illustrates the benefits of using domain-specific retrievers tailored to specialized datasets. In contrast, general-purpose retrievers like Contriever, which incorporate Wikipedia data during training, excel in retrieving information from Wikipedia, especially for tasks like MMLU-Med and MedQA-US. On the other hand, SPECTER, which focuses more on regularizing pairwise article distances than optimizing query-to-article relevance, underperforms on the MedCorp corpus. The study also explores combining multiple retrievers using Reciprocal Rank Fusion (RRF). However, results show that adding more retrievers does not always lead to better outcomes; for instance, excluding SPECTER in RRF-2 on Wikipedia yields better results than RRF-4, indicating that simply increasing the number of retrievers is not beneficial unless their strengths align with the retrieval task.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 145,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-146",
      "content": "uding SPECTER in RRF-2 on Wikipedia yields better results than RRF-4, indicating that simply increasing the number of retrievers is not beneficial unless their strengths align with the retrieval task.\n\nFigure 5a illustrates how eRAG investigates the correlation between LLM performance and retrieval effectiveness on the NQ dataset using three retrievers with different characteristics: BM25 (lexical sparse), RetroMAE (dense) [143], and SPLADEv3 (learned sparse) [80]. The initial retrievals are re-ranked using a DeBERTa-v3 [79] cross-encoder. The analysis demonstrates that as retrieval quality improves, LLM performance increases significantly across various models. Notably, reranking with SPLADEv3 and DeBERTa-v3 consistently achieves the best results across datasets and metrics. This underscores the critical role that high-quality retrieval plays in determining overall RAG system effectiveness, suggesting that IR-focused LLMs could be a valuable asset in enhancing generation performance.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 146,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-147",
      "content": "res the critical role that high-quality retrieval plays in determining overall RAG system effectiveness, suggesting that IR-focused LLMs could be a valuable asset in enhancing generation performance.\n\nImpact of the Generator. The BERGEN study [114] compares the performance of LLMs with gold passages (Oracle) against closed-book settings without retrieval, as shown in Figure 5b. Surprisingly, the experiments do not reveal a straightforward relationship between model size and the performance gains from retrieval. For instance, smaller models like LLaMA2-7B benefit more\n\n<!-- image -->\n\n- (a) Impact of retrieval performance on RAG performance for SOLAR-10.7B [75] on NQ with different ranking systems. RR means with additional re-ranking using DeBERTa-v3.\n- (b) Performance gains w/ and w/o oracle retrieval for LLMs with different sizes. Comparing closed book vs oracle passages averaged over all QA datasets in KILT.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 147,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-148",
      "content": "ing using DeBERTa-v3.\n- (b) Performance gains w/ and w/o oracle retrieval for LLMs with different sizes. Comparing closed book vs oracle passages averaged over all QA datasets in KILT.\n\n<!-- image -->\n\n(c) The correlation between eRAG and the downstream performance of different LLM sizes. In this experiment, T5small (60M parameters) and T5-base (220M parameters) with FiD are used. The documents are retrieved using BM25.\n\n<!-- image -->\n\nFig. 5. Retriever and generator experiment results sourced from eRAG [119] and BERGEN [114].\n\nfrom retrieval than larger models like LLaMA2-70B. In fact, LLaMA2-7B with retrieval outperforms LLaMA2-70B in a closed-book setting, suggesting that retrieval augmentation can make smaller models more competitive. Similarly, results from the eRAG experiments in Figure 5c indicate that varying LLM sizes (e.g., T5-small vs. T5-base) does not significantly affect the correlation between eRAG and downstream performance.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 148,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-149",
      "content": "arly, results from the eRAG experiments in Figure 5c indicate that varying LLM sizes (e.g., T5-small vs. T5-base) does not significantly affect the correlation between eRAG and downstream performance. These findings highlight that retrieval quality has a more substantial impact on RAG performance than the choice of generator, reinforcing the notion that investing in better retrieval strategies often yields more benefits than relying solely on larger LLMs.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "8.2 Retriever and Generator",
        "chunkIndex": 149,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-150",
      "content": "The evolving landscape of RAG systems faces significant challenges that impact the quality of generated outputs, system efficiency, and the integration of multimodal data. As these systems become more prevalent across a range of applications, addressing these challenges is essential for improving their effectiveness and scalability.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9 Challenges and Future Directions",
        "chunkIndex": 150,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-151",
      "content": "The quality of retrieval is fundamental to any effective RAG system, directly influencing the relevance and accuracy of the generated content [46, 119, 138, 164]. Current retrieval methods, however, frequently struggle with issues like noise, irrelevant documents, and fragmented information, all of which compromise the generation process.\n\nNoise Robustness. Irrelevant or misleading documents within the retrieved set can introduce noise, leading to hallucinations or unreliable answers. This challenge highlights the need for more sophisticated filtering and context-aware retrieval methods that can better differentiate relevant from irrelevant content. However, Cuconasu et al. [26] present an interesting perspective by showing that, under certain conditions, the inclusion of irrelevant documents can enhance overall accuracy.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.1 Retrieval Quality",
        "chunkIndex": 151,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-152",
      "content": "from irrelevant content. However, Cuconasu et al. [26] present an interesting perspective by showing that, under certain conditions, the inclusion of irrelevant documents can enhance overall accuracy. This finding challenges conventional retrieval strategies and suggests the potential for developing specialized approaches that strategically integrate noise within the retrieval process.\n\nNegative Rejection. When retrieval fails to return relevant results, models often attempt to generate responses regardless, increasing the risk of incorrect outputs. This issue is particularly problematic when queries are poorly expressed or lack sufficient context, making it difficult for retrieval models to surface relevant documents. Techniques like generating a pseudo-document that captures the query's essence, as demonstrated by HyDE [36], can help bridge this gap.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.1 Retrieval Quality",
        "chunkIndex": 152,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-153",
      "content": "it difficult for retrieval models to surface relevant documents. Techniques like generating a pseudo-document that captures the query's essence, as demonstrated by HyDE [36], can help bridge this gap. By allowing retrieval systems to find more relevant documents even from suboptimal queries, HyDE improves retrieval accuracy, albeit with a trade-off in computational cost. Future research could focus on optimizing this process to balance improved retrieval accuracy with reduced latency.\n\nInformation Integration. Complex queries often require synthesizing information from multiple documents, yet fragmented or conflicting information can result in incoherent or incomplete answers. Pre- and post-retrieval techniques play a critical role in addressing this challenge. Enhancing retrieval granularity and incorporating techniques like entity-level retrieval and re-ranking can improve the cohesiveness of retrieved documents. However, many post-retrieval methods, as investigated by Zhu et al.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.1 Retrieval Quality",
        "chunkIndex": 153,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-154",
      "content": "ularity and incorporating techniques like entity-level retrieval and re-ranking can improve the cohesiveness of retrieved documents. However, many post-retrieval methods, as investigated by Zhu et al. [165], rely heavily on calling LLM APIs, which incurs significant costs. Exploring alternatives such as knowledge distillation to lightweight models could offer more scalable solutions, making advanced retrieval strategies more practical in online settings.\n\nRecent research highlights the development of generative models for search as a promising direction for improving retrieval quality. Models like GERE [15] and PARADE [84] enhance document re-ranking and fact verification by directly generating relevant document titles or evidence sentences. Fine-tuning pre-trained models like RankT5 [166] for ranking-specific tasks has also demonstrated potential in boosting out-of-domain performance, which is crucial for generalizing RAG systems across diverse contexts.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.1 Retrieval Quality",
        "chunkIndex": 154,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-155",
      "content": "for ranking-specific tasks has also demonstrated potential in boosting out-of-domain performance, which is crucial for generalizing RAG systems across diverse contexts.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.1 Retrieval Quality",
        "chunkIndex": 155,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-156",
      "content": "System efficiency remains a significant bottleneck, especially as RAG systems scale to handle large datasets and real-time applications. The multi-step nature of RAG workflows-including query classification, retrieval, re-ranking, and generation-adds complexity and latency, which can hinder overall performance.\n\nLatency in Retrieval Processes. As document collections grow, retrieval and re-ranking processes increasingly become sources of latency. Lightweight search methods and hybrid retrieval approaches that combine sparse and dense techniques offer potential solutions by balancing speed and accuracy. For example, indexing, a traditionally resource-intensive process, has seen innovations through\n\ndifferentiable search indices such as DSI [128] and SEAL [7]. These methods integrate retrieval within Transformer models, enabling direct mapping of text queries to document identifiers and thereby improving both performance and retrieval efficiency.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.2 System Efficiency",
        "chunkIndex": 156,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-157",
      "content": "SEAL [7]. These methods integrate retrieval within Transformer models, enabling direct mapping of text queries to document identifiers and thereby improving both performance and retrieval efficiency.\n\nComputational Costs. The introduction of deep learning-based re-ranking models like monoT5 [102] and RankLLaMA [96] brings significant computational overhead, particularly in scenarios requiring iterative reasoning. Future research could focus on optimizing these models or developing retrieval pruning techniques that reduce the number of documents passed to the generation phase without sacrificing performance [145].\n\nModular Workflow Optimization. The complexity of RAG systems often stems from interdependencies between components like chunking strategies, embedding models, and re-ranking algorithms. Modular designs that enable independent optimization of each step while accounting for cross-component interactions are key to enhancing system throughput [39].",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.2 System Efficiency",
        "chunkIndex": 157,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-158",
      "content": "ding models, and re-ranking algorithms. Modular designs that enable independent optimization of each step while accounting for cross-component interactions are key to enhancing system throughput [39]. Advanced chunking methods and hybrid search strategies could offer trade-offs that maximize both retrieval precision and speed. An example is the Hybrid with HyDE [139] approach, which integrates both sparse and dense retrieval to capture relevant documents from both lexical and semantic perspectives.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.2 System Efficiency",
        "chunkIndex": 158,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-159",
      "content": "The expansion of RAG systems to support multimodal data-encompassing text, images, and audio-presents new challenges. Integrating diverse modalities requires not only effective retrieval but also seamless alignment and generation across different data types.\n\nCross-Modal Alignment. Aligning multimodal documents with text-based queries remains a core challenge. The complexity of mapping diverse data types into a unified retrieval framework necessitates improved cross-modal retrieval strategies capable of simultaneously handling text, image, and potentially video or audio data.\n\nCoherent Multimodal Generation. Generating responses that meaningfully integrate information from multiple modalities is another difficult task. Advanced generation models capable of reasoning across different modalities are required to produce outputs that are both contextually relevant and visually coherent.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.3 Multimodal RAG",
        "chunkIndex": 159,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-160",
      "content": "lities is another difficult task. Advanced generation models capable of reasoning across different modalities are required to produce outputs that are both contextually relevant and visually coherent.\n\nRecent advancements in multimodal RAG, such as MuRAG [17], REVEAL [49], and Re-ViLM [152], have shown potential in incorporating multimodal retrieval and generation into real-world applications like visual question answering [18], image captioning [120], and text-to-audio generation [158]. Moving forward, research will likely focus on refining these techniques, especially in scaling multimodal retrieval to handle larger datasets and more complex queries. Extending retrieval capabilities to include more diverse media types, such as video and speech, also represents a promising direction for the continued evolution of RAG systems.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "9.3 Multimodal RAG",
        "chunkIndex": 160,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-161",
      "content": "In this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing the capabilities of LLMs. Through a structured overview of RAG, categorizing various methods, and an in-depth analysis of its core technologies and evaluation methods, this study illuminates the path for future research. It identifies crucial areas for improvement and outlines potential directions for advancing RAG applications, especially in textual contexts. This survey aims to elucidate the core concepts of the RAG field from a retrieval perspective, and it is intended to facilitate further exploration and innovation in the accurate retrieval and generation of information.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "10 Conclusions",
        "chunkIndex": 161,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-162",
      "content": "This research is supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Acknowledgments",
        "chunkIndex": 162,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-163",
      "content": "- [1] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and etc. 2023. GPT-4 Technical Report. arXiv (2023).\n- [2] Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep Listwise Context Model for Ranking Refinement. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval . ACM.\n- [3] Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu. 1998. An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions. J. ACM 45, 6 (1998), 891-923. https://doi.org/10. 1145/293347.293348\n- [4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.11511.\n- [5] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 2023.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 163,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-164",
      "content": "h Self-Reflection. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.11511.\n- [5] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 2023. Optimizing Retrievalaugmented Reader Models via Token Elimination. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 1506-1524.\n- [6] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.).",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 164,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-165",
      "content": "e on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper\\_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-AbstractConference.html\n- [7] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [8] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 165,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-166",
      "content": "ell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of BigScience Episode #5 - Workshop on Challenges &amp; Perspectives in Creating Large Language Models , Vol. abs/2204.06745. Association for Computational Linguistics.\n- [9] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning (ICML) . 2206-2240.\n- [10] Tom B.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 166,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-167",
      "content": ", Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning (ICML) . 2206-2240.\n- [10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Conference on Neural Information Processing Systems (NeurIPS) , Vol. abs/2005.14165.\n- [11] Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, and Tal Schuster. 2022. Tomayto, Tomahto.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 167,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-168",
      "content": "rence on Neural Information Processing Systems (NeurIPS) , Vol. abs/2005.14165.\n- [11] Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, and Tal Schuster. 2022. Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 291-305.\n- [12] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. arXiv abs/2404.00610 (2024).\n- [13] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. arXiv abs/2310.05029 (2023).\n- [14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large Language Models in RetrievalAugmented Generation.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 168,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-169",
      "content": "text Limit through Interactive Reading. arXiv abs/2310.05029 (2023).\n- [14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large Language Models in RetrievalAugmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 16 (2024), 17754-17762.\n- [15] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022. GERE: Generative Evidence Retrieval for Fact Verification. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "References",
        "chunkIndex": 169,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-170",
      "content": "- [16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 170,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-171",
      "content": "night, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv abs/2107.03374 (2021).\n- [17] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022. MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n- [18] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023. Re-Imagen: Retrieval-Augmented Text-toImage Generator. In International Conference on Learning Representations (ICLR) .\n- [19] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix: Democratizing ChatGPT across Languages.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 171,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-172",
      "content": "hen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix: Democratizing ChatGPT across Languages. arXiv abs/2304.10453 (2023).\n- [20] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 12318-12337.\n- [21] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory.. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [22] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 172,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-173",
      "content": "Systems (NeurIPS) .\n- [22] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\n- [23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shiv",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 173,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-174",
      "content": "Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research (JMLR) 24 (2023), 240:1-240:113.\n- [24] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Ad",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 174,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-175",
      "content": "rtha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv abs/2210.11416 (2022).\n- [25] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 8440-8451.\n- [26] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 175,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-176",
      "content": "Computational Linguistics, 8440-8451.\n- [26] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) , Vol. abs/2401.14887.\n- [27] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot Dense Retrieval From 8 Examples. In International Conference on Learning Representations (ICLR) .\n- [28] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-sensitive hashing scheme based on p-stable distributions.. In International Symposium on Computational Geometry (SoCG) . 253-262.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 176,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-177",
      "content": ", Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-sensitive hashing scheme based on p-stable distributions.. In International Symposium on Computational Geometry (SoCG) . 253-262.\n\n- [29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the Conference of the North . Association for Computational Linguistics, 4171-4186.\n- [30] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge-Powered Conversational Agents. In International Conference on Learning Representations (ICLR) .\n- [31] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 177,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-178",
      "content": "2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics.\n- [32] Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In International Conference on Language Resources and Evaluation (LREC) .\n- [33] Shahul ES, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2023. RAGAs: Automated Evaluation of Retrieval Augmented Generation. Conference of the European Chapter of the Association for Computational Linguistics abs/2309.15217 (2023).\n- [34] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-Generation Synergy Augmented Large Language Models.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 178,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-179",
      "content": "on for Computational Linguistics abs/2309.15217 (2023).\n- [34] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-Generation Synergy Augmented Large Language Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing , Vol. abs/2310.05149. IEEE.\n- [35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv abs/2101.00027 (2021).\n- [36] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot Dense Retrieval without Relevance Labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 1762-1777.\n- [37] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 179,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-180",
      "content": "tics (Volume 1: Long Papers) . Association for Computational Linguistics, 1762-1777.\n- [37] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 6894-6910.\n- [38] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv abs/2312.10997 (2023).\n- [39] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024. Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks. arXiv (2024).\n- [40] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank, Generate.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 180,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-181",
      "content": "like Reconfigurable Frameworks. arXiv (2024).\n- [40] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank, Generate. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 2701-2715.\n- [41] Simon Gottschalk and Elena Demidova. 2018. EventKG: A Multilingual Event-Centric Temporal Knowledge Graph . Springer International Publishing. 272-287 pages.\n- [42] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language Model Pre-Training. In International Conference on Machine Learning (ICML) . 3929-3938.\n- [43] William L. Hamilton. 2020. Graph representation learning . Springer International Publishing.\n- [44] Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen E. Robertson, Steve Walker, and P. W. Williams. 1996.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 181,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-182",
      "content": ". 2020. Graph representation learning . Springer International Publishing.\n- [44] Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen E. Robertson, Steve Walker, and P. W. Williams. 1996. Okapi at TREC-5. In Proceedings of The Fifth Text REtrieval Conference, TREC 1996, Gaithersburg, Maryland, USA, November 20-22, 1996 (NIST Special Publication, Vol. 500-238) , Ellen M. Voorhees and Donna K. Harman (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz\n- [45] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding.. In International Conference on Learning Representations (ICLR) .\n- [46] Enrique HerreraViedma, Gabriella Pasi, Antonio G. LopezHerrera, and Carlos Porcel. 2006. Evaluating the information quality of Web sites: A methodology based on fuzzy computing with words.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 182,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-183",
      "content": "R) .\n- [46] Enrique HerreraViedma, Gabriella Pasi, Antonio G. LopezHerrera, and Carlos Porcel. 2006. Evaluating the information quality of Web sites: A methodology based on fuzzy computing with words. Journal of the American Society for Information Science and Technology 57, 4 (2006), 538-549.\n- [47] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-light: Efficient and effective retrievalaugmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1437-1447.\n- [48] Yucheng Hu and Yuxing Lu. 2024. RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing. arXiv abs/2404.19543 (2024).\n- [49] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi. 2023. Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 183,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-184",
      "content": "Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi. 2023. Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2336923379.\n\n- [50] Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen-mei Hwu. 2022. Understanding Jargon: Combining Extraction and Generation for Definition Modeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.\n- [51] Jimmy Xiangji Huang, Jun Miao, and Ben He. 2013. High performance query expansion using adaptive co-training. Inf. Process. Manag. 49, 2 (2013), 441-453. https://doi.org/10.1016/J.IPM.2012.08.002\n- [52] Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, and Lilian H. Y. Tang. 2023. Learning Retrieval Augmentation for Personalized Dialogue Generation..",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 184,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-185",
      "content": "org/10.1016/J.IPM.2012.08.002\n- [52] Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, and Lilian H. Y. Tang. 2023. Learning Retrieval Augmentation for Personalized Dialogue Generation.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 2523-2540.\n- [53] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Z Pan. 2023. Retrieval Augmented Generation with Rich Answer Encoding. Proc. of IJCNLP-AACL 2023 (2023).\n- [54] Xiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for biomedical information retrieval. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009 , James Allan, Javed A. Aslam, Mark Sanderson, ChengXiang Zhai, and Justin Zobel (Eds.). ACM, 307-314. https://doi.org/10.1145/1571941.1571995\n- [55] Yizheng Huang and Jimmy Huang. 2024.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 185,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-186",
      "content": "y 19-23, 2009 , James Allan, Javed A. Aslam, Mark Sanderson, ChengXiang Zhai, and Justin Zobel (Eds.). ACM, 307-314. https://doi.org/10.1145/1571941.1571995\n- [55] Yizheng Huang and Jimmy Huang. 2024. Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges. CoRR abs/2402.11203 (2024). https://doi.org/10.48550/ARXIV.2402.11203 arXiv:2402.11203\n- [56] Yizheng Huang and Jimmy X. Huang. 2023. Diversified Prior Knowledge Enhanced General Language Model for Biomedical Information Retrieval. In ECAI 2023 - 26th European Conference on Artificial Intelligence, September 30 October 4, 2023, Kraków, Poland - Including 12th Conference on Prestigious Applications of Intelligent Systems (PAIS 2023) (Frontiers in Artificial Intelligence and Applications, Vol. 372) , Kobi Gal, Ann Nowé, Grzegorz J. Nalepa, Roy Fairstein, and Roxana Radulescu (Eds.). IOS Press, 1109-1115.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 186,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-187",
      "content": "ligent Systems (PAIS 2023) (Frontiers in Artificial Intelligence and Applications, Vol. 372) , Kobi Gal, Ann Nowé, Grzegorz J. Nalepa, Roy Fairstein, and Roxana Radulescu (Eds.). IOS Press, 1109-1115. https://doi.org/10.3233/FAIA230385\n- [57] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Transactions on Machine Learning Research (TMLR) 2022 (2022).\n- [58] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, 874-880.\n- [59] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 187,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-188",
      "content": "Linguistics, 874-880.\n- [59] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models. Journal of Machine Learning Research (JMLR) 24 (2023), 251:1-251:43.\n- [60] Israt Jahan, Md. Tahmid Rahman Laskar, Chun Peng, and Jimmy Xiangji Huang. 2023. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. CoRR abs/2306.04504 (2023). https://doi.org/10.48550/ARXIV.2306.04504 arXiv:2306.04504\n- [61] Bernard J. Jansen, Danielle L. Booth, and Amanda Spink. 2009. Patterns of query reformulation during Web searching. J. Assoc. Inf. Sci. Technol. 60, 7 (2009), 1358-1371. https://doi.org/10.1002/ASI.21071\n- [62] H Jégou, M Douze, and C Schmid. 2011. Product Quantization for Nearest Neighbor Search.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 188,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-189",
      "content": "ng Web searching. J. Assoc. Inf. Sci. Technol. 60, 7 (2009), 1358-1371. https://doi.org/10.1002/ASI.21071\n- [62] H Jégou, M Douze, and C Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 1 (2011), 117-128.\n- [63] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, and Gustavo Alonso. 2023. Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models. arXiv abs/2310.09949 (2023).\n- [64] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. 2024. PipeRAG: Fast RetrievalAugmented Generation via Algorithm-System Co-design. arXiv abs/2403.05676 (2024).\n- [65] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 189,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-190",
      "content": "g Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7969-7992.\n- [66] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams. Applied Sciences 11, 14 (2021), 6421.\n- [67] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 2567-2577.\n- [68] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535-547. https://doi.org/10.1109/TBDATA.2019.2921572\n- [69] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 190,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-191",
      "content": "Similarity Search with GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535-547. https://doi.org/10.1109/TBDATA.2019.2921572\n- [69] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 1601-1611.\n- [70] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation. arXiv abs/2305.18846 (2023).\n\n- [71] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 191,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-192",
      "content": "s, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 6769-6781.\n- [72] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR) .\n- [73] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. arXiv abs/2212.14024 (2022).\n- [74] Omar Khattab and Matei Zaharia. 2020. ColBERT - Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 192,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-193",
      "content": "ficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 39-48.\n- [75] Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. 2024. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track) , Vol. abs/2312.15166. Association for Computational Linguistics.\n- [76] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 193,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-194",
      "content": "Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 453-466.\n- [77] Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. CoRR abs/2305.18486 (2023). https://doi.org/10.48550/ARXIV.2305.18486 arXiv:2305.18486\n- [78] Md. Tahmid Rahman Laskar, Enamul Hoque, and Jimmy X. Huang. 2020. Query Focused Abstractive Summarization via Incorporating Query Relevance and Transfer Learning with Transformer Models.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 194,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-195",
      "content": ".18486\n- [78] Md. Tahmid Rahman Laskar, Enamul Hoque, and Jimmy X. Huang. 2020. Query Focused Abstractive Summarization via Incorporating Query Relevance and Transfer Learning with Transformer Models. In Advances in Artificial Intelligence - 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13-15, 2020, Proceedings (Lecture Notes in Computer Science, Vol. 12109) , Cyril Goutte and Xiaodan Zhu (Eds.). Springer, 342-348. https://doi.org/10.1007/978-3-030-47358-7\\_35\n- [79] Carlos Lassance and Stéphane Clinchant. 2022. Naver Labs Europe (SPLADE) @ TREC NeuCLIR 2022.. In Text Retrieval Conference (TREC) .\n- [80] Carlos Lassance, Hervé Déjean, Thibault Formal, and Stéphane Clinchant. 2024. SPLADE-v3: New baselines for SPLADE. arXiv abs/2403.06789 (2024).\n- [81] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 195,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-196",
      "content": "PLADE. arXiv abs/2403.06789 (2024).\n- [81] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) . Association for Computational Linguistics.\n- [82] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 7871-7880.\n- [83] Patrick S. H.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 196,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-197",
      "content": "Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 7871-7880.\n- [83] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [84] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2024. PARADE: Passage Representation Aggregation forDocument Reranking. ACM Transactions on Information Systems 42, 2 (2024), 1-26.\n- [85] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Survey on Retrieval-Augmented Text Generation. arXiv abs/2202.01110 (2022).\n- [86] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq R. Joty, Soujanya Poria, and Lidong Bing. 2024.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 197,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-198",
      "content": "2022. A Survey on Retrieval-Augmented Text Generation. arXiv abs/2202.01110 (2022).\n- [86] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq R. Joty, Soujanya Poria, and Lidong Bing. 2024. Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. In The Twelfth International Conference on Learning Representations .\n- [87] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out . Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013\n- [88] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 6385-6400.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 198,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-199",
      "content": "our Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 6385-6400.\n\n- [89] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. RA-DIT: Retrieval-Augmented Dual Instruction Tuning. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.01352.\n- [90] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 199,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-200",
      "content": "Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.\n- [91] Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge. arXiv abs/2311.08147 (2023).\n- [92] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Augmented Large Language Models with Parametric Knowledge Guiding. arXiv abs/2305.04757 (2023).\n- [93] Hengzhao Ma, Jianzhong Li, and Yong Zhang. 2024. Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 200,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-201",
      "content": "Ma, Jianzhong Li, and Yong Zhang. 2024. Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) . IEEE.\n- [94] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented Large Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 5303-5315.\n- [95] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 1. ACM, 2421-2425.\n- [96] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text Retrieval.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 201,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-202",
      "content": "n Research and Development in Information Retrieval , Vol. 1. ACM, 2421-2425.\n- [96] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 2421-2425. https://doi.org/10.1145/3626772.3657951\n- [97] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2020), 824-836. https://doi.org/10.1109/TPAMI.2018.2889473\n- [98] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to information retrieval . Cambridge University Press.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 202,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-203",
      "content": "), 824-836. https://doi.org/10.1109/TPAMI.2018.2889473\n- [98] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to information retrieval . Cambridge University Press. https://doi.org/10.1017/CBO9780511809071\n- [99] Niklas Muennighoff. 2022. SGPT: GPT Sentence Embeddings for Semantic Search. arXiv abs/2202.08904 (2022).\n- [100] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. arXiv abs/2112.09332 (2021).\n- [101] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, MingWei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 203,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-204",
      "content": "01] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, MingWei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 9844-9855.\n- [102] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics.\n- [103] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. CoRR abs/1910.14424 (2019).\n- [104] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 204,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-205",
      "content": "Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. CoRR abs/1910.14424 (2019).\n- [104] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [105] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering.. In Conference on Health, Inference, and Learning (CHIL) . 248-260.\n- [106] Yu Pan, Jianxin Sun, and Hongfeng Yu. 2023. LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic Graph-Based ANN Indexing.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 205,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-206",
      "content": "In Conference on Health, Inference, and Learning (CHIL) . 248-260.\n- [106] Yu Pan, Jianxin Sun, and Hongfeng Yu. 2023. LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic Graph-Based ANN Indexing. In 2023 IEEE International Conference on Big Data (BigData) . IEEE.\n- [107] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL '02) . Association for Computational Linguistics, USA, 311-318. https://doi.org/10. 3115/1073083.1073135\n- [108] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming\n\nCurated Corpora with Web Data Only.. In Conference on Neural Information Processing Systems (NeurIPS) .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 206,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-207",
      "content": "tesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming\n\nCurated Corpora with Web Data Only.. In Conference on Neural Information Processing Systems (NeurIPS) .\n\n- [109] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 2523-2544.\n- [110] Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information retrieval metrics.. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 667-674.\n- [111] Colin Raffel, Noam M.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 207,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-208",
      "content": "ng the sensitivity of information retrieval metrics.. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 667-674.\n- [111] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR) 21 (2020), 140:1-140:67.\n- [112] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316-1331.\n- [113] Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022. Learning to Retrieve Passages without Supervision. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 208,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-209",
      "content": "2022. Learning to Retrieve Passages without Supervision. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics.\n- [114] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane Clinchant. 2024. BERGEN: A Benchmarking Library for Retrieval-Augmented Generation. arXiv abs/2407.01102 (2024).\n- [115] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, 3980-3990.\n- [116] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 209,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-210",
      "content": "al Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, 3980-3990.\n- [116] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333-389.\n- [117] Jon Saad-Falcon, O. Khattab, Christopher Potts, and Matei Zaharia. 2023. ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2311.09476.\n- [118] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 752-762.\n- [119] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 210,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-211",
      "content": "ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 752-762.\n- [119] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 21. ACM, 2395-2400.\n- [120] Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022. Retrieval-Augmented Transformer for Image Captioning. In International Conference on Content-based Multimedia Indexing . ACM.\n- [121] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing RetrievalAugmented Large Language Models with Iterative Retrieval-Generation Synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 9248-9274.\n- [122] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 211,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-212",
      "content": "stics: EMNLP 2023 . Association for Computational Linguistics, 9248-9274.\n- [122] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2301.12652.\n- [123] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. 2024. ERAGent: Enhancing RetrievalAugmented Language Models with Improved Accuracy, Efficiency, and Personalization. arXiv abs/2405.06683 (2024).\n- [124] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. arXiv abs/2403.10081 (2024).\n- [125] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-Augmented Language Models.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 212,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-213",
      "content": "he Real-time Information Needs of Large Language Models. arXiv abs/2403.10081 (2024).\n- [125] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-Augmented Language Models. In International Conference on Learning Representations (ICLR) .\n- [126] Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, and Jun Deguchi. 2024. AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval. arXiv abs/2404.06004 (2024).\n- [127] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying Language Learning Paradigms. In International Conference on Learning Representations (ICLR) .\n- [128] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 213,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-214",
      "content": "ions (ICLR) .\n- [128] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer Memory as a Differentiable Search Index. In Conference on Neural Information Processing Systems (NeurIPS) .\n\n- [129] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . Association for Computational Linguistics, 809-819.\n- [130] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 214,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-215",
      "content": "lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 215,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-216",
      "content": "iyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv abs/2307.09288 (2023).\n- [131] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 10014-10037.\n- [132] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion And",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 216,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-217",
      "content": ", Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC Bioinformatics 16, 1 (2015).\n- [133] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Neural Information Processing Systems . 5998-6008.\n- [134] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Conference on Neural Information Processing Systems (NeurIPS) . 3261-3275.\n- [135] Haoyu Wang, Tuo Zhao, and Jing Gao. 2024.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 217,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-218",
      "content": "A Stickier Benchmark for General-Purpose Language Understanding Systems. In Conference on Neural Information Processing Systems (NeurIPS) . 3261-3275.\n- [135] Haoyu Wang, Tuo Zhao, and Jing Gao. 2024. BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering. arXiv abs/2402.11129 (2024).\n- [136] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv abs/2212.03533 (2022).\n- [137] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 9414-9423.\n- [138] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. 2022. WebFormer: The Web-page Transformer for Structure Information Extraction.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 218,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-219",
      "content": "tational Linguistics, 9414-9423.\n- [138] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. 2022. WebFormer: The Web-page Transformer for Structure Information Extraction. In Proceedings of the ACM Web Conference 2022 . ACM.\n- [139] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, and Xuanjing Huang. 2024. Searching for Best Practices in Retrieval-Augmented Generation. arXiv abs/2407.01219 (2024).\n- [140] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023. KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. arXiv abs/2308.11761 (2023).\n- [141] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 219,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-220",
      "content": "Knowledge Bases. arXiv abs/2308.11761 (2023).\n- [141] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation. arXiv abs/2311.08377 (2023).\n- [142] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, and etc. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv abs/2211.05100 (2022).\n- [143] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 538-548.\n- [144] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking Retrieval-Augmented Generation for Medicine. arXiv abs/2402.13178 (2024).\n- [145] Jie Xiong, Li Yu, Xi Niu, and Youfang Leng. 2023. XRR: Extreme multi-label text classification with candidate retrieving and deep ranking. Information Sciences 622 (2023), 115-132.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 220,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-221",
      "content": "178 (2024).\n- [145] Jie Xiong, Li Yu, Xi Niu, and Youfang Leng. 2023. XRR: Extreme multi-label text classification with candidate retrieving and deep ranking. Information Sciences 622 (2023), 115-132.\n\n- [146] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In International Conference on Learning Representations (ICLR) .\n- [147] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.04408.\n- [148] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation. In Proceedings of the ACM on Web Conference 2024 , Vol. 21.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 221,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-222",
      "content": "Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation. In Proceedings of the ACM on Web Conference 2024 , Vol. 21. ACM, 1330-1340.\n- [149] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. arXiv abs/2401.15884 (2024).\n- [150] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: MultiRound Retrieval-Augmented Generation Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 33. ACM, 730-740.\n- [151] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 222,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-223",
      "content": "Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 5364-5375.\n- [152] Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023. Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 11844-11857.\n- [153] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 223,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-224",
      "content": "nov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 2369-2380.\n- [154] Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval-Augmented Generation: A Survey. arXiv abs/2405.07437 (2024).\n- [155] Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul N. Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. FewShot Generative Conversational Query Rewriting. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1933-1936.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 224,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-225",
      "content": "nt in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1933-1936. https://doi.org/10.1145/3397271.3401323\n- [156] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than Retrieve: Large Language Models are Strong Context Generators. In International Conference on Learning Representations (ICLR) .\n- [157] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 2421-2436.\n- [158] Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2024. Retrieval-Augmented Text-to-Audio Generation.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 225,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-226",
      "content": "apers) . Association for Computational Linguistics, 2421-2436.\n- [158] Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2024. Retrieval-Augmented Text-to-Audio Generation. In ICASSP - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Vol. abs/2309.08051. IEEE.\n- [159] Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, and Dong Wang. 2024. Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2403.14952.\n- [160] Saber Zerhoudi and Michael Granitzer. 2024. PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents. arXiv (2024).\n- [161] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 226,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-227",
      "content": "cing Retrieval-Augmented Generation Systems with User-Centric Agents. arXiv (2024).\n- [161] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. arXiv abs/2205.01068 (2022).\n- [162] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv abs/2402.19473 (2024).\n- [163] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. 2024. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.06117.",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 227,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-228",
      "content": "uoc V Le, and Denny Zhou. 2024. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.06117.\n\n- [164] Ning Zhong, Yuefeng Li, and Sheng-Tang Wu. 2012. Effective Pattern Discovery for Text Mining. IEEE Transactions on Knowledge and Data Engineering 24, 1 (2012), 30-44.\n- [165] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. arXiv abs/2308.07107 (2023).\n- [166] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. 2023. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM.\n\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 228,
        "totalChunks": 230
      }
    },
    {
      "id": "2404.10981v2-chunk-229",
      "content": "46th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM.\n\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
      "metadata": {
        "source": "arxiv:2404.10981v2",
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
          "Yizheng Huang",
          "Jimmy Huang"
        ],
        "section": "Information Retrieval . ACM.",
        "chunkIndex": 229,
        "totalChunks": 230
      }
    }
  ],
  "fullText": "## The Survey of Retrieval-Augmented Text Generation in Large Language Models\n\nYIZHENG HUANG and JIMMY X. HUANG, York University, Canada\n\nRetrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's mechanics and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.\n\n## CCS Concepts: · Computing methodologies → Natural language generation ; · Information systems → Information retrieval .\n\nAdditional Key Words and Phrases: retrieval-augmented generation, information retrieval, large language model\n\n## ACMReference Format:\n\nYizheng Huang and Jimmy X. Huang. 2018. The Survey of Retrieval-Augmented Text Generation in Large Language Models. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX). ACM, New York, NY, USA, 37 pages. https://doi.org/XXXXXXX.XXXXXXX\n\n## 1 Introduction\n\nThe advent of ChatGPT has significantly impacted both academia and industry due to its interactive capabilities and widespread application, establishing itself as a leading artificial intelligence tool [55, 60, 77]. At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by [1], which has seen numerous enhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language Processing (NLP) tasks [78]. Despite these advancements, the adoption of LLMs has highlighted several critical issues primarily due to their reliance on extensive datasets. This reliance restricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data to maximize accessibility and applicability results in subpar performance in specialized areas. Second, the rapid creation of online data, combined with the significant resources required for data annotation and model training,\n\nAuthors' Contact Information: Yizheng Huang, hyz@yorku.ca; Jimmy X. Huang, jhuang@yorku.ca, York University, Toronto, Ontario, Canada.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nConference acronym 'XX, June 03-05, 2018, Woodstock, NY\n\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n\nACM ISBN 978-1-4503-XXXX-X/18/06\n\nhttps://doi.org/XXXXXXX.XXXXXXX\n\nFig. 1. An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the scope of the training data and generates correct results.\n\n<!-- image -->\n\nhinders LLMs' ability to stay updated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as 'hallucinations', which can mislead users.\n\nAddressing these challenges is crucial for LLMs to be effectively utilized across various domains. A promising solution is the integration of Retrieval-Augmented Generation (RAG) technology, which supplements models by fetching external data in response to queries, thus ensuring more accurate and current outputs. Figure 1 illustrates how RAG can enable ChatGPT to provide precise answers beyond its initial training data.\n\nSince its introduction by Lewis et al. [83] in 2020, RAG has seen rapid development, especially with the rise of models like ChatGPT. Despite these advancements, there remains a noticeable gap in the literature regarding a comprehensive analysis of the mechanisms underlying RAG and the progress achieved by subsequent studies. Moreover, the field suffers from fragmented research focuses and inconsistent terminology for similar methods, leading to confusion. This survey seeks to bridge this gap by offering a structured overview of RAG, categorizing various approaches, and providing an in-depth understanding of the current research landscape, with a focus on textual applications given their prominence in recent research.\n\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10.\n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications.\n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.\n\nFig. 2. The unified RAG core concepts with basic workflow.\n\n<!-- image -->\n\n## 2 RAG Framework\n\nThe hallucinations are largely attributed to LLMs' inability to access up-to-date information. This limitation stems from the models' reliance on their training datasets. RAG proposes a solution to this issue by supplementing the LLM's training data with current information from external sources through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more cost-effective alternative to the extensive training and fine-tuning processes typically required for LLMs. It allows for the dynamic incorporation of fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly integrate this new data into the LLM. This feature makes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through RAG is derived from real-world data, authored by humans, which not only simplifies the generation process but also increases the reliability of the generated responses.\n\nResearch by Khandelwal et al. [72] demonstrates that accessing relevant information from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG has evolved from a means of providing supplementary information to enabling multiple interactions between the retrieval and generation components. This involves conducting several rounds of retrieval to refine the accuracy of the information retrieved and iteratively improve the quality of the generated output. Toolkits such as LangChain 1 and LlamaIndex 2 have modularized the RAG approach, enhancing its adaptability and expanding its range of applications. Despite these toolkits employing diverse methodologies to tackle different aspects of RAG-from multiple search iterations to iterative generation-they maintain adherence to the fundamental RAG workflow. This consistency is crucial for understanding their operation and pinpointing opportunities for further development.\n\n## 2.1 Basic RAG Workflow\n\nFigure 2 represents the unified RAG core concepts with basic workflow. The workflow of RAG begins with the creation of an index comprising external sources. This index serves as the basis for retrieving relevant information through a retriever model based on a specific query. The final step\n\n1 https://www.langchain.com\n\n2 https://www.llamaindex.ai\n\ninvolves a generator model, which combines the retrieved information with the query to produce the desired output.\n\n2.1.1 Indexing. Efficient retrieval begins with comprehensive indexing, where data preparation is key. This stage involves text normalization processes such as tokenization, stemming, and the removal of stop words to enhance the text's suitability for indexing [98]. Text segments are then organized into sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing through the use of pretrained LMs for generating semantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly enhancing retrieval efficiency.\n\n2.1.2 Retrieval. While traditional retrieval methods, such as the BM25 algorithm [44], focus on term frequency and presence for document ranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT [29], which capture the semantic essence of queries more effectively. These models improve search accuracy by considering synonyms and the structure of phrases, thereby refining document ranking through the detection of semantic similarities. This is typically achieved by measuring vector distances between documents and queries, combining traditional retrieval metrics with semantic understanding to yield search results that are both relevant and aligned with user intent.\n\n2.1.3 Generation. The generation phase is tasked with producing text that is both relevant to the query and reflective of the information found in the retrieved documents. The usual method involves concatenating the query with the retrieved information, which is then fed into an LLM for text generation [85]. Although ensuring the generated text's alignment and accuracy with the retrieved content presents challenges, it is also essential to strike a balance between adhering closely to the source material and infusing the output with creativity. The generated text should accurately convey the information from the retrieved documents and align with the query's intent, while also offering the flexibility to introduce new insights or perspectives not explicitly contained within the retrieved data.\n\n## 2.2 RAG Paradigm\n\nThe RAG paradigm organizes research within the domain, offering a straightforward yet robust framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for generating high-quality outcomes. Therefore, this paradigm is structured into four main phases from a retrieval perspective: pre-retrieval, retrieval, post-retrieval, and generation. Both single-hop and multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase structure. Figure 3 is the taxonomy tree of RAG's core techniques.\n\n2.2.1 Pre-Retrieval. The pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and query preparation, ensuring efficient information retrieval. This phase includes essential tasks to prepare for effective data access.\n\nIndexing. The process starts with indexing, which establishes an organized system to enable fast and accurate retrieval of information. The specificity of indexing depends on the task and data type. For example, sentence-level indexing is beneficial for question-answering systems to precisely locate answers, while document-level indexing is more appropriate for summarizing documents to understand their main concepts and ideas.\n\nQuery Manipulation. After indexing, query manipulation is performed to adjust user queries for a better match with the indexed data. This involves query reformulation [61, 155], which rewrites\n\nthe query to align more closely with the user's intention; query expansion [51], which extends the query to capture more relevant results through synonyms or related terms; and query normalization, which resolves differences in spelling or terminology for consistent query matching.\n\nData Modification. Data modification is also critical in enhancing retrieval efficiency. This step includes preprocessing techniques like removing irrelevant or redundant information to improve the quality of results and enriching the data with additional information such as metadata to boost the relevance and diversity of the retrieved content [6].\n\n## 2.2.2 Retrieval.\n\nSearch &amp; Ranking. The retrieval stage is the combination of search and ranking. It focuses on selecting and prioritizing documents from a dataset to enhance the quality of the generation model's outputs. This stage employs search algorithms to navigate through the indexed data, finding documents that match a user's query. After identifying relevant documents, the process of initially ranking these documents starts to sort them according to their relevance to the query.\n\n2.2.3 Post-Retrieval. The post-retrieval phase serves to refine the initially retrieved documents to improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed at optimizing the document selection for the final generation task.\n\nRe-Ranking. In the re-ranking step, the documents previously retrieved are reassessed, scored, and reorganized. The objective is to more accurately highlight the documents most relevant to the query and diminish the importance of the less relevant ones. This step involves incorporating additional metrics and external knowledge sources to enhance precision. In this context, pre-trained models with superior accuracy but lower efficiency can be effectively employed due to the limited set of candidate documents available [54].\n\nFiltering. Filtering aims to remove documents that fail to meet specified quality or relevance standards [56, 74]. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below a certain relevance level. Furthermore, the use of feedback from users or prior relevance evaluations assists in adjusting the filtering process, guaranteeing that only the most relevant documents are retained for text generation.\n\n2.2.4 Generation. The generation stage is a crucial component of the RAG process, responsible for leveraging retrieved information to enhance the quality of the generated response. This stage encompasses several sub-steps aimed at producing content that is readable, engaging, and informative.\n\nEnhancing. At the heart of the generation phase is the enhancement step, where the objective is to merge the retrieved information with the user's query to create a coherent and relevant response. This includes the process of elaboration, adding extra details to the retrieved content to enrich it. Efforts are focused on improving the output's quality by increasing its clarity, coherence, and stylistic appeal through methods such as rephrasing and restructuring. Information from various sources is combined to offer a comprehensive perspective, and verification is conducted to ensure the accuracy and relevance of the content.\n\nCustomization. Customization is user-centric. It encompasses tailoring content in two primary ways. First, it aligns the generated output with relevant information retrieved in earlier stages, ensuring consistency and accuracy by incorporating key knowledge. Second, it adapts the content to suit user-specific factors such as intended audience, situational context, and personal preferences, shaping the response to be both contextually relevant and user-centric. This dual focus on\n\nFig. 3. Taxonomy tree of RAG's core techniques\n\n<!-- image -->\n\nintegrating relevant knowledge and adjusting to diverse contextual demands forms the basis of effective customization in RAG.\n\n## 3 Pre-Retrieval\n\n## 3.1 Indexing\n\nOne of the most commonly used indexing structures in traditional information retrieval systems is the inverted index. This structure associates documents with words to form a vocabulary list, allowing users to quickly locate references where a specific word appears within a collection of documents. The vocabulary list here refers to the set of all unique words present in the document collection, while the reference includes the documents where the word appears, along with the word's position and weight within those documents. However, traditional indexing structures struggle to retrieve documents that are semantically related to a user's query but do not contain the exact query terms.\n\nTo address this limitation, retrieval methods using dense vectors generated by deep learning models have become the preferred choice. These vectors, also known as embeddings, capture the semantic meaning of words and documents, allowing for more flexible and accurate retrieval. Dense vector-based indexing methods can be categorized into three main types: graphs, product quantization (PQ) [62], and locality-sensitive hashing (LSH) [28]. Since generating dense vectors with large language models requires substantial resources, and the document collections to be searched are typically vast, the core strategy of these indexing methods is based on approximate nearest neighbor search (ANNS) [3]. This approach significantly speeds up the search process at the cost of a slight reduction in search accuracy.\n\nGraph. Using graphs to build indexes is a common practice in RAG. By indexing vectors with a graph structure, the range of nodes where distances need to be computed during retrieval can be limited to a local subgraph, thereby enhancing search speed. Several prominent methods and tools have been developed using this approach. For example, k-nearest neighbor language models kNN-LMs [72], as demonstrated by Khandelwal et al., integrate the kNN algorithm with pre-trained language models. This method employs a datastore created from collections of texts to dynamically retrieve contextually relevant examples, enhancing model performance without requiring additional training. FAISS [68], a tool widely adopted for indexing in many studies [72, 73, 83], integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation\n\n[97] to further speed up retrieval [83]. WebGPT [100] showcases another practical application by utilizing the Bing API 3 for indexing based on actual user search histories, which illustrates the potential of integrating real-world user data into the retrieval process. Additionally, other methods like MEMWALKER [13] introduces innovative approaches to overcome limitations such as context window size in large language models. It creates a memory tree from input text, segmenting the text into smaller pieces and summarizing these segments into a hierarchical structure. Moreover, LRUS-CoverTree method [93] designed another tree structure for k-Maximum Inner-Product Search (k-MIPS) and achieves performance comparable with significantly lower index construction time. These techniques facilitate efficient indexing and management of large information volumes, demonstrating the versatility and effectiveness of graph-based approaches.\n\nProduct Quantization. PQ is one of the most representative methods for handling large-scale data. It accelerates searches by segmenting vectors and then clustering each part for quantization. Unlike graph-based methods, which speed up searches by reducing the number of vectors for distance calculation, PQ achieves faster searches by reducing the time spent on calculating word distances. Several implementations of PQ have emerged in RAG, each improving its efficiency and scalability in different ways. PipeRAG [64] integrates PQ within a pipeline-parallelism framework to enhance retrieval-augmented generation by optimizing retrieval intervals. Chameleon system [63] leverages PQin a disaggregated accelerator environment to balance memory usage and retrieval speed in RAG tasks. AiSAQ [126] introduces an all-in-storage ANNS method that offloads PQ vectors from DRAM to storage, drastically reducing memory usage while maintaining high recall. It demonstrates that even with billion-scale datasets, memory usage can be minimized to around 10 MB with only minor latency increases, making it a highly scalable solution for RAG systems.\n\nLocality-sensitive Hashing. The core idea of LSH is to place similar vectors into the same hash bucket with high probability. LSH uses hash functions that map similar vectors to the same or nearby hash values, making it easier to find approximate nearest neighbours. In LSH, when a query vector is hashed, the system quickly retrieves candidate vectors that share the same hash value. This method reduces the dimensionality of the problem and can be implemented efficiently, but it may introduce some inaccuracies due to the hashing process itself. While LSH is rarely used in RAG systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over the slight loss in accuracy.\n\n## 3.2 Query Manipulation\n\nQuery manipulation is pivotal in enhancing the effectiveness and accuracy of modern IR systems. By refining users' original queries, it addresses challenges such as ambiguous phrasing and vocabulary mismatches between the query and target documents. This process involves more than merely replacing words with synonyms; it requires a deep understanding of user intent and the context of the query, particularly in complex tasks like RAG. Effective query manipulation significantly boosts retrieval performance, which in turn can greatly impact the quality of generated outputs. The three primary approaches to query manipulation are query expansion, query reformulation, and prompt-based rewriting.\n\nQuery Expansion. Query expansion involves augmenting the original query with additional terms or phrases that are related to or synonymous with the query terms. In the context of LLMs, query expansion can be more sophisticated, utilizing the model's extensive knowledge to generate contextually relevant expansions. This technique aims to improve recall by ensuring that the retrieval process captures a broader range of relevant documents, accommodating different\n\n3 https://www.microsoft.com/en-us/bing/apis/bing-web-search-api\n\nterminologies or expressions. Techniques such as synonym expansion, semantic similarity, or leveraging external knowledge bases are commonly employed in query expansion. For example, the method described in FiD [58] expands the query by retrieving a wider range of passages using both sparse and dense retrieval techniques, enabling the model to aggregate evidence from multiple sources and thereby improving the accuracy and robustness of generated answers. A more advanced form of query expansion is demonstrated in Query2doc [137], where pseudo-documents generated by LLMs enhance the original query, effectively bridging the gap between the user's input and the corpus information, which benefits both sparse and dense retrieval systems. Similarly, KnowledGPT [140] broadens the scope of information accessed during retrieval by leveraging external knowledge bases, further refining the retrieval process. The RARG [159] framework uses an evidence-driven query expansion approach, incorporating a wide array of supporting documents to generate informed and accurate counter-misinformation responses.\n\nQuery Reformulation. Query reformulation involves rephrasing or restructuring the original query to enhance its effectiveness. This might include making the wording more specific, removing vague terms, or adjusting the syntax to better align with the retrieval system's requirements. With LLMs, query reformulation can be dynamically driven by understanding the user's intent and context, allowing for more precise modifications that lead to improved retrieval results. This reformulation process can also be informed by past queries or user interactions, adapting the query to better fit the specific retrieval task. For instance, the RQ-RAG [12] model represents an advanced form of query reformulation by rewriting, decomposing, and disambiguating queries, making it particularly effective in scenarios that demand complex query handling. This approach ensures that the refined query better matches the needed context, improving the relevance of retrieved information. Rewrite-Retrieve-Read framework [94] adjusts the original query to optimize the retrieval process, allowing the system to more effectively leverage retrieved data for generating accurate responses. Additionally, FLARE [65] exemplifies query reformulation through its active retrieval-augmented generation approach, which iteratively refines the query based on a feedback loop between retrieval and generation, thereby enhancing the accuracy and relevance of the retrieved information.\n\nPrompt-based Rewriting. Prompt-based rewriting, particularly in the context of LLMs, represents an innovative approach where the original query is embedded within a larger prompt or context to guide the LLM's response. This technique harnesses the model's ability to understand and generate language within a specific context, effectively rewriting the query to align with the desired output. Prompt-based rewriting is especially powerful in scenarios where the retrieval process is integrated into a generative workflow, allowing the system to adapt the query to various stages of retrieval and generation. This approach may also involve dynamic prompts that evolve based on interaction, further refining the retrieval process. For example, Step-Back [163] refines the query context through carefully crafted prompts that guide the LLM's reasoning process, ensuring that the outputs are more aligned with the user's intent, particularly in complex reasoning tasks. The CoK [86] method focuses on dynamically adapting the knowledge source and using prompts to rewrite the context in which a query is interpreted. This approach leverages prompt-based rewriting to enable the LLM to effectively integrate and ground its responses based on various heterogeneous knowledge sources. Additionally, Promptagator [27] discusses using prompt-based techniques to adapt and rewrite the query to better align with the retrieval system's expectations, particularly in few-shot learning scenarios. These prompts guide the model in generating or refining the query to optimize retrieval results.\n\n## 3.3 Data Modification\n\nDocument modification techniques play a critical role in enhancing retrieval performance, particularly when integrated with LLMs. These techniques can be broadly categorized into Internal Data Augmentation and External Data Enrichment. Internal Data Augmentation focuses on maximizing the value of existing information within documents or models, while External Data Enrichment introduces supplementary data from outside sources to fill gaps, provide additional context, or broaden the scope of the content.\n\nInternal Data Augmentation. Internal Data Augmentation leverages information already present within documents or taps into the inherent knowledge embedded in LLMs. Techniques like paraphrasing, where content is rewritten for improved readability or multiple perspectives, and summarization, which condenses information while retaining core content, are commonly employed. Other methods involve generating supplementary content or explanations that are contextually related without introducing external data. For instance, RECITE [125] utilizes a model's internal memory to recite relevant information before generating responses, thus enhancing performance in tasks like closed-book question answering without external data. KnowledGPT [140] similarly refines the internal knowledge embedded within LLMs, optimizing its use during generation. GENREAD [156] further demonstrates how pre-existing knowledge within LLMs can be used to generate context that enhances task performance, bypassing the need for external sources. In another example, the Selfmem [21] framework allows the model to iteratively use its own outputs as memory in subsequent generation tasks. By selecting and utilizing the best internal outputs as memory, this approach boosts model performance without depending on external memory resources.\n\nExternal Data Enrichment. External Data Enrichment enhances document content by incorporating new information from external sources, enriching the overall context and accuracy. This process can involve integrating facts, data, or contextual knowledge from external datasets or knowledge bases. For example, RA-DIT [89] augments input prompts during fine-tuning by leveraging large datasets like Wikipedia and CommonCrawl, enhancing the model's capability in knowledgeintensive tasks. The dual instruction tuning technique optimizes both the language model and the retriever to more effectively incorporate retrieved information. UPRISE [20] demonstrates how retrieving prompts from diverse task datasets improves model generalization in zero-shot scenarios by enriching the context during inference. Additionally, RARG [159] exemplifies external data enrichment by integrating scientific evidence from academic databases to strengthen responses countering misinformation. This method involves a two-stage retrieval pipeline that identifies and ranks relevant documents, which are then used to support and enhance the factual accuracy of generated responses.\n\n## 4 Retrieval\n\n## 4.1 Search &amp; Ranking\n\nThe search and ranking process within RAG is crucial for improving the relevance and accuracy of generated outputs. Several methodologies have been developed to refine this process, each contributing unique strategies for enhancing retrieval and ranking. For example, Atlas [59] and AAR [157] both aim to improve the relevance of retrieved documents, but they approach this challenge differently. Atlas focuses on optimizing the retriever's ability to select contextually relevant documents, especially in new domains with limited data, by employing few-shot learning techniques such as Attention Distillation and Perplexity Distillation. AAR, on the other hand, adapts retrieval preferences to better align with the requirements of LLMs, enhancing retrieval generalization across tasks by training a smaller source model.\n\nFig. 4. An example of a typical RAG framework with interative retrieval strategy.\n\n<!-- image -->\n\nAdditionally, IRCOT [131] and FLARE [65] introduce dynamic interactions within the retrieval process, albeit with distinct goals. IRCOT integrates retrieval with chain-of-thought (CoT) reasoning, interleaving these processes to ensure that each retrieval step supports the ongoing reasoning task. FLARE, in contrast, adopts a confidence-based active retrieval mechanism, dynamically triggering retrieval when the model generates low-confidence tokens. This approach is particularly useful in scenarios where model confidence varies, as it allows the system to fetch additional information to resolve uncertainties during the generation process.\n\nWhenaddressing domain-specific retrieval challenges, SURGE [70] and PRCA [151] offer different solutions. SURGE uses a subgraph retriever to extract relevant subgraphs from knowledge graphs,\n\nintegrating structured data into the retrieval process to improve the contextual understanding of generated responses. The relational structure of knowledge graphs allows for more accurate and informed retrieval. PRCA, in contrast, focuses on domain-specific abstractive summarization, using a reward-driven approach to refine the retrieved content. This strategy is designed to optimize content for the generator, particularly in scenarios where the generator functions as a black box, thereby enhancing alignment between retrieval and generation.\n\nMEMWALKER [13] presents a unique approach to handling long-context question answering by incorporating an internal search and ranking mechanism within a memory tree structure. This method navigates extensive memory stores, ensuring that the most relevant information is retrieved and used for complex queries. Unlike other methods, MEMWALKER emphasizes efficient processing of long texts through iterative navigation and summarization, rather than solely optimizing the initial retrieval phase.\n\n## 4.2 Retrieval Strategy\n\nThe retrieval strategies within RAG are vital for customizing the retrieval process to specific application needs, with each strategy offering distinct advantages and addressing particular challenges. In RAG, it is mostly the utilization of retrieval techniques rather than the exploration of retrieval algorithms that is involved, so it is the strategy of retrieval that is usually considered in searching and ranking. While basic RAGs are usually single-hop searches, i.e., they are retrieved only once as generated supplementary material, today's RAGs are mostly multi-hop searches, i.e., they are searched several times through different search strategies until they are satisfied. In terms of practical applications these strategies belong to the design on the engineering pipeline. Figure 4 shows a typical case of the RAG framework with iterative retrieval strategy. There are five main retrieval strategies in RAG:\n\nBasic Retrieval Strategy. Basic retrieval strategies typically follow a linear workflow, moving sequentially through pre-retrieval, retrieval, post-retrieval, and generation phases. The Atlas [94] framework exemplifies this straightforward approach, guiding the retrieval process efficiently from start to finish without iterations or complex conditional modifications. REPLUG [122] similarly follows this basic strategy, augmenting black-box language models with retrieval in a simple manner, where the retrieved information is directly used to enhance the generation process.\n\nIterative Retrieval Strategy. For more complex scenarios, iterative retrieval strategies (Algorithm 1) are employed, where information is retrieved in multiple steps, each informed by previous results. IRCOT [131] exemplifies this by integrating retrieval with chain-of-thought reasoning, where the retrieval process is sequential and closely tied to reasoning steps. This method is particularly effective in scenarios requiring multi-step problem-solving, such as research assistance or complex queries that benefit from detailed exploration. ITER-RETGEN [121] also employs iterative retrieval, refining the process based on generated responses, allowing for continuous improvement and closer alignment between retrieval and generation. RQ-RAG [12] advances this approach by using techniques like query rewriting, decomposition, and disambiguation, refining the retrieval step-bystep to enhance the final output. PlanRAG [81] also fits within this strategy, iteratively refining the retrieval process based on generated content and feedback, ensuring that each step is better informed than the last.\n\nRecursive Retrieval Strategy. Recursive retrieval (Algorithm 2) involves retrieval that can call itself, creating a hierarchy or tree of retrievals. This method effectively handles hierarchical or layered information by breaking down complex queries into simpler sub-queries. It is particularly useful for hierarchical data exploration, knowledge base construction, and detailed information\n\n## Algorithm 1 Iterative Retrieval Strategy in RAG\n\n```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: while 𝑖 ≤ 𝑁 do 3: Pre-retrieval Phase 4: 𝑞 ′ ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ) // Indexing, Query Manipulation, Data Modification 5: Retrieval Phase 6: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷 ) // Search and initial ranking of documents 7: Post-retrieval Phase 8: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 ) // Re-ranking and filtering to refine documents 9: Generation Phase 10: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 ) // Generate output based on refined documents 11: if stopping condition met based on 𝑦 𝑖 then 12: BREAK // Stop iterations if output is satisfactory 13: end if 14: Update 𝑞 ′ ← UpdateQuery ( 𝑞,𝑦 𝑖 ) // Refine query based on the generated output 15: 𝑖 ← 𝑖 + 1 // Increment iteration counter 16: end while 17: Final Synthesis 18: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 19: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nretrieval. SURGE [70] leverages this strategy through knowledge graphs, where relevant subgraphs are extracted to enhance contextual understanding. The relational structure of knowledge graphs facilitates navigating multiple layers of information, ensuring accurate and contextually relevant retrieval. MEMWALKER [13] similarly adopts a recursive approach, processing long texts by constructing a memory tree of summaries. The system navigates through this tree to retrieve relevant information, effectively breaking down complex queries into manageable segments, which is particularly useful for handling long-context question answering. IMRAG [150] introduces a multi-round retrieval mechanism, where each round of retrieval is based on the model's internal monologues, progressively refining the search with each iteration. Selfmem [21] employs a selfmemory module, enabling the system to store and retrieve information recursively, building upon previously retrieved knowledge in a hierarchical manner. This recursive strategy enhances the system's ability to manage and integrate vast amounts of information across multiple retrieval iterations.\n\nConditional Retrieval Strategy. Conditional retrieval strategies (Algorithm 3) are governed by specific conditions or rules, which may be predefined or dynamically determined during the process. This method ensures that retrieval aligns with specific constraints or criteria, enhancing relevance and specificity. It is particularly useful for compliance checking, rule-based recommendation systems, and context-sensitive information retrieval. PRCA [151] is a prime example, where retrieval strategies are adapted based on reward-driven adjustments, refining the context used by large language models to enhance precision and relevance. RARG [159] similarly emphasizes retrieval based on specific evidence conditions, ensuring that the retrieval process aligns with predefined requirements, which is critical for generating factual and polite responses. CRAG [149] adds another\n\n## Algorithm 2 Recursive Retrieval Strategy in RAG\n\n```\nRequire: Initial Query 𝑞 , Documents 𝐷 , Maximum Depth 𝐿 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Sub-query Generation Function 𝐹 𝑠𝑢𝑏𝑞 , Hierarchical Layer Building Function 𝐹 𝑏𝑢𝑖𝑙𝑑 , Hierarchical Information Operating Function 𝐹 ℎ𝑖𝑒𝑟 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Build Hierarchical Layers (Pre-retrieval)\n```\n\n```\n2: 𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦 ← 𝐹 𝑏𝑢𝑖𝑙𝑑 ( 𝑞 ) // Build hierarchical layers based on the initial query 3: Initialize 𝑙 ← 0 // Start depth counter from 0 4: Initialize 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 ← [ 𝑞 ] // Initialize list of sub-queries 5: while 𝑙 ≤ 𝐿 do 6: for each 𝑞 ′ 𝑙 ∈ 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 do 7: Pre-retrieval Phase 8: 𝑞 ′ 𝑙 ← 𝐹 ℎ𝑖𝑒𝑟 ( 𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦,𝑞 ′ 𝑙 ) // Adjust query based on hierarchical layers 9: 𝑞 ′ 𝑙 ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ 𝑙 ) // Query Manipulation, Indexing, and Data Modification 10: Retrieval Phase 11: 𝐷 𝑙 ← 𝑅 ( 𝑞 ′ 𝑙 , 𝐷 ) // Retrieve documents for the current sub-query 12: Post-retrieval Phase 13: 𝐷 ′ 𝑙 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ 𝑙 , 𝐷 𝑙 ) // Re-ranking and filtering to refine documents 14: Generation Phase 15: 𝑦 𝑙 ← 𝐺 ( 𝑞 ′ 𝑙 , 𝐷 ′ 𝑙 ) // Generate output based on refined documents 16: Sub-query Generation (if needed) 17: if additional refinement needed based on 𝑦 𝑙 then 18: 𝑆𝑢𝑏 _ 𝑞𝑢𝑒𝑟𝑖𝑒𝑠 ← 𝐹 𝑠𝑢𝑏𝑞 ( 𝑦 𝑙 ) // Generate new sub-queries based on current output 19: end if 20: end for 21: 𝑙 ← 𝑙 + 1 // Increment depth counter 22: end while 23: Final Synthesis 24: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 0 , 𝑦 1 , . . . , 𝑦 𝑙 }) // Merge results 25: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nlayer to this approach by incorporating a retrieval evaluator that assesses the quality of retrieved documents and triggers different actions based on confidence thresholds, ensuring that only the most relevant and accurate information is used in the generation process.\n\nAdaptive Retrieval Strategy. Adaptive retrieval (Algorithm 4) dynamically adjusts the retrieval strategy based on the context and nature of the query or the data retrieved so far. This highly flexible method tailors retrieval approaches on-the-fly to optimize for relevance and precision, making it ideal for personalized search engines, adaptive learning systems, and real-time decision support. AAR [157] exemplifies adaptive retrieval by adjusting its strategy based on the preferences of LLMs, learning from a small source model and generalizing to unseen tasks. FLARE [65] takes a similar adaptive approach but focuses on dynamically fetching additional information when model confidence is low, thereby improving the relevance of generated responses. SelfRAG [4] goes further by incorporating self-reflective processes, where the retrieval strategy evolves based on critiques of the generated content. CoK [86], on the other hand, implements a dynamic mechanism that adjusts retrieval strategies based on the evolving needs of the task. The retrieval process in CoK is not static but adapts according to the specific scenario and the nature of the\n\n## Algorithm 3 Conditional Retrieval Strategy in RAG\n\n```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Condition Evaluation Function 𝐹 𝑐𝑜𝑛𝑑 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: 𝑞 ′ ← 𝑞 // Initialize query 3: while 𝑖 ≤ 𝑁 do 4: Pre-retrieval Phase 5: 𝑞 ′ ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ ) // Perform query manipulation and data modification 6: Retrieval Phase 7: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷 ) // Retrieve documents based on the current query 8: Post-retrieval Phase 9: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 ) // Re-rank and filter documents based on conditions 10: Generation Phase 11: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 ) // Generate output using the refined documents 12: Conditional Branching 13: if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition A then 14: Apply Strategy A // e.g., refine the query based on feedback 15: else if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition B then 16: Apply Strategy B // e.g., expand the scope or adjust parameters 17: else if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is Condition C then 18: Apply Strategy C // e.g., modify retrieval strategy or output processing 19: else 20: Continue without changes // If no conditions are met, proceed without adjustments 21: end if 22: Check Termination Condition 23: if 𝐹 𝑐𝑜𝑛𝑑 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) meets stopping criteria then 24: BREAK // Exit the loop if the stopping condition is met 25: end if 26: 𝑖 ← 𝑖 + 1 // Increment iteration counter 27: end while 28: Final Synthesis 29: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 30: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\ninformation being accessed, making it highly effective for context-sensitive applications. DRAGIN [124] discusses a real-time dynamic retrieval mechanism that adapts to the evolving needs of the language model, ensuring that the retrieval strategy remains responsive and aligned with the immediate task requirements, thus optimizing the relevance and precision of the retrieved information.\n\nIn summary, the choice of retrieval strategy within RAG depends on the specific requirements of the application at hand. While basic retrieval strategies offer simplicity and efficiency, iterative retrieval is well-suited for tasks requiring detailed exploration and refinement. Recursive retrieval excels in managing hierarchical information, while adaptive retrieval provides flexibility in dynamic environments. Conditional retrieval ensures strict adherence to predefined criteria, making it indispensable in applications where compliance and specific constraints are critical. By carefully\n\n## Algorithm 4 Adaptive Retrieval Strategy in RAG\n\n```\nRequire: Query 𝑞 , Documents 𝐷 , Maximum Iterations 𝑁 , Retriever 𝑅 , Generator 𝐺 , Pre-retrieval Function 𝐹 𝑝𝑟𝑒 , Post-retrieval Function 𝐹 𝑝𝑜𝑠𝑡 , Adaptive Adjustment Function 𝐹 𝑎𝑑𝑎𝑝𝑡 , Feedback Function 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 Ensure: Final Output 𝑦 𝑓 𝑖𝑛𝑎𝑙 1: Initialize 𝑖 ← 1 // Start iteration counter 2: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝑞, ∅ // Initialize query and context 3: while 𝑖 ≤ 𝑁 do 4: Pre-retrieval Phase 5: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝐹 𝑝𝑟𝑒 ( 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Query Manipulation, Indexing, Data Modification, and Context Setup 6: Dynamic Retrieval Phase 7: 𝐷 𝑖 ← 𝑅 ( 𝑞 ′ , 𝐷, 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Retrieve documents based on the current query and context 8: Adaptive Post-retrieval Phase 9: 𝐷 ′ 𝑖 ← 𝐹 𝑝𝑜𝑠𝑡 ( 𝑞 ′ , 𝐷 𝑖 , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Re-ranking and filtering based on adaptive criteria 10: Generation Phase 11: 𝑦 𝑖 ← 𝐺 ( 𝑞 ′ , 𝐷 ′ 𝑖 , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ) // Generate output using the refined documents 12: Adaptive Adjustment 13: if 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 ( 𝑦 𝑖 , 𝐷 ′ 𝑖 ) is negative then 14: 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡 ← 𝐹 𝑎𝑑𝑎𝑝𝑡 ( 𝑞 ′ , 𝐶𝑜𝑛𝑡𝑒𝑥𝑡,𝑦 𝑖 , 𝐷 ′ 𝑖 ) // Dynamically adjust the query and context 15: end if 16: Feedback Integration 17: if 𝐹 𝑓 𝑒𝑒𝑑𝑏𝑎𝑐𝑘 ( 𝑦 𝑖 ) is positive then 18: BREAK // Stop iterations if output is satisfactory 19: end if 20: 𝑖 ← 𝑖 + 1 // Increment iteration counter 21: end while 22: Final Synthesis 23: 𝑦 𝑓 𝑖𝑛𝑎𝑙 ← SynthesizeResults ({ 𝑦 1 , 𝑦 2 , . . . , 𝑦 𝑖 }) // Merge results 24: return 𝑦 𝑓 𝑖𝑛𝑎𝑙\n```\n\nselecting and combining these strategies, RAG systems can be tailored to effectively handle a wide range of information retrieval scenarios, leveraging the strengths of each approach to deliver robust and precise results.\n\n## 5 Post-Retrieval\n\n## 5.1 Re-Ranking\n\nAs retrieval mechanisms often return a large number of potentially relevant documents, re-ranking methods are employed to reorder these documents, prioritizing those most likely to contribute meaningfully to the final output. By leveraging various strategies, including unsupervised techniques, supervised learning, and data augmentation, re-ranking aims to optimize the alignment between the retrieved content and the desired response, thereby improving the overall effectiveness of RAG systems [165].\n\nUnsupervised Re-ranking. Unsupervised re-rankers do not rely on labeled data for training. They use strategies such as pointwise, listwise, or pairwise methods to rank documents based on LLM outputs without the need for supervised fine-tuning. For example, In-Context RALM [112] employs\n\na zero-shot approach where an off-the-shelf language model is used to re-rank the top-k documents retrieved by a BM25 retriever. This process involves selecting the document that maximizes the likelihood of the generated text, effectively using the LM's semantic understanding to improve document relevance without requiring additional supervised training. The paper also explores training a dedicated re-ranker using self-supervised learning to further enhance the selection of relevant documents, demonstrating that training a re-ranker with domain-specific data can be more effective than zero-shot re-ranking.\n\nSupervised Re-ranking. Supervised re-rankers involve fine-tuning LLMs on specific ranking datasets. This category can be further divided into models like BERT that process query-document pairs to compute relevance scores, models like T5 that treat ranking as a generation task and use generated tokens to determine relevance, and models like RankLLaMA [95] that employ a prompt-based approach, focusing on the last token's representation for relevance calculation [165]. For instance, the re-ranker in Re2G [40] is based on a BERT model trained on labeled data (such as MS MARCO) and fine-tuned to improve the relevance ranking of retrieved documents. FiD-Light [47] employs a supervised approach where the model is fine-tuned on specific datasets to learn how to re-rank passages effectively using source pointers during autoregressive text generation. The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank relevant passages based on the output generated during the text generation process. GenRT [148] utilizes a combination of an encoder to capture global list-level features and a sequential decoder to reorder documents based on relevance. The model is trained to learn relevance scores through supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever. This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it to better capture the semantic relevance of documents relative to the task input.\n\nData Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the training process by generating additional training data, such as pseudo-relevance labels, using LLMs. This data augmentation provides more varied training examples, which helps improve the performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving the retrieval process by incorporating rich answer encodings. These methods involve generating additional training signals or utilizing enriched data representations to improve the retrieval and ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data generated through LLM-based query generation to enhance the training of the reranker. This data augmentation approach allows the re-ranker to refine candidate passages more effectively, using a cross-attention model trained on these additional examples to boost retrieval accuracy.\n\n## 5.2 Filtering\n\nFiltering and re-ranking are distinct processes in the post-retrieval stage of RAG systems. Filtering focuses on eliminating irrelevant or low-quality documents from the retrieved set, thereby reducing the document set size and improving efficiency and effectiveness in subsequent processing. In contrast, re-ranking orders the remaining documents based on their relevance or utility for the task, often prioritizing those that enhance the quality of the generated output, especially in responseaware scenarios.\n\nSeveral filtering methods have been developed to refine document sets in RAG systems, each with unique mechanisms but sharing common goals of improving relevance and reducing computational load. Self-RAG [4] employs a self-reflection mechanism, utilizing special 'reflection tokens'\n\ngenerated by the model to evaluate the relevance and quality of retrieved passages and the model's own generated outputs. This self-reflection ensures that only the most pertinent documents are retained, leveraging the model's internal capabilities without relying on external models during inference. Similarly, BlendFilter [135] utilizes the LLM itself as the filter, assessing and removing irrelevant or less useful documents by applying filtering separately to knowledge retrieved from original, externally augmented, and internally augmented queries. Both Self-RAG and BlendFilter highlight the model's intrinsic ability to perform filtering, reducing the need for additional models and enhancing computational efficiency.\n\nIn contrast, RECOMP [147] and CRAG [149] employ more external or structural strategies. RECOMPfocuses on selective augmentation, where summaries generated from retrieved documents are selectively prepended to the input for the language model. If the retrieved documents are deemed irrelevant, the compressor can generate an empty summary, effectively filtering out unnecessary information. This method allows for a dynamic approach to filtering, where only helpful content is retained. CRAG, on the other hand, uses a decompose-then-recompose approach, where retrieved documents are split into finer knowledge strips. These strips are evaluated for relevance using a finetuned T5 model, and only the relevant strips are recomposed to form a refined set of information for the generation task. This granular filtering process ensures that the final document set is both relevant and concise, tailored specifically to the generation task.\n\nDynamic filtering techniques are also employed in methods like FiD-TF [5] and CoK [86]. FiD-TF introduces Token Filtering during the decoding process, where less relevant tokens are dynamically filtered out based on cross-attention scores. This approach reduces the computational load by eliminating tokens deemed uninformative for generating the final answer, enhancing efficiency with minimal impact on performance. CoK employs a filtering technique based on self-consistency, identifying and processing only those questions with 'uncertain' answers. This method works by sampling various reasoning paths and answers, preserving only predictions with high consistency. Questions that do not meet the specified consistency threshold undergo further processing, effectively preventing the propagation of errors in the generation process.\n\nFinally, FILCO [141] implements a comprehensive filtering approach using three distinct strategies: String Inclusion (STRINC) to match exact outputs, Lexical Overlap to measure word-level similarity, and Conditional Cross-Mutual Information (CXMI) to assess how much the context improves output likelihood. FILCO applies these filtering strategies at the sentence level, refining the retrieved content for better relevance. Additionally, FILCO trains a context filtering model using these strategies, which predicts the most useful context at inference time, thereby enhancing the accuracy and relevance of the generation model's output.\n\n## 6 Generation\n\n## 6.1 Enhancing\n\nEnhancing methods are strategies aimed at improving the quality and relevance of generated outputs by integrating retrieved content in various ways. These methods differ in how they combine, aggregate, or refine retrieved information, offering multiple approaches to enrich the final output. Broadly, these techniques can be grouped into three categories: enhancing with queries, enhancing with ensemble approaches, and enhancing with feedback loops.\n\nEnhance with Query. This approach integrates the retrieved documents with the original query, enabling the generator to leverage both sources in producing the final output. By combining the query with the retrieved content, the generation process ensures that the response remains closely aligned with the user's intent while being enriched by relevant information. The focus here is on the seamless fusion of the query and context, allowing the generated output to maintain both relevance\n\nand completeness. For instance, the RETRO [9] model enhances generation by integrating retrieved text chunks with the user's query using a chunked cross-attention mechanism, where relevant information from the retrieved neighbors is directly injected into the generation process. This method involves first retrieving similar document chunks based on the query and then using a crossattention module to align and combine these chunks with the input sequence during generation. In-Context RALM [112] takes a comparable approach, directly prepending the retrieved documents to the input query. In this way, the language model can generate responses conditioned on both the query and the retrieved content without requiring changes to the model's architecture. Both examples illustrate a straightforward yet effective method: concatenating the query and retrieved documents into a single input sequence that the LLMs process together, yielding outputs that are contextually enhanced.\n\nEnhance with Ensemble. When multiple sources are synthesized, the generation process can achieve a more coherent and well-rounded response. Rather than relying solely on a single source, this approach aggregates information from various documents, allowing the generator to reconcile conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output. The ensemble process can manifest in different ways: it may involve combining insights from several sources into a unified narrative, or generating multiple candidate outputs and choosing the best one based on criteria like consistency, relevance, or factual accuracy. An instance of this strategy is seen in FiD [58], which encodes multiple retrieved passages independently before fusing them in the decoder to create a coherent answer. By treating each passage separately during encoding and then merging them during decoding, the model effectively combines evidence from multiple sources. Meanwhile, in REPLUG [122], an ensemble approach is adopted where each retrieved document is independently prepended to the query and processed separately. The outputs are then aggregated, with relevance scores guiding the weighting of each document's contribution. Through this process, the model capitalizes on diverse information across several sources, leading to improvements in answer accuracy, coverage, and scalability as more data becomes available.\n\nEnhance with Feedback. In contrast to approaches that process retrieved information in a single pass, this method introduces iterative refinement into the generation process by incorporating feedback loops. Initially, the generator produces a draft response, which is then evaluated and adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on factual accuracy and fluency. This iterative approach aims to incrementally improve the output by identifying and correcting errors or fine-tuning content to better align with quality standards, ultimately producing a polished and reliable response. PRCA [151] offers an example by positioning itself between the retriever and generator, distilling retrieved information based on feedback from the generator. This distilled information serves as a reward model to guide context optimization, leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which details should be emphasized or downplayed. DSP [73], on the other hand, refines both queries and retrieved passages through a multi-hop retrieval process that incorporates programmatically bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant passages, and updates the context in subsequent steps-each stage building on the last to refine the final output. Feedback-driven enhancements are also evident in models like Selfmem [21], which focus on generating self-memory. The model first produces an unbounded pool of outputs and then selects the most relevant one as memory for the next generation, guided by metrics like BLEU or ROUGE. Finally, RECITE [122] integrates feedback by generating multiple recitations from the model's internal knowledge and using self-consistency techniques to aggregate the outputs. By introducing diversity in the recitations and leveraging passage hints during generation, this approach selects the best content through majority voting. Together, these methods demonstrate\n\nhow feedback loops and iterative refinements can lead to outputs that are not only more accurate but also increasingly coherent and contextually grounded as they evolve.\n\n## 6.2 Customization\n\nCustomization focuses on tailoring content to the user's personality and needs. It involves adjusting the output either to align with specific knowledge retrieved during earlier stages (content alignment) or to adapt the generated response to meet the user's preferences, context, or audience needs (contextual adaptation).\n\nIn LAPDOG [52], customization is achieved primarily through content alignment by integrating persona profiles with external stories to enrich the context used for generation. The story retriever identifies relevant narratives based on the persona, expanding the limited profiles with additional information. The generator then combines this enriched knowledge with the dialogue history, ensuring that responses align closely with the persona's traits and background. This approach allows for a nuanced understanding of the user's personality, making the output more engaging and contextually appropriate.\n\nOn the other hand, PersonaRAG [160] emphasizes real-time adaptation by customizing generated content based on dynamic user profiles, session behavior, and ongoing feedback. A multi-agent system continuously analyzes user interactions to refine responses, ensuring alignment with the user's preferences and context. By integrating personalized insights at each step, the system can adjust its output to suit specific informational needs and situational contexts. This level of responsiveness allows the system to evolve in line with the user's changing requirements, creating more relevant and targeted responses.\n\nERAGent [123] also focuses on customization but through the use of a Personalized LLM Reader, which adapts responses using user-specific profiles. This module integrates rewritten questions, filtered knowledge, and user preferences to tailor responses according to both content relevance and user needs. For instance, it takes into account preferences like environmental consciousness or dietary restrictions, ensuring that the generated content is not only aligned with retrieved knowledge but also personalized to the user's particular values and requirements. This deep level of customization ensures that the output is both relevant and personally meaningful, enhancing user engagement.\n\nROPG [118] proposes a dynamic pre- and post-generation retriever selection model, enhancing personalization by aligning the retrieval process with both the input context and the user's preferences. The pre-generation model determines which retrieval strategy-such as recency-based, keyword matching, or semantic retrieval-is most appropriate before generation begins. By tailoring the retrieval process in this way, the model ensures that the documents retrieved from the user profile closely match the current input, thereby aligning the content with relevant user-specific knowledge. Following this, the post-generation model evaluates the outputs generated by different retrieval strategies and selects the most personalized result. This selection is guided by feedback from the generated content, which is then used to adjust future retrievals. By combining content alignment (through pre-generation retrieval) with contextual adaptation (through post-generation evaluation), this approach offers a comprehensive solution for customization within RAG.\n\n## 7 Evaluation in RAG\n\nTo assess how effectively language models can generate more accurate, relevant, and robust responses by leveraging external knowledge, the evaluation of RAG systems has emerged as a crucial research focus. Given the rising popularity of dialogue-based interactions, much recent work has concentrated on evaluating RAG models' performance on such downstream tasks using established metrics like Exact Match (EM) and F1 scores. These metrics have been applied across a\n\nTable 1. The Comparison of Different RAG Evaluation Frameworks.\n\n| Evaluation Framework   | Aspects                       | Methods                        | Metrics                                                     | Datasets                                                                 |\n|------------------------|-------------------------------|--------------------------------|-------------------------------------------------------------|--------------------------------------------------------------------------|\n| RAGAS [33]             | Quality of RAG Systems        | Context Relevance              | Extracted Sentences / Total Sentences                       | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Answer Relevance               | Average Cosine Similarity                                   | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Faithfulness                   | Supported Statements / Total Statements                     | WikiEval 7                                                               |\n| ARES [117]             | Improving RAGAS               | Context Relevance              | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Relevance               | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Faithfulness            | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| RECALL [91]            | Counterfactual Robustness     | Response Quality               | Accuracy (QA) BLEU, ROUGE-L (Generation)                    | EventKG [41] UJ [50]                                                     |\n| RECALL [91]            | Counterfactual Robustness     | Robustness                     | Misleading Rate (QA) Mistake Reappearance Rate (Generation) | EventKG [41] UJ [50]                                                     |\n| RGB [14]               | Impact of RAG on LLMs         | Noise Robustness               | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Negative Rejection             | Rejection Rate                                              | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Information Integration        | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Counterfactual Robustness      | Error Detection Rate Error Correction Rate                  | Synthetic                                                                |\n| MIRAGE [144]           | RAG in Medical QA             | Zero-Shot Learning             | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Multi-Choice Evaluation        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Retrieval-Augmented Generation | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Question-Only Retrieval        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| eRAG [119]             | Retrieval Quality in RAG      | Downstream Task                | Accuracy, ROUGE                                             | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Set-based                      | Precision, Recall, Hit Rate                                 | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Ranking                        | MAP, MRR, NDCG                                              | KILT                                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Surface-Based                  | EM, F1, Precision, Recall                                   | QA Datasets [69, 76]                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Semantic                       | BEM [11], LLMeval [114]                                     | QA Datasets [69, 76]                                                     |\n\nwide array of datasets, including TriviaQA [69], HotpotQA [153], FEVER [129], Natural Questions (NQ) [76], Wizard of Wikipedia (WoW) [30], and T-REX [32], which are often used to benchmark the effectiveness of retrieval and generation components in knowledge-intensive tasks.\n\nWhile downstream task evaluations provide valuable insights, they fail to address the multifaceted challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has proposed various frameworks and benchmarks that aim to evaluate these systems from multiple perspectives, considering not only the quality of the generated text but also the relevance of retrieved documents and the system's resilience to misinformation, as shown in Table 1. These evaluations include metrics that assess noise robustness, negative prompting, information integration, and counterfactual robustness, all of which reflect the complex challenges RAG systems face in realworld applications. The ongoing development of comprehensive evaluation frameworks and metrics is essential for advancing the field, broadening the applicability of RAG systems, and ensuring that they meet the demands of an increasingly dynamic and complex information landscape [154].\n\n## 7.1 Retrieval-based Aspect\n\nIn information retrieval, standard metrics such as Mean Average Precision (MAP), Precision, Reciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) [103, 110, 115] have traditionally been used to evaluate the relevance of retrieved documents to a given query. These metrics are essential in assessing the effectiveness of traditional information retrieval systems, where the primary goal is to measure how well the retrieved documents match the user's query.\n\nWhen applied to RAG systems, these retrieval-based metrics extend their focus to consider how the retrieved information contributes to the quality of the generated output. In this context, Accuracy becomes a crucial metric, assessing how precisely the retrieved documents provide correct information for answering queries. Additionally, Rejection Rate [14], which measures the system's ability to decline answering when no relevant information is available, has emerged as a\n\nkey indicator of responsible output generation. Similarly, Error Detection Rate [14] evaluates the model's capability to identify and filter out incorrect or misleading information, ensuring that the generation process is based on trustworthy sources.\n\nAnother important consideration is Context Relevance, which assesses the alignment of retrieved documents with the specific query, emphasizing the need for content directly relevant to the generation task's context. Faithfulness [33] is also critical in determining whether the generated text accurately reflects the information found in the retrieved documents, thereby minimizing the risk of generating misleading or incorrect content.\n\nThe eRAG framework [119] introduces a more refined approach to evaluating retrieval quality in RAG systems by focusing on individual documents rather than the entire retrieval process. It operates by feeding each document in the retrieval list into the LLM alongside the query and evaluating the generated output against downstream task metrics such as Accuracy. The documentlevel scores are then aggregated using ranking metrics like MAP to produce a single evaluation score. This focus on document-level contributions offers a more precise assessment of retrieval quality while being significantly more computationally efficient than traditional end-to-end evaluations.\n\nNotably, eRAG demonstrates that its document-level evaluation correlates more strongly with downstream RAG performance compared to conventional methods like human annotations or provenance labels. This correlation underscores that the LLM, as the primary consumer of the retrieved results, is the most reliable judge of retrieval performance [119]. Regardless of the retrieval model or the number of retrieved documents, eRAG consistently outperforms other evaluation approaches, indicating that directly evaluating how each document supports the LLM's output is the most effective way to measure retrieval quality in RAG systems.\n\n## 7.2 Generation-based Aspect\n\nThe evaluation of text produced by large language models involves analyzing performance across a range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy, and alignment with ground-truth data. Metrics like BLEU [107] and ROUGE-L [87] are often used to measure fluency, similarity to human-produced text, and the overlap with reference summaries, respectively, providing insights into how well the generated content captures key ideas and phrases.\n\nIn addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap with ground-truth data are evaluated using EM and F1 scores, which respectively measure the percentage of completely correct answers and offer a balanced view of precision and recall. This ensures that relevant answers are retrieved while inaccuracies are minimized.\n\nBeyond these standard evaluation techniques, more specialized criteria have been introduced to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate [91] have been developed to measure a model's ability to avoid generating incorrect or misleading content. Other advanced metrics include Answer Relevance [33], which assesses the precision of responses to queries, Kendall's tau [117], used for evaluating the accuracy of system rankings, and Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers. Prediction Accuracy further complements these by directly measuring how closely the generated responses align with the expected answers, offering a clear measure of a system's effectiveness in producing accurate content.\n\nTable 2. The comprehensive summary of RAG studies. A ! in the 'Multi-hop' column signifies that the research involves multiple search rounds. Similarly, a ! in the 'Training' column indicates that the study included training phases. It is important to note that in this context, 'Training' encompasses both initial model training and fine-tuning processes.\n\n<!-- image -->\n\n| Research                     | Year      | Retrieval   | Source     | Multi-hop   | Training   |            | Pre-Retrieval      |                   | Retrieval          | Post-Retrieval   | Post-Retrieval   | Generation   | Generation    |\n|------------------------------|-----------|-------------|------------|-------------|------------|------------|--------------------|-------------------|--------------------|------------------|------------------|--------------|---------------|\n| REALM [42]                   |           | Internal    | External ! |             | !          | Indexing ! | Query Manipulation | Data Modification | Search & Ranking ! | Re-Ranking       | Filtering        | Enhancing    | Customization |\n| kNN-LMs [72]                 | 2020 2020 | !           | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| RAG [83]                     | 2020      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| FiD [58]                     | 2021      |             | !          |             |            |            |                    |                   | !                  |                  |                  | !            |               |\n| Webgpt [100]                 | 2021      |             | !          | !           | !          | !          | !                  |                   | !                  |                  | !                | !            |               |\n| Re2G [40]                    | 2022      | !           |            | !           | !          |            |                    |                   |                    | !                |                  |              |               |\n| RETRO [9]                    | 2022      |             | !          | !           | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| DSP [73]                     | 2022      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| CoK [86]                     | 2023      |             | !          | !           |            |            | !                  |                   |                    | !                |                  |              |               |\n| IRCOT [131]                  | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  | !            |               |\n| ITRG [34]                    | 2023      | !           | !          | !           |            |            |                    |                   | !                  |                  |                  | !            |               |\n| PKG [92]                     | 2023      | !           |            |             |            |            |                    |                   |                    |                  |                  |              |               |\n| RA-DIT [89]                  | 2023      |             | !          | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| Self-RAG [4]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n|                              | 2023      | !           |            |             |            |            |                    |                   | !                  |                  |                  |              |               |\n| SURGE [70] FiD-TF [5]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                | !                |              |               |\n| PRCA [151]                   | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  | !            |               |\n| REPLUG [122]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  |                  | !            |               |\n| AAR [157]                    | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  |              |               |\n| Query2doc [137]              | 2023      | !           |            |             |            |            | !                  |                   |                    |                  |                  |              |               |\n| Step-Back [163]              | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  |              |               |\n| ITER-RETGEN [121]            | 2023      |             | !          | !           |            |            |                    |                   | !                  | !                |                  |              |               |\n| RECITE [125]                 | 2023      | !           |            | !           | !          |            |                    | !                 |                    |                  |                  | !            |               |\n| PROMPTAGATOR [27]            | 2023      | !           |            | !           |            |            | !                  |                   |                    | !                | !                |              |               |\n| UPRISE [20]                  | 2023      | ! !         |            | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| GENREAD [156]                | 2023      |             |            |             |            |            |                    | !                 |                    |                  |                  | !            |               |\n| LAPDOG [52]                  | 2023      |             | !          |             | !          |            | !                  |                   | !                  | !                |                  | !            | !             |\n| KnowledGPT [140]             | 2023      |             | ! !        | ! !         | !          |            | !                  | !                 |                    | !                |                  | !            |               |\n| Selfmem [21]                 | 2023      |             | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| MEMWALKER [13] RECOMP [147]  | 2023 2023 |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| Rewrite-Retrieve-Read [94]   | 2023      |             | !          |             | !          |            | !                  |                   |                    |                  |                  |              |               |\n| Atlas [94]                   | 2023      |             | !          | !           | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| DKS-RAC [53]                 | 2023      |             | !          | !           | !          |            |                    |                   |                    | !                | !                |              |               |\n| In-Context RALM [112]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                |                  |              |               |\n| Fid-light [47]               | 2023      |             | ! !        | !           |            |            | !                  |                   | !                  | !                |                  |              |               |\n| FLARE [65]                   | 2023      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| Chameleon [63] ERAGent [123] | 2023      | !           | !          |             | !          |            | !                  |                   | !                  |                  | !                | !            | !             |\n| PipeRAG [64]                 | 2024 2024 |             | !          | !           | !          | !          |                    |                   |                    |                  |                  | !            |               |\n|                              |           | !           |            |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| GenRT [148] PersonaRAG [160] | 2024 2024 | !           | !          | !           | !          |            | !                  |                   | !                  | ! !              |                  | !            | !             |\n| CRAG [149]                   | 2024      | !           | !          |             | !          |            |                    | !                 |                    | !                | !                | !            |               |\n| IMRAG [150]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  | !            |               |\n| AiSAQ [126]                  | 2024      | !           |            |             |            | !          |                    |                   | !                  | !                |                  |              |               |\n| ROPG [118]                   | 2024      | !           |            |             | !          |            |                    |                   | !                  |                  |                  | !            | !             |\n| RQ-RAG [12]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  |              |               |\n| PlanRAG [81]                 | 2024      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| RARG [159]                   | 2024      |             | !          |             | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| DRAGIN [124]                 | 2024      |             | !          | !           |            |            | !                  |                   | !                  |                  |                  | !            |               |\n| [93]                         | 2024      | !           |            |             | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| LRUS-CoverTree               |           |             |            |             |            |            |                    |                   |                    |                  |                  |              |               |\n\n## 8 Comparisons of RAG\n\n## 8.1 The Comprehensive Summary of RAG\n\nTable 2 presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows that the majority of these studies have utilized external data sources to enrich the content of LLMs. A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search rounds generally yield superior results. In other words, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval stage, more studies focus on manipulating the query to improve retrieval performance. Additionally, there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in the research. However, there seems to be a scarcity of studies concentrating on customization in the generation stage, pointing to this as a potential area for future exploration. Overall, while the goal of RAG is to enhance the response quality of LLMs, greater efforts have been directed towards improving retrieval aspects.\n\nTable 3. The summary of Retrievers and Generators. The retrieval models and pre-trained language models explicitly mentioned in these studies have been recorded.\n\n| Research                   |   Year | Retriever                                          | Generator                                                  |\n|----------------------------|--------|----------------------------------------------------|------------------------------------------------------------|\n| REALM [42]                 |   2020 | BERT [29]                                          | Transformers [133]                                         |\n| kNN-LMs [72]               |   2020 | FAISS [68]                                         | Transformers                                               |\n| RAG [83]                   |   2020 | DPR [71]                                           | BART-Large [82]                                            |\n| FiD [58]                   |   2021 | BM25 [116], DPR                                    | T5 [111]                                                   |\n| Webgpt [100]               |   2021 | Bing                                               | GPT-3 [10]                                                 |\n| Re2G [40]                  |   2022 | BM25, DPR                                          | BART                                                       |\n| RETRO [9]                  |   2022 | BERT                                               | Transformer                                                |\n| DSP [73]                   |   2022 | ColBERTv2                                          | GPT-3.5 (text-davinci-002)                                 |\n| CoK [86]                   |   2023 | [74] LLaMA2-7B [130], ChatGPT (gpt-3.5-turbo-0613) | ChatGPT (gpt-3.5-turbo-0613)                               |\n| IRCOT [131]                |   2023 | BM25                                               | GPT-3 (code-davinci-002), Flan-T5 [24]                     |\n| ITRG [34]                  |   2023 | Atlas [94]                                         | LLaMA-33B                                                  |\n| PKG [92]                   |   2023 | LLaMA-7B                                           | InstructGPT-3.5 (text-davinic-002) [104]                   |\n| RA-DIT [89]                |   2023 | DRAGON+ [88]                                       | LLaMA                                                      |\n| Self-RAG [4]               |   2023 | Contriever [57]                                    | LLaMA2 (7B and 13B) , GPT-4 [1]                            |\n| SURGE [70]                 |   2023 | Graph Neural Networks (GNN) [43]                   | Transformers                                               |\n| FiD-TF [5]                 |   2023 | BM25, SBERT [115]                                  | T5                                                         |\n| PRCA [151]                 |   2023 | BM25, DPR, Contriver, SimCSE [37], SBERT           | T5, Phoenix-7B [19], Vicuna-7B [22], ChatGLM [31], GPT-3.5 |\n| REPLUG [122]               |   2023 | Contriever                                         | GPT-3                                                      |\n| AAR [157]                  |   2023 | ANCE [146], Contriever                             | Flan-T5, InstructGPT                                       |\n| Query2doc [137]            |   2023 | BM25, DPR                                          | GPT-3 (text-davinci-003)                                   |\n| Step-Back [163]            |   2023 | PaLM-2L [23]                                       | PaLM-2L, GPT-4                                             |\n| ITER-RETGEN [121]          |   2023 | Contriever                                         | InstructGPT (text-davinci-003), LLaMA2                     |\n| RECITE [125]               |   2023 |                                                    | PaLM, UL2 [127], OPT [161], Codex [16]                     |\n| PROMPTAGATOR [27]          |   2023 | T5                                                 | FLAN                                                       |\n| UPRISE [20]                |   2023 | GPT-Neo-2.7B [8]                                   | BLOOM-7.1B [142], OPT-66B, GPT-3-175B                      |\n| GENREAD [156]              |   2023 |                                                    | InstructGPT                                                |\n| LAPDOG [52]                |   2023 | Contriever                                         | T5                                                         |\n| KnowledGPT [140]           |   2023 |                                                    | GPT-4                                                      |\n| Selfmem [21]               |   2023 | BM25                                               | XGLM [90], XLM-Rbase [25]                                  |\n| MEMWALKER [13]             |   2023 | LLaMA2                                             | LLaMA2                                                     |\n| RECOMP [147]               |   2023 | BM25                                               | T5-Large                                                   |\n| Rewrite-Retrieve-Read [94] |   2023 | Bing                                               | T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B               |\n| Atlas [94]                 |   2023 | Contriever                                         | T5                                                         |\n| DKS-RAC [53]               |   2023 | DPR                                                | BART                                                       |\n| In-Context RALM [112]      |   2023 | BM25, BERT-base, Contriever, Spider [113]          | GPT-2, GPT-Neo, GPT-J [35], OPT, and LLaMA                 |\n| Fid-light [47]             |   2023 | GTR-Base [101]                                     | T5                                                         |\n| FLARE [65]                 |   2023 | BM25, Bing                                         | GPT-3.5 (text-davinci-003)                                 |\n| Chameleon [63]             |   2023 | ChamVS [63]                                        | ChamLM [63]                                                |\n| ERAGent [123]              |   2024 | Bing                                               | GPT-3.5, Falcon 1B [108]                                   |\n| PipeRAG [64]               |   2024 | SBERT                                              | RETRO [9]                                                  |\n| GenRT [148]                |   2024 | LambdaMart [2]                                     |                                                            |\n| PersonaRAG [160]           |   2024 | BM25                                               | GPT-3.5                                                    |\n| CRAG [149]                 |   2024 | Contriever                                         | LLaMA2                                                     |\n| IMRAG [150]                |   2024 | DPR                                                | Vicuna-7B                                                  |\n| AiSAQ [126]                |   2024 | DiskANN [106]                                      |                                                            |\n| ROPG [118]                 |   2024 | BM25, Contriever                                   | FlanT5-XXL                                                 |\n| RQ-RAG [12]                |   2024 | DuckDuckGo 8                                       | LLaMA2-7B                                                  |\n| PlanRAG [81]               |   2024 | GPT-4                                              | GPT-4                                                      |\n| RARG [159]                 |   2024 | BM25, E5 [136]                                     | LLaMA2-7B                                                  |\n| DRAGIN [124]               |   2024 | BM25, SGPT [99]                                    | LLaMA2 (7B and 13B), Vicuna-13B                            |\n| LRUS-CoverTree [93]        |   2024 | k-MIPS                                             |                                                            |\n\n## 8.2 Retriever and Generator\n\nIn RAG, the retriever and generator are central components, each playing a distinct role in the system's overall performance. Table 3 summarizes the retrievers and generators used across the studies discussed in this paper. The table reveals that while a wide range of advanced language models are employed as generators, many systems still rely on traditional retrievers like BM25, valued for their efficiency. This highlights the continued importance of optimizing retrieval methods while balancing computational demands. Interestingly, despite the availability of powerful models such as LLaMA2, GPT-3.5, and GPT-4, these are not widely adopted as generators. Instead, models like T5 remain prevalent, while more foundational retrieval approaches, such as those based on BERT, see limited use. The relative scarcity of IR-focused LLMs in retrievers suggests a promising avenue for future research and development in this domain.\n\nTable 4. Part results of Accuracy (%) of GPT-3.5 across different corpora and retrievers on Mirage. Red and green highlight declines and improvements compared to CoT (first row), with shading intensity reflecting the degree of change. Data sourced from Mirage [144].\n\n|                   |            | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   |         |\n|-------------------|------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------|\n| Corpus            | Retriever  | MMLU-Med                   | MedQA-US                   | MedMCQA                    | PubMedQA*                  | BioASQ-Y/N                 | Average |\n| None              | None       | 72.91 ± 1.35               | 65.04 ± 1.34               | 55.25 ± 0.77               | 36.00 ± 2.15               | 74.27 ± 1.76               | 60.69   |\n| PubMed (23.9M)    | BM25       | 72.27 ± 1.36               | 63.71 ± 1.35               | 55.49 ± 0.77               | 66.20 ± 2.12               | 88.51 ± 1.28               | 69.23   |\n| PubMed (23.9M)    | Contriever | 71.72 ± 1.36               | 63.94 ± 1.35               | 54.29 ± 0.77               | 65.60 ± 2.12               | 85.44 ± 1.42               | 68.20   |\n| PubMed (23.9M)    | SPECTER    | 73.19 ± 1.34               | 65.20 ± 1.34               | 53.12 ± 0.77               | 54.80 ± 2.23               | 75.73 ± 1.72               | 64.41   |\n| PubMed (23.9M)    | MedCPT     | 73.09 ± 1.34               | 66.69 ± 1.32               | 54.94 ± 0.77               | 66.40 ± 2.11               | 85.76 ± 1.41               | 69.38   |\n| PubMed (23.9M)    | RRF-2      | 75.57 ± 1.30               | 64.34 ± 1.34               | 55.34 ± 0.77               | 69.00 ± 2.07               | 87.06 ± 1.35               | 70.26   |\n| PubMed (23.9M)    | RRF-4      | 73.37 ± 1.34               | 64.73 ± 1.34               | 54.75 ± 0.77               | 67.20 ± 2.10               | 88.51 ± 1.28               | 69.71   |\n| Wikipedia (29.9M) | BM25       | 73.37 ± 1.34               | 63.47 ± 1.35               | 54.10 ± 0.77               | 26.40 ± 1.97               | 71.36 ± 1.82               | 57.74   |\n| Wikipedia (29.9M) | Contriever | 74.10 ± 1.33               | 65.99 ± 1.33               | 54.03 ± 0.77               | 26.40 ± 1.97               | 69.90 ± 1.85               | 58.08   |\n|                   | SPECTER    | 72.18 ± 1.36               | 63.63 ± 1.35               | 52.71 ± 0.77               | 22.20 ± 1.86               | 66.83 ± 1.89               | 55.51   |\n|                   | MedCPT     | 71.99 ± 1.36               | 65.12 ± 1.34               | 55.15 ± 0.77               | 29.00 ± 2.03               | 73.46 ± 1.78               | 58.95   |\n|                   | RRF-2      | 74.20 ± 1.33               | 64.57 ± 1.34               | 54.72 ± 0.77               | 31.00 ± 2.07               | 76.21 ± 1.71               | 60.14   |\n|                   | RRF-4      | 73.19 ± 1.34               | 64.96 ± 1.34               | 54.53 ± 0.77               | 31.00 ± 2.07               | 72.01 ± 1.81               | 59.14   |\n\nImpact of the Retriever. The results shown in Table 4 highlight the accuracy of GPT-3.5 across different corpora and retrievers on the Mirage benchmark [144]. These findings underscore how retriever performance closely depends on the alignment between training data and the target corpus. For example, in the MEDRAG system, MedCPT-trained specifically on PubMed user logs-significantly improves retrieval performance when accessing the PubMed corpus. This illustrates the benefits of using domain-specific retrievers tailored to specialized datasets. In contrast, general-purpose retrievers like Contriever, which incorporate Wikipedia data during training, excel in retrieving information from Wikipedia, especially for tasks like MMLU-Med and MedQA-US. On the other hand, SPECTER, which focuses more on regularizing pairwise article distances than optimizing query-to-article relevance, underperforms on the MedCorp corpus. The study also explores combining multiple retrievers using Reciprocal Rank Fusion (RRF). However, results show that adding more retrievers does not always lead to better outcomes; for instance, excluding SPECTER in RRF-2 on Wikipedia yields better results than RRF-4, indicating that simply increasing the number of retrievers is not beneficial unless their strengths align with the retrieval task.\n\nFigure 5a illustrates how eRAG investigates the correlation between LLM performance and retrieval effectiveness on the NQ dataset using three retrievers with different characteristics: BM25 (lexical sparse), RetroMAE (dense) [143], and SPLADEv3 (learned sparse) [80]. The initial retrievals are re-ranked using a DeBERTa-v3 [79] cross-encoder. The analysis demonstrates that as retrieval quality improves, LLM performance increases significantly across various models. Notably, reranking with SPLADEv3 and DeBERTa-v3 consistently achieves the best results across datasets and metrics. This underscores the critical role that high-quality retrieval plays in determining overall RAG system effectiveness, suggesting that IR-focused LLMs could be a valuable asset in enhancing generation performance.\n\nImpact of the Generator. The BERGEN study [114] compares the performance of LLMs with gold passages (Oracle) against closed-book settings without retrieval, as shown in Figure 5b. Surprisingly, the experiments do not reveal a straightforward relationship between model size and the performance gains from retrieval. For instance, smaller models like LLaMA2-7B benefit more\n\n<!-- image -->\n\n- (a) Impact of retrieval performance on RAG performance for SOLAR-10.7B [75] on NQ with different ranking systems. RR means with additional re-ranking using DeBERTa-v3.\n- (b) Performance gains w/ and w/o oracle retrieval for LLMs with different sizes. Comparing closed book vs oracle passages averaged over all QA datasets in KILT.\n\n<!-- image -->\n\n(c) The correlation between eRAG and the downstream performance of different LLM sizes. In this experiment, T5small (60M parameters) and T5-base (220M parameters) with FiD are used. The documents are retrieved using BM25.\n\n<!-- image -->\n\nFig. 5. Retriever and generator experiment results sourced from eRAG [119] and BERGEN [114].\n\nfrom retrieval than larger models like LLaMA2-70B. In fact, LLaMA2-7B with retrieval outperforms LLaMA2-70B in a closed-book setting, suggesting that retrieval augmentation can make smaller models more competitive. Similarly, results from the eRAG experiments in Figure 5c indicate that varying LLM sizes (e.g., T5-small vs. T5-base) does not significantly affect the correlation between eRAG and downstream performance. These findings highlight that retrieval quality has a more substantial impact on RAG performance than the choice of generator, reinforcing the notion that investing in better retrieval strategies often yields more benefits than relying solely on larger LLMs.\n\n## 9 Challenges and Future Directions\n\nThe evolving landscape of RAG systems faces significant challenges that impact the quality of generated outputs, system efficiency, and the integration of multimodal data. As these systems become more prevalent across a range of applications, addressing these challenges is essential for improving their effectiveness and scalability.\n\n## 9.1 Retrieval Quality\n\nThe quality of retrieval is fundamental to any effective RAG system, directly influencing the relevance and accuracy of the generated content [46, 119, 138, 164]. Current retrieval methods, however, frequently struggle with issues like noise, irrelevant documents, and fragmented information, all of which compromise the generation process.\n\nNoise Robustness. Irrelevant or misleading documents within the retrieved set can introduce noise, leading to hallucinations or unreliable answers. This challenge highlights the need for more sophisticated filtering and context-aware retrieval methods that can better differentiate relevant from irrelevant content. However, Cuconasu et al. [26] present an interesting perspective by showing that, under certain conditions, the inclusion of irrelevant documents can enhance overall accuracy. This finding challenges conventional retrieval strategies and suggests the potential for developing specialized approaches that strategically integrate noise within the retrieval process.\n\nNegative Rejection. When retrieval fails to return relevant results, models often attempt to generate responses regardless, increasing the risk of incorrect outputs. This issue is particularly problematic when queries are poorly expressed or lack sufficient context, making it difficult for retrieval models to surface relevant documents. Techniques like generating a pseudo-document that captures the query's essence, as demonstrated by HyDE [36], can help bridge this gap. By allowing retrieval systems to find more relevant documents even from suboptimal queries, HyDE improves retrieval accuracy, albeit with a trade-off in computational cost. Future research could focus on optimizing this process to balance improved retrieval accuracy with reduced latency.\n\nInformation Integration. Complex queries often require synthesizing information from multiple documents, yet fragmented or conflicting information can result in incoherent or incomplete answers. Pre- and post-retrieval techniques play a critical role in addressing this challenge. Enhancing retrieval granularity and incorporating techniques like entity-level retrieval and re-ranking can improve the cohesiveness of retrieved documents. However, many post-retrieval methods, as investigated by Zhu et al. [165], rely heavily on calling LLM APIs, which incurs significant costs. Exploring alternatives such as knowledge distillation to lightweight models could offer more scalable solutions, making advanced retrieval strategies more practical in online settings.\n\nRecent research highlights the development of generative models for search as a promising direction for improving retrieval quality. Models like GERE [15] and PARADE [84] enhance document re-ranking and fact verification by directly generating relevant document titles or evidence sentences. Fine-tuning pre-trained models like RankT5 [166] for ranking-specific tasks has also demonstrated potential in boosting out-of-domain performance, which is crucial for generalizing RAG systems across diverse contexts.\n\n## 9.2 System Efficiency\n\nSystem efficiency remains a significant bottleneck, especially as RAG systems scale to handle large datasets and real-time applications. The multi-step nature of RAG workflows-including query classification, retrieval, re-ranking, and generation-adds complexity and latency, which can hinder overall performance.\n\nLatency in Retrieval Processes. As document collections grow, retrieval and re-ranking processes increasingly become sources of latency. Lightweight search methods and hybrid retrieval approaches that combine sparse and dense techniques offer potential solutions by balancing speed and accuracy. For example, indexing, a traditionally resource-intensive process, has seen innovations through\n\ndifferentiable search indices such as DSI [128] and SEAL [7]. These methods integrate retrieval within Transformer models, enabling direct mapping of text queries to document identifiers and thereby improving both performance and retrieval efficiency.\n\nComputational Costs. The introduction of deep learning-based re-ranking models like monoT5 [102] and RankLLaMA [96] brings significant computational overhead, particularly in scenarios requiring iterative reasoning. Future research could focus on optimizing these models or developing retrieval pruning techniques that reduce the number of documents passed to the generation phase without sacrificing performance [145].\n\nModular Workflow Optimization. The complexity of RAG systems often stems from interdependencies between components like chunking strategies, embedding models, and re-ranking algorithms. Modular designs that enable independent optimization of each step while accounting for cross-component interactions are key to enhancing system throughput [39]. Advanced chunking methods and hybrid search strategies could offer trade-offs that maximize both retrieval precision and speed. An example is the Hybrid with HyDE [139] approach, which integrates both sparse and dense retrieval to capture relevant documents from both lexical and semantic perspectives.\n\n## 9.3 Multimodal RAG\n\nThe expansion of RAG systems to support multimodal data-encompassing text, images, and audio-presents new challenges. Integrating diverse modalities requires not only effective retrieval but also seamless alignment and generation across different data types.\n\nCross-Modal Alignment. Aligning multimodal documents with text-based queries remains a core challenge. The complexity of mapping diverse data types into a unified retrieval framework necessitates improved cross-modal retrieval strategies capable of simultaneously handling text, image, and potentially video or audio data.\n\nCoherent Multimodal Generation. Generating responses that meaningfully integrate information from multiple modalities is another difficult task. Advanced generation models capable of reasoning across different modalities are required to produce outputs that are both contextually relevant and visually coherent.\n\nRecent advancements in multimodal RAG, such as MuRAG [17], REVEAL [49], and Re-ViLM [152], have shown potential in incorporating multimodal retrieval and generation into real-world applications like visual question answering [18], image captioning [120], and text-to-audio generation [158]. Moving forward, research will likely focus on refining these techniques, especially in scaling multimodal retrieval to handle larger datasets and more complex queries. Extending retrieval capabilities to include more diverse media types, such as video and speech, also represents a promising direction for the continued evolution of RAG systems.\n\n## 10 Conclusions\n\nIn this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing the capabilities of LLMs. Through a structured overview of RAG, categorizing various methods, and an in-depth analysis of its core technologies and evaluation methods, this study illuminates the path for future research. It identifies crucial areas for improvement and outlines potential directions for advancing RAG applications, especially in textual contexts. This survey aims to elucidate the core concepts of the RAG field from a retrieval perspective, and it is intended to facilitate further exploration and innovation in the accurate retrieval and generation of information.\n\n## Acknowledgments\n\nThis research is supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada.\n\n## References\n\n- [1] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and etc. 2023. GPT-4 Technical Report. arXiv (2023).\n- [2] Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep Listwise Context Model for Ranking Refinement. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval . ACM.\n- [3] Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu. 1998. An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions. J. ACM 45, 6 (1998), 891-923. https://doi.org/10. 1145/293347.293348\n- [4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.11511.\n- [5] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 2023. Optimizing Retrievalaugmented Reader Models via Token Elimination. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 1506-1524.\n- [6] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper\\_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-AbstractConference.html\n- [7] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [8] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of BigScience Episode #5 - Workshop on Challenges &amp; Perspectives in Creating Large Language Models , Vol. abs/2204.06745. Association for Computational Linguistics.\n- [9] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning (ICML) . 2206-2240.\n- [10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Conference on Neural Information Processing Systems (NeurIPS) , Vol. abs/2005.14165.\n- [11] Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, and Tal Schuster. 2022. Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 291-305.\n- [12] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation. arXiv abs/2404.00610 (2024).\n- [13] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. arXiv abs/2310.05029 (2023).\n- [14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large Language Models in RetrievalAugmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 16 (2024), 17754-17762.\n- [15] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022. GERE: Generative Evidence Retrieval for Fact Verification. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in\n\n## Information Retrieval . ACM.\n\n- [16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv abs/2107.03374 (2021).\n- [17] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022. MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n- [18] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023. Re-Imagen: Retrieval-Augmented Text-toImage Generator. In International Conference on Learning Representations (ICLR) .\n- [19] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix: Democratizing ChatGPT across Languages. arXiv abs/2304.10453 (2023).\n- [20] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. 2023. UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 12318-12337.\n- [21] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory.. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [22] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\n- [23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research (JMLR) 24 (2023), 240:1-240:113.\n- [24] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv abs/2210.11416 (2022).\n- [25] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 8440-8451.\n- [26] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) , Vol. abs/2401.14887.\n- [27] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot Dense Retrieval From 8 Examples. In International Conference on Learning Representations (ICLR) .\n- [28] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-sensitive hashing scheme based on p-stable distributions.. In International Symposium on Computational Geometry (SoCG) . 253-262.\n\n- [29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the Conference of the North . Association for Computational Linguistics, 4171-4186.\n- [30] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia: Knowledge-Powered Conversational Agents. In International Conference on Learning Representations (ICLR) .\n- [31] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics.\n- [32] Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In International Conference on Language Resources and Evaluation (LREC) .\n- [33] Shahul ES, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2023. RAGAs: Automated Evaluation of Retrieval Augmented Generation. Conference of the European Chapter of the Association for Computational Linguistics abs/2309.15217 (2023).\n- [34] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-Generation Synergy Augmented Large Language Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing , Vol. abs/2310.05149. IEEE.\n- [35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv abs/2101.00027 (2021).\n- [36] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot Dense Retrieval without Relevance Labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 1762-1777.\n- [37] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 6894-6910.\n- [38] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv abs/2312.10997 (2023).\n- [39] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024. Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks. arXiv (2024).\n- [40] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank, Generate. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 2701-2715.\n- [41] Simon Gottschalk and Elena Demidova. 2018. EventKG: A Multilingual Event-Centric Temporal Knowledge Graph . Springer International Publishing. 272-287 pages.\n- [42] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language Model Pre-Training. In International Conference on Machine Learning (ICML) . 3929-3938.\n- [43] William L. Hamilton. 2020. Graph representation learning . Springer International Publishing.\n- [44] Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen E. Robertson, Steve Walker, and P. W. Williams. 1996. Okapi at TREC-5. In Proceedings of The Fifth Text REtrieval Conference, TREC 1996, Gaithersburg, Maryland, USA, November 20-22, 1996 (NIST Special Publication, Vol. 500-238) , Ellen M. Voorhees and Donna K. Harman (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz\n- [45] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding.. In International Conference on Learning Representations (ICLR) .\n- [46] Enrique HerreraViedma, Gabriella Pasi, Antonio G. LopezHerrera, and Carlos Porcel. 2006. Evaluating the information quality of Web sites: A methodology based on fuzzy computing with words. Journal of the American Society for Information Science and Technology 57, 4 (2006), 538-549.\n- [47] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-light: Efficient and effective retrievalaugmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1437-1447.\n- [48] Yucheng Hu and Yuxing Lu. 2024. RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing. arXiv abs/2404.19543 (2024).\n- [49] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi. 2023. Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2336923379.\n\n- [50] Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen-mei Hwu. 2022. Understanding Jargon: Combining Extraction and Generation for Definition Modeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.\n- [51] Jimmy Xiangji Huang, Jun Miao, and Ben He. 2013. High performance query expansion using adaptive co-training. Inf. Process. Manag. 49, 2 (2013), 441-453. https://doi.org/10.1016/J.IPM.2012.08.002\n- [52] Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, and Lilian H. Y. Tang. 2023. Learning Retrieval Augmentation for Personalized Dialogue Generation.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 2523-2540.\n- [53] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Z Pan. 2023. Retrieval Augmented Generation with Rich Answer Encoding. Proc. of IJCNLP-AACL 2023 (2023).\n- [54] Xiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for biomedical information retrieval. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009 , James Allan, Javed A. Aslam, Mark Sanderson, ChengXiang Zhai, and Justin Zobel (Eds.). ACM, 307-314. https://doi.org/10.1145/1571941.1571995\n- [55] Yizheng Huang and Jimmy Huang. 2024. Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges. CoRR abs/2402.11203 (2024). https://doi.org/10.48550/ARXIV.2402.11203 arXiv:2402.11203\n- [56] Yizheng Huang and Jimmy X. Huang. 2023. Diversified Prior Knowledge Enhanced General Language Model for Biomedical Information Retrieval. In ECAI 2023 - 26th European Conference on Artificial Intelligence, September 30 October 4, 2023, Kraków, Poland - Including 12th Conference on Prestigious Applications of Intelligent Systems (PAIS 2023) (Frontiers in Artificial Intelligence and Applications, Vol. 372) , Kobi Gal, Ann Nowé, Grzegorz J. Nalepa, Roy Fairstein, and Roxana Radulescu (Eds.). IOS Press, 1109-1115. https://doi.org/10.3233/FAIA230385\n- [57] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Transactions on Machine Learning Research (TMLR) 2022 (2022).\n- [58] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume . Association for Computational Linguistics, 874-880.\n- [59] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models. Journal of Machine Learning Research (JMLR) 24 (2023), 251:1-251:43.\n- [60] Israt Jahan, Md. Tahmid Rahman Laskar, Chun Peng, and Jimmy Xiangji Huang. 2023. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. CoRR abs/2306.04504 (2023). https://doi.org/10.48550/ARXIV.2306.04504 arXiv:2306.04504\n- [61] Bernard J. Jansen, Danielle L. Booth, and Amanda Spink. 2009. Patterns of query reformulation during Web searching. J. Assoc. Inf. Sci. Technol. 60, 7 (2009), 1358-1371. https://doi.org/10.1002/ASI.21071\n- [62] H Jégou, M Douze, and C Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE Transactions on Pattern Analysis and Machine Intelligence 33, 1 (2011), 117-128.\n- [63] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, and Gustavo Alonso. 2023. Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models. arXiv abs/2310.09949 (2023).\n- [64] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. 2024. PipeRAG: Fast RetrievalAugmented Generation via Algorithm-System Co-design. arXiv abs/2403.05676 (2024).\n- [65] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active Retrieval Augmented Generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 7969-7992.\n- [66] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams. Applied Sciences 11, 14 (2021), 6421.\n- [67] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 2567-2577.\n- [68] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Transactions on Big Data 7, 3 (2021), 535-547. https://doi.org/10.1109/TBDATA.2019.2921572\n- [69] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 1601-1611.\n- [70] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation. arXiv abs/2305.18846 (2023).\n\n- [71] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 6769-6781.\n- [72] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR) .\n- [73] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. arXiv abs/2212.14024 (2022).\n- [74] Omar Khattab and Matei Zaharia. 2020. ColBERT - Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 39-48.\n- [75] Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. 2024. SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track) , Vol. abs/2312.15166. Association for Computational Linguistics.\n- [76] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 453-466.\n- [77] Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. CoRR abs/2305.18486 (2023). https://doi.org/10.48550/ARXIV.2305.18486 arXiv:2305.18486\n- [78] Md. Tahmid Rahman Laskar, Enamul Hoque, and Jimmy X. Huang. 2020. Query Focused Abstractive Summarization via Incorporating Query Relevance and Transfer Learning with Transformer Models. In Advances in Artificial Intelligence - 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13-15, 2020, Proceedings (Lecture Notes in Computer Science, Vol. 12109) , Cyril Goutte and Xiaodan Zhu (Eds.). Springer, 342-348. https://doi.org/10.1007/978-3-030-47358-7\\_35\n- [79] Carlos Lassance and Stéphane Clinchant. 2022. Naver Labs Europe (SPLADE) @ TREC NeuCLIR 2022.. In Text Retrieval Conference (TREC) .\n- [80] Carlos Lassance, Hervé Déjean, Thibault Formal, and Stéphane Clinchant. 2024. SPLADE-v3: New baselines for SPLADE. arXiv abs/2403.06789 (2024).\n- [81] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) . Association for Computational Linguistics.\n- [82] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, 7871-7880.\n- [83] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [84] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2024. PARADE: Passage Representation Aggregation forDocument Reranking. ACM Transactions on Information Systems 42, 2 (2024), 1-26.\n- [85] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Survey on Retrieval-Augmented Text Generation. arXiv abs/2202.01110 (2022).\n- [86] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq R. Joty, Soujanya Poria, and Lidong Bing. 2024. Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources. In The Twelfth International Conference on Learning Representations .\n- [87] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out . Association for Computational Linguistics, Barcelona, Spain, 74-81. https://aclanthology.org/W04-1013\n- [88] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 6385-6400.\n\n- [89] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. RA-DIT: Retrieval-Augmented Dual Instruction Tuning. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.01352.\n- [90] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.\n- [91] Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge. arXiv abs/2311.08147 (2023).\n- [92] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Augmented Large Language Models with Parametric Knowledge Guiding. arXiv abs/2305.04757 (2023).\n- [93] Hengzhao Ma, Jianzhong Li, and Yong Zhang. 2024. Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree. In 2024 IEEE 40th International Conference on Data Engineering (ICDE) . IEEE.\n- [94] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented Large Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 5303-5315.\n- [95] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 1. ACM, 2421-2425.\n- [96] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 2421-2425. https://doi.org/10.1145/3626772.3657951\n- [97] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2020), 824-836. https://doi.org/10.1109/TPAMI.2018.2889473\n- [98] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to information retrieval . Cambridge University Press. https://doi.org/10.1017/CBO9780511809071\n- [99] Niklas Muennighoff. 2022. SGPT: GPT Sentence Embeddings for Semantic Search. arXiv abs/2202.08904 (2022).\n- [100] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with human feedback. arXiv abs/2112.09332 (2021).\n- [101] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, MingWei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 9844-9855.\n- [102] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained Sequence-to-Sequence Model. In Findings of the Association for Computational Linguistics: EMNLP 2020 . Association for Computational Linguistics.\n- [103] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. CoRR abs/1910.14424 (2019).\n- [104] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Conference on Neural Information Processing Systems (NeurIPS) .\n- [105] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering.. In Conference on Health, Inference, and Learning (CHIL) . 248-260.\n- [106] Yu Pan, Jianxin Sun, and Hongfeng Yu. 2023. LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic Graph-Based ANN Indexing. In 2023 IEEE International Conference on Big Data (BigData) . IEEE.\n- [107] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL '02) . Association for Computational Linguistics, USA, 311-318. https://doi.org/10. 3115/1073083.1073135\n- [108] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming\n\nCurated Corpora with Web Data Only.. In Conference on Neural Information Processing Systems (NeurIPS) .\n\n- [109] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics, 2523-2544.\n- [110] Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information retrieval metrics.. In Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR) . 667-674.\n- [111] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (JMLR) 21 (2020), 140:1-140:67.\n- [112] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context Retrieval-Augmented Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316-1331.\n- [113] Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022. Learning to Retrieve Passages without Supervision. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies . Association for Computational Linguistics.\n- [114] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane Clinchant. 2024. BERGEN: A Benchmarking Library for Retrieval-Augmented Generation. arXiv abs/2407.01102 (2024).\n- [115] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . Association for Computational Linguistics, 3980-3990.\n- [116] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333-389.\n- [117] Jon Saad-Falcon, O. Khattab, Christopher Potts, and Matei Zaharia. 2023. ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2311.09476.\n- [118] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. Optimization Methods for Personalizing Large Language Models through Retrieval Augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM, 752-762.\n- [119] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 21. ACM, 2395-2400.\n- [120] Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022. Retrieval-Augmented Transformer for Image Captioning. In International Conference on Content-based Multimedia Indexing . ACM.\n- [121] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing RetrievalAugmented Large Language Models with Iterative Retrieval-Generation Synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 9248-9274.\n- [122] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2301.12652.\n- [123] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. 2024. ERAGent: Enhancing RetrievalAugmented Language Models with Improved Accuracy, Efficiency, and Personalization. arXiv abs/2405.06683 (2024).\n- [124] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models. arXiv abs/2403.10081 (2024).\n- [125] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-Augmented Language Models. In International Conference on Learning Representations (ICLR) .\n- [126] Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, and Jun Deguchi. 2024. AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval. arXiv abs/2404.06004 (2024).\n- [127] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying Language Learning Paradigms. In International Conference on Learning Representations (ICLR) .\n- [128] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer Memory as a Differentiable Search Index. In Conference on Neural Information Processing Systems (NeurIPS) .\n\n- [129] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . Association for Computational Linguistics, 809-819.\n- [130] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv abs/2307.09288 (2023).\n- [131] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 10014-10037.\n- [132] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC Bioinformatics 16, 1 (2015).\n- [133] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Neural Information Processing Systems . 5998-6008.\n- [134] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Conference on Neural Information Processing Systems (NeurIPS) . 3261-3275.\n- [135] Haoyu Wang, Tuo Zhao, and Jing Gao. 2024. BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering. arXiv abs/2402.11129 (2024).\n- [136] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv abs/2212.03533 (2022).\n- [137] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 9414-9423.\n- [138] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. 2022. WebFormer: The Web-page Transformer for Structure Information Extraction. In Proceedings of the ACM Web Conference 2022 . ACM.\n- [139] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, and Xuanjing Huang. 2024. Searching for Best Practices in Retrieval-Augmented Generation. arXiv abs/2407.01219 (2024).\n- [140] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang. 2023. KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. arXiv abs/2308.11761 (2023).\n- [141] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context for Retrieval-Augmented Generation. arXiv abs/2311.08377 (2023).\n- [142] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, and etc. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv abs/2211.05100 (2022).\n- [143] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder.. In Conference on Empirical Methods in Natural Language Processing (EMNLP) . 538-548.\n- [144] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking Retrieval-Augmented Generation for Medicine. arXiv abs/2402.13178 (2024).\n- [145] Jie Xiong, Li Yu, Xi Niu, and Youfang Leng. 2023. XRR: Extreme multi-label text classification with candidate retrieving and deep ranking. Information Sciences 622 (2023), 115-132.\n\n- [146] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In International Conference on Learning Representations (ICLR) .\n- [147] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.04408.\n- [148] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation. In Proceedings of the ACM on Web Conference 2024 , Vol. 21. ACM, 1330-1340.\n- [149] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. arXiv abs/2401.15884 (2024).\n- [150] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: MultiRound Retrieval-Augmented Generation Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval , Vol. 33. ACM, 730-740.\n- [151] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 5364-5375.\n- [152] Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023. Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. In Findings of the Association for Computational Linguistics: EMNLP 2023 . Association for Computational Linguistics, 11844-11857.\n- [153] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, 2369-2380.\n- [154] Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval-Augmented Generation: A Survey. arXiv abs/2405.07437 (2024).\n- [155] Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul N. Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. FewShot Generative Conversational Query Rewriting. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 , Jimmy X. Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1933-1936. https://doi.org/10.1145/3397271.3401323\n- [156] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than Retrieve: Large Language Models are Strong Context Generators. In International Conference on Learning Representations (ICLR) .\n- [157] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 2421-2436.\n- [158] Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2024. Retrieval-Augmented Text-to-Audio Generation. In ICASSP - IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , Vol. abs/2309.08051. IEEE.\n- [159] Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, and Dong Wang. 2024. Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation. In North American Chapter of the Association for Computational Linguistics , Vol. abs/2403.14952.\n- [160] Saber Zerhoudi and Michael Granitzer. 2024. PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents. arXiv (2024).\n- [161] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. arXiv abs/2205.01068 (2022).\n- [162] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv abs/2402.19473 (2024).\n- [163] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. 2024. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. In The Twelfth International Conference on Learning Representations , Vol. abs/2310.06117.\n\n- [164] Ning Zhong, Yuefeng Li, and Sheng-Tang Wu. 2012. Effective Pattern Discovery for Text Mining. IEEE Transactions on Knowledge and Data Engineering 24, 1 (2012), 30-44.\n- [165] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for Information Retrieval: A Survey. arXiv abs/2308.07107 (2023).\n- [166] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. 2023. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . ACM.\n\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009",
  "tables": [
    {
      "index": 0,
      "markdown": "| Evaluation Framework   | Aspects                       | Methods                        | Metrics                                                     | Datasets                                                                 |\n|------------------------|-------------------------------|--------------------------------|-------------------------------------------------------------|--------------------------------------------------------------------------|\n| RAGAS [33]             | Quality of RAG Systems        | Context Relevance              | Extracted Sentences / Total Sentences                       | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Answer Relevance               | Average Cosine Similarity                                   | WikiEval 7                                                               |\n| RAGAS [33]             | Quality of RAG Systems        | Faithfulness                   | Supported Statements / Total Statements                     | WikiEval 7                                                               |\n| ARES [117]             | Improving RAGAS               | Context Relevance              | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Relevance               | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| ARES [117]             | Improving RAGAS               | Answer Faithfulness            | Confidence Intervals                                        | KILT [109] SuperGLUE [134]                                               |\n| RECALL [91]            | Counterfactual Robustness     | Response Quality               | Accuracy (QA) BLEU, ROUGE-L (Generation)                    | EventKG [41] UJ [50]                                                     |\n| RECALL [91]            | Counterfactual Robustness     | Robustness                     | Misleading Rate (QA) Mistake Reappearance Rate (Generation) | EventKG [41] UJ [50]                                                     |\n| RGB [14]               | Impact of RAG on LLMs         | Noise Robustness               | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Negative Rejection             | Rejection Rate                                              | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Information Integration        | Accuracy                                                    | Synthetic                                                                |\n| RGB [14]               | Impact of RAG on LLMs         | Counterfactual Robustness      | Error Detection Rate Error Correction Rate                  | Synthetic                                                                |\n| MIRAGE [144]           | RAG in Medical QA             | Zero-Shot Learning             | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Multi-Choice Evaluation        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Retrieval-Augmented Generation | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| MIRAGE [144]           | RAG in Medical QA             | Question-Only Retrieval        | Accuracy                                                    | MMLU-Med [45] MedQA-US [66] MedMCQA [105] PubMedQA [67] BioASQ-Y/N [132] |\n| eRAG [119]             | Retrieval Quality in RAG      | Downstream Task                | Accuracy, ROUGE                                             | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Set-based                      | Precision, Recall, Hit Rate                                 | KILT                                                                     |\n| eRAG [119]             | Retrieval Quality in RAG      | Ranking                        | MAP, MRR, NDCG                                              | KILT                                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Surface-Based                  | EM, F1, Precision, Recall                                   | QA Datasets [69, 76]                                                     |\n| BERGEN [114]           | Standardizing RAG Experiments | Semantic                       | BEM [11], LLMeval [114]                                     | QA Datasets [69, 76]                                                     |"
    },
    {
      "index": 1,
      "markdown": "| Research                     | Year      | Retrieval   | Source     | Multi-hop   | Training   |            | Pre-Retrieval      |                   | Retrieval          | Post-Retrieval   | Post-Retrieval   | Generation   | Generation    |\n|------------------------------|-----------|-------------|------------|-------------|------------|------------|--------------------|-------------------|--------------------|------------------|------------------|--------------|---------------|\n| REALM [42]                   |           | Internal    | External ! |             | !          | Indexing ! | Query Manipulation | Data Modification | Search & Ranking ! | Re-Ranking       | Filtering        | Enhancing    | Customization |\n| kNN-LMs [72]                 | 2020 2020 | !           | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| RAG [83]                     | 2020      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| FiD [58]                     | 2021      |             | !          |             |            |            |                    |                   | !                  |                  |                  | !            |               |\n| Webgpt [100]                 | 2021      |             | !          | !           | !          | !          | !                  |                   | !                  |                  | !                | !            |               |\n| Re2G [40]                    | 2022      | !           |            | !           | !          |            |                    |                   |                    | !                |                  |              |               |\n| RETRO [9]                    | 2022      |             | !          | !           | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| DSP [73]                     | 2022      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| CoK [86]                     | 2023      |             | !          | !           |            |            | !                  |                   |                    | !                |                  |              |               |\n| IRCOT [131]                  | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  | !            |               |\n| ITRG [34]                    | 2023      | !           | !          | !           |            |            |                    |                   | !                  |                  |                  | !            |               |\n| PKG [92]                     | 2023      | !           |            |             |            |            |                    |                   |                    |                  |                  |              |               |\n| RA-DIT [89]                  | 2023      |             | !          | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| Self-RAG [4]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n|                              | 2023      | !           |            |             |            |            |                    |                   | !                  |                  |                  |              |               |\n| SURGE [70] FiD-TF [5]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                | !                |              |               |\n| PRCA [151]                   | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  | !            |               |\n| REPLUG [122]                 | 2023      |             | !          |             | !          |            |                    |                   |                    |                  |                  | !            |               |\n| AAR [157]                    | 2023      |             | !          |             | !          |            |                    |                   | !                  |                  |                  |              |               |\n| Query2doc [137]              | 2023      | !           |            |             |            |            | !                  |                   |                    |                  |                  |              |               |\n| Step-Back [163]              | 2023      |             | !          | !           |            |            | !                  |                   |                    |                  |                  |              |               |\n| ITER-RETGEN [121]            | 2023      |             | !          | !           |            |            |                    |                   | !                  | !                |                  |              |               |\n| RECITE [125]                 | 2023      | !           |            | !           | !          |            |                    | !                 |                    |                  |                  | !            |               |\n| PROMPTAGATOR [27]            | 2023      | !           |            | !           |            |            | !                  |                   |                    | !                | !                |              |               |\n| UPRISE [20]                  | 2023      | ! !         |            | !           | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| GENREAD [156]                | 2023      |             |            |             |            |            |                    | !                 |                    |                  |                  | !            |               |\n| LAPDOG [52]                  | 2023      |             | !          |             | !          |            | !                  |                   | !                  | !                |                  | !            | !             |\n| KnowledGPT [140]             | 2023      |             | ! !        | ! !         | !          |            | !                  | !                 |                    | !                |                  | !            |               |\n| Selfmem [21]                 | 2023      |             | !          |             |            | !          |                    |                   | !                  |                  |                  | !            |               |\n| MEMWALKER [13] RECOMP [147]  | 2023 2023 |             | !          |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| Rewrite-Retrieve-Read [94]   | 2023      |             | !          |             | !          |            | !                  |                   |                    |                  |                  |              |               |\n| Atlas [94]                   | 2023      |             | !          | !           | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| DKS-RAC [53]                 | 2023      |             | !          | !           | !          |            |                    |                   |                    | !                | !                |              |               |\n| In-Context RALM [112]        | 2023      |             | !          |             |            |            |                    |                   |                    | !                |                  |              |               |\n| Fid-light [47]               | 2023      |             | ! !        | !           |            |            | !                  |                   | !                  | !                |                  |              |               |\n| FLARE [65]                   | 2023      |             | !          |             | !          | !          |                    |                   | !                  |                  |                  |              |               |\n| Chameleon [63] ERAGent [123] | 2023      | !           | !          |             | !          |            | !                  |                   | !                  |                  | !                | !            | !             |\n| PipeRAG [64]                 | 2024 2024 |             | !          | !           | !          | !          |                    |                   |                    |                  |                  | !            |               |\n|                              |           | !           |            |             | !          |            |                    |                   |                    |                  | !                |              |               |\n| GenRT [148] PersonaRAG [160] | 2024 2024 | !           | !          | !           | !          |            | !                  |                   | !                  | ! !              |                  | !            | !             |\n| CRAG [149]                   | 2024      | !           | !          |             | !          |            |                    | !                 |                    | !                | !                | !            |               |\n| IMRAG [150]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  | !            |               |\n| AiSAQ [126]                  | 2024      | !           |            |             |            | !          |                    |                   | !                  | !                |                  |              |               |\n| ROPG [118]                   | 2024      | !           |            |             | !          |            |                    |                   | !                  |                  |                  | !            | !             |\n| RQ-RAG [12]                  | 2024      |             | !          | !           | !          |            | !                  |                   |                    | !                |                  |              |               |\n| PlanRAG [81]                 | 2024      |             | !          | !           |            |            | !                  |                   |                    | !                |                  | !            |               |\n| RARG [159]                   | 2024      |             | !          |             | !          |            |                    | !                 | !                  |                  |                  | !            |               |\n| DRAGIN [124]                 | 2024      |             | !          | !           |            |            | !                  |                   | !                  |                  |                  | !            |               |\n| [93]                         | 2024      | !           |            |             | !          | !          |                    |                   | !                  | !                |                  |              |               |\n| LRUS-CoverTree               |           |             |            |             |            |            |                    |                   |                    |                  |                  |              |               |"
    },
    {
      "index": 2,
      "markdown": "| Research                   |   Year | Retriever                                          | Generator                                                  |\n|----------------------------|--------|----------------------------------------------------|------------------------------------------------------------|\n| REALM [42]                 |   2020 | BERT [29]                                          | Transformers [133]                                         |\n| kNN-LMs [72]               |   2020 | FAISS [68]                                         | Transformers                                               |\n| RAG [83]                   |   2020 | DPR [71]                                           | BART-Large [82]                                            |\n| FiD [58]                   |   2021 | BM25 [116], DPR                                    | T5 [111]                                                   |\n| Webgpt [100]               |   2021 | Bing                                               | GPT-3 [10]                                                 |\n| Re2G [40]                  |   2022 | BM25, DPR                                          | BART                                                       |\n| RETRO [9]                  |   2022 | BERT                                               | Transformer                                                |\n| DSP [73]                   |   2022 | ColBERTv2                                          | GPT-3.5 (text-davinci-002)                                 |\n| CoK [86]                   |   2023 | [74] LLaMA2-7B [130], ChatGPT (gpt-3.5-turbo-0613) | ChatGPT (gpt-3.5-turbo-0613)                               |\n| IRCOT [131]                |   2023 | BM25                                               | GPT-3 (code-davinci-002), Flan-T5 [24]                     |\n| ITRG [34]                  |   2023 | Atlas [94]                                         | LLaMA-33B                                                  |\n| PKG [92]                   |   2023 | LLaMA-7B                                           | InstructGPT-3.5 (text-davinic-002) [104]                   |\n| RA-DIT [89]                |   2023 | DRAGON+ [88]                                       | LLaMA                                                      |\n| Self-RAG [4]               |   2023 | Contriever [57]                                    | LLaMA2 (7B and 13B) , GPT-4 [1]                            |\n| SURGE [70]                 |   2023 | Graph Neural Networks (GNN) [43]                   | Transformers                                               |\n| FiD-TF [5]                 |   2023 | BM25, SBERT [115]                                  | T5                                                         |\n| PRCA [151]                 |   2023 | BM25, DPR, Contriver, SimCSE [37], SBERT           | T5, Phoenix-7B [19], Vicuna-7B [22], ChatGLM [31], GPT-3.5 |\n| REPLUG [122]               |   2023 | Contriever                                         | GPT-3                                                      |\n| AAR [157]                  |   2023 | ANCE [146], Contriever                             | Flan-T5, InstructGPT                                       |\n| Query2doc [137]            |   2023 | BM25, DPR                                          | GPT-3 (text-davinci-003)                                   |\n| Step-Back [163]            |   2023 | PaLM-2L [23]                                       | PaLM-2L, GPT-4                                             |\n| ITER-RETGEN [121]          |   2023 | Contriever                                         | InstructGPT (text-davinci-003), LLaMA2                     |\n| RECITE [125]               |   2023 |                                                    | PaLM, UL2 [127], OPT [161], Codex [16]                     |\n| PROMPTAGATOR [27]          |   2023 | T5                                                 | FLAN                                                       |\n| UPRISE [20]                |   2023 | GPT-Neo-2.7B [8]                                   | BLOOM-7.1B [142], OPT-66B, GPT-3-175B                      |\n| GENREAD [156]              |   2023 |                                                    | InstructGPT                                                |\n| LAPDOG [52]                |   2023 | Contriever                                         | T5                                                         |\n| KnowledGPT [140]           |   2023 |                                                    | GPT-4                                                      |\n| Selfmem [21]               |   2023 | BM25                                               | XGLM [90], XLM-Rbase [25]                                  |\n| MEMWALKER [13]             |   2023 | LLaMA2                                             | LLaMA2                                                     |\n| RECOMP [147]               |   2023 | BM25                                               | T5-Large                                                   |\n| Rewrite-Retrieve-Read [94] |   2023 | Bing                                               | T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B               |\n| Atlas [94]                 |   2023 | Contriever                                         | T5                                                         |\n| DKS-RAC [53]               |   2023 | DPR                                                | BART                                                       |\n| In-Context RALM [112]      |   2023 | BM25, BERT-base, Contriever, Spider [113]          | GPT-2, GPT-Neo, GPT-J [35], OPT, and LLaMA                 |\n| Fid-light [47]             |   2023 | GTR-Base [101]                                     | T5                                                         |\n| FLARE [65]                 |   2023 | BM25, Bing                                         | GPT-3.5 (text-davinci-003)                                 |\n| Chameleon [63]             |   2023 | ChamVS [63]                                        | ChamLM [63]                                                |\n| ERAGent [123]              |   2024 | Bing                                               | GPT-3.5, Falcon 1B [108]                                   |\n| PipeRAG [64]               |   2024 | SBERT                                              | RETRO [9]                                                  |\n| GenRT [148]                |   2024 | LambdaMart [2]                                     |                                                            |\n| PersonaRAG [160]           |   2024 | BM25                                               | GPT-3.5                                                    |\n| CRAG [149]                 |   2024 | Contriever                                         | LLaMA2                                                     |\n| IMRAG [150]                |   2024 | DPR                                                | Vicuna-7B                                                  |\n| AiSAQ [126]                |   2024 | DiskANN [106]                                      |                                                            |\n| ROPG [118]                 |   2024 | BM25, Contriever                                   | FlanT5-XXL                                                 |\n| RQ-RAG [12]                |   2024 | DuckDuckGo 8                                       | LLaMA2-7B                                                  |\n| PlanRAG [81]               |   2024 | GPT-4                                              | GPT-4                                                      |\n| RARG [159]                 |   2024 | BM25, E5 [136]                                     | LLaMA2-7B                                                  |\n| DRAGIN [124]               |   2024 | BM25, SGPT [99]                                    | LLaMA2 (7B and 13B), Vicuna-13B                            |\n| LRUS-CoverTree [93]        |   2024 | k-MIPS                                             |                                                            |"
    },
    {
      "index": 3,
      "markdown": "|                   |            | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   | Mirage Benchmark Dataset   |         |\n|-------------------|------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|---------|\n| Corpus            | Retriever  | MMLU-Med                   | MedQA-US                   | MedMCQA                    | PubMedQA*                  | BioASQ-Y/N                 | Average |\n| None              | None       | 72.91 ± 1.35               | 65.04 ± 1.34               | 55.25 ± 0.77               | 36.00 ± 2.15               | 74.27 ± 1.76               | 60.69   |\n| PubMed (23.9M)    | BM25       | 72.27 ± 1.36               | 63.71 ± 1.35               | 55.49 ± 0.77               | 66.20 ± 2.12               | 88.51 ± 1.28               | 69.23   |\n| PubMed (23.9M)    | Contriever | 71.72 ± 1.36               | 63.94 ± 1.35               | 54.29 ± 0.77               | 65.60 ± 2.12               | 85.44 ± 1.42               | 68.20   |\n| PubMed (23.9M)    | SPECTER    | 73.19 ± 1.34               | 65.20 ± 1.34               | 53.12 ± 0.77               | 54.80 ± 2.23               | 75.73 ± 1.72               | 64.41   |\n| PubMed (23.9M)    | MedCPT     | 73.09 ± 1.34               | 66.69 ± 1.32               | 54.94 ± 0.77               | 66.40 ± 2.11               | 85.76 ± 1.41               | 69.38   |\n| PubMed (23.9M)    | RRF-2      | 75.57 ± 1.30               | 64.34 ± 1.34               | 55.34 ± 0.77               | 69.00 ± 2.07               | 87.06 ± 1.35               | 70.26   |\n| PubMed (23.9M)    | RRF-4      | 73.37 ± 1.34               | 64.73 ± 1.34               | 54.75 ± 0.77               | 67.20 ± 2.10               | 88.51 ± 1.28               | 69.71   |\n| Wikipedia (29.9M) | BM25       | 73.37 ± 1.34               | 63.47 ± 1.35               | 54.10 ± 0.77               | 26.40 ± 1.97               | 71.36 ± 1.82               | 57.74   |\n| Wikipedia (29.9M) | Contriever | 74.10 ± 1.33               | 65.99 ± 1.33               | 54.03 ± 0.77               | 26.40 ± 1.97               | 69.90 ± 1.85               | 58.08   |\n|                   | SPECTER    | 72.18 ± 1.36               | 63.63 ± 1.35               | 52.71 ± 0.77               | 22.20 ± 1.86               | 66.83 ± 1.89               | 55.51   |\n|                   | MedCPT     | 71.99 ± 1.36               | 65.12 ± 1.34               | 55.15 ± 0.77               | 29.00 ± 2.03               | 73.46 ± 1.78               | 58.95   |\n|                   | RRF-2      | 74.20 ± 1.33               | 64.57 ± 1.34               | 54.72 ± 0.77               | 31.00 ± 2.07               | 76.21 ± 1.71               | 60.14   |\n|                   | RRF-4      | 73.19 ± 1.34               | 64.96 ± 1.34               | 54.53 ± 0.77               | 31.00 ± 2.07               | 72.01 ± 1.81               | 59.14   |"
    }
  ],
  "stats": {
    "pages": 37,
    "chunksCreated": 230,
    "totalCharacters": 164414,
    "totalWords": 21697,
    "numTables": 4,
    "processingTimeMs": 60934
  }
}