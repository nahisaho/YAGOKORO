{
  "paper": {
    "id": "2310.11453v1",
    "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
    "abstract": "The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.",
    "authors": [
      "Hongyu Wang",
      "Shuming Ma",
      "Li Dong",
      "Shaohan Huang",
      "Huaijie Wang",
      "Lingxiao Ma",
      "Fan Yang",
      "Ruiping Wang",
      "Yi Wu",
      "Furu Wei"
    ],
    "published": "2023-10-17T17:59:15.000Z",
    "updated": "2023-10-17T17:59:15.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.11453v1",
    "absUrl": "https://arxiv.org/abs/2310.11453v1"
  },
  "chunks": [
    {
      "id": "2310.11453v1-chunk-0",
      "content": "Hongyu Wang ∗†‡ Shuming Ma ∗† Li Dong † Shaohan Huang † Huaijie Wang § Lingxiao Ma † Fan Yang † Ruiping Wang ‡ Yi Wu § Furu Wei †⋄ † Microsoft Research ‡ University of Chinese Academy of Sciences § Tsinghua University https://aka.ms/GeneralAI",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "chunkIndex": 0,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-1",
      "content": "The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-2",
      "content": "BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.\n\nFigure 1: BitNet trains 1-bit Transformers from scratch, obtaining competitive results in an energyefficient way. BitNet significantly outperforms state-of-the-art quantization methods. As the model size scales up, the cost savings become more significant while achieving competitive performance with the models trained with FP16.\n\n<!-- image -->\n\n∗ Equal contribution. ⋄ Corresponding author.\n\n<!-- image -->\n\n'",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-3",
      "content": "The rapid growth of large language models [BMR + 20, Ope23, CND + 22, ADF + 23, TLI + 23, TMS + 23] has led to significant improvements in various tasks. However, it is expensive to host large language models due to the high inference costs and energy consumption. As the size of these models grows, the memory bandwidth required for accessing and processing the model parameters becomes a major bottleneck, limiting the overall inference performance. Moreover, when deploying these models on distributed systems or multi-device platforms, the inter-device communication overhead can significantly impact the inference latency and energy consumption. Model quantization [FAHA23, CCKS23, XLS + 23] has emerged as a promising solution, as it can significantly reduce the memory footprint and computational cost of large-scale models while maintaining competitive performance.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-4",
      "content": "HA23, CCKS23, XLS + 23] has emerged as a promising solution, as it can significantly reduce the memory footprint and computational cost of large-scale models while maintaining competitive performance.\n\nMost existing quantization approaches for large language models are post-training. They are simple and easy to apply since it does not require any changes to the training pipeline or retraining the model. However, it will result in a more significant loss of accuracy especially when the precision goes lower, because the model is not optimized for the quantized representation during training.\n\nAnother strand of quantizing deep neural networks is quantization-aware training. Compared to post-training, it typically results in better accuracy, as the model is trained to account for the reduced precision from the beginning. Moreover, it allows the model to continue-train or do fine-tuning, which is essential for large language models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-5",
      "content": "uracy, as the model is trained to account for the reduced precision from the beginning. Moreover, it allows the model to continue-train or do fine-tuning, which is essential for large language models. The challenge of quantization-aware training mainly lies in optimization, i.e., the model becomes more difficult to converge as the precision goes lower. Besides, it is unknown whether quantization-aware training follows the scaling law of neural language models.\n\nIn this work, we focus on binarization (i.e., 1-bit), which is the extreme case of quantization, applied to large language models. Previous studies on binarized neural networks [RORF16, BT19] have mostly revolved around convolutional neural networks. Recently, there has been some research on binarized Transformers. However, these studies have focused on machine translation or BERT pretraining, which is quite different from large language models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-6",
      "content": ". Recently, there has been some research on binarized Transformers. However, these studies have focused on machine translation or BERT pretraining, which is quite different from large language models. For example, machine translation employs an encoder-decoder architecture, BERT pretraining utilizes a bidirectional encoder, and large language models use a unidirectional decoder. Furthermore, large language models are typically scaled up to a much larger model size, while BERT and machine translation models do not undergo such extensive scaling.\n\nTo the best of our knowledge, this work is the first to investigate quantization-aware training for 1-bit large language models. We propose BitNet, a 1-bit Transformer architecture for large language models, which aims to scale efficiently in terms of both memory and computation. BitNet employs low-precision binary weights and quantized activations, while maintaining high precision for the optimizer states and gradients during training.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-7",
      "content": "in terms of both memory and computation. BitNet employs low-precision binary weights and quantized activations, while maintaining high precision for the optimizer states and gradients during training. Our approach is designed to be scalable and stable, with the ability to handle large language models efficiently. The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. Furthermore, it complements other acceleration methods for large language models, such as PagedAttention [KLZ + 23], FlashAttention [DFE + 22, Dao23], and speculative decoding [LKM23].\n\nWe evaluate BitNet on a range of language modeling benchmarks, comparing with state-of-the-art quantization methods and FP16 Transformers. Experimental results demonstrate that BitNet achieves competitive performance in terms of both perplexity and downstream task accuracy.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-8",
      "content": "ith state-of-the-art quantization methods and FP16 Transformers. Experimental results demonstrate that BitNet achieves competitive performance in terms of both perplexity and downstream task accuracy. More importantly, BitNet significantly reduces memory footprint and energy consumption compared to the baselines. Furthermore, we show that BitNet follows a scaling law similar to that of full-precision Transformers, indicating that it can be effectively scaled to even larger language models with potential benefits in terms of performance and efficiency.\n\nWilliam Henry Gates III\n\n'\n\nFigure 2: (a) The computation flow of BitLinear . (b) The architecture of BitNet, consisting of the stacks of attentions and FFNs, where matrix multiplication is implemented as BitLinear .\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-9",
      "content": "As shown in Figure 2, BitNet uses the same layout as Transformers, stacking blocks of self-attention and feed-forward networks. Compared with vanilla Transformer, BitNet uses BitLinear (Eq. 11) instead of conventional matrix multiplication, which employs binarized (i.e., 1-bit) model weights. We leave the other components high-precision, e.g., 8-bit in our experiments. We summarized the reasons as follows. First, the residual connections and the layer normalization contribute negligible computation costs to large language models. Second, the computation cost of QKV transformation is much smaller than the parametric projection as the model grows larger. Third, we preserve the precision for the input/output embedding because the language models have to use high-precision probabilities to perform sampling.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2 BitNet",
        "chunkIndex": 9,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-10",
      "content": "We first binarize the weights to either +1 or -1 with the signum function. Following [LOP + 22], we centralize the weights to be zero-mean before binarization to increase the capacity within a limited numerical range. A scaling factor β is used after binarization to reduce the l 2 error between the real-valued and the binarized weights. The binarization of a weight W ∈ R n × m can be formulated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe further quantize the activations to b -bit precision. Following [DLBZ22], we use absmax quantization, which scales activations into the range [ -Q b , Q b ] ( Q b = 2 b -1 ) by multiplying with Q b and dividing by the absolute maximum of the input matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere ϵ is a small floating-point number that prevents overflow when performing the clipping.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 10,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-11",
      "content": "by the absolute maximum of the input matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere ϵ is a small floating-point number that prevents overflow when performing the clipping.\n\nFor the activations before the non-linear functions (e.g., ReLU), we scale them into the range [0 , Q b ] by subtracting the minimum of the inputs so that all values are non-negative:\n\n<!-- formula-not-decoded -->\n\nIn this work, we quantize the activation to 8-bit and leave lower precision in future work. Moreover, the quantization is performed per tensor during training while per token during inference for both stability and efficiency.\n\nWith the above quantization equations, the matrix multiplication can be written as:\n\n<!-- formula-not-decoded -->\n\nWe assume that the elements in W and x are mutually independent and share the same distribution, and W and x are independent of each other. Then the variance of the output y is estimated as:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 11,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-12",
      "content": "lements in W and x are mutually independent and share the same distribution, and W and x are independent of each other. Then the variance of the output y is estimated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor the full-precision computation, the variance of the output Var( y ) is at the scale of 1 with the standard initialization methods (e.g., Kaiming initialization or Xavier initialization), which has a great benefit to the training stability. To preserve the variance after quantization, we introduce a LayerNorm [BKH16] function before the activation quantization. In this way, the variance of the output y is then estimated as Var( y ) ≈ E [LN( ˜ x ) 2 ] = 1 , which has the same magnitude as the full-precision counterpart Var( y ) . In the context of Transformers, it has the exact implementation as SubLN [WMH + 22]. With SubLN and the quantization methods above, we have BitLinear , which is formulated as:",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 12,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-13",
      "content": "n counterpart Var( y ) . In the context of Transformers, it has the exact implementation as SubLN [WMH + 22]. With SubLN and the quantization methods above, we have BitLinear , which is formulated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFigure 2 provides an illustration of the computation flow of BitLinear . After the SubLN operation, the activations are quantized with the absmax function. The matrix multiplication is performed between the 1-bit weights and the quantized activations. The output activations are rescaled with { β, γ } to dequantize them to the original precision.\n\nModel parallelism with Group Quantization and Normalization One essential technique to scale up large language models is model parallelism [SPP + 19], which partitions the matrix multiplication on multiple devices. A prerequisite for the existing model parallelism approaches is that the tensors are independent along the partition dimension.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 13,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-14",
      "content": "P + 19], which partitions the matrix multiplication on multiple devices. A prerequisite for the existing model parallelism approaches is that the tensors are independent along the partition dimension. However, all of the parameters α , β , γ , and η are calculated from the whole tensors, breaking the independent prerequisite. One solution is to introduce one all-reduce operation for each parameter. However, even though the communication for each parameter is small, the amount of synchronization is growing as the model becomes deeper, which significantly slows the forward pass. The problem also exists in SubLN , where the mean and the variance should be estimated across the partition dimension.\n\nTo this end, we propose a simple approach that makes the model parallelism more efficient. We divide the weights and activations into groups and then independently estimate each group's parameters. This way, the parameters can be calculated locally without requiring additional communication.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 14,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-15",
      "content": "e divide the weights and activations into groups and then independently estimate each group's parameters. This way, the parameters can be calculated locally without requiring additional communication. This approach, called Group Quantization, is formulated as follows:\n\nFor a weight matrix W ∈ R n × m , we divide it into G groups along the partition dimension, and each group has a size of n G × m . We then estimate the parameters for each group independently:\n\n<!-- formula-not-decoded -->\n\nTable 1: Energy consumption of BitNet and Transformer varying different model size. Results are reported with 512 as input length.\n\n| Models      | Size   | WBits   | 7nm Energy (J)   | 7nm Energy (J)   | 45nm Energy (J)   | 45nm Energy (J)   |\n|-------------|--------|---------|------------------|------------------|-------------------|-------------------|\n| Models      | Size   | WBits   | MUL              | ADD              | MUL               | ADD               |\n| Transformer | 6.7B   | 32 16   |",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 15,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-16",
      "content": "----------|-------------------|-------------------|\n| Models      | Size   | WBits   | MUL              | ADD              | MUL               | ADD               |\n| Transformer | 6.7B   | 32 16   | 4.41 1.14        | 1.28 0.54        | 12.46 3.70        | 3.03 1.35         |\n| BitNet      | 6.7B   |         |                  |                  |                   |                   |\n|             | 6.7B   | 1       | 0.02             | 0.04             | 0.08              | 0.13              |\n| Transformer | 13B    | 32      | 8.58             | 2.49             | 24.23             | 5.89              |\n|             | 13B    | 16      | 2.23             | 1.05             | 7.20              | 2.62              |\n| BitNet      | 13B    | 1       | 0.04             | 0.06             | 0.12              | 0.24              |\n| Transformer | 30B    | 32      | 20.09            | 5.83             | 56.73             | 13.80             |\n|             | 30B    | 16      | 5.21",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 16,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-17",
      "content": "| 0.12              | 0.24              |\n| Transformer | 30B    | 32      | 20.09            | 5.83             | 56.73             | 13.80             |\n|             | 30B    | 16      | 5.21             | 2.45             | 16.87             | 6.13              |\n| BitNet      | 30B    | 1       | 0.06             | 0.14             | 0.20              | 0.53              |\n\nwhere W ( g ) denotes the g -th group of the weight matrix. Similarly, for the activations, we can divide the input matrix x ∈ R n × m into G groups and calculate the parameters for each group:\n\n<!-- formula-not-decoded -->\n\nFor LN, we can apply the group normalization technique [WH20] to compute the mean and variance for each group independently:\n\n<!-- formula-not-decoded -->\n\nIn this way, we can efficiently implement model parallelism with Group Quantization and Normalization, which requires no additional communication and can scale to large language models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 17,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-18",
      "content": "ement model parallelism with Group Quantization and Normalization, which requires no additional communication and can scale to large language models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.1 BitLinear",
        "chunkIndex": 18,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-19",
      "content": "Straight-through estimator. To train our 1-bit model, we employ the straight-through estimator (STE)[BLC13] to approximate the gradient during backpropagation. This method bypasses the nondifferentiable functions, such as the Sign (Eq. 2) and Clip (Eq. 5) functions, during the backward pass. STE allows gradients to flow through the network without being affected by these non-differentiable functions, making it possible to train our quantized model.\n\nMixed precision training. While the weights and the activations are quantized to low precision, the gradients and the optimizer states are stored in high precision to ensure training stability and accuracy. Following the previous work [LSL + 21], we maintain a latent weight in a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.2 Model Training",
        "chunkIndex": 19,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-20",
      "content": "a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process.\n\nLarge learning rate. One challenge for the optimization is that a small update on the latent weights often makes no difference in the 1-bit weights. This results in a biased gradient and update which are estimated based on the 1-bit weights. This problem is even worse at the beginning of the training, where the models are supposed to converge as fast as possible. To address this challenge, we explore various methods, concluding that increasing the learning rate is the simplest and best way to accelerate the optimization. Our experiments show that BitNet benefits from a large learning rate in terms of convergence, while the FP16 Transformer diverges at the beginning of training with the same learning rate. More details can be found in Section 3.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.2 Model Training",
        "chunkIndex": 20,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-21",
      "content": "terms of convergence, while the FP16 Transformer diverges at the beginning of training with the same learning rate. More details can be found in Section 3.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.2 Model Training",
        "chunkIndex": 21,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-22",
      "content": "We estimate the computational efficiency of BitNet in terms of both arithmetic operations energy and memory footprint. We mainly focus on the calculation for the matrix multiplication, since it contributes the most to the cost of large language models.\n\nArithmetic operations energy. According to the energy model in [Hor14, ZZL22], the energy consumption for different arithmetic operations can be estimated as follows:\n\nTable 2: ADD and MUL energy consumption [Hor14, ZZL22] for different bit representations at 45nm and 7nm process nodes.\n\n| Bits   |   ADD Energy 45nm |   ˆ E add (pJ) 7nm |   MUL Energy 45nm |   ˆ E mul (pJ) 7nm |\n|--------|-------------------|--------------------|-------------------|--------------------|\n| FP32   |              0.9  |              0.38  |               3.7 |               1.31 |\n| FP16   |              0.4  |              0.16  |               1.1 |               0.34 |\n| INT8   |              0.03 |              0.007 |               0.2 |",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.3 Computational Efficiency",
        "chunkIndex": 22,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-23",
      "content": "1.31 |\n| FP16   |              0.4  |              0.16  |               1.1 |               0.34 |\n| INT8   |              0.03 |              0.007 |               0.2 |               0.07 |\n\nIn vanilla Transformers, for matrix multiplication with dimensions m × n and n × p , the energy consumption can be calculated as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor BitNet, the energy consumption of the matrix multiplication is dominated by the addition operations, as the weights are 1-bit. The multiplication operations are only applied to scale the output with the scalars β and γ Q b , so the energy consumption for multiplication can be computed as:\n\n<!-- formula-not-decoded -->\n\nwhich is significantly smaller than that in Transformers. The energy savings of W1A8 BitNet compared to a full-precision (32-32) and half-precision (16-16) Transformer are shown in Table 1.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.3 Computational Efficiency",
        "chunkIndex": 23,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-24",
      "content": "coded -->\n\nwhich is significantly smaller than that in Transformers. The energy savings of W1A8 BitNet compared to a full-precision (32-32) and half-precision (16-16) Transformer are shown in Table 1. As can be seen, BitNet provides significant energy savings, especially for the multiplication operations, which are the major component of the matrix multiplication energy consumption.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "2.3 Computational Efficiency",
        "chunkIndex": 24,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-25",
      "content": "We train a series of autoregressive language models with BitNet of various scales, ranging from 125M to 30B. The models are trained on an English-language corpus, which consists of the Pile dataset, Common Crawl snapshots, RealNews, and CC-Stories datasets. We use the Sentencpiece tokenizer to preprocess data and the vocabulary size is 16K. Besides BitNet, we also train the Transformer baselines with the same datasets and settings for a fair comparison. More details can be found in the appendix.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.1 Setup",
        "chunkIndex": 25,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-26",
      "content": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term:\n\n<!-- formula-not-decoded -->\n\nFigure 3: Scaling curves of BitNet and FP16 Transformers.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.2 Inference-Optimal Scaling Law",
        "chunkIndex": 26,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-27",
      "content": "nsformer, which follows a power-law. We then fit the scaling law with an irreducible loss term:\n\n<!-- formula-not-decoded -->\n\nFigure 3: Scaling curves of BitNet and FP16 Transformers.\n\n<!-- image -->\n\nTo evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.2 Inference-Optimal Scaling Law",
        "chunkIndex": 27,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-28",
      "content": "te by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.2 Inference-Optimal Scaling Law",
        "chunkIndex": 28,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-29",
      "content": "In addition to the loss, we are also concerned about the capabilities with the scaling of BitNet. Compared with the loss, the capacity is more difficult to predict due to the emergent nature of neural language models. To evaluate the capabilities with the interpretable metrics, we test both the 0-shot and 4-shot results on four downstream tasks, including Hellaswag [ZHB + 19], Winogrande [SBBC20], Winograd [LDM12], and Storycloze [MCH + 16]. Figure 4 reports the average results of BitNet and FP16 Transformer with various scales. Similar to the loss scaling curve, the performance on the downstream tasks can scale as the computation budget grows. Besides, the scaling efficiency of capabilities is much higher than the FP16 Transformer baseline, in terms of both zero-shot and few-shot performance.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.3 Results on Downstream Tasks",
        "chunkIndex": 29,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-30",
      "content": "The major challenge for training low-bit Transformers is the stability in optimization. Therefore, we perform stability tests for both BitNet and the FP16 baseline by training a series of models with varying peak learning rates. Figure 5a illustrates the results of the stability test. It shows that BitNet can converge with a large learning rate while FP16 Transformer can not, demonstrating better training stability of BitNet. This advantage in optimization enables the training with larger learning rates. Figure 5b shows that BitNet can benefit from the increase in learning rate, achieving better convergence in terms of PPL.\n\nFigure 4: Zero-shot (Left) and few-shot (Right) performance of BitNet and FP16 Transformer against the inference cost.\n\n<!-- image -->\n\nFigure 5: BitNet is more stable than FP16 Transformer with a same learning rate (Left). The training stability enables BitNet a larger learning rate, resulting in better convergence (Right).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "3.4 Stability Test",
        "chunkIndex": 30,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-31",
      "content": "We train BitNet with the same setup as described in Section 3.1. We compare BitNet with state-of-theart quantization methods, including Absmax [DLBZ22], SmoothQuant [XLS + 23], GPTQ [FAHA23], and QuIP [CCKS23]. These methods are post-training quantization over an FP16 Transformer model, which follows the same training setting and data as BitNet. Among them, Absmax and SmoothQuant quantize both the weights and the activations, while GPTQ and QuIP only reduce the precision of weights. We apply the methods to various quantization levels. For the weight-only quantization (i.e., GPTQ and QuIP), we experiment with W4A16 and W2A16. For weight-and-activation quantization (i.e., Absmax and SmoothQuant), we use them to quantize the FP16 Transformers to W8A8, W4A4, and W1A8. Our implementation of BitNet is binary weight 8-bit activation (W1A8), which has lower or equal bits than the baselines.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.1 Setup",
        "chunkIndex": 31,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-32",
      "content": "Table 3 presents a detailed comparative analysis of the zero-shot performance of our proposed method, BitNet, against various baseline approaches on four benchmark datasets, namely Winogrande, Winograd, Storycloze, and Hellaswag. All models have the model sizes of 6.7B for a fair comparison.\n\n�� Figure 6: Zero-shot (Left) and few-shot (Right) results for BitNet and the post-training quantization baselines on downstream tasks.\n\n<!-- image -->\n\n��\n\nTable 3: Zero-shot results for BitNet and the baselines ( PTQ : Post-training quantization, WGe : Winogrande, WG : Winograd, SC : Storycloze, and HS : Hellaswag dataset).\n\n| WBits   | ���� Methods            | PTQ   | ���� PPL ↓        | WG ↑            | WGe ↑          | HS ↑           | ���� SC ↑      | Avg ↑          |\n|---------|-------------------------|-------|-------------------|-----------------|----------------|----------------|----------------|----------------|\n| -       | Random                  | ✗     | -                 | �������",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 32,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-33",
      "content": "-----------|-------|-------------------|-----------------|----------------|----------------|----------------|----------------|\n| -       | Random                  | ✗     | -                 | ���������� 50.0 | 50.0           | 25.0           | 50.0           | 43.8           |\n| 16      | Transformer             | ✗     | 15.19             | 66.7            | 54.3           | 42.9           | 67.4           | 57.8           |\n| 8       | Absmax SmoothQuant      | ✓ ✓   | 21.43 15.67       | 60.4 65.3       | 52.0 53.1      | 38.3 40.9      | 62.7 67.6      | 53.4 56.7      |\n| 4       | GPTQ Absmax SmoothQuant | ✓ ✓ ✓ | 16.05 4.8e4 1.6e6 | 57.2 55.8 53.7  | 51.2 50.9 48.3 | 39.9 25.0 24.8 | 63.4 53.1 53.6 | 52.9 46.2 45.1 |\n| 2       | GPTQ QuIP               | ✓ ✓   | 1032 70.43        | 51.6 56.1       | 50.1 51.2      | 25.8 30.3      | 53.4 58.4      | 45.2 49.0      |\n| 1       | Absmax SmoothQuant      | ✓ ✓   | 3.5e23 3.3e21     | 49.8 50.5       | 50.0 49.5      | 24.8 24.6",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 33,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-34",
      "content": "51.6 56.1       | 50.1 51.2      | 25.8 30.3      | 53.4 58.4      | 45.2 49.0      |\n| 1       | Absmax SmoothQuant      | ✓ ✓   | 3.5e23 3.3e21     | 49.8 50.5       | 50.0 49.5      | 24.8 24.6      | 53.6 53.1      | 44.6 44.4      |\n| 1       | BitNet                  | ✗     | 17.07             |                 | 51.4           | 38.9           | 66.9           | 55.9           |\n|         |                         |       |                   | 66.3            |                |                |                |                |\n\nThe methods are evaluated across several weight bit levels, spanning from 16 down to 1. Besides the zero-shot accuracy on the downstream tasks, the evaluation metrics include language model perplexity on the validation set, which provides a comprehensive understanding of each method's performance.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 34,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-35",
      "content": "e zero-shot accuracy on the downstream tasks, the evaluation metrics include language model perplexity on the validation set, which provides a comprehensive understanding of each method's performance.\n\nThe results demonstrate the effectiveness of BitNet in achieving competitive performance levels compared to the baseline approaches, particularly for lower bit levels. The zero-shot scores of BitNet are comparable with the 8-bit models, while the inference cost is much lower. For the 4-bit models, the weight-only quantization methods outperform the weight-and-activation quantizers, mainly because the activation is more difficult to quantify. BitNet, as a 1-bit model, significantly achieves better results than both the weight-and-activation quantization methods and the weight-only methods. As for the lower-bit models, BitNet has consistently superior scores over all baselines.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 35,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-36",
      "content": "y achieves better results than both the weight-and-activation quantization methods and the weight-only methods. As for the lower-bit models, BitNet has consistently superior scores over all baselines. This proves the advantages of the quantization-aware training approaches over the post-training quantization methods. Figure 6 summarizes both the zero-shot accuracy and few-shot accuracy of our method and the baselines while scaling up the model size from 1.3B to 6.7B. It proves that the advantage is consistent across different scales.\n\nTable 4: Ablation of BitNet ( WGe : Winogrande, WG : Winograd, SC : Storycloze, and HS : Hellaswag dataset). Elastic is an activation quantization method from [LOP + 22], while BMT is the architecture from [ZGC + 23] to stabilize the training of low-bit models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 36,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-37",
      "content": ", SC : Storycloze, and HS : Hellaswag dataset). Elastic is an activation quantization method from [LOP + 22], while BMT is the architecture from [ZGC + 23] to stabilize the training of low-bit models.\n\n| Methods            | PPL ↓   | HS ↑   | WGe ↑   | WG ↑   | SC ↑   | Avg ↑   |\n|--------------------|---------|--------|---------|--------|--------|---------|\n| Zero-Shot Learning |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.2   | 52.1    | 60.7   | 63.2   | 52.3    |\n| Elastic + Pre-LN   | 24.05   | 29.6   | 52.9    | 56.8   | 61.3   | 50.2    |\n| Absmax + Pre-LN    | 22.11   | 31.6   | 50.0    | 61.8   | 61.6   | 51.3    |\n| Absmax + BMT       | 22.98   | 31.2   | 52.1    | 60.4   | 62.7   | 51.6    |\n| Few-Shot Learning  |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.5   | 50.4    | 62.1   | 63.8   | 52.5    |\n| Elastic + Pre-LN   | 24.05   | 29.9   | 51.7    | 57.5   | 61.1   | 50.1",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 37,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-38",
      "content": "|         |        |        |         |\n| BitNet             | 20.34   | 33.5   | 50.4    | 62.1   | 63.8   | 52.5    |\n| Elastic + Pre-LN   | 24.05   | 29.9   | 51.7    | 57.5   | 61.1   | 50.1    |\n| Absmax + Pre-LN    | 22.11   | 31.4   | 51.9    | 63.9   | 61.6   | 52.2    |\n| Absmax + BMT       | 22.98   | 31.3   | 51.5    | 57.5   | 62.6   | 50.7    |",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "4.2 Results",
        "chunkIndex": 38,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-39",
      "content": "In Table 4, we present an ablation study of our compared with several alternative approaches. We ablate the effect of our choices in activation quantization approaches as well as the techniques to stabilize the model training. BitNet implement absmax to quantize the activation and use SubLN for training stability. One quantization alternative is the elastic function [LOP + 22], which dynamically adjusts the scales with learnable parameters. In our experiments, we find that absmax has better performance than the elastic function. Besides, the absmax function leads to more stable training, which enables a larger learning rate for BitNet. We further compare SubLN with the Pre-LN and the BMT architecture [ZGC + 23]. Pre-LN is the default architecture for GPT pertaining, while BMT has proven to improve the stability of binarized models. Our experiments show that SubLN outperforms both Pre-LN and BMT. Therefore, we choose absmax and SubLN as the implementation in BitNet.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "5 Ablation Studies",
        "chunkIndex": 39,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-40",
      "content": "We present BitNet, a novel 1-bit Transformer architecture for large language models. Our approach is designed to be scalable and stable, with the ability to handle large language models efficiently. The experimental results demonstrate that BitNet achieves competitive performance in terms of both perplexity and downstream task performance, while significantly reducing memory footprint and energy consumption compared to the baselines. Moreover, BitNet follows a scaling law similar to that of full-precision Transformers, indicating that it can be effectively scaled to even larger language models with potential benefits in terms of performance and efficiency. In the future, we would like to scale up BitNet in terms of model size and training steps. We are also interested in applying BitNet in other architectures (e.g., RetNet [SDH + 23]) for training large language models.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "6 Conclusion and Future Work",
        "chunkIndex": 40,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-41",
      "content": "- [ADF + 23] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, and et al. PaLM 2 technical report. CoRR , abs/2305.10403, 2023.\n- [BKH16] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , 2016.\n- [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR , abs/1308.3432, 2013.\n- [BMR + 20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, and et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 , 2020.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 41,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-42",
      "content": "Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, and et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 , 2020.\n\n- [BT19] Adrian Bulat and Georgios Tzimiropoulos. XNOR-Net++: improved binary neural networks. In BMVC 2019 , 2019.\n- [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large language models with guarantees. CoRR , abs/2307.13304, 2023.\n- [CND + 22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, and et al. PaLM: scaling language modeling with pathways. CoRR , abs/2204.02311, 2022.\n- [Dao23] Tri Dao. FlashAttention-2: faster attention with better parallelism and work partitioning.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 42,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-43",
      "content": "oam Shazeer, and et al. PaLM: scaling language modeling with pathways. CoRR , abs/2204.02311, 2022.\n- [Dao23] Tri Dao. FlashAttention-2: faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023.\n- [DFE + 22] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: fast and memory-efficient exact attention with io-awareness. In NeurIPS , 2022.\n- [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. CoRR , 2022.\n- [FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations , 2023.\n- [HBM + 22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Ka",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 43,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-44",
      "content": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR , abs/2203.15556, 2022.\n- [HKK + 20] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. CoRR , abs/2010.14701, 2020.\n- [Hor14] Mark Horowitz. 1.1 computing's energy problem (and what we can do about it).",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 44,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-45",
      "content": "rio Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. CoRR , abs/2010.14701, 2020.\n- [Hor14] Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014 , pages 10-14, 2014.\n- [KLZ + 23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. CoRR , 2023.\n- [KMH + 20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR , abs/2001.08361, 2020.\n- [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 45,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-46",
      "content": "ford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR , abs/2001.08361, 2020.\n- [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning . Citeseer, 2012.\n- [LKM23] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.\n- [LOP + 22] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar Mehdad. BiT: robustly binarized multi-distilled transformer. In NeurIPS , 2022.\n\n- [LSL + 21] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and KwangTing Cheng. How do adam and training strategies help bnns optimization.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 46,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-47",
      "content": "stilled transformer. In NeurIPS , 2022.\n\n- [LSL + 21] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and KwangTing Cheng. How do adam and training strategies help bnns optimization. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 6936-6946. PMLR, 2021.\n- [MCH + 16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James F. Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. CoRR , abs/1604.01696, 2016.\n- [Ope23] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.\n- [RORF16] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: imagenet classification using binary convolutional neural networks.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 47,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-48",
      "content": "nical report. CoRR , abs/2303.08774, 2023.\n- [RORF16] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: imagenet classification using binary convolutional neural networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV , Lecture Notes in Computer Science, 2016.\n- [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence , pages 8732-8740, 2020.\n- [SDH + 23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to Transformer for large language models, 2023.\n- [SPP + 19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: training multi-billion parameter language models using model parallelism.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 48,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-49",
      "content": "- [SPP + 19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: training multi-billion parameter language models using model parallelism. CoRR , abs/1909.08053, 2019.\n- [TLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and efficient foundation language models. CoRR , abs/2302.13971, 2023.\n- [TMS + 23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. CoRR , abs/2307.09288, 2023.\n- [WH20] Yuxin Wu and Kaiming He. Group normalization. Int.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 49,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-50",
      "content": "urull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. CoRR , abs/2307.09288, 2023.\n- [WH20] Yuxin Wu and Kaiming He. Group normalization. Int. J. Comput. Vis. , 128(3):742-755, 2020.\n- [WMH + 22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR , 2022.\n- [XLS + 23] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.\n- [ZGC + 23] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 50,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-51",
      "content": "2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.\n- [ZGC + 23] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. CoRR , 2023.\n- [ZHB + 19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: can a machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics , pages 4791-4800, 2019.\n- [ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight accuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12465-12475. IEEE, 2022.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "References",
        "chunkIndex": 51,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-52",
      "content": "| Params   |   # Hidden |   # Layers |   # Heads |   Learning Rate |\n|----------|------------|------------|-----------|-----------------|\n| 125M     |        768 |         12 |        12 |         0.0024  |\n| 350M     |       1024 |         24 |        16 |         0.0012  |\n| 760M     |       1536 |         24 |        16 |         0.001   |\n| 1.3B     |       2048 |         24 |        32 |         0.0008  |\n| 2.7B     |       2560 |         32 |        32 |         0.00064 |\n| 6.7B     |       4096 |         32 |        32 |         0.00048 |\n| 13B      |       5120 |         40 |        40 |         0.0004  |\n| 30B      |       7168 |         48 |        56 |         0.0004  |\n\nTable 5: Model configuration for BitNet in the scaling experiments.\n\nTable 6: Hyperparameters for BitNet and the FP16 Transformers in the scaling experiments. For 13B and 30B model, we set weight decay to 0.05 for training stability.",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "A Hyperparameters",
        "chunkIndex": 52,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-53",
      "content": "BitNet in the scaling experiments.\n\nTable 6: Hyperparameters for BitNet and the FP16 Transformers in the scaling experiments. For 13B and 30B model, we set weight decay to 0.05 for training stability.\n\n| Hyperparameters                                                                 | Value                                     |\n|---------------------------------------------------------------------------------|-------------------------------------------|\n| Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                        | ✗ ✗ ✗ 0.01                                |\n\nTable 7: Hyperparameters for the stability test of BitNet and FP16 Transformer.\n\n| Hyperparameters                                                                   | Value                                      |\n|------------------------------------------------------------------------",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "A Hyperparameters",
        "chunkIndex": 53,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-54",
      "content": "perparameters                                                                   | Value                                      |\n|-----------------------------------------------------------------------------------|--------------------------------------------|\n| Peak learning rate Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-3 128K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                          | ✗ ✗ ✗ 0.01                                 |\n\nTable 8: Hyperparameters for the ablations of BitNet.\n\n| Hyperparameters                                                                                    | Elastic Absmax                                      |\n|----------------------------------------------------------------------------------------------------|-----------------------------------------------------|\n| Peak learning rate Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-4 8e",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "A Hyperparameters",
        "chunkIndex": 54,
        "totalChunks": 56
      }
    },
    {
      "id": "2310.11453v1-chunk-55",
      "content": "----------------------------------|-----------------------------------------------------|\n| Peak learning rate Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-4 8e-4 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                                           | ✗ ✗ ✗ 0.01                                          |",
      "metadata": {
        "source": "arxiv:2310.11453v1",
        "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
        "authors": [
          "Hongyu Wang",
          "Shuming Ma",
          "Li Dong",
          "Shaohan Huang",
          "Huaijie Wang",
          "Lingxiao Ma",
          "Fan Yang",
          "Ruiping Wang",
          "Yi Wu",
          "Furu Wei"
        ],
        "section": "A Hyperparameters",
        "chunkIndex": 55,
        "totalChunks": 56
      }
    }
  ],
  "fullText": "## BitNet: Scaling 1-bit Transformers for Large Language Models\n\nHongyu Wang ∗†‡ Shuming Ma ∗† Li Dong † Shaohan Huang † Huaijie Wang § Lingxiao Ma † Fan Yang † Ruiping Wang ‡ Yi Wu § Furu Wei †⋄ † Microsoft Research ‡ University of Chinese Academy of Sciences § Tsinghua University https://aka.ms/GeneralAI\n\n## Abstract\n\nThe increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.\n\nFigure 1: BitNet trains 1-bit Transformers from scratch, obtaining competitive results in an energyefficient way. BitNet significantly outperforms state-of-the-art quantization methods. As the model size scales up, the cost savings become more significant while achieving competitive performance with the models trained with FP16.\n\n<!-- image -->\n\n∗ Equal contribution. ⋄ Corresponding author.\n\n<!-- image -->\n\n'\n\n## 1 Introduction\n\nThe rapid growth of large language models [BMR + 20, Ope23, CND + 22, ADF + 23, TLI + 23, TMS + 23] has led to significant improvements in various tasks. However, it is expensive to host large language models due to the high inference costs and energy consumption. As the size of these models grows, the memory bandwidth required for accessing and processing the model parameters becomes a major bottleneck, limiting the overall inference performance. Moreover, when deploying these models on distributed systems or multi-device platforms, the inter-device communication overhead can significantly impact the inference latency and energy consumption. Model quantization [FAHA23, CCKS23, XLS + 23] has emerged as a promising solution, as it can significantly reduce the memory footprint and computational cost of large-scale models while maintaining competitive performance.\n\nMost existing quantization approaches for large language models are post-training. They are simple and easy to apply since it does not require any changes to the training pipeline or retraining the model. However, it will result in a more significant loss of accuracy especially when the precision goes lower, because the model is not optimized for the quantized representation during training.\n\nAnother strand of quantizing deep neural networks is quantization-aware training. Compared to post-training, it typically results in better accuracy, as the model is trained to account for the reduced precision from the beginning. Moreover, it allows the model to continue-train or do fine-tuning, which is essential for large language models. The challenge of quantization-aware training mainly lies in optimization, i.e., the model becomes more difficult to converge as the precision goes lower. Besides, it is unknown whether quantization-aware training follows the scaling law of neural language models.\n\nIn this work, we focus on binarization (i.e., 1-bit), which is the extreme case of quantization, applied to large language models. Previous studies on binarized neural networks [RORF16, BT19] have mostly revolved around convolutional neural networks. Recently, there has been some research on binarized Transformers. However, these studies have focused on machine translation or BERT pretraining, which is quite different from large language models. For example, machine translation employs an encoder-decoder architecture, BERT pretraining utilizes a bidirectional encoder, and large language models use a unidirectional decoder. Furthermore, large language models are typically scaled up to a much larger model size, while BERT and machine translation models do not undergo such extensive scaling.\n\nTo the best of our knowledge, this work is the first to investigate quantization-aware training for 1-bit large language models. We propose BitNet, a 1-bit Transformer architecture for large language models, which aims to scale efficiently in terms of both memory and computation. BitNet employs low-precision binary weights and quantized activations, while maintaining high precision for the optimizer states and gradients during training. Our approach is designed to be scalable and stable, with the ability to handle large language models efficiently. The implementation of the BitNet architecture is quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch) in the Transformer. Furthermore, it complements other acceleration methods for large language models, such as PagedAttention [KLZ + 23], FlashAttention [DFE + 22, Dao23], and speculative decoding [LKM23].\n\nWe evaluate BitNet on a range of language modeling benchmarks, comparing with state-of-the-art quantization methods and FP16 Transformers. Experimental results demonstrate that BitNet achieves competitive performance in terms of both perplexity and downstream task accuracy. More importantly, BitNet significantly reduces memory footprint and energy consumption compared to the baselines. Furthermore, we show that BitNet follows a scaling law similar to that of full-precision Transformers, indicating that it can be effectively scaled to even larger language models with potential benefits in terms of performance and efficiency.\n\nWilliam Henry Gates III\n\n'\n\nFigure 2: (a) The computation flow of BitLinear . (b) The architecture of BitNet, consisting of the stacks of attentions and FFNs, where matrix multiplication is implemented as BitLinear .\n\n<!-- image -->\n\n## 2 BitNet\n\nAs shown in Figure 2, BitNet uses the same layout as Transformers, stacking blocks of self-attention and feed-forward networks. Compared with vanilla Transformer, BitNet uses BitLinear (Eq. 11) instead of conventional matrix multiplication, which employs binarized (i.e., 1-bit) model weights. We leave the other components high-precision, e.g., 8-bit in our experiments. We summarized the reasons as follows. First, the residual connections and the layer normalization contribute negligible computation costs to large language models. Second, the computation cost of QKV transformation is much smaller than the parametric projection as the model grows larger. Third, we preserve the precision for the input/output embedding because the language models have to use high-precision probabilities to perform sampling.\n\n## 2.1 BitLinear\n\nWe first binarize the weights to either +1 or -1 with the signum function. Following [LOP + 22], we centralize the weights to be zero-mean before binarization to increase the capacity within a limited numerical range. A scaling factor β is used after binarization to reduce the l 2 error between the real-valued and the binarized weights. The binarization of a weight W ∈ R n × m can be formulated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nWe further quantize the activations to b -bit precision. Following [DLBZ22], we use absmax quantization, which scales activations into the range [ -Q b , Q b ] ( Q b = 2 b -1 ) by multiplying with Q b and dividing by the absolute maximum of the input matrix:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere ϵ is a small floating-point number that prevents overflow when performing the clipping.\n\nFor the activations before the non-linear functions (e.g., ReLU), we scale them into the range [0 , Q b ] by subtracting the minimum of the inputs so that all values are non-negative:\n\n<!-- formula-not-decoded -->\n\nIn this work, we quantize the activation to 8-bit and leave lower precision in future work. Moreover, the quantization is performed per tensor during training while per token during inference for both stability and efficiency.\n\nWith the above quantization equations, the matrix multiplication can be written as:\n\n<!-- formula-not-decoded -->\n\nWe assume that the elements in W and x are mutually independent and share the same distribution, and W and x are independent of each other. Then the variance of the output y is estimated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor the full-precision computation, the variance of the output Var( y ) is at the scale of 1 with the standard initialization methods (e.g., Kaiming initialization or Xavier initialization), which has a great benefit to the training stability. To preserve the variance after quantization, we introduce a LayerNorm [BKH16] function before the activation quantization. In this way, the variance of the output y is then estimated as Var( y ) ≈ E [LN( ˜ x ) 2 ] = 1 , which has the same magnitude as the full-precision counterpart Var( y ) . In the context of Transformers, it has the exact implementation as SubLN [WMH + 22]. With SubLN and the quantization methods above, we have BitLinear , which is formulated as:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFigure 2 provides an illustration of the computation flow of BitLinear . After the SubLN operation, the activations are quantized with the absmax function. The matrix multiplication is performed between the 1-bit weights and the quantized activations. The output activations are rescaled with { β, γ } to dequantize them to the original precision.\n\nModel parallelism with Group Quantization and Normalization One essential technique to scale up large language models is model parallelism [SPP + 19], which partitions the matrix multiplication on multiple devices. A prerequisite for the existing model parallelism approaches is that the tensors are independent along the partition dimension. However, all of the parameters α , β , γ , and η are calculated from the whole tensors, breaking the independent prerequisite. One solution is to introduce one all-reduce operation for each parameter. However, even though the communication for each parameter is small, the amount of synchronization is growing as the model becomes deeper, which significantly slows the forward pass. The problem also exists in SubLN , where the mean and the variance should be estimated across the partition dimension.\n\nTo this end, we propose a simple approach that makes the model parallelism more efficient. We divide the weights and activations into groups and then independently estimate each group's parameters. This way, the parameters can be calculated locally without requiring additional communication. This approach, called Group Quantization, is formulated as follows:\n\nFor a weight matrix W ∈ R n × m , we divide it into G groups along the partition dimension, and each group has a size of n G × m . We then estimate the parameters for each group independently:\n\n<!-- formula-not-decoded -->\n\nTable 1: Energy consumption of BitNet and Transformer varying different model size. Results are reported with 512 as input length.\n\n| Models      | Size   | WBits   | 7nm Energy (J)   | 7nm Energy (J)   | 45nm Energy (J)   | 45nm Energy (J)   |\n|-------------|--------|---------|------------------|------------------|-------------------|-------------------|\n| Models      | Size   | WBits   | MUL              | ADD              | MUL               | ADD               |\n| Transformer | 6.7B   | 32 16   | 4.41 1.14        | 1.28 0.54        | 12.46 3.70        | 3.03 1.35         |\n| BitNet      | 6.7B   |         |                  |                  |                   |                   |\n|             | 6.7B   | 1       | 0.02             | 0.04             | 0.08              | 0.13              |\n| Transformer | 13B    | 32      | 8.58             | 2.49             | 24.23             | 5.89              |\n|             | 13B    | 16      | 2.23             | 1.05             | 7.20              | 2.62              |\n| BitNet      | 13B    | 1       | 0.04             | 0.06             | 0.12              | 0.24              |\n| Transformer | 30B    | 32      | 20.09            | 5.83             | 56.73             | 13.80             |\n|             | 30B    | 16      | 5.21             | 2.45             | 16.87             | 6.13              |\n| BitNet      | 30B    | 1       | 0.06             | 0.14             | 0.20              | 0.53              |\n\nwhere W ( g ) denotes the g -th group of the weight matrix. Similarly, for the activations, we can divide the input matrix x ∈ R n × m into G groups and calculate the parameters for each group:\n\n<!-- formula-not-decoded -->\n\nFor LN, we can apply the group normalization technique [WH20] to compute the mean and variance for each group independently:\n\n<!-- formula-not-decoded -->\n\nIn this way, we can efficiently implement model parallelism with Group Quantization and Normalization, which requires no additional communication and can scale to large language models.\n\n## 2.2 Model Training\n\nStraight-through estimator. To train our 1-bit model, we employ the straight-through estimator (STE)[BLC13] to approximate the gradient during backpropagation. This method bypasses the nondifferentiable functions, such as the Sign (Eq. 2) and Clip (Eq. 5) functions, during the backward pass. STE allows gradients to flow through the network without being affected by these non-differentiable functions, making it possible to train our quantized model.\n\nMixed precision training. While the weights and the activations are quantized to low precision, the gradients and the optimizer states are stored in high precision to ensure training stability and accuracy. Following the previous work [LSL + 21], we maintain a latent weight in a high-precision format for the learnable parameters to accumulate the parameter updates. The latent weights are binarized on the fly during the forward pass and never used for the inference process.\n\nLarge learning rate. One challenge for the optimization is that a small update on the latent weights often makes no difference in the 1-bit weights. This results in a biased gradient and update which are estimated based on the 1-bit weights. This problem is even worse at the beginning of the training, where the models are supposed to converge as fast as possible. To address this challenge, we explore various methods, concluding that increasing the learning rate is the simplest and best way to accelerate the optimization. Our experiments show that BitNet benefits from a large learning rate in terms of convergence, while the FP16 Transformer diverges at the beginning of training with the same learning rate. More details can be found in Section 3.\n\n## 2.3 Computational Efficiency\n\nWe estimate the computational efficiency of BitNet in terms of both arithmetic operations energy and memory footprint. We mainly focus on the calculation for the matrix multiplication, since it contributes the most to the cost of large language models.\n\nArithmetic operations energy. According to the energy model in [Hor14, ZZL22], the energy consumption for different arithmetic operations can be estimated as follows:\n\nTable 2: ADD and MUL energy consumption [Hor14, ZZL22] for different bit representations at 45nm and 7nm process nodes.\n\n| Bits   |   ADD Energy 45nm |   ˆ E add (pJ) 7nm |   MUL Energy 45nm |   ˆ E mul (pJ) 7nm |\n|--------|-------------------|--------------------|-------------------|--------------------|\n| FP32   |              0.9  |              0.38  |               3.7 |               1.31 |\n| FP16   |              0.4  |              0.16  |               1.1 |               0.34 |\n| INT8   |              0.03 |              0.007 |               0.2 |               0.07 |\n\nIn vanilla Transformers, for matrix multiplication with dimensions m × n and n × p , the energy consumption can be calculated as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor BitNet, the energy consumption of the matrix multiplication is dominated by the addition operations, as the weights are 1-bit. The multiplication operations are only applied to scale the output with the scalars β and γ Q b , so the energy consumption for multiplication can be computed as:\n\n<!-- formula-not-decoded -->\n\nwhich is significantly smaller than that in Transformers. The energy savings of W1A8 BitNet compared to a full-precision (32-32) and half-precision (16-16) Transformer are shown in Table 1. As can be seen, BitNet provides significant energy savings, especially for the multiplication operations, which are the major component of the matrix multiplication energy consumption.\n\n## 3 Comparison with FP16 Transformers\n\n## 3.1 Setup\n\nWe train a series of autoregressive language models with BitNet of various scales, ranging from 125M to 30B. The models are trained on an English-language corpus, which consists of the Pile dataset, Common Crawl snapshots, RealNews, and CC-Stories datasets. We use the Sentencpiece tokenizer to preprocess data and the vocabulary size is 16K. Besides BitNet, we also train the Transformer baselines with the same datasets and settings for a fair comparison. More details can be found in the appendix.\n\n## 3.2 Inference-Optimal Scaling Law\n\nNeural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term:\n\n<!-- formula-not-decoded -->\n\nFigure 3: Scaling curves of BitNet and FP16 Transformers.\n\n<!-- image -->\n\nTo evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.\n\n## 3.3 Results on Downstream Tasks\n\nIn addition to the loss, we are also concerned about the capabilities with the scaling of BitNet. Compared with the loss, the capacity is more difficult to predict due to the emergent nature of neural language models. To evaluate the capabilities with the interpretable metrics, we test both the 0-shot and 4-shot results on four downstream tasks, including Hellaswag [ZHB + 19], Winogrande [SBBC20], Winograd [LDM12], and Storycloze [MCH + 16]. Figure 4 reports the average results of BitNet and FP16 Transformer with various scales. Similar to the loss scaling curve, the performance on the downstream tasks can scale as the computation budget grows. Besides, the scaling efficiency of capabilities is much higher than the FP16 Transformer baseline, in terms of both zero-shot and few-shot performance.\n\n## 3.4 Stability Test\n\nThe major challenge for training low-bit Transformers is the stability in optimization. Therefore, we perform stability tests for both BitNet and the FP16 baseline by training a series of models with varying peak learning rates. Figure 5a illustrates the results of the stability test. It shows that BitNet can converge with a large learning rate while FP16 Transformer can not, demonstrating better training stability of BitNet. This advantage in optimization enables the training with larger learning rates. Figure 5b shows that BitNet can benefit from the increase in learning rate, achieving better convergence in terms of PPL.\n\nFigure 4: Zero-shot (Left) and few-shot (Right) performance of BitNet and FP16 Transformer against the inference cost.\n\n<!-- image -->\n\nFigure 5: BitNet is more stable than FP16 Transformer with a same learning rate (Left). The training stability enables BitNet a larger learning rate, resulting in better convergence (Right).\n\n<!-- image -->\n\n## 4 Comparison with Post-training Quantization\n\n## 4.1 Setup\n\nWe train BitNet with the same setup as described in Section 3.1. We compare BitNet with state-of-theart quantization methods, including Absmax [DLBZ22], SmoothQuant [XLS + 23], GPTQ [FAHA23], and QuIP [CCKS23]. These methods are post-training quantization over an FP16 Transformer model, which follows the same training setting and data as BitNet. Among them, Absmax and SmoothQuant quantize both the weights and the activations, while GPTQ and QuIP only reduce the precision of weights. We apply the methods to various quantization levels. For the weight-only quantization (i.e., GPTQ and QuIP), we experiment with W4A16 and W2A16. For weight-and-activation quantization (i.e., Absmax and SmoothQuant), we use them to quantize the FP16 Transformers to W8A8, W4A4, and W1A8. Our implementation of BitNet is binary weight 8-bit activation (W1A8), which has lower or equal bits than the baselines.\n\n## 4.2 Results\n\nTable 3 presents a detailed comparative analysis of the zero-shot performance of our proposed method, BitNet, against various baseline approaches on four benchmark datasets, namely Winogrande, Winograd, Storycloze, and Hellaswag. All models have the model sizes of 6.7B for a fair comparison.\n\n�� Figure 6: Zero-shot (Left) and few-shot (Right) results for BitNet and the post-training quantization baselines on downstream tasks.\n\n<!-- image -->\n\n��\n\nTable 3: Zero-shot results for BitNet and the baselines ( PTQ : Post-training quantization, WGe : Winogrande, WG : Winograd, SC : Storycloze, and HS : Hellaswag dataset).\n\n| WBits   | ���� Methods            | PTQ   | ���� PPL ↓        | WG ↑            | WGe ↑          | HS ↑           | ���� SC ↑      | Avg ↑          |\n|---------|-------------------------|-------|-------------------|-----------------|----------------|----------------|----------------|----------------|\n| -       | Random                  | ✗     | -                 | ���������� 50.0 | 50.0           | 25.0           | 50.0           | 43.8           |\n| 16      | Transformer             | ✗     | 15.19             | 66.7            | 54.3           | 42.9           | 67.4           | 57.8           |\n| 8       | Absmax SmoothQuant      | ✓ ✓   | 21.43 15.67       | 60.4 65.3       | 52.0 53.1      | 38.3 40.9      | 62.7 67.6      | 53.4 56.7      |\n| 4       | GPTQ Absmax SmoothQuant | ✓ ✓ ✓ | 16.05 4.8e4 1.6e6 | 57.2 55.8 53.7  | 51.2 50.9 48.3 | 39.9 25.0 24.8 | 63.4 53.1 53.6 | 52.9 46.2 45.1 |\n| 2       | GPTQ QuIP               | ✓ ✓   | 1032 70.43        | 51.6 56.1       | 50.1 51.2      | 25.8 30.3      | 53.4 58.4      | 45.2 49.0      |\n| 1       | Absmax SmoothQuant      | ✓ ✓   | 3.5e23 3.3e21     | 49.8 50.5       | 50.0 49.5      | 24.8 24.6      | 53.6 53.1      | 44.6 44.4      |\n| 1       | BitNet                  | ✗     | 17.07             |                 | 51.4           | 38.9           | 66.9           | 55.9           |\n|         |                         |       |                   | 66.3            |                |                |                |                |\n\nThe methods are evaluated across several weight bit levels, spanning from 16 down to 1. Besides the zero-shot accuracy on the downstream tasks, the evaluation metrics include language model perplexity on the validation set, which provides a comprehensive understanding of each method's performance.\n\nThe results demonstrate the effectiveness of BitNet in achieving competitive performance levels compared to the baseline approaches, particularly for lower bit levels. The zero-shot scores of BitNet are comparable with the 8-bit models, while the inference cost is much lower. For the 4-bit models, the weight-only quantization methods outperform the weight-and-activation quantizers, mainly because the activation is more difficult to quantify. BitNet, as a 1-bit model, significantly achieves better results than both the weight-and-activation quantization methods and the weight-only methods. As for the lower-bit models, BitNet has consistently superior scores over all baselines. This proves the advantages of the quantization-aware training approaches over the post-training quantization methods. Figure 6 summarizes both the zero-shot accuracy and few-shot accuracy of our method and the baselines while scaling up the model size from 1.3B to 6.7B. It proves that the advantage is consistent across different scales.\n\nTable 4: Ablation of BitNet ( WGe : Winogrande, WG : Winograd, SC : Storycloze, and HS : Hellaswag dataset). Elastic is an activation quantization method from [LOP + 22], while BMT is the architecture from [ZGC + 23] to stabilize the training of low-bit models.\n\n| Methods            | PPL ↓   | HS ↑   | WGe ↑   | WG ↑   | SC ↑   | Avg ↑   |\n|--------------------|---------|--------|---------|--------|--------|---------|\n| Zero-Shot Learning |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.2   | 52.1    | 60.7   | 63.2   | 52.3    |\n| Elastic + Pre-LN   | 24.05   | 29.6   | 52.9    | 56.8   | 61.3   | 50.2    |\n| Absmax + Pre-LN    | 22.11   | 31.6   | 50.0    | 61.8   | 61.6   | 51.3    |\n| Absmax + BMT       | 22.98   | 31.2   | 52.1    | 60.4   | 62.7   | 51.6    |\n| Few-Shot Learning  |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.5   | 50.4    | 62.1   | 63.8   | 52.5    |\n| Elastic + Pre-LN   | 24.05   | 29.9   | 51.7    | 57.5   | 61.1   | 50.1    |\n| Absmax + Pre-LN    | 22.11   | 31.4   | 51.9    | 63.9   | 61.6   | 52.2    |\n| Absmax + BMT       | 22.98   | 31.3   | 51.5    | 57.5   | 62.6   | 50.7    |\n\n## 5 Ablation Studies\n\nIn Table 4, we present an ablation study of our compared with several alternative approaches. We ablate the effect of our choices in activation quantization approaches as well as the techniques to stabilize the model training. BitNet implement absmax to quantize the activation and use SubLN for training stability. One quantization alternative is the elastic function [LOP + 22], which dynamically adjusts the scales with learnable parameters. In our experiments, we find that absmax has better performance than the elastic function. Besides, the absmax function leads to more stable training, which enables a larger learning rate for BitNet. We further compare SubLN with the Pre-LN and the BMT architecture [ZGC + 23]. Pre-LN is the default architecture for GPT pertaining, while BMT has proven to improve the stability of binarized models. Our experiments show that SubLN outperforms both Pre-LN and BMT. Therefore, we choose absmax and SubLN as the implementation in BitNet.\n\n## 6 Conclusion and Future Work\n\nWe present BitNet, a novel 1-bit Transformer architecture for large language models. Our approach is designed to be scalable and stable, with the ability to handle large language models efficiently. The experimental results demonstrate that BitNet achieves competitive performance in terms of both perplexity and downstream task performance, while significantly reducing memory footprint and energy consumption compared to the baselines. Moreover, BitNet follows a scaling law similar to that of full-precision Transformers, indicating that it can be effectively scaled to even larger language models with potential benefits in terms of performance and efficiency. In the future, we would like to scale up BitNet in terms of model size and training steps. We are also interested in applying BitNet in other architectures (e.g., RetNet [SDH + 23]) for training large language models.\n\n## References\n\n- [ADF + 23] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, and et al. PaLM 2 technical report. CoRR , abs/2305.10403, 2023.\n- [BKH16] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , 2016.\n- [BLC13] Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR , abs/1308.3432, 2013.\n- [BMR + 20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, and et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33 , 2020.\n\n- [BT19] Adrian Bulat and Georgios Tzimiropoulos. XNOR-Net++: improved binary neural networks. In BMVC 2019 , 2019.\n- [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large language models with guarantees. CoRR , abs/2307.13304, 2023.\n- [CND + 22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, and et al. PaLM: scaling language modeling with pathways. CoRR , abs/2204.02311, 2022.\n- [Dao23] Tri Dao. FlashAttention-2: faster attention with better parallelism and work partitioning. CoRR , abs/2307.08691, 2023.\n- [DFE + 22] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: fast and memory-efficient exact attention with io-awareness. In NeurIPS , 2022.\n- [DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. CoRR , 2022.\n- [FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations , 2023.\n- [HBM + 22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. CoRR , abs/2203.15556, 2022.\n- [HKK + 20] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. CoRR , abs/2010.14701, 2020.\n- [Hor14] Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014 , pages 10-14, 2014.\n- [KLZ + 23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. CoRR , 2023.\n- [KMH + 20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR , abs/2001.08361, 2020.\n- [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning . Citeseer, 2012.\n- [LKM23] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.\n- [LOP + 22] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar Mehdad. BiT: robustly binarized multi-distilled transformer. In NeurIPS , 2022.\n\n- [LSL + 21] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and KwangTing Cheng. How do adam and training strategies help bnns optimization. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , volume 139 of Proceedings of Machine Learning Research , pages 6936-6946. PMLR, 2021.\n- [MCH + 16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James F. Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. CoRR , abs/1604.01696, 2016.\n- [Ope23] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.\n- [RORF16] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: imagenet classification using binary convolutional neural networks. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV , Lecture Notes in Computer Science, 2016.\n- [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence , pages 8732-8740, 2020.\n- [SDH + 23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to Transformer for large language models, 2023.\n- [SPP + 19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: training multi-billion parameter language models using model parallelism. CoRR , abs/1909.08053, 2019.\n- [TLI + 23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and efficient foundation language models. CoRR , abs/2302.13971, 2023.\n- [TMS + 23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. CoRR , abs/2307.09288, 2023.\n- [WH20] Yuxin Wu and Kaiming He. Group normalization. Int. J. Comput. Vis. , 128(3):742-755, 2020.\n- [WMH + 22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR , 2022.\n- [XLS + 23] Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.\n- [ZGC + 23] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. CoRR , 2023.\n- [ZHB + 19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: can a machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics , pages 4791-4800, 2019.\n- [ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight accuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12465-12475. IEEE, 2022.\n\n## A Hyperparameters\n\n| Params   |   # Hidden |   # Layers |   # Heads |   Learning Rate |\n|----------|------------|------------|-----------|-----------------|\n| 125M     |        768 |         12 |        12 |         0.0024  |\n| 350M     |       1024 |         24 |        16 |         0.0012  |\n| 760M     |       1536 |         24 |        16 |         0.001   |\n| 1.3B     |       2048 |         24 |        32 |         0.0008  |\n| 2.7B     |       2560 |         32 |        32 |         0.00064 |\n| 6.7B     |       4096 |         32 |        32 |         0.00048 |\n| 13B      |       5120 |         40 |        40 |         0.0004  |\n| 30B      |       7168 |         48 |        56 |         0.0004  |\n\nTable 5: Model configuration for BitNet in the scaling experiments.\n\nTable 6: Hyperparameters for BitNet and the FP16 Transformers in the scaling experiments. For 13B and 30B model, we set weight decay to 0.05 for training stability.\n\n| Hyperparameters                                                                 | Value                                     |\n|---------------------------------------------------------------------------------|-------------------------------------------|\n| Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                        | ✗ ✗ ✗ 0.01                                |\n\nTable 7: Hyperparameters for the stability test of BitNet and FP16 Transformer.\n\n| Hyperparameters                                                                   | Value                                      |\n|-----------------------------------------------------------------------------------|--------------------------------------------|\n| Peak learning rate Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-3 128K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                          | ✗ ✗ ✗ 0.01                                 |\n\nTable 8: Hyperparameters for the ablations of BitNet.\n\n| Hyperparameters                                                                                    | Elastic Absmax                                      |\n|----------------------------------------------------------------------------------------------------|-----------------------------------------------------|\n| Peak learning rate Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-4 8e-4 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                                           | ✗ ✗ ✗ 0.01                                          |",
  "tables": [
    {
      "index": 0,
      "markdown": "| Models      | Size   | WBits   | 7nm Energy (J)   | 7nm Energy (J)   | 45nm Energy (J)   | 45nm Energy (J)   |\n|-------------|--------|---------|------------------|------------------|-------------------|-------------------|\n| Models      | Size   | WBits   | MUL              | ADD              | MUL               | ADD               |\n| Transformer | 6.7B   | 32 16   | 4.41 1.14        | 1.28 0.54        | 12.46 3.70        | 3.03 1.35         |\n| BitNet      | 6.7B   |         |                  |                  |                   |                   |\n|             | 6.7B   | 1       | 0.02             | 0.04             | 0.08              | 0.13              |\n| Transformer | 13B    | 32      | 8.58             | 2.49             | 24.23             | 5.89              |\n|             | 13B    | 16      | 2.23             | 1.05             | 7.20              | 2.62              |\n| BitNet      | 13B    | 1       | 0.04             | 0.06             | 0.12              | 0.24              |\n| Transformer | 30B    | 32      | 20.09            | 5.83             | 56.73             | 13.80             |\n|             | 30B    | 16      | 5.21             | 2.45             | 16.87             | 6.13              |\n| BitNet      | 30B    | 1       | 0.06             | 0.14             | 0.20              | 0.53              |"
    },
    {
      "index": 1,
      "markdown": "| Bits   |   ADD Energy 45nm |   ˆ E add (pJ) 7nm |   MUL Energy 45nm |   ˆ E mul (pJ) 7nm |\n|--------|-------------------|--------------------|-------------------|--------------------|\n| FP32   |              0.9  |              0.38  |               3.7 |               1.31 |\n| FP16   |              0.4  |              0.16  |               1.1 |               0.34 |\n| INT8   |              0.03 |              0.007 |               0.2 |               0.07 |"
    },
    {
      "index": 2,
      "markdown": "| WBits   | ���� Methods            | PTQ   | ���� PPL ↓        | WG ↑            | WGe ↑          | HS ↑           | ���� SC ↑      | Avg ↑          |\n|---------|-------------------------|-------|-------------------|-----------------|----------------|----------------|----------------|----------------|\n| -       | Random                  | ✗     | -                 | ���������� 50.0 | 50.0           | 25.0           | 50.0           | 43.8           |\n| 16      | Transformer             | ✗     | 15.19             | 66.7            | 54.3           | 42.9           | 67.4           | 57.8           |\n| 8       | Absmax SmoothQuant      | ✓ ✓   | 21.43 15.67       | 60.4 65.3       | 52.0 53.1      | 38.3 40.9      | 62.7 67.6      | 53.4 56.7      |\n| 4       | GPTQ Absmax SmoothQuant | ✓ ✓ ✓ | 16.05 4.8e4 1.6e6 | 57.2 55.8 53.7  | 51.2 50.9 48.3 | 39.9 25.0 24.8 | 63.4 53.1 53.6 | 52.9 46.2 45.1 |\n| 2       | GPTQ QuIP               | ✓ ✓   | 1032 70.43        | 51.6 56.1       | 50.1 51.2      | 25.8 30.3      | 53.4 58.4      | 45.2 49.0      |\n| 1       | Absmax SmoothQuant      | ✓ ✓   | 3.5e23 3.3e21     | 49.8 50.5       | 50.0 49.5      | 24.8 24.6      | 53.6 53.1      | 44.6 44.4      |\n| 1       | BitNet                  | ✗     | 17.07             |                 | 51.4           | 38.9           | 66.9           | 55.9           |\n|         |                         |       |                   | 66.3            |                |                |                |                |"
    },
    {
      "index": 3,
      "markdown": "| Methods            | PPL ↓   | HS ↑   | WGe ↑   | WG ↑   | SC ↑   | Avg ↑   |\n|--------------------|---------|--------|---------|--------|--------|---------|\n| Zero-Shot Learning |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.2   | 52.1    | 60.7   | 63.2   | 52.3    |\n| Elastic + Pre-LN   | 24.05   | 29.6   | 52.9    | 56.8   | 61.3   | 50.2    |\n| Absmax + Pre-LN    | 22.11   | 31.6   | 50.0    | 61.8   | 61.6   | 51.3    |\n| Absmax + BMT       | 22.98   | 31.2   | 52.1    | 60.4   | 62.7   | 51.6    |\n| Few-Shot Learning  |         |        |         |        |        |         |\n| BitNet             | 20.34   | 33.5   | 50.4    | 62.1   | 63.8   | 52.5    |\n| Elastic + Pre-LN   | 24.05   | 29.9   | 51.7    | 57.5   | 61.1   | 50.1    |\n| Absmax + Pre-LN    | 22.11   | 31.4   | 51.9    | 63.9   | 61.6   | 52.2    |\n| Absmax + BMT       | 22.98   | 31.3   | 51.5    | 57.5   | 62.6   | 50.7    |"
    },
    {
      "index": 4,
      "markdown": "| Params   |   # Hidden |   # Layers |   # Heads |   Learning Rate |\n|----------|------------|------------|-----------|-----------------|\n| 125M     |        768 |         12 |        12 |         0.0024  |\n| 350M     |       1024 |         24 |        16 |         0.0012  |\n| 760M     |       1536 |         24 |        16 |         0.001   |\n| 1.3B     |       2048 |         24 |        32 |         0.0008  |\n| 2.7B     |       2560 |         32 |        32 |         0.00064 |\n| 6.7B     |       4096 |         32 |        32 |         0.00048 |\n| 13B      |       5120 |         40 |        40 |         0.0004  |\n| 30B      |       7168 |         48 |        56 |         0.0004  |"
    },
    {
      "index": 5,
      "markdown": "| Hyperparameters                                                                 | Value                                     |\n|---------------------------------------------------------------------------------|-------------------------------------------|\n| Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                        | ✗ ✗ ✗ 0.01                                |"
    },
    {
      "index": 6,
      "markdown": "| Hyperparameters                                                                   | Value                                      |\n|-----------------------------------------------------------------------------------|--------------------------------------------|\n| Peak learning rate Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-3 128K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                          | ✗ ✗ ✗ 0.01                                 |"
    },
    {
      "index": 7,
      "markdown": "| Hyperparameters                                                                                    | Elastic Absmax                                      |\n|----------------------------------------------------------------------------------------------------|-----------------------------------------------------|\n| Peak learning rate Training updates Tokens per sample Adam β Learning rate schedule Warmup updates | 1e-4 8e-4 40K 256K (0.9, 0.98) Polynomial decay 750 |\n| Gradient clipping Dropout Attention dropout Weight decay                                           | ✗ ✗ ✗ 0.01                                          |"
    }
  ],
  "stats": {
    "pages": 14,
    "chunksCreated": 56,
    "totalCharacters": 40100,
    "totalWords": 5904,
    "numTables": 8,
    "processingTimeMs": 19782
  }
}