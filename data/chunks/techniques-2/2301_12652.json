{
  "paper": {
    "id": "2301.12652v4",
    "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
    "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",
    "authors": [
      "Weijia Shi",
      "Sewon Min",
      "Michihiro Yasunaga",
      "Minjoon Seo",
      "Rich James",
      "Mike Lewis",
      "Luke Zettlemoyer",
      "Wen-tau Yih"
    ],
    "published": "2023-01-30T04:18:09.000Z",
    "updated": "2023-05-24T05:08:07.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2301.12652v4",
    "absUrl": "https://arxiv.org/abs/2301.12652v4"
  },
  "chunks": [
    {
      "id": "2301.12652v4-chunk-0",
      "content": "Weijia Shi, 1 * Sewon Min, 1 Michihiro Yasunaga, 2 Minjoon Seo, 3 Rich James, 4 Mike Lewis, 4 Luke Zettlemoyer 1 4 Wen-tau Yih 4",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "chunkIndex": 0,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-1",
      "content": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-2",
      "content": "Large language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022;",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 2,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-3",
      "content": "ng hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022;\n\n1 University of Washington 2 Stanford University 3 KAIST 4 Meta AI. Work done while the first author was interning at Meta AI.\n\n* Correspondence to: Weijia Shi &lt;swj0419@uw.edu&gt;.\n\nFigure 1. Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., &gt;100B parameters), which are often served via APIs.\n\n<!-- image -->\n\nIzacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 3,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-4",
      "content": "100B parameters), which are often served via APIs.\n\n<!-- image -->\n\nIzacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.\n\nIn this work, we introduce REPLUG ( Re trieve and Plug ), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 4,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-5",
      "content": "elevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy. As shown in\n\nFigure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.\n\nWe also introduce REPLUG LSR (REPLUG with L MS upervised R etrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 5,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-6",
      "content": "sion signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\n\nOur experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) leads to additional improvements, including up to 6.3% increase in GPT-3 175B language modeling.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 6,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-7",
      "content": "tion-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) leads to additional improvements, including up to 6.3% increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (&gt;100B model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:\n\n- We introduce REPLUG (§3), the first retrievalaugmented language modeling framework for enhancing large black-box language models with retrieval.\n- We propose a training scheme (§4) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.\n- Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 7,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-8",
      "content": "- Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "1. Introduction",
        "chunkIndex": 8,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-9",
      "content": "Black-box Language Models Large language models (i.e., &gt;100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant computational resources to run and finetune locally. For example, finetuning BLOOM-176B requires 72 A100 GPUs (80GB memory, $15k each (Younes Belkda, 2022)), making them inaccessible to researchers and developers with limited resources. Traditionally, retrieval-augmented model frameworks (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have focused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 9,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-10",
      "content": "t al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have focused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents. However, the increasing scale and black-box nature of large language models makes this approach infeasible. To address the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot access or modify its parameters.\n\nRetrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022).",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 10,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-11",
      "content": "P tasks, including language modeling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 11,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-12",
      "content": "riever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020; Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 12,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-13",
      "content": "epresentations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval. While concurrent work (Mallen et al., 2022; Si et al., 2023; Yu et al., 2023; Khattab et al., 2022) has demonstrated that using a frozen retriever can improve GPT3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also propose an ensemble method to incorporate more documents\n\nFigure 2. REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ).\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 13,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-14",
      "content": "triever (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ).\n\n<!-- image -->\n\nand a training scheme to further adapt the retriever to large LMs.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "2. Background and Related Work",
        "chunkIndex": 14,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-15",
      "content": "We introduce REPLUG ( Re trieve and Plug ), a new retrievalaugmented LM paradigm where the language model is treated as black box and the retrieval component is added as a potentially tuneable module.\n\nAs shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities (§3.2).",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "3. REPLUG",
        "chunkIndex": 15,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-16",
      "content": "Given an input context x , the retriever aims to retrieve a small set of documents from a corpus D = { d 1 ...d m } that are relevant to x . Following prior work (Qu et al., 2021; Izacard &amp; Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context x and the document d . Specifically, the encoder maps each document d ∈ D to an embedding E ( d ) by taking the mean pooling of the last hidden representation over the tokens in d . At query time, the same encoder is applied to the input context x to obtain a query embedding E ( x ) . The similarity between the query embedding and the document embedding is computed by their cosine similarity:\n\n<!-- formula-not-decoded -->\n\nThe topk documents that have the highest similarity scores when compared with the input x are retrieved in this step.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "3.1. Document Retrieval",
        "chunkIndex": 16,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-17",
      "content": "t embedding is computed by their cosine similarity:\n\n<!-- formula-not-decoded -->\n\nThe topk documents that have the highest similarity scores when compared with the input x are retrieved in this step.\n\nFor efficient retrieval, we precompute the embedding of each document d ∈ D and construct FAISS index (Johnson et al., 2019) over these embeddings.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "3.1. Document Retrieval",
        "chunkIndex": 17,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-18",
      "content": "The retrieved topk documents provide rich information about the original input context x and can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend x with all k documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., k ) we can include, given the language model's context window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume D ′ ⊂ D consists of k most relevant documents to x , according to the scoring function in Eq. (1). We prepend each document d ∈ D ′ to x , pass this concatenation to the LM separately, and then ensemble output probabilities from all k passes. Formally, given the input context x and its topk relevant documents D ′ , the output probability of the next token y is computed as a weighted average ensemble:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "3.2. Input Reformulation",
        "chunkIndex": 18,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-19",
      "content": "k passes. Formally, given the input context x and its topk relevant documents D ′ , the output probability of the next token y is computed as a weighted average ensemble:\n\n<!-- formula-not-decoded -->\n\nwhere ◦ denotes the concatenation of two sequences and the weight λ ( d, x ) is based on the similarity score between the document d and the input context x :\n\n<!-- formula-not-decoded -->\n\nAlthough our ensemble method requires running the LM k times, the cross attention is performed between each retrieved document and the input context. Therefore, compared with the method of prepending all the retrieved docu-\n\nments, our ensemble methods do not incur additional computational cost overhead.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "3.2. Input Reformulation",
        "chunkIndex": 19,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-20",
      "content": "Instead of relying only on existing neural dense retrieval models (Karpukhin et al., 2020a; Izacard et al., 2022a; Su et al., 2022), we further propose REPLUG LSR (REPLUG with LM-Supervised Retrieval), which adapts the retriever in REPLUG by using the LM itself to provide supervision about which documents should be retrieved.\n\nInspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model. In other words, we would like the retriever to find documents that result in lower perplexity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (§4.1), (2) scoring the retrieved documents by the language model (§4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (§4.3), and (4",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4. REPLUG LSR: Training the Dense Retriever",
        "chunkIndex": 20,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-21",
      "content": "eved documents by the language model (§4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (§4.3), and (4) asynchronous update of the datastore index (§4.4).",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4. REPLUG LSR: Training the Dense Retriever",
        "chunkIndex": 21,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-22",
      "content": "We retrieve k documents D ′ ⊂ D with the highest similarity scores from a corpus D given an input context x , as described in §3.1. We then compute the retrieval likelihood of each retrieved document d :\n\n<!-- formula-not-decoded -->\n\nwhere γ is a hyperparameter that controls the temerature of the softmax. Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpus D , which is intractable in practice. Therefore, we approximate the retrieval likelihood by only marginalizing over the retrieved documents D ′ .",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4.1. Computing Retrieval Likelihood",
        "chunkIndex": 22,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-23",
      "content": "We use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifically, we first compute P LM ( y | d, x ) , the LM probability of the ground truth output y given the input context x and a document d . The higher the probability, the better the document d i is at improving the LM's perplexity. We then compute the LM likelihood of each document d as follows:\n\n<!-- formula-not-decoded -->\n\nwhere β is another hyperparameter.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4.2. Computing LM likelihood",
        "chunkIndex": 23,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-24",
      "content": "Given the input context x and the corresponding ground truth continuation y , we compute the retrieval likelihood and the language model likelihood. The dense retriever is trained by minimizing the KL divergence between these two distributions:\n\n<!-- formula-not-decoded -->\n\nwhere B is a set of input contexts. When minimizing the loss, we can only update the retrieval model parameters. The LM parameters are fixed due to our black-box assumption.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4.3. Loss Function",
        "chunkIndex": 24,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-25",
      "content": "Because the parameters in the retriever are updated during the training process, the previously computed document embeddings are no longer up to date. Therefore, following Guu et al. (2020), we recompute the document embeddings and rebuild the efficient search index using the new embeddings every T training steps. Then we use the new document embeddings and index for retrieval, and repeat the training procedure.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "4.4. Asynchronous Update of the Datastore Index",
        "chunkIndex": 25,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-26",
      "content": "In this section, we describe the details of our training procedure. We first describe the model setting in REPLUG (§5.1) and then describe the procedure for training the retriever in REPLUG LSR (§5.2).",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "5. Training Setup",
        "chunkIndex": 26,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-27",
      "content": "In theory, any type of retriever, either dense (Karpukhin et al., 2020b; Ni et al., 2021) or sparse (Robertson et al., 2009), could be used for REPLUG. Following prior work (Izacard et al., 2022b), we use the Contriever (Izacard et al., 2022a) as the retrieval model for REPLUG, as it has demonstrated strong performance.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "5.1. REPLUG",
        "chunkIndex": 27,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-28",
      "content": "For REPLUG LSR, we initialize the retriever with the Contriever model (Izacard et al., 2022a). We use GPT-3 Curie (Brown et al., 2020b) as the supervision LM to compute the LM likelihood.\n\nTraining data We use 800K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2020), as our training queries. Each query is split into two parts: the first 128 tokens are used as the input context x , and the last 128 tokens are used as the ground truth continuation y . For the external corpus D , we sample 36M documents\n\nFigure 3. REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision signals.\n\n<!-- image -->\n\nof 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "5.2. REPLUG LSR",
        "chunkIndex": 28,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-29",
      "content": "-->\n\nof 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled.\n\nTraining details To make the training process more efficient, we pre-compute the document embeddings of the external corpus D and create a FAISS index (Johnson et al., 2019) for fast similarity search. Given a query x , we retrieve the top 20 documents from the FAISS index and compute the retrieval likelihood and the LM likelihood with a temperature of 0.1. We train the retriever using the Adam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 2e-5, a batch size of 64, and a warmup ratio of 0.1. We re-compute the document embeddings every 3k steps and fine-tune the retriever for a total of 25k steps.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "5.2. REPLUG LSR",
        "chunkIndex": 29,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-30",
      "content": "We perform evaluations on both language modeling (§6.1) and downstream tasks such as MMLU (§6.2) and opendomain QA (§6.3). In all settings, REPLUG ˜ improve the performance of various black-box language models, showing the effectiveness and generality of our approach.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6. Experiments",
        "chunkIndex": 30,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-31",
      "content": "Datasets The Pile (Gao et al., 2020) is a language modeling benchmark that consists of text sources from diverse domains such as web pages, code and academic papers. Following prior work, we report bits per UTF-8 encoded byte (BPB) as the metric on each subset domain.\n\nBaselines We consider GPT-3 and GPT-2 family language model as the baselines. The four models from GPT-3 (Davinci, Curie, Baddage and Ada) are black-box models that are only accessible through API\n\nOur model We add REPLUG and REPLUG LSR to the baselines. We randomly subsampled Pile training data (367M documents of 128 tokens) and use them as the retrieval corpus for all models. As the Pile dataset has made efforts to deduplicate documents across train, validation and test splits (Gao et al., 2020), we did not do additional filtering. For both REPLUG and REPLUG LSR, we use a length of 128-token context to do retrieval and adopt the ensemble method (Section 3.2) to incorporate top 10 retrieved documents during inference.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.1. Language Modeling",
        "chunkIndex": 31,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-32",
      "content": "filtering. For both REPLUG and REPLUG LSR, we use a length of 128-token context to do retrieval and adopt the ensemble method (Section 3.2) to incorporate top 10 retrieved documents during inference.\n\nResults Table 1 reports the results of the original baselines, baselines augmented with the REPLUG, and baselines augmented with the REPLUG LSR. We observe that both REPLUG and REPLUG LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks. Furthermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.1. Language Modeling",
        "chunkIndex": 32,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-33",
      "content": "compared to 4.7% improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.1. Language Modeling",
        "chunkIndex": 33,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-34",
      "content": "Datasets Massive Multi-task Language Understanding (MMLU (Hendrycks et al., 2021)) is a multiple choice QA dataset that covers exam questions from 57 tasks including mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humanities, STEM, social sciences and other. Following Chung\n\nREPLUG: Retrieval-Augmented Black-Box Language Models\n\n| Model       |         | # Parameters   |   Original |   + REPLUG |   Gain% |   + REPLUG LSR |   Gain% |\n|-------------|---------|----------------|------------|------------|---------|----------------|---------|\n| GPT-2       | Small   | 117M           |       1.33 |       1.26 |     5.3 |           1.21 |     9   |\n|             | Medium  | 345M           |       1.2  |       1.14 |     5   |           1.11 |     7.5 |\n|             | Large   | 774M           |       1.19 |       1.15 |     3.4 |           1.09 |     8.4 |\n|             | XL      | 1.5B           |       1.16 |       1.09 |     6   |",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 34,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-35",
      "content": "7.5 |\n|             | Large   | 774M           |       1.19 |       1.15 |     3.4 |           1.09 |     8.4 |\n|             | XL      | 1.5B           |       1.16 |       1.09 |     6   |           1.07 |     7.8 |\n| GPT-3       | Ada     | 350M           |       1.05 |       0.98 |     6.7 |           0.96 |     8.6 |\n| (black-box) | Babbage | 1.3B           |       0.95 |       0.9  |     5.3 |           0.88 |     7.4 |\n| (black-box) | Curie   | 6.7B           |       0.88 |       0.85 |     3.4 |           0.82 |     6.8 |\n| (black-box) | Davinci | 175B           |       0.8  |       0.77 |     3.8 |           0.75 |     6.3 |\n\nTable 1. Both REPLUG and REPLUG LSR consistently enhanced the performance of different language models. Bits per byte (BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrieval-augmented versions (+REPLUG and +REPLUG LSR. The gain % shows the relative improvement of our models compared to the original language model.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 35,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-36",
      "content": "T-3 and GPT-2 family models (Original) and their retrieval-augmented versions (+REPLUG and +REPLUG LSR. The gain % shows the relative improvement of our models compared to the original language model.\n\nTable 2. REPLUG and REPLUG LSR improves Codex by 4.5% and 5.1% respectively. Performance on MMLU broken down into 4 categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning with direct prompting.\n\n| Model              | # Parameters   | Humanities   | Social.   | STEM   | Other   |   All |\n|--------------------|----------------|--------------|-----------|--------|---------|-------|\n| Codex              | 175B           | 74.2         | 76.9      | 57.8   | 70.1    |  68.3 |\n| PaLM               | 540B           | 77.0         | 81.0      | 55.6   | 69.6    |  69.3 |\n| Flan-PaLM          | 540B           | -            | -         | -      | -       |  72.2 |\n| Atlas              | 11B            | 46.1",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 36,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-37",
      "content": "77.0         | 81.0      | 55.6   | 69.6    |  69.3 |\n| Flan-PaLM          | 540B           | -            | -         | -      | -       |  72.2 |\n| Atlas              | 11B            | 46.1         | 54.6      | 38.8   | 52.8    |  47.9 |\n| Codex + REPLUG     | 175B           | 76.0         | 79.7      | 58.8   | 72.1    |  71.4 |\n| Codex + REPLUG LSR | 175B           | 76.5         | 79.9      | 58.9   | 73.2    |  71.8 |\n\net al. (2022a), we evaluate REPLUG in the 5-shot in-context learning setting.\n\nBaselines Weconsider two groups of strong previous models as baselines for comparisons. The first group of baselines is the state-of-the-art LLMs including Codex 1 (Chen et al., 2021b), PaLM (Chowdhery et al., 2022), and FlanPaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. The second group of baselines consists of retrieval-augmented language models.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 37,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-38",
      "content": "nPaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. The second group of baselines consists of retrieval-augmented language models. We only include Atlas (Izacard et al., 2022b) in this group, as no other retrievalaugmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting.\n\nOur model We add REPLUG and REPLUG LSR only to Codex because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retrieve 10 relevant documents from Wikipedia (2018, December) and prepend each retrieved document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together.\n\n1 Code-Davinci-002",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 38,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-39",
      "content": "d document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together.\n\n1 Code-Davinci-002\n\nResults Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG and REPLUG LSR improve the original Codex model by 4.5% and 5.1%, respectively. In addition, REPLUG LSR largely outperforms the previous retrieval-augmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting. Although our models slightly underperform FlanPaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 39,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-40",
      "content": "erperform FlanPaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model.\n\nAnother interesting observation is that the REPLUG LSR outperforms the original model by 1.9% even in the STEM category. This suggests that retrieval may improve a language model's problem-solving abilities.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.2. MMLU",
        "chunkIndex": 40,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-41",
      "content": "Lastly, we conduct evaluation on two open-domain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).\n\nDatasets NQ and TriviaQA are two open-domain QA datasets consisting of questions, answers collected from\n\nSi et al. (2022) augment Codex with concatenation of 10\n\n2 documents retrieved by contriever.\n\nTable 3. Performance on NQ and TQA. We report results for both few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full training data settings. REPLUG LSR improves Codex by 12.0% on NQ and 5.0% on TQA, making it the best-performing model in the few-shot setting. Note that models with † are finetuned using training examples, while other models use in-context learning.\n\n|                         | NQ       | NQ   | TQA      | TQA   |\n|-------------------------|----------|------|----------|-------|\n| Model                   | Few-shot | Full | Few-shot | Full  |\n| Chinchilla              | 35.5     | -    | 64",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 41,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-42",
      "content": "| TQA      | TQA   |\n|-------------------------|----------|------|----------|-------|\n| Model                   | Few-shot | Full | Few-shot | Full  |\n| Chinchilla              | 35.5     | -    | 64.6     | -     |\n| PaLM                    | 39.6     | -    | -        | -     |\n| Codex                   | 40.6     | -    | 73.6     | -     |\n| RETRO †                 | -        | 45.5 | -        | -     |\n| R2-D2 †                 | -        | 55.9 | -        | 69.9  |\n| Atlas †                 | 42.4     | 60.4 | 74.5     | 79.8  |\n| Codex + Contriever cc 2 | 44.2     | -    | 76.0     | -     |\n| Codex + REPLUG          | 44.7     | -    | 76.8     | -     |\n| Codex + REPLUG LSR      | 45.5     | -    | 77.3     | -     |\n\nWikipedia and the Web. Following prior work (Izacard &amp; Grave, 2021a; Si et al., 2022), we report results for the filtered set of TriviaQA.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 42,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-43",
      "content": "+ REPLUG LSR      | 45.5     | -    | 77.3     | -     |\n\nWikipedia and the Web. Following prior work (Izacard &amp; Grave, 2021a; Si et al., 2022), we report results for the filtered set of TriviaQA. For evaluation, we consider the fewshot setting where the model is only given a few training examples and full data where the model is given all the training examples.\n\nBaselines We compare our model with several state-ofthe-art baselines, both in a few-shot setting and with full training data. The first group of models consists of powerful large language models, including Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), and Codex. These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 43,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-44",
      "content": "M (Chowdhery et al., 2022), and Codex. These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots. The second group of models for comparison includes retrievalaugmented language models such as RETRO (Borgeaud et al., 2021), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard et al., 2022b). All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting.\n\nOur model We add REPLUG and REPLUG LSR to Codex with Wikipedia (2018, December) as the retrieval corpus to evaluate the model in a 16-shot in context learning. Similar to the setting in language modeling and MMLU, we incorporate top-10 retrieved documents using our proposed ensemble method.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 44,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-45",
      "content": "ieval corpus to evaluate the model in a 16-shot in context learning. Similar to the setting in language modeling and MMLU, we incorporate top-10 retrieved documents using our proposed ensemble method.\n\nResults As shown in Table 3, REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA. It outperforms the previous best\n\nFigure 4. Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG and REPLUG LSR) when varying the number of documents (i.e.; number of ensemble times.)\n\n<!-- image -->\n\nmodel, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of nearduplicate test questions in the training set (e.g., Lewis et al.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 45,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-46",
      "content": "the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of nearduplicate test questions in the training set (e.g., Lewis et al. (2021) found that 32.5% of test questions overlap with the training sets in NQ).",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "6.3. Open Domain QA",
        "chunkIndex": 46,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-47",
      "content": "The core of our method design is the use of an ensemble method that combines output probabilities of different passes, in which each retrieved document is prepended separately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concatenated each random document with the input, and ensemble the outputs of different runs (referred to as \"random\"). As shown in Figure 6, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by REPLUG, and documents retrieved by REPLUG LSR. We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not solely come from the ensembling effect. Instead, ensembling the relevant documents is crucial for the success of REPLUG.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.1. REPLUG performance gain does not simply come from the ensembling effect",
        "chunkIndex": 47,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-48",
      "content": "ds to worse performance, indicating that the performance gains of REPLUG do not solely come from the ensembling effect. Instead, ensembling the relevant documents is crucial for the success of REPLUG. Additionally, as more documents were ensembled, the performance of REPLUG and REPLUG LSR improved monotonically. However, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.\n\nFigure 5. GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG. The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.1. REPLUG performance gain does not simply come from the ensembling effect",
        "chunkIndex": 48,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-49",
      "content": "Here we further study whether REPLUG could enhance diverse language model families that have been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022b) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Stephen et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khandelwal et al., 2020), we use Wikitext-103 training data as the retrieval corpus.\n\nFigure 5 shows the performance of different-sized language models with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.2. REPLUG is applicable to diverse language models",
        "chunkIndex": 49,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-50",
      "content": "e retrieval corpus.\n\nFigure 5 shows the performance of different-sized language models with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT with 125M parameters achieves 6.9% perplexity improvement, while OPT with 66B parameters achieves 5.6% perplexity improvement. Additionally, REPLUG improves the perplexity of all the model families. This indicates that REPLUG is applicable to diverse language models with different sizes.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.2. REPLUG is applicable to diverse language models",
        "chunkIndex": 50,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-51",
      "content": "To understand why the REPLUG improves language modeling performance, we conducted manual analysis of examples in which the REPLUG results in a decrease in perplexity. We find that REPLUG is more helpful when texts contain rare entities. Figure 6 shows a test context and its continuation from the Wikitext-103 test set. For REPLUG, we use the test context as a query to retrieve a relevant docu- ment from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its REPLUG enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by 11%. Among all tokens in the continuation, we found that REPLUG is most helpful for the rare entity name \"Li Bai\". This is likely because the original LM does not have sufficient information about this rare entity name.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.3. Qualitative Analysis: rare entities benefit from retrieval",
        "chunkIndex": 51,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-52",
      "content": "ns in the continuation, we found that REPLUG is most helpful for the rare entity name \"Li Bai\". This is likely because the original LM does not have sufficient information about this rare entity name. However, by incorporating the retrieved document, REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance.\n\nFigure 6. Rare entities benefit from retrieval . After incorporating the retrieved document during inference, the entity \" Li Bai \" and the token \" greatest \" in the continuation show the most improvement in perplexity (15% for \" Li Bai \" and 5% for \" greatest \"). Other tokens' perplexity changes are within 5%.\n\n<!-- image -->",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "7.3. Qualitative Analysis: rare entities benefit from retrieval",
        "chunkIndex": 52,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-53",
      "content": "We introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance\n\non language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into largescale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "8. Conclusion",
        "chunkIndex": 53,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-54",
      "content": "Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426 , 2021.\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning , pp. 2206-2240. PMLR, 2022.\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-55",
      "content": ", Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proc. of NeurIPS , 2020b.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-56",
      "content": ", M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proc. of NeurIPS , 2020b. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings,\n\nD., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-57",
      "content": "Cummings,\n\nD., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021a. URL https://arxiv.org/abs/2107.03374 .\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 57,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-58",
      "content": "P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021b. URL https://arxiv.org/abs/2107.03374 .\n\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 58,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-59",
      "content": "g, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022a.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022b.\n\nFajcik, M., Docekal, M., Ondrej, K., and Smrz, P. R2D2: A modular baseline for open-domain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pp. 854-870, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.73.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 59,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-60",
      "content": "Association for Computational Linguistics: EMNLP 2021 , pp. 854-870, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.73. URL https://aclanthology.org/ 2021.findings-emnlp.73 .\n\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\n\n- Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R., Badia, A. P., Guez, A., Mirza, M., Humphreys, P. C., Konyushova, K., et al. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning , pp. 7740-7765. PMLR, 2022.\n- Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International Conference on Machine Learning , pp. 39293938.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 60,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-61",
      "content": "g , pp. 7740-7765. PMLR, 2022.\n- Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International Conference on Machine Learning , pp. 39293938. PMLR, 2020.\n- Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations , 2021. URL https: //openreview.net/forum?id=d7KBjmI3GmQ .\n- Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- Hu, Y., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699 , 2022.\n- Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 61,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-62",
      "content": ": Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699 , 2022.\n- Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 874-880, Online, April 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main.74 .\n- Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In Proc. of EACL , 2021b. URL https://arxiv.org/ abs/2007.01282 .\n- Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research , 2022a.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 62,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-63",
      "content": "., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research , 2022a. URL https://openreview.net/forum?id=jKN1pXi7b0 .\n- Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022b.\n- Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity search with gpus. IEEE Transactions on Big Data , 7 (3):535-547, 2019.\n- Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 63,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-64",
      "content": "al Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769-6781, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 .\n- Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 64,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-65",
      "content": "Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769-6781, 2020b.\n- Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\n- Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledgeintensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\n- Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n- Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 65,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-66",
      "content": "2015.\n- Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_ 00276. URL https://aclanthology.org/Q19-1026 .\n- Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2020. URL https://arxiv.org/abs/2005.11401 .\n- Lewis, P., Stenetorp, P., and Riedel, S. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000-1008, 2021.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 66,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-67",
      "content": "n overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000-1008, 2021.\n\n- Mallen, A., Asai, A., Zhong, V., Das, R., Hajishirzi, H., and Khashabi, D. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. arXiv preprint arXiv:2212.10511 , 2022.\n- Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Hajishirzi, H., and Zettlemoyer, L. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349 , 2022.\n- Ni, J., Qu, C., Lu, J., Dai, Z., Ábrego, G. H., Ma, J., Zhao, V. Y., Luan, Y., Hall, K. B., Chang, M., and Yang, Y. Large dual encoders are generalizable retrievers, 2021. URL https://arxiv.org/abs/2112.07899 .\n- Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 67,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-68",
      "content": ", and Yang, Y. Large dual encoders are generalizable retrievers, 2021. URL https://arxiv.org/abs/2112.07899 .\n- Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 5835-5847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.466. URL https: //aclanthology.org/2021.naacl-main.466 .\n- Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009.\n- Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 68,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-69",
      "content": "levance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009.\n- Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2655-2671, 2022.\n- Sachan, D. S., Lewis, M., Yogatama, D., Zettlemoyer, L., Pineau, J., and Zaheer, M. Questions are all you need to train a dense passage retriever. arXiv preprint arXiv:2206.10658 , 2022.\n- Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´ c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\n- Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L. Nearest neighbor zero-shot inference. 2022.\n- Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 69,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-70",
      "content": "211.05100 , 2022.\n- Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L. Nearest neighbor zero-shot inference. 2022.\n- Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 , 2022.\n- Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. In Proc. of ICLR , 2023. URL https://openreview.net/ forum?id=98p5x51L5af .\n- Stephen, M., Caiming, X., James, B., and Socher, R. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017.\n- Su, H., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder, any task: Instruction-finetuned text embeddings.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 70,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-71",
      "content": "ack Proceedings , 2017.\n- Su, H., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741 , 2022.\n- Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li, F., Zhu, H., Luo, J., Xu, L., et al. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725 , 2021.\n- Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561 , 2022.\n- Younes Belkda, T. D. A gentle introduction to 8-bit matrix multiplication, 2022. URL https://huggingface.co/ blog/hf-bitsandbytes-integration .\n- Yu, W. Retrieval-augmented generation across heterogeneous knowledge.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 71,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-72",
      "content": "T. D. A gentle introduction to 8-bit matrix multiplication, 2022. URL https://huggingface.co/ blog/hf-bitsandbytes-integration .\n- Yu, W. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop , pp. 52-58, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-srw.7. URL https://aclanthology.org/2022. naacl-srw.7 .\n- Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate rather than retrieve: Large language models are strong context generators. 2023.\n- Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing access to large-scale language models with opt-175b. Meta AI , 2022a.\n- Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 72,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-73",
      "content": "Democratizing access to large-scale language models with opt-175b. Meta AI , 2022a.\n- Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b.\n- Zhong, Z., Lei, T., and Chen, D. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP) , 2022.\n\nKnowledge : Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 73,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-74",
      "content": "of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European\n\nQuestion : As of 2015, since 1990 forests have in Europe and have in Africa and the Americas.\n\nA. \"increased, increased\" B. \"increased, decreased\" C. \"decreased, increased\" D. \"decreased, decreased\"\n\nAnswer : B\n\nKnowledge : Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population.",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 74,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-75",
      "content": ": B\n\nKnowledge : Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, 56% of those age 18 to 29 favor gay marriage, 68% state environmental protection to be as important as job creation, 52% \"think immigrants ´ strengthen the country with their hard work and talents, ´ \" 62% favor a \"tax financed, government-administrated universal health care\" program and 74% \"say ´ people´ s will´ should have more influence on U.S. laws than the Bible, compared to 37%, 49%, 38%, 47% and 58% among the\n\nQuestion : As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?\n\nA. 31% B. 46% C. 61% D. 76%\n\nAnswer : B\n\n...",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 75,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-76",
      "content": "49%, 38%, 47% and 58% among the\n\nQuestion : As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?\n\nA. 31% B. 46% C. 61% D. 76%\n\nAnswer : B\n\n...\n\nKnowledge : last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule.\" Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries\n\nQuestion\n\n: Which of the following countries generated the most total energy from solar sources in 2019?\n\nA. China B. United States C. Germany D. Japan\n\nTable 4. Prompt for MMLU",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 76,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-77",
      "content": "ranking for countries\n\nQuestion\n\n: Which of the following countries generated the most total energy from solar sources in 2019?\n\nA. China B. United States C. Germany D. Japan\n\nTable 4. Prompt for MMLU\n\nKnowledge : received 122,000 buys (excluding WWE Network views), down from the previous year´ s 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of \"Raw\", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the\n\nQuestion : Who won the mens money in the bank match?\n\nAnswer : Braun Strowman",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 77,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-78",
      "content": "ng week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the\n\nQuestion : Who won the mens money in the bank match?\n\nAnswer : Braun Strowman\n\nKnowledge : in 3D on March 17, 2017. The first official presentation of the film took place at Disney´ s three-day D23 Expo in August 2015. The world premiere of \"Beauty and the Beast\" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in\n\nQuestion : When does beaty and the beast take place\n\nAnswer : Rococo-era\n\n...",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 78,
        "totalChunks": 80
      }
    },
    {
      "id": "2301.12652v4-chunk-79",
      "content": "nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in\n\nQuestion : When does beaty and the beast take place\n\nAnswer : Rococo-era\n\n...\n\nKnowledge : Love Yourself \"Love Yourself\" is a song recorded by Canadian singer Justin Bieber for his fourth studio album \"Purpose\" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album´ s third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, \"Love Yourself\" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers.\n\nLyrically, the song is a kiss-off to a narcissistic ex-lover who did\n\nQuestion\n\n: love yourself by justin bieber is about who",
      "metadata": {
        "source": "arxiv:2301.12652v4",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "authors": [
          "Weijia Shi",
          "Sewon Min",
          "Michihiro Yasunaga",
          "Minjoon Seo",
          "Rich James",
          "Mike Lewis",
          "Luke Zettlemoyer",
          "Wen-tau Yih"
        ],
        "section": "References",
        "chunkIndex": 79,
        "totalChunks": 80
      }
    }
  ],
  "fullText": "## REPLUG: Retrieval-Augmented Black-Box Language Models\n\nWeijia Shi, 1 * Sewon Min, 1 Michihiro Yasunaga, 2 Minjoon Seo, 3 Rich James, 4 Mike Lewis, 4 Luke Zettlemoyer 1 4 Wen-tau Yih 4\n\n## Abstract\n\nWe introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%.\n\n## 1. Introduction\n\nLarge language models (LLMs) such as GPT-3 (Brown et al., 2020a) and Codex (Chen et al., 2021a), have demonstrated impressive performance on a wide range of language tasks. These models are typically trained on very large datasets and store a substantial amount of world or domain knowledge implicitly in their parameters. However, they are also prone to hallucination and cannot represent the full long tail of knowledge from the training corpus. Retrieval-augmented language models (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Yasunaga et al., 2022), in contrast, can retrieve knowledge from an external datastore when needed, potentially reducing hallucination and increasing coverage. Previous approaches of retrieval-augmented language models require access to the internal LM representations (e.g., to train the model (Borgeaud et al., 2022;\n\n1 University of Washington 2 Stanford University 3 KAIST 4 Meta AI. Work done while the first author was interning at Meta AI.\n\n* Correspondence to: Weijia Shi &lt;swj0419@uw.edu&gt;.\n\nFigure 1. Different from previous retrieval-augmented approaches (Borgeaud et al., 2022) that enhance a language model with retrieval by updating the LM's parameters, REPLUG treats the language model as a black box and augments it with a frozen or tunable retriever. This black-box assumption makes REPLUG applicable to large LMs (i.e., &gt;100B parameters), which are often served via APIs.\n\n<!-- image -->\n\nIzacard et al., 2022b) or to index the datastore (Khandelwal et al., 2020)), and are thus difficult to be applied to very large LMs. In addition, many best-in-class LLMs can only be accessed through APIs. Internal representations of such models are not exposed and fine-tuning is not supported.\n\nIn this work, we introduce REPLUG ( Re trieve and Plug ), a new retrieval-augmented LM framework where the language model is viewed as a black box and the retrieval component is added as a tuneable plug-and-play module. Given an input context, REPLUG first retrieves relevant documents from an external corpus using an off-the-shelf retrieval model. The retrieved documents are prepended to the input context and fed into the black-box LM to make the final prediction. Because the LM context length limits the number of documents that can be prepended, we also introduce a new ensemble scheme that encodes the retrieved documents in parallel with the same black-box LM, allowing us to easily trade compute for accuracy. As shown in\n\nFigure 1, REPLUG is extremely flexible and can be used with any existing black-box LM and retrieval model.\n\nWe also introduce REPLUG LSR (REPLUG with L MS upervised R etrieval), a training scheme that can further improve the initial retrieval model in REPLUG with supervision signals from a black-box language model. The key idea is to adapt the retriever to the LM, which is in contrast to prior work (Borgeaud et al., 2022) that adapts language models to the retriever. We use a training objective which prefers retrieving documents that improve language model perplexity, while treating the LM as a frozen, black-box scoring function.\n\nOur experiments show that REPLUG can improve the performance of diverse black-box LMs on both language modeling and downstream tasks, including MMLU (Hendrycks et al., 2021) and open-domain QA (Kwiatkowski et al., 2019; Joshi et al., 2017). For instance, REPLUG can improve Codex (175B) performance on MMLU by 4.5%, achieving comparable results to the 540B, instruction-finetuned Flan-PaLM. Furthermore, tuning the retriever with our training scheme (i.e., REPLUG LSR) leads to additional improvements, including up to 6.3% increase in GPT-3 175B language modeling. To the best of our knowledge, our work is the first to show the benefits of retrieval to large LMs (&gt;100B model parameters), for both reducing LM perplexity and and improving in-context learning performance. We summarize our contributions as follows:\n\n- We introduce REPLUG (§3), the first retrievalaugmented language modeling framework for enhancing large black-box language models with retrieval.\n- We propose a training scheme (§4) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.\n- Evaluations on language modeling (§6), open-domain QA and MMLU demonstrate that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.\n\n## 2. Background and Related Work\n\nBlack-box Language Models Large language models (i.e., &gt;100B), such as GPT-3 (Brown et al., 2020a), Codex (Chen et al., 2021a), and Yuan 1.0 (Wu et al., 2021), are not open-sourced due to commercial considerations and are only available as black-box APIs, through which users can send queries and receive responses. On the other hand, even open sourced language models such as OPT-175B (Zhang et al., 2022a) and BLOOM-176B (Scao et al., 2022) require significant computational resources to run and finetune locally. For example, finetuning BLOOM-176B requires 72 A100 GPUs (80GB memory, $15k each (Younes Belkda, 2022)), making them inaccessible to researchers and developers with limited resources. Traditionally, retrieval-augmented model frameworks (Khandelwal et al., 2020; Borgeaud et al., 2022; Yu, 2022; Izacard et al., 2022b; Goyal et al., 2022) have focused on the white-box setting, where language models are fine-tuned to incorporate retrieved documents. However, the increasing scale and black-box nature of large language models makes this approach infeasible. To address the challenges posed by large language models, we investigate retrieval-augmentation in the black-box setting , where users only have access to the model predictions and cannot access or modify its parameters.\n\nRetrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022; Borgeaud et al., 2022; Khandelwal et al., 2020) and open-domain question answering (Lewis et al., 2020; Izacard et al., 2022b; Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022; Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2020; Borgeaud et al., 2022; Shi et al., 2022; Rubin et al., 2022). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2022) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2020; Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference. Although kNN-LM does not require additional training, it requires access to internal LM representations to compute the kNN distribution, which are not always available for large LMs such as GPT-3. In this work, we investigate ways to improve large black-box language models with retrieval. While concurrent work (Mallen et al., 2022; Si et al., 2023; Yu et al., 2023; Khattab et al., 2022) has demonstrated that using a frozen retriever can improve GPT3 performance on open-domain question answering, we approach the problem in a more general setting, including language modeling and understanding tasks. We also propose an ensemble method to incorporate more documents\n\nFigure 2. REPLUG at inference (§3). Given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1 Document Retrieval ). Then it prepends each document separately to the input context and ensembles output probabilities from different passes (§3.2 Input Reformulation ).\n\n<!-- image -->\n\nand a training scheme to further adapt the retriever to large LMs.\n\n## 3. REPLUG\n\nWe introduce REPLUG ( Re trieve and Plug ), a new retrievalaugmented LM paradigm where the language model is treated as black box and the retrieval component is added as a potentially tuneable module.\n\nAs shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities (§3.2).\n\n## 3.1. Document Retrieval\n\nGiven an input context x , the retriever aims to retrieve a small set of documents from a corpus D = { d 1 ...d m } that are relevant to x . Following prior work (Qu et al., 2021; Izacard &amp; Grave, 2021b; Ni et al., 2021), we use a dense retriever based on the dual encoder architecture, where an encoder is used to encode both the input context x and the document d . Specifically, the encoder maps each document d ∈ D to an embedding E ( d ) by taking the mean pooling of the last hidden representation over the tokens in d . At query time, the same encoder is applied to the input context x to obtain a query embedding E ( x ) . The similarity between the query embedding and the document embedding is computed by their cosine similarity:\n\n<!-- formula-not-decoded -->\n\nThe topk documents that have the highest similarity scores when compared with the input x are retrieved in this step.\n\nFor efficient retrieval, we precompute the embedding of each document d ∈ D and construct FAISS index (Johnson et al., 2019) over these embeddings.\n\n## 3.2. Input Reformulation\n\nThe retrieved topk documents provide rich information about the original input context x and can potentially help the LM to make a better prediction. One simple way to incorporate the retrieved documents as part of the input to the LM is to prepend x with all k documents. However, this simple scheme is fundamentally restricted by the number of documents (i.e., k ) we can include, given the language model's context window size. To address this limitation, we adopt an ensemble strategy described as follows. Assume D ′ ⊂ D consists of k most relevant documents to x , according to the scoring function in Eq. (1). We prepend each document d ∈ D ′ to x , pass this concatenation to the LM separately, and then ensemble output probabilities from all k passes. Formally, given the input context x and its topk relevant documents D ′ , the output probability of the next token y is computed as a weighted average ensemble:\n\n<!-- formula-not-decoded -->\n\nwhere ◦ denotes the concatenation of two sequences and the weight λ ( d, x ) is based on the similarity score between the document d and the input context x :\n\n<!-- formula-not-decoded -->\n\nAlthough our ensemble method requires running the LM k times, the cross attention is performed between each retrieved document and the input context. Therefore, compared with the method of prepending all the retrieved docu-\n\nments, our ensemble methods do not incur additional computational cost overhead.\n\n## 4. REPLUG LSR: Training the Dense Retriever\n\nInstead of relying only on existing neural dense retrieval models (Karpukhin et al., 2020a; Izacard et al., 2022a; Su et al., 2022), we further propose REPLUG LSR (REPLUG with LM-Supervised Retrieval), which adapts the retriever in REPLUG by using the LM itself to provide supervision about which documents should be retrieved.\n\nInspired by Sachan et al. (2022), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model. In other words, we would like the retriever to find documents that result in lower perplexity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (§4.1), (2) scoring the retrieved documents by the language model (§4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM's score distribution (§4.3), and (4) asynchronous update of the datastore index (§4.4).\n\n## 4.1. Computing Retrieval Likelihood\n\nWe retrieve k documents D ′ ⊂ D with the highest similarity scores from a corpus D given an input context x , as described in §3.1. We then compute the retrieval likelihood of each retrieved document d :\n\n<!-- formula-not-decoded -->\n\nwhere γ is a hyperparameter that controls the temerature of the softmax. Ideally, the retrieval likelihood is computed by marginalizing over all the documents in the corpus D , which is intractable in practice. Therefore, we approximate the retrieval likelihood by only marginalizing over the retrieved documents D ′ .\n\n## 4.2. Computing LM likelihood\n\nWe use the LM as a scoring function to measure how much each document could improve the LM perplexity. Specifically, we first compute P LM ( y | d, x ) , the LM probability of the ground truth output y given the input context x and a document d . The higher the probability, the better the document d i is at improving the LM's perplexity. We then compute the LM likelihood of each document d as follows:\n\n<!-- formula-not-decoded -->\n\nwhere β is another hyperparameter.\n\n## 4.3. Loss Function\n\nGiven the input context x and the corresponding ground truth continuation y , we compute the retrieval likelihood and the language model likelihood. The dense retriever is trained by minimizing the KL divergence between these two distributions:\n\n<!-- formula-not-decoded -->\n\nwhere B is a set of input contexts. When minimizing the loss, we can only update the retrieval model parameters. The LM parameters are fixed due to our black-box assumption.\n\n## 4.4. Asynchronous Update of the Datastore Index\n\nBecause the parameters in the retriever are updated during the training process, the previously computed document embeddings are no longer up to date. Therefore, following Guu et al. (2020), we recompute the document embeddings and rebuild the efficient search index using the new embeddings every T training steps. Then we use the new document embeddings and index for retrieval, and repeat the training procedure.\n\n## 5. Training Setup\n\nIn this section, we describe the details of our training procedure. We first describe the model setting in REPLUG (§5.1) and then describe the procedure for training the retriever in REPLUG LSR (§5.2).\n\n## 5.1. REPLUG\n\nIn theory, any type of retriever, either dense (Karpukhin et al., 2020b; Ni et al., 2021) or sparse (Robertson et al., 2009), could be used for REPLUG. Following prior work (Izacard et al., 2022b), we use the Contriever (Izacard et al., 2022a) as the retrieval model for REPLUG, as it has demonstrated strong performance.\n\n## 5.2. REPLUG LSR\n\nFor REPLUG LSR, we initialize the retriever with the Contriever model (Izacard et al., 2022a). We use GPT-3 Curie (Brown et al., 2020b) as the supervision LM to compute the LM likelihood.\n\nTraining data We use 800K sequences of 256 tokens each, sampled from the Pile training data (Gao et al., 2020), as our training queries. Each query is split into two parts: the first 128 tokens are used as the input context x , and the last 128 tokens are used as the ground truth continuation y . For the external corpus D , we sample 36M documents\n\nFigure 3. REPLUG LSR training process (§4). The retriever is trained using the output of a frozen language model as supervision signals.\n\n<!-- image -->\n\nof 128 tokens from the Pile training data. To avoid trivial retrieval, we ensure that the external corpus documents do not overlap with the documents from which the training queries are sampled.\n\nTraining details To make the training process more efficient, we pre-compute the document embeddings of the external corpus D and create a FAISS index (Johnson et al., 2019) for fast similarity search. Given a query x , we retrieve the top 20 documents from the FAISS index and compute the retrieval likelihood and the LM likelihood with a temperature of 0.1. We train the retriever using the Adam optimizer (Kingma &amp; Ba, 2015) with a learning rate of 2e-5, a batch size of 64, and a warmup ratio of 0.1. We re-compute the document embeddings every 3k steps and fine-tune the retriever for a total of 25k steps.\n\n## 6. Experiments\n\nWe perform evaluations on both language modeling (§6.1) and downstream tasks such as MMLU (§6.2) and opendomain QA (§6.3). In all settings, REPLUG ˜ improve the performance of various black-box language models, showing the effectiveness and generality of our approach.\n\n## 6.1. Language Modeling\n\nDatasets The Pile (Gao et al., 2020) is a language modeling benchmark that consists of text sources from diverse domains such as web pages, code and academic papers. Following prior work, we report bits per UTF-8 encoded byte (BPB) as the metric on each subset domain.\n\nBaselines We consider GPT-3 and GPT-2 family language model as the baselines. The four models from GPT-3 (Davinci, Curie, Baddage and Ada) are black-box models that are only accessible through API\n\nOur model We add REPLUG and REPLUG LSR to the baselines. We randomly subsampled Pile training data (367M documents of 128 tokens) and use them as the retrieval corpus for all models. As the Pile dataset has made efforts to deduplicate documents across train, validation and test splits (Gao et al., 2020), we did not do additional filtering. For both REPLUG and REPLUG LSR, we use a length of 128-token context to do retrieval and adopt the ensemble method (Section 3.2) to incorporate top 10 retrieved documents during inference.\n\nResults Table 1 reports the results of the original baselines, baselines augmented with the REPLUG, and baselines augmented with the REPLUG LSR. We observe that both REPLUG and REPLUG LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks. Furthermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.\n\n## 6.2. MMLU\n\nDatasets Massive Multi-task Language Understanding (MMLU (Hendrycks et al., 2021)) is a multiple choice QA dataset that covers exam questions from 57 tasks including mathematics, computer science, law, US history and etc. The 57 tasks are grouped into 4 categories: humanities, STEM, social sciences and other. Following Chung\n\nREPLUG: Retrieval-Augmented Black-Box Language Models\n\n| Model       |         | # Parameters   |   Original |   + REPLUG |   Gain% |   + REPLUG LSR |   Gain% |\n|-------------|---------|----------------|------------|------------|---------|----------------|---------|\n| GPT-2       | Small   | 117M           |       1.33 |       1.26 |     5.3 |           1.21 |     9   |\n|             | Medium  | 345M           |       1.2  |       1.14 |     5   |           1.11 |     7.5 |\n|             | Large   | 774M           |       1.19 |       1.15 |     3.4 |           1.09 |     8.4 |\n|             | XL      | 1.5B           |       1.16 |       1.09 |     6   |           1.07 |     7.8 |\n| GPT-3       | Ada     | 350M           |       1.05 |       0.98 |     6.7 |           0.96 |     8.6 |\n| (black-box) | Babbage | 1.3B           |       0.95 |       0.9  |     5.3 |           0.88 |     7.4 |\n| (black-box) | Curie   | 6.7B           |       0.88 |       0.85 |     3.4 |           0.82 |     6.8 |\n| (black-box) | Davinci | 175B           |       0.8  |       0.77 |     3.8 |           0.75 |     6.3 |\n\nTable 1. Both REPLUG and REPLUG LSR consistently enhanced the performance of different language models. Bits per byte (BPB) of the Pile using GPT-3 and GPT-2 family models (Original) and their retrieval-augmented versions (+REPLUG and +REPLUG LSR. The gain % shows the relative improvement of our models compared to the original language model.\n\nTable 2. REPLUG and REPLUG LSR improves Codex by 4.5% and 5.1% respectively. Performance on MMLU broken down into 4 categories. The last column averages the performance over these categories. All models are evaluated based on 5-shot in-context learning with direct prompting.\n\n| Model              | # Parameters   | Humanities   | Social.   | STEM   | Other   |   All |\n|--------------------|----------------|--------------|-----------|--------|---------|-------|\n| Codex              | 175B           | 74.2         | 76.9      | 57.8   | 70.1    |  68.3 |\n| PaLM               | 540B           | 77.0         | 81.0      | 55.6   | 69.6    |  69.3 |\n| Flan-PaLM          | 540B           | -            | -         | -      | -       |  72.2 |\n| Atlas              | 11B            | 46.1         | 54.6      | 38.8   | 52.8    |  47.9 |\n| Codex + REPLUG     | 175B           | 76.0         | 79.7      | 58.8   | 72.1    |  71.4 |\n| Codex + REPLUG LSR | 175B           | 76.5         | 79.9      | 58.9   | 73.2    |  71.8 |\n\net al. (2022a), we evaluate REPLUG in the 5-shot in-context learning setting.\n\nBaselines Weconsider two groups of strong previous models as baselines for comparisons. The first group of baselines is the state-of-the-art LLMs including Codex 1 (Chen et al., 2021b), PaLM (Chowdhery et al., 2022), and FlanPaLM (Chung et al., 2022b). According to Chung et al. (2022b), these three models rank top-3 in the leaderboard of MMLU. The second group of baselines consists of retrieval-augmented language models. We only include Atlas (Izacard et al., 2022b) in this group, as no other retrievalaugmented LMs have been evaluated on the MMLU dataset. Atlas trains both the retriever and the language model, which we consider a white-box retrieval LM setting.\n\nOur model We add REPLUG and REPLUG LSR only to Codex because other models such as PaLM and Flan-PaLM are not accessible to the public. We use the test question as the query to retrieve 10 relevant documents from Wikipedia (2018, December) and prepend each retrieved document to the test question, resulting in 10 separate inputs. These inputs are then separately fed into the language models, and the output probabilities are ensemble together.\n\n1 Code-Davinci-002\n\nResults Table 2 presents the results from the baselines, REPLUG, and REPLUG LSR on the MMLU dataset. We observe that both the REPLUG and REPLUG LSR improve the original Codex model by 4.5% and 5.1%, respectively. In addition, REPLUG LSR largely outperforms the previous retrieval-augmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting. Although our models slightly underperform FlanPaLM, this is still a strong result because Flan-PaLM has three times more parameters. We would expect that the REPLUG LSR could further improve Flan-PaLM, if we had access to the model.\n\nAnother interesting observation is that the REPLUG LSR outperforms the original model by 1.9% even in the STEM category. This suggests that retrieval may improve a language model's problem-solving abilities.\n\n## 6.3. Open Domain QA\n\nLastly, we conduct evaluation on two open-domain QA datasets: Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017).\n\nDatasets NQ and TriviaQA are two open-domain QA datasets consisting of questions, answers collected from\n\nSi et al. (2022) augment Codex with concatenation of 10\n\n2 documents retrieved by contriever.\n\nTable 3. Performance on NQ and TQA. We report results for both few-shot (64 shots for Chinchilla, PaLM, and Atlas; 16 shots for Codex-based models) and full training data settings. REPLUG LSR improves Codex by 12.0% on NQ and 5.0% on TQA, making it the best-performing model in the few-shot setting. Note that models with † are finetuned using training examples, while other models use in-context learning.\n\n|                         | NQ       | NQ   | TQA      | TQA   |\n|-------------------------|----------|------|----------|-------|\n| Model                   | Few-shot | Full | Few-shot | Full  |\n| Chinchilla              | 35.5     | -    | 64.6     | -     |\n| PaLM                    | 39.6     | -    | -        | -     |\n| Codex                   | 40.6     | -    | 73.6     | -     |\n| RETRO †                 | -        | 45.5 | -        | -     |\n| R2-D2 †                 | -        | 55.9 | -        | 69.9  |\n| Atlas †                 | 42.4     | 60.4 | 74.5     | 79.8  |\n| Codex + Contriever cc 2 | 44.2     | -    | 76.0     | -     |\n| Codex + REPLUG          | 44.7     | -    | 76.8     | -     |\n| Codex + REPLUG LSR      | 45.5     | -    | 77.3     | -     |\n\nWikipedia and the Web. Following prior work (Izacard &amp; Grave, 2021a; Si et al., 2022), we report results for the filtered set of TriviaQA. For evaluation, we consider the fewshot setting where the model is only given a few training examples and full data where the model is given all the training examples.\n\nBaselines We compare our model with several state-ofthe-art baselines, both in a few-shot setting and with full training data. The first group of models consists of powerful large language models, including Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022), and Codex. These models are all evaluated using in-context learning under the few-shot setting, with Chinchilla and PaLM evaluated using 64 shots, and Codex using 16 shots. The second group of models for comparison includes retrievalaugmented language models such as RETRO (Borgeaud et al., 2021), R2-D2 (Fajcik et al., 2021), and Atlas (Izacard et al., 2022b). All of these retrieval-augmented models are finetuned on the training data, either in a few-shot setting or with full training data. Specifically, Atlas is finetuned on 64 examples in the few-shot setting.\n\nOur model We add REPLUG and REPLUG LSR to Codex with Wikipedia (2018, December) as the retrieval corpus to evaluate the model in a 16-shot in context learning. Similar to the setting in language modeling and MMLU, we incorporate top-10 retrieved documents using our proposed ensemble method.\n\nResults As shown in Table 3, REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA. It outperforms the previous best\n\nFigure 4. Ensembling random documents does not result in improved performance. BPB of Curie augmented with different methods (random, REPLUG and REPLUG LSR) when varying the number of documents (i.e.; number of ensemble times.)\n\n<!-- image -->\n\nmodel, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data. This is likely due to the presence of nearduplicate test questions in the training set (e.g., Lewis et al. (2021) found that 32.5% of test questions overlap with the training sets in NQ).\n\n## 7. Analysis\n\n## 7.1. REPLUG performance gain does not simply come from the ensembling effect\n\nThe core of our method design is the use of an ensemble method that combines output probabilities of different passes, in which each retrieved document is prepended separately to the input and fed into a language model. To study whether the gains come solely from the ensemble method, we compare our method to ensembling random documents. For this, we randomly sample several documents, concatenated each random document with the input, and ensemble the outputs of different runs (referred to as \"random\"). As shown in Figure 6, we evaluated the performance of GPT-3 Curie on Pile when augmented with random documents, documents retrieved by REPLUG, and documents retrieved by REPLUG LSR. We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not solely come from the ensembling effect. Instead, ensembling the relevant documents is crucial for the success of REPLUG. Additionally, as more documents were ensembled, the performance of REPLUG and REPLUG LSR improved monotonically. However, a small number of documents (e.g., 10) was sufficient to achieve large performance gains.\n\nFigure 5. GPT-2, BLOOM and OPT models of varying sizes consistently benefit from REPLUG. The x-axis indicates the size of the language model and the y-axis is its perplexity on Wikitext-103.\n\n<!-- image -->\n\n## 7.2. REPLUG is applicable to diverse language models\n\nHere we further study whether REPLUG could enhance diverse language model families that have been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020a), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022b) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Stephen et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khandelwal et al., 2020), we use Wikitext-103 training data as the retrieval corpus.\n\nFigure 5 shows the performance of different-sized language models with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT with 125M parameters achieves 6.9% perplexity improvement, while OPT with 66B parameters achieves 5.6% perplexity improvement. Additionally, REPLUG improves the perplexity of all the model families. This indicates that REPLUG is applicable to diverse language models with different sizes.\n\n## 7.3. Qualitative Analysis: rare entities benefit from retrieval\n\nTo understand why the REPLUG improves language modeling performance, we conducted manual analysis of examples in which the REPLUG results in a decrease in perplexity. We find that REPLUG is more helpful when texts contain rare entities. Figure 6 shows a test context and its continuation from the Wikitext-103 test set. For REPLUG, we use the test context as a query to retrieve a relevant docu- ment from Wikitext-103 training data. We then compute the perplexity of the continuation using the original GPT-2 1.5B and its REPLUG enhanced version. After incorporating the retrieved document, the perplexity of the continuation improves by 11%. Among all tokens in the continuation, we found that REPLUG is most helpful for the rare entity name \"Li Bai\". This is likely because the original LM does not have sufficient information about this rare entity name. However, by incorporating the retrieved document, REPLUG was able to match the name with the relevant information in the retrieved document, resulting in better performance.\n\nFigure 6. Rare entities benefit from retrieval . After incorporating the retrieved document during inference, the entity \" Li Bai \" and the token \" greatest \" in the continuation show the most improvement in perplexity (15% for \" Li Bai \" and 5% for \" greatest \"). Other tokens' perplexity changes are within 5%.\n\n<!-- image -->\n\n## 8. Conclusion\n\nWe introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model. Our evaluation shows that REPLUG can be integrated with any existing language model to improve their performance\n\non language modeling or downstream tasks. This work opens up new possibilities for integrating retrieval into largescale black-box language models and demonstrates even the state-of-the-art large-scale LMs could benefit from retrieval. However, REPLUG lacks interpretability as it is unclear when the model relies on retrieved knowledge or parametric knowledge. Future research could focus on developing more interpretable retrieval-augmented language models.\n\n## References\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426 , 2021.\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning , pp. 2206-2240. PMLR, 2022.\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Proc. of NeurIPS , 2020b. URL https: //proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings,\n\nD., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021a. URL https://arxiv.org/abs/2107.03374 .\n\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., HerbertVoss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code. CoRR , abs/2107.03374, 2021b. URL https://arxiv.org/abs/2107.03374 .\n\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022a.\n\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022b.\n\nFajcik, M., Docekal, M., Ondrej, K., and Smrz, P. R2D2: A modular baseline for open-domain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pp. 854-870, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.73. URL https://aclanthology.org/ 2021.findings-emnlp.73 .\n\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\n\n- Goyal, A., Friesen, A., Banino, A., Weber, T., Ke, N. R., Badia, A. P., Guez, A., Mirza, M., Humphreys, P. C., Konyushova, K., et al. Retrieval-augmented reinforcement learning. In International Conference on Machine Learning , pp. 7740-7765. PMLR, 2022.\n- Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International Conference on Machine Learning , pp. 39293938. PMLR, 2020.\n- Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations , 2021. URL https: //openreview.net/forum?id=d7KBjmI3GmQ .\n- Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- Hu, Y., Hua, H., Yang, Z., Shi, W., Smith, N. A., and Luo, J. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699 , 2022.\n- Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 874-880, Online, April 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main.74 .\n- Izacard, G. and Grave, E. Leveraging passage retrieval with generative models for open domain question answering. In Proc. of EACL , 2021b. URL https://arxiv.org/ abs/2007.01282 .\n- Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research , 2022a. URL https://openreview.net/forum?id=jKN1pXi7b0 .\n- Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022b.\n- Johnson, J., Douze, M., and Jégou, H. Billion-scale similarity search with gpus. IEEE Transactions on Big Data , 7 (3):535-547, 2019.\n- Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147 .\n- Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769-6781, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 .\n- Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6769-6781, 2020b.\n- Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\n- Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. Demonstrate-search-predict: Composing retrieval and language models for knowledgeintensive nlp. arXiv preprint arXiv:2212.14024 , 2022.\n- Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n- Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466, 2019. doi: 10.1162/tacl\\_a\\_ 00276. URL https://aclanthology.org/Q19-1026 .\n- Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2020. URL https://arxiv.org/abs/2005.11401 .\n- Lewis, P., Stenetorp, P., and Riedel, S. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 1000-1008, 2021.\n\n- Mallen, A., Asai, A., Zhong, V., Das, R., Hajishirzi, H., and Khashabi, D. When not to trust language models: Investigating effectiveness and limitations of parametric and nonparametric memories. arXiv preprint arXiv:2212.10511 , 2022.\n- Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Hajishirzi, H., and Zettlemoyer, L. Nonparametric masked language modeling. arXiv preprint arXiv:2212.01349 , 2022.\n- Ni, J., Qu, C., Lu, J., Dai, Z., Ábrego, G. H., Ma, J., Zhao, V. Y., Luan, Y., Hall, K. B., Chang, M., and Yang, Y. Large dual encoders are generalizable retrievers, 2021. URL https://arxiv.org/abs/2112.07899 .\n- Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W. X., Dong, D., Wu, H., and Wang, H. RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 5835-5847, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.466. URL https: //aclanthology.org/2021.naacl-main.466 .\n- Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval , 3(4):333-389, 2009.\n- Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 2655-2671, 2022.\n- Sachan, D. S., Lewis, M., Yogatama, D., Zettlemoyer, L., Pineau, J., and Zaheer, M. Questions are all you need to train a dense passage retriever. arXiv preprint arXiv:2206.10658 , 2022.\n- Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili´ c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 , 2022.\n- Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L. Nearest neighbor zero-shot inference. 2022.\n- Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. arXiv preprint arXiv:2210.09150 , 2022.\n- Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., and Wang, L. Prompting gpt-3 to be reliable. In Proc. of ICLR , 2023. URL https://openreview.net/ forum?id=98p5x51L5af .\n- Stephen, M., Caiming, X., James, B., and Socher, R. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings , 2017.\n- Su, H., Kasai, J., Wang, Y ., Hu, Y ., Ostendorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L., Yu, T., et al. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741 , 2022.\n- Wu, S., Zhao, X., Yu, T., Zhang, R., Shen, C., Liu, H., Li, F., Zhu, H., Luo, J., Xu, L., et al. Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning. arXiv preprint arXiv:2110.04725 , 2021.\n- Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Retrieval-augmented multimodal language modeling. arXiv preprint arXiv:2211.12561 , 2022.\n- Younes Belkda, T. D. A gentle introduction to 8-bit matrix multiplication, 2022. URL https://huggingface.co/ blog/hf-bitsandbytes-integration .\n- Yu, W. Retrieval-augmented generation across heterogeneous knowledge. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop , pp. 52-58, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. naacl-srw.7. URL https://aclanthology.org/2022. naacl-srw.7 .\n- Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M. Generate rather than retrieve: Large language models are strong context generators. 2023.\n- Zhang, S., Diab, M., and Zettlemoyer, L. Democratizing access to large-scale language models with opt-175b. Meta AI , 2022a.\n- Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022b.\n- Zhong, Z., Lei, T., and Chen, D. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP) , 2022.\n\nKnowledge : Arctic Ocean. Although over half of Europe's original forests disappeared through the centuries of deforestation, Europe still has over one quarter of its land area as forest, such as the broadleaf and mixed forests, taiga of Scandinavia and Russia, mixed rainforests of the Caucasus and the Cork oak forests in the western Mediterranean. During recent times, deforestation has been slowed and many trees have been planted. However, in many cases monoculture plantations of conifers have replaced the original mixed natural forest, because these grow quicker. The plantations now cover vast areas of land, but offer poorer habitats for many European\n\nQuestion : As of 2015, since 1990 forests have in Europe and have in Africa and the Americas.\n\nA. \"increased, increased\" B. \"increased, decreased\" C. \"decreased, increased\" D. \"decreased, decreased\"\n\nAnswer : B\n\nKnowledge : Over the past decades, the political outlook of Americans has become more progressive, with those below the age of thirty being considerably more liberal than the overall population. According to recent polls, 56% of those age 18 to 29 favor gay marriage, 68% state environmental protection to be as important as job creation, 52% \"think immigrants ´ strengthen the country with their hard work and talents, ´ \" 62% favor a \"tax financed, government-administrated universal health care\" program and 74% \"say ´ people´ s will´ should have more influence on U.S. laws than the Bible, compared to 37%, 49%, 38%, 47% and 58% among the\n\nQuestion : As of 2019, about what percentage of Americans agree that the state is run for the benefit of all the people?\n\nA. 31% B. 46% C. 61% D. 76%\n\nAnswer : B\n\n...\n\nKnowledge : last week at a United Nations climate meeting in Germany, China and India should easily exceed the targets they set for themselves in the 2015 Paris Agreement... India is now expected to obtain 40 percent of its electricity from non-fossil fuel sources by 2022, eight years ahead of schedule.\" Solar power in Japan has been expanding since the late 1990s. By the end of 2017, cumulative installed PV capacity reached over 50 GW with nearly 8 GW installed in the year 2017. The country is a leading manufacturer of solar panels and is in the top 4 ranking for countries\n\nQuestion\n\n: Which of the following countries generated the most total energy from solar sources in 2019?\n\nA. China B. United States C. Germany D. Japan\n\nTable 4. Prompt for MMLU\n\nKnowledge : received 122,000 buys (excluding WWE Network views), down from the previous year´ s 199,000 buys. The event is named after the Money In The Bank ladder match, in which multiple wrestlers use ladders to retrieve a briefcase hanging above the ring. The winner is guaranteed a match for the WWE World Heavyweight Championship at a time of their choosing within the next year. On the June 2 episode of \"Raw\", Alberto Del Rio qualified for the match by defeating Dolph Ziggler. The following week, following Daniel Bryan being stripped of his WWE World Championship due to injury, Stephanie McMahon changed the\n\nQuestion : Who won the mens money in the bank match?\n\nAnswer : Braun Strowman\n\nKnowledge : in 3D on March 17, 2017. The first official presentation of the film took place at Disney´ s three-day D23 Expo in August 2015. The world premiere of \"Beauty and the Beast\" took place at Spencer House in London, England on February 23, 2017; and the film later premiered at the El Capitan Theatre in Hollywood, California, on March 2, 2017. The stream was broadcast onto YouTube. A sing along version of the film released in over 1,200 US theaters nationwide on April 7, 2017. The United Kingdom received the same version on April 21, 2017. The film was re-released in\n\nQuestion : When does beaty and the beast take place\n\nAnswer : Rococo-era\n\n...\n\nKnowledge : Love Yourself \"Love Yourself\" is a song recorded by Canadian singer Justin Bieber for his fourth studio album \"Purpose\" (2015). The song was released first as a promotional single on November 8, 2015, and later was released as the album´ s third single. It was written by Ed Sheeran, Benny Blanco and Bieber, and produced by Blanco. An acoustic pop song, \"Love Yourself\" features an electric guitar and a brief flurry of trumpets as its main instrumentation. During the song, Bieber uses a husky tone in the lower registers.\n\nLyrically, the song is a kiss-off to a narcissistic ex-lover who did\n\nQuestion\n\n: love yourself by justin bieber is about who",
  "tables": [
    {
      "index": 0,
      "markdown": "| Model       |         | # Parameters   |   Original |   + REPLUG |   Gain% |   + REPLUG LSR |   Gain% |\n|-------------|---------|----------------|------------|------------|---------|----------------|---------|\n| GPT-2       | Small   | 117M           |       1.33 |       1.26 |     5.3 |           1.21 |     9   |\n|             | Medium  | 345M           |       1.2  |       1.14 |     5   |           1.11 |     7.5 |\n|             | Large   | 774M           |       1.19 |       1.15 |     3.4 |           1.09 |     8.4 |\n|             | XL      | 1.5B           |       1.16 |       1.09 |     6   |           1.07 |     7.8 |\n| GPT-3       | Ada     | 350M           |       1.05 |       0.98 |     6.7 |           0.96 |     8.6 |\n| (black-box) | Babbage | 1.3B           |       0.95 |       0.9  |     5.3 |           0.88 |     7.4 |\n| (black-box) | Curie   | 6.7B           |       0.88 |       0.85 |     3.4 |           0.82 |     6.8 |\n| (black-box) | Davinci | 175B           |       0.8  |       0.77 |     3.8 |           0.75 |     6.3 |"
    },
    {
      "index": 1,
      "markdown": "| Model              | # Parameters   | Humanities   | Social.   | STEM   | Other   |   All |\n|--------------------|----------------|--------------|-----------|--------|---------|-------|\n| Codex              | 175B           | 74.2         | 76.9      | 57.8   | 70.1    |  68.3 |\n| PaLM               | 540B           | 77.0         | 81.0      | 55.6   | 69.6    |  69.3 |\n| Flan-PaLM          | 540B           | -            | -         | -      | -       |  72.2 |\n| Atlas              | 11B            | 46.1         | 54.6      | 38.8   | 52.8    |  47.9 |\n| Codex + REPLUG     | 175B           | 76.0         | 79.7      | 58.8   | 72.1    |  71.4 |\n| Codex + REPLUG LSR | 175B           | 76.5         | 79.9      | 58.9   | 73.2    |  71.8 |"
    },
    {
      "index": 2,
      "markdown": "|                         | NQ       | NQ   | TQA      | TQA   |\n|-------------------------|----------|------|----------|-------|\n| Model                   | Few-shot | Full | Few-shot | Full  |\n| Chinchilla              | 35.5     | -    | 64.6     | -     |\n| PaLM                    | 39.6     | -    | -        | -     |\n| Codex                   | 40.6     | -    | 73.6     | -     |\n| RETRO †                 | -        | 45.5 | -        | -     |\n| R2-D2 †                 | -        | 55.9 | -        | 69.9  |\n| Atlas †                 | 42.4     | 60.4 | 74.5     | 79.8  |\n| Codex + Contriever cc 2 | 44.2     | -    | 76.0     | -     |\n| Codex + REPLUG          | 44.7     | -    | 76.8     | -     |\n| Codex + REPLUG LSR      | 45.5     | -    | 77.3     | -     |"
    }
  ],
  "stats": {
    "pages": 12,
    "chunksCreated": 80,
    "totalCharacters": 52425,
    "totalWords": 8169,
    "numTables": 3,
    "processingTimeMs": 16683
  }
}