# 生成AI進化タイムライン

> 収集論文: 193件

## 2017年

### Alignment

- **Deep reinforcement learning from human preferences** (arXiv:1706.03741)

### Foundation

- **Attention Is All You Need** (arXiv:1706.03762)

## 2018年

### Foundation

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (arXiv:1810.04805)

## 2020年

### Efficiency

- **Scaling Laws for Neural Language Models** (arXiv:2001.08361)

### RAG

- **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (arXiv:2005.11401)
- **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension** (10.18653/v1/2020.acl-main.703)
- **AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts** (10.18653/v1/2020.emnlp-main.346)

### LLM

- **Language Models are Few-Shot Learners** (arXiv:2005.14165)
- **Array programming with NumPy** (10.1038/s41586-020-2649-2)

### Multimodal

- **Denoising Diffusion Probabilistic Models** (arXiv:2006.11239)

### Foundation

- **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** (arXiv:2010.11929)

### Evaluation

- **Beyond Accuracy: Behavioral Testing of NLP Models with CheckList** (10.18653/v1/2020.acl-main.442)
- **Consistency of a Recurrent Language Model With Respect to Incomplete Decoding** (10.18653/v1/2020.emnlp-main.448)

### Architecture

- **Unsupervised Cross-lingual Representation Learning at Scale** (10.18653/v1/2020.acl-main.747)

### Safety

- **RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models** (10.18653/v1/2020.findings-emnlp.301)

## 2021年

### Efficiency

- **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** (arXiv:2101.03961)
- **LoRA: Low-Rank Adaptation of Large Language Models** (arXiv:2106.09685)

### Multimodal

- **Learning Transferable Visual Models From Natural Language Supervision** (arXiv:2103.00020)
- **High-Resolution Image Synthesis with Latent Diffusion Models** (arXiv:2112.10752)
- **The Power of Scale for Parameter-Efficient Prompt Tuning** (10.18653/v1/2021.emnlp-main.243)

### RAG

- **Improving language models by retrieving from trillions of tokens** (arXiv:2112.04426)
- **StereoSet: Measuring stereotypical bias in pretrained language models** (10.18653/v1/2021.acl-long.416)

### Science

- **Highly accurate protein structure prediction with AlphaFold** (10.1038/s41586-021-03819-2)

### Evaluation

- **Societal Biases in Language Generation: Progress and Challenges** (10.18653/v1/2021.acl-long.330)

### Architecture

- **RoFormer: Enhanced Transformer with Rotary Position Embedding** (arXiv:2104.09864)

### LLM

- **Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation** (arXiv:2108.12409)

### Code

- **Evaluating Large Language Models Trained on Code** (arXiv:2107.03374)

## 2022年

### Reasoning

- **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** (arXiv:2201.11903)
- **ReAct: Synergizing Reasoning and Acting in Language Models** (arXiv:2210.03629)
- **Self-Instruct: Aligning Language Models with Self-Generated Instructions** (arXiv:2212.10560)
- **Solving Quantitative Reasoning Problems with Language Models** (arXiv:2206.14858)
- **Editing Models with Task Arithmetic** (arXiv:2212.04089)

### Alignment

- **Training language models to follow instructions with human feedback** (arXiv:2203.02155)
- **Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback** (arXiv:2204.05862)
- **Constitutional AI: Harmlessness from AI Feedback** (arXiv:2212.08073)
- **AdapLeR: Speeding up Inference by Adaptive Length Reduction** (10.18653/v1/2022.acl-long.1)
- **Scaling Instruction-Finetuned Language Models** (arXiv:2210.11416)

### Efficiency

- **Training Compute-Optimal Large Language Models** (arXiv:2203.15556)
- **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (arXiv:2205.14135)
- **Cross-Task Generalization via Natural Language Crowdsourcing Instructions** (10.18653/v1/2022.acl-long.244)
- **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (arXiv:2205.14135)
- **GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers** (arXiv:2210.17323)

### LLM

- **PaLM: Scaling Language Modeling with Pathways** (arXiv:2204.02311)
- **Fast Inference from Transformers via Speculative Decoding** (arXiv:2211.17192)

### Multimodal

- **Flamingo: a Visual Language Model for Few-Shot Learning** (arXiv:2204.14198)
- **Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding** (arXiv:2205.11487)

### Evaluation

- **Emergent Abilities of Large Language Models** (arXiv:2206.07682)
- **Holistic Evaluation of Language Models** (arXiv:2211.09110)

### Prompting

- **When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer** (10.18653/v1/2022.naacl-main.264)

### Agent

- **ReAct: Synergizing Reasoning and Acting in Language Models** (arXiv:2210.03629)

### Embedding

- **Text Embeddings by Weakly-Supervised Contrastive Pre-training** (arXiv:2212.03533)

### Safety

- **Constitutional AI: Harmlessness from AI Feedback** (arXiv:2212.08073)

## 2023年

### Multimodal

- **BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models** (arXiv:2301.12597)
- **Adding Conditional Control to Text-to-Image Diffusion Models** (arXiv:2302.05543)
- **Visual Instruction Tuning** (arXiv:2304.08485)
- **Visual Instruction Tuning** (arXiv:2304.08485)
- **Improved Baselines with Visual Instruction Tuning** (arXiv:2310.03744)
- **Gemini: A Family of Highly Capable Multimodal Models** (arXiv:2312.11805)
- **SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities** (arXiv:2305.11000)
- **Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM** (arXiv:2305.15255)
- **Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding** (arXiv:2306.02858)
- **AudioPaLM: A Large Language Model That Can Speak and Listen** (arXiv:2306.12925)
- **MM-VID: Advancing Video Understanding with GPT-4V(ision)** (arXiv:2310.19773)
- **Video-LLaVA: Learning United Visual Representation by Alignment Before Projection** (arXiv:2311.10122)
- **Gemini: A Family of Highly Capable Multimodal Models** (arXiv:2312.11805)

### LLM

- **LLaMA: Open and Efficient Foundation Language Models** (arXiv:2302.13971)
- **Llama 2: Open Foundation and Fine-Tuned Chat Models** (arXiv:2307.09288)
- **Mistral 7B** (arXiv:2310.06825)
- **Gemini: A Family of Highly Capable Multimodal Models** (arXiv:2312.11805)
- **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face** (arXiv:2303.17580)
- **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** (arXiv:2305.10601)
- **Orca: Progressive Learning from Complex Explanation Traces of GPT-4** (arXiv:2306.02707)
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** (arXiv:2306.05685)
- **Augmenting Language Models with Long-Term Memory** (arXiv:2306.07174)
- **Textbooks Are All You Need** (arXiv:2306.11644)
- **Lost in the Middle: How Language Models Use Long Contexts** (arXiv:2307.03172)
- **Universal and Transferable Adversarial Attacks on Aligned Language Models** (arXiv:2307.15043)
- **Large Language Models as Optimizers** (arXiv:2309.03409)
- **Textbooks Are All You Need II: phi-1.5 technical report** (arXiv:2309.05463)
- **MemGPT: Towards LLMs as Operating Systems** (arXiv:2310.08560)
- **BitNet: Scaling 1-bit Transformers for Large Language Models** (arXiv:2310.11453)
- **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection** (arXiv:2310.11511)
- **Accelerating Large Language Model Decoding with Speculative Sampling** (arXiv:2302.01318)
- **FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance** (arXiv:2305.05176)
- **TIES-Merging: Resolving Interference When Merging Models** (arXiv:2306.01708)
- **Can Large Language Models Infer Causation from Correlation?** (arXiv:2306.05836)
- **Explore, Establish, Exploit: Red Teaming Language Models from Scratch** (arXiv:2306.09442)
- **Continual Pre-Training of Large Language Models: How to (re)warm your model?** (arXiv:2308.04014)
- **Large Language Models for Compiler Optimization** (arXiv:2309.07062)
- **RAIN: Your Language Models Can Align Themselves without Finetuning** (arXiv:2309.07124)
- **Adapting Large Language Models to Domains via Reading Comprehension** (arXiv:2309.09530)
- **Compressing Context to Enhance Inference Efficiency of Large Language Models** (arXiv:2310.06201)
- **Mistral 7B** (arXiv:2310.06825)
- **Retrieve Anything To Augment Large Language Models** (arXiv:2310.07554)
- **Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch** (arXiv:2311.03099)
- **LM-Cocktail: Resilient Tuning of Language Models via Model Merging** (arXiv:2311.13534)

### Evaluation

- **GPT-4 Technical Report** (arXiv:2303.08774)
- **GPQA: A Graduate-Level Google-Proof Q&A Benchmark** (arXiv:2311.12022)

### Reasoning

- **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** (arXiv:2305.10601)
- **Let's Verify Step by Step** (arXiv:2305.20050)
- **Reasoning with Language Model is Planning with World Model** (arXiv:2305.14992)
- **WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct** (arXiv:2308.09583)
- **MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning** (arXiv:2309.05653)
- **MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models** (arXiv:2309.12284)
- **Llemma: An Open Language Model For Mathematics** (arXiv:2310.10631)
- **Orca 2: Teaching Small Language Models How to Reason** (arXiv:2311.11045)

### Alignment

- **Instruction Tuning with GPT-4** (arXiv:2304.03277)
- **LIMA: Less Is More for Alignment** (arXiv:2305.11206)
- **Direct Preference Optimization: Your Language Model is Secretly a Reward Model** (arXiv:2305.18290)
- **The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only** (arXiv:2306.01116)
- **Instruction Tuning for Large Language Models: A Survey** (arXiv:2308.10792)
- **A General Theoretical Paradigm to Understand Learning from Human Preferences** (arXiv:2310.12036)
- **WizardLM: Empowering large pre-trained language models to follow complex instructions** (arXiv:2304.12244)
- **Enhancing Chat Language Models by Scaling High-quality Instructional Conversations** (arXiv:2305.14233)
- **Self-Alignment with Instruction Backtranslation** (arXiv:2308.06259)
- **Zephyr: Direct Distillation of LM Alignment** (arXiv:2310.16944)

### Agent

- **Voyager: An Open-Ended Embodied Agent with Large Language Models** (arXiv:2305.16291)
- **ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs** (arXiv:2307.16789)
- **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation** (arXiv:2308.08155)
- **Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models** (arXiv:2310.04406)
- **Improving Factuality and Reasoning in Language Models through Multiagent Debate** (arXiv:2305.14325)

### Efficiency

- **AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration** (arXiv:2306.00978)
- **FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning** (arXiv:2307.08691)
- **YaRN: Efficient Context Window Extension of Large Language Models** (arXiv:2309.00071)
- **Efficient Memory Management for Large Language Model Serving with PagedAttention** (arXiv:2309.06180)
- **QLoRA: Efficient Finetuning of Quantized LLMs** (arXiv:2305.14314)
- **MiniLLM: Knowledge Distillation of Large Language Models** (arXiv:2306.08543)
- **Efficient Memory Management for Large Language Model Serving with PagedAttention** (arXiv:2309.06180)
- **FlashDecoding++: Faster Large Language Model Inference on GPUs** (arXiv:2311.01282)

### Architecture

- **Extending Context Window of Large Language Models via Positional Interpolation** (arXiv:2306.15595)
- **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** (arXiv:2312.00752)

### Safety

- **Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!** (arXiv:2310.03693)
- **Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation** (arXiv:2310.06987)
- **Jailbreaking Black Box Large Language Models in Twenty Queries** (arXiv:2310.08419)

### RAG

- **Retrieval-Augmented Generation for Large Language Models: A Survey** (arXiv:2312.10997)
- **REPLUG: Retrieval-Augmented Black-Box Language Models** (arXiv:2301.12652)
- **Query Rewriting for Retrieval-Augmented Large Language Models** (arXiv:2305.14283)
- **Making Retrieval-Augmented Language Models Robust to Irrelevant Context** (arXiv:2310.01558)

### Prompting

- **A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT** (arXiv:2302.11382)
- **Learning to Compress Prompts with Gist Tokens** (arXiv:2304.08467)
- **Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution** (arXiv:2309.16797)

### Code

- **StarCoder: may the source be with you!** (arXiv:2305.06161)
- **The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models** (arXiv:2308.00245)
- **OctoPack: Instruction Tuning Code Large Language Models** (arXiv:2308.07124)
- **Code Llama: Open Foundation Models for Code** (arXiv:2308.12950)

### Training

- **Llama 2: Open Foundation and Fine-Tuned Chat Models** (arXiv:2307.09288)

### Embedding

- **Improving Text Embeddings with Large Language Models** (arXiv:2401.00368)

## 2024年

### LLM

- **Mixtral of Experts** (arXiv:2401.04088)
- **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning** (arXiv:2401.01325)
- **Mixtral of Experts** (arXiv:2401.04088)
- **Self-Rewarding Language Models** (arXiv:2401.10020)
- **The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits** (arXiv:2402.17764)
- **Gemma: Open Models Based on Gemini Research and Technology** (arXiv:2403.08295)
- **Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone** (arXiv:2404.14219)
- **The Llama 3 Herd of Models** (arXiv:2407.21783)
- **TinyLlama: An Open-Source Small Language Model** (arXiv:2401.02385)
- **DeepSeek LLM: Scaling Open-Source Language Models with Longtermism** (arXiv:2401.02954)
- **Probabilistic Mobility Load Balancing for Multi-band 5G and Beyond Networks** (arXiv:2401.13792)
- **MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases** (arXiv:2402.14905)
- **Yi: Open Foundation Models by 01.AI** (arXiv:2403.04652)
- **Qwen2 Technical Report** (arXiv:2407.10671)

### RAG

- **From Local to Global: A Graph RAG Approach to Query-Focused Summarization** (arXiv:2404.16130)
- **RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval** (arXiv:2401.18059)
- **RAG-Fusion: a New Take on Retrieval-Augmented Generation** (arXiv:2402.03367)
- **A Survey on Retrieval-Augmented Text Generation for Large Language Models** (arXiv:2404.10981)
- **RAFT: Adapting Language Model to Domain Specific RAG** (arXiv:2403.10131)

### Efficiency

- **MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts** (arXiv:2401.04081)
- **Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality** (arXiv:2405.21060)
- **Implicit Diffusion: Efficient Optimization through Stochastic Sampling** (arXiv:2402.05468)
- **A Survey on Efficient Inference for Large Language Models** (arXiv:2404.14294)
- **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model** (arXiv:2405.04434)

### Architecture

- **DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models** (arXiv:2401.06066)
- **Scaling Laws for Fine-Grained Mixture of Experts** (arXiv:2402.07871)

### Alignment

- **KTO: Model Alignment as Prospect Theoretic Optimization** (arXiv:2402.01306)
- **ORPO: Monolithic Preference Optimization without Reference Model** (arXiv:2403.07691)
- **Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive** (arXiv:2402.13228)

### Reasoning

- **Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation** (arXiv:2402.03268)
- **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** (arXiv:2402.03300)
- **Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step** (arXiv:2402.16906)

### Multimodal

- **World Model on Million-Length Video And Language With Blockwise RingAttention** (arXiv:2402.08268)
- **Efficient Multimodal Learning from Data-centric Perspective** (arXiv:2402.11530)
- **MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training** (arXiv:2403.09611)
- **Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context** (arXiv:2403.05530)

### Agent

- **Data Interpreter: An LLM Agent For Data Science** (arXiv:2402.18679)
- **ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools** (arXiv:2406.12793)

### Evaluation

- **RewardBench: Evaluating Reward Models for Language Modeling** (arXiv:2403.13787)

### Code

- **DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence** (arXiv:2401.14196)
- **StarCoder 2 and The Stack v2: The Next Generation** (arXiv:2402.19173)
- **Long Code Arena: a Set of Benchmarks for Long-Context Code Models** (arXiv:2406.11612)

### Prompting

- **On Prompt-Driven Safeguarding for Large Language Models** (arXiv:2401.18018)

### Embedding

- **Multilingual E5 Text Embeddings: A Technical Report** (arXiv:2402.05672)

### Training

- **Model Stock: All we need is just a few fine-tuned models** (arXiv:2403.19522)
- **Spectra: Surprising Effectiveness of Pretraining Ternary Language Models at Scale** (arXiv:2407.12327)

