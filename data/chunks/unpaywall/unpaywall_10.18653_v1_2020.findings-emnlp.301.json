{
  "doi": "10.18653/v1/2020.findings-emnlp.301",
  "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
  "category": "Safety",
  "year": 2020,
  "paper": {
    "doi": "10.18653/v1/2020.findings-emnlp.301",
    "doi_url": "https://doi.org/10.18653/v1/2020.findings-emnlp.301",
    "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
    "genre": "proceedings-article",
    "is_paratext": false,
    "published_date": "2020-01-01",
    "year": 2020,
    "journal_name": "Findings of the Association for Computational Linguistics: EMNLP 2020",
    "journal_issns": null,
    "journal_issn_l": null,
    "journal_is_oa": false,
    "journal_is_in_doaj": false,
    "publisher": "Association for Computational Linguistics",
    "is_oa": true,
    "oa_status": "gold",
    "has_repository_copy": true,
    "best_oa_location": {
      "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
      "url_for_pdf": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
      "url_for_landing_page": "https://doi.org/10.18653/v1/2020.findings-emnlp.301",
      "evidence": "deprecated",
      "license": "cc-by",
      "version": "publishedVersion",
      "host_type": null,
      "is_best": true,
      "pmh_id": null,
      "endpoint_id": null,
      "repository_institution": null,
      "oa_date": "2020-01-01",
      "updated": "deprecated"
    },
    "first_oa_location": {
      "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
      "url_for_pdf": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
      "url_for_landing_page": "https://doi.org/10.18653/v1/2020.findings-emnlp.301",
      "evidence": "deprecated",
      "license": "cc-by",
      "version": "publishedVersion",
      "host_type": null,
      "is_best": true,
      "pmh_id": null,
      "endpoint_id": null,
      "repository_institution": null,
      "oa_date": "2020-01-01",
      "updated": "deprecated"
    },
    "oa_locations": [
      {
        "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
        "url_for_pdf": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf",
        "url_for_landing_page": "https://doi.org/10.18653/v1/2020.findings-emnlp.301",
        "evidence": "deprecated",
        "license": "cc-by",
        "version": "publishedVersion",
        "host_type": null,
        "is_best": true,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": null,
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      },
      {
        "url": "https://arxiv.org/pdf/2009.11462",
        "url_for_pdf": null,
        "url_for_landing_page": "https://arxiv.org/pdf/2009.11462",
        "evidence": "deprecated",
        "license": null,
        "version": "submittedVersion",
        "host_type": "repository",
        "is_best": false,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": "Cornell University",
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      },
      {
        "url": "https://doi.org/10.48550/arxiv.2009.11462",
        "url_for_pdf": null,
        "url_for_landing_page": "https://doi.org/10.48550/arxiv.2009.11462",
        "evidence": "deprecated",
        "license": null,
        "version": null,
        "host_type": "repository",
        "is_best": false,
        "pmh_id": null,
        "endpoint_id": null,
        "repository_institution": "Cornell University",
        "oa_date": "2020-01-01",
        "updated": "deprecated"
      }
    ],
    "oa_locations_embargoed": [],
    "data_standard": 2,
    "z_authors": [
      {
        "author_position": "first",
        "raw_author_name": "Samuel Gehman",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "University of Washington ;"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Suchin Gururangan",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "University of Washington ;"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Maarten Sap",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "University of Washington ;"
        ]
      },
      {
        "author_position": "middle",
        "raw_author_name": "Yejin Choi",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "University of Washington ;"
        ]
      },
      {
        "author_position": "last",
        "raw_author_name": "Noah A. Smith",
        "is_corresponding": false,
        "raw_affiliation_strings": [
          "University of Washington ;"
        ]
      }
    ],
    "updated": "2025-12-01T22:21:40Z"
  },
  "chunks": [
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-0",
      "content": "## REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models\n\nSamuel Gehman /diamondmath Suchin Gururangan /diamondmath Maarten Sap /diamondmath Yejin Choi /diamondmath Noah A. Smith /diamondmath\n\n/diamondmath Paul G. Allen School of Computer Science &amp; Engineering, University of Washington Allen Institute for Artificial Intelligence\n\nSeattle, USA\n\n¶ sgehman,sg01,msap,yejin,nasmith ♦ @cs.washington.edu\n\n## Abstract",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 0
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-1",
      "content": "Intelligence Seattle, USA ¶ sgehman,sg01,msap,yejin,nasmith ♦ @cs.washington.edu ## Abstract \n\nPretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release REALTOXICITYPROMPTS, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widelyused toxicity classifier. Using REALTOXICITYPROMPTS, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning 'bad' words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et al., 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 1
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-2",
      "content": "GPT-2; Radford et al., 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining. \n\n## 1 Introduction\n\nAlthough they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 2
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-3",
      "content": "2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their \n\nFigure 1: Non-toxic examples from REALTOXICITYPROMPTS, a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by PERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E).\n\n<!-- image -->\n\nsafe deployment (McGuffie and Newhouse, 2020).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 3
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-4",
      "content": "Table 17 in Appendix §E). <!-- image --> safe deployment (McGuffie and Newhouse, 2020). \n\nWe first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release REALTOXICITYPROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (PERSPECTIVE API). We show that popular LMs produce toxic generations when conditioned on our prompts, even those that are non-toxic (§4.2).\n\nThen, as a possible mitigation strategy, we evaluate controllable generation methods and quantify their ability to steer away from toxic content using REALTOXICITYPROMPTS (§5). We find that certain controllable methods (e.g., toxicity control tokens, swearword filters) are less successful than",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 4
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-5",
      "content": "to steer away from toxic content using REALTOXICITYPROMPTS (§5). We find that certain controllable methods (e.g., toxicity control tokens, swearword filters) are less successful than \n\nmore computationally or data-intensive methods (e.g., finetuning on non-toxic corpora). However, we show that even our best steering methods can still generate highly toxic content.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 5
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-6",
      "content": "best steering methods can still generate highly toxic content. \n\nFinally, to further investigate the potential cause of these phenomena, we present the first largescale analysis of toxicity in GPT-2's training corpus, OpenAI WebText, (OPENAI-WT; Radford et al., 2019), as well as an in-depth analysis of its open-source replica, OPENWEBTEXT CORPUS (OWTC; Gokaslan and Cohen, 2019, §6). We find non-negligible amounts of toxic, harmful, and abusive text in these corpora, which were used in pretraining of several language models (including RoBERTa, CTRL, and GPT-2; Liu et al., 2019; Keskar et al., 2019, §6.1). We identify additional issues with the data and its provenance, including large numbers of news articles shared on banned Internet communities or from factually unreliable sources (§6.2).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 6
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-7",
      "content": "with the data and its provenance, including large numbers of news articles shared on banned Internet communities or from factually unreliable sources (§6.2). \n\nOur findings highlight the difficulty of avoiding toxicity in natural language generation (NLG) and illustrate a need to actively reconsider the content used in LM pretraining. We release our code and data for tracking the progress towards combating the critical issue of neural toxic degeneration. 1 ↪ 2\n\n## 2 Operationalizing Toxicity",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 7
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-8",
      "content": "the critical issue of neural toxic degeneration. 1 ↪ 2 ## 2 Operationalizing Toxicity \n\nCharacterizing the toxicity of large corpora of naturally occurring or machine generated text is crucial to understanding toxic degeneration by language models. Unfortunately, such large scale prevents human annotations of toxicity (e.g., we score at least 80 GB of text in §6). Therefore, we rely on PERSPECTIVE API 3 , an automated tool for toxic language and hate speech detection. We acknowledge, however, that such tools are imperfect and subject to a variety of biases, as discussed in §2.2 and §7.\n\n## 2.1 PERSPECTIVE API TOXICITY\n\nWe use the TOXICITY 4 score from PERSPECTIVE API, a widely used, commercially deployed toxic-\n\n1 Due to their prevalence, we focus our study only on neural language models, and therefore use the term 'neural toxic degeneration.' Future work could examine whether non-neural language models exhibit similar behavior.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 8
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-9",
      "content": "we focus our study only on neural language models, and therefore use the term 'neural toxic degeneration.' Future work could examine whether non-neural language models exhibit similar behavior. \n\n2 http://toxicdegeneration.allenai.org/ 3 https://github.com/conversationai/ perspectiveapi\n\n4 PERSPECTIVE API defines TOXICITY as a 'rude, disrespectful, or unreasonable comment; likely to make people leave a discussion.'\n\nity detection tool. Accessed through an API, TOXICITY corresponds to the prediction output of a CNN (Lecun et al., 1998) trained on a proprietary corpus of comments from Wikipedia , New York Times , and other news sites with an AUC of 0.97. Since the model is calibrated using isotonic regression (Zadrozny and Elkan, 2002), 5 we can meaningfully interpret the score as a probability of toxicity. In our analyses, we label a prompt as toxic if it has TOXICITY ≥ 0.5, and non-toxic otherwise. 6\n\n## 2.2 Biases in Toxic Language Detection",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 9
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-10",
      "content": "of toxicity. In our analyses, we label a prompt as toxic if it has TOXICITY ≥ 0.5, and non-toxic otherwise. 6 ## 2.2 Biases in Toxic Language Detection \n\nAlthough widely used, the PERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., 'I'm a gay man'; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors' over-reliance on lexical cues of toxicity (including swearwords, slurs, and other 'bad' words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 10
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-11",
      "content": "examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). \n\n## 3 Out-of-the-Box Generation Toxicity\n\nWe focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al., 2017) language models: GPT-1,\n\n5 https://github.com/conversationai/ perspectiveapi/blob/master/3-concepts/ score-normalization.md",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 11
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-12",
      "content": "Transformer-based (Vaswani et al., 2017) language models: GPT-1, 5 https://github.com/conversationai/ perspectiveapi/blob/master/3-concepts/ score-normalization.md \n\n6 To assess PERSPECTIVE API on human-generated text, the first three authors performed manual judgments of toxicity of a sample of 100 documents from OWTC, and found an 88% pairwise agreement (Pearson ρ =0.83) with TOXICITY scores. To assess the API on machine-generated text, among 100 generations from GPT-2, our judgments had 80% pairwise agreement and Pearson ρ =0.65 with TOXICITY. For further model information, we refer the reader to the model card for TOX-\n\nICITY: https://github.com/conversationai/ perspectiveapi/blob/master/2-api/modelcards/English/toxicity.md",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 12
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-13",
      "content": "For further model information, we refer the reader to the model card for TOX- ICITY: https://github.com/conversationai/ perspectiveapi/blob/master/2-api/modelcards/English/toxicity.md \n\nGPT-2, GPT-3, CTRL, and CTRL-WIKI. GPT-1 (Radford et al., 2018) is a 117M-parameter model pretrained on a large corpus of English books (Zhu et al., 2015). GPT-2 (specifically, GPT-2-small; Radford et al., 2019), is a similarly sized model pretrained on OPENAI-WT, which contains 40GB of English web text and is described in §6. 7 GPT-3 (Brown et al., 2020) is pretrained on a mix of Common Crawl, an expanded version of OPENAI-WT, books corpora, and Wikipedia. 8 In all experiments, we use the 175B parameter GPT-3 model, also known as DA VINCI in the OpenAI API.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 13
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-14",
      "content": "Wikipedia. 8 In all experiments, we use the 175B parameter GPT-3 model, also known as DA VINCI in the OpenAI API. \n\nCTRL (Keskar et al., 2019) is a 1.63B parameter model that uses domain-specific control tokens for conditional language modelling. We analyze generations in two domains: web text (CTRL, Links control token), and English Wikipedia (CTRLWIKI, Wiki control token).\n\nGenerating from Models Unless otherwise noted, we use nucleus sampling (Holtzman et al., 2020) with p = 0.9 to generate up to 20 tokens (see Appendix §B.4 for additional details). All experiments are carried out with the Hugging Face Transformers library (Wolf et al., 2019).\n\n## 3.1 Unprompted Toxicity in Neural Models",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 14
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-15",
      "content": "experiments are carried out with the Hugging Face Transformers library (Wolf et al., 2019). ## 3.1 Unprompted Toxicity in Neural Models \n\nTo quantify the risk associated with using pretrained language models for generation, we first measure their propensity to generate toxic output conditioned only on their respective start-ofsentence tokens . 9 For each model, we first generate a pool of 10K spans, and then perform bootstrap estimation of the expected maximum toxicity for n ≤ 10K generations, by sampling (with replacement) n generations from the pool 1K times each.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 15
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-16",
      "content": "for n ≤ 10K generations, by sampling (with replacement) n generations from the pool 1K times each. \n\nOur results (Figure 2) show that all five language models can degenerate into toxicity of over 0.5 within 100 generations, and most only require 1K generations to exceed a maximum toxicity of 0.9 (see Table 15 and 16 in Appendix §E for examples). We find similar patterns of expected maximum toxicity for GPT-2 and CTRL, which have significantly more overlap in pretraining data than with GPT-1. Though trained on a much larger corpus, GPT-3's unprompted toxicity also mirrors\n\n7 We find similar toxic behavior in GPT-2-small and GPT2-medium, see Appendix §B.7 for details.\n\n8 We access the GPT-3 model through OpenAI's API ( https://openai.com/api/ ).\n\n9 For CTRL and CTRL-WIKI, we use the Links and Wiki control tokens; for GPT-2 and GPT-3, we use the &lt;|endoftext|&gt; token; for GPT-1, we use '. '.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 16
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-17",
      "content": "). 9 For CTRL and CTRL-WIKI, we use the Links and Wiki control tokens; for GPT-2 and GPT-3, we use the &lt;|endoftext|&gt; token; for GPT-1, we use '. '. \n\nFigure 2: Neural models generate toxicity, even with no prompting. Here we display bootstrap estimates of the expected maximum toxicity for N generations, with variance bounds as shades. For example, we observe that GPT-2 generates an expected maximum toxicity of 0.65 with just 100 unprompted generations.\n\n<!-- image -->",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 17
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-18",
      "content": "an expected maximum toxicity of 0.65 with just 100 unprompted generations. <!-- image --> \n\nthat of GPT-2, which may be due to the fact that GPT-3's training data was designed to be similar to GPT-2's training data (Brown et al., 2020). On the other hand, GPT-1 generates higher levels of expected toxicity with fewer generations. This may be explained by the correspondingly high levels of toxicity in GPT-1's pretraining corpus (see Appendix §D.3 for details). We also observe that CTRL-WIKI has a significantly lower expected maximum toxicity than the other models. These results suggest that models acquire toxicity from their pretraining data, which we analyze further in §6.\n\n## 4 REALTOXICITYPROMPTS",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 18
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-19",
      "content": "models. These results suggest that models acquire toxicity from their pretraining data, which we analyze further in §6. ## 4 REALTOXICITYPROMPTS \n\nTo systematically evaluate and compare the generations from language models, we create REALTOXICITYPROMPTS as a testbed for toxicity in conditional language generation that mirrors real world applications (e.g., autocomplete systems; Chen et al., 2019; King, 2019). With this dataset, we quantify the effect of prompt toxicity on the toxicity of generation from our five language models.\n\n## 4.1 Prompt Creation and Selection\n\nWe select our prompts from sentences in the OPENWEBTEXT CORPUS (Gokaslan and Cohen, 2019), a large corpus of English web text scraped from\n\nTable 1: Data statistics of prompts and continuations in REALTOXICITYPROMPTS.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 19
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-20",
      "content": "and Cohen, 2019), a large corpus of English web text scraped from Table 1: Data statistics of prompts and continuations in REALTOXICITYPROMPTS. \n\n|               | REALTOXICITYPROMPTS                    | REALTOXICITYPROMPTS                          |\n|---------------|----------------------------------------|----------------------------------------------|\n| # Prompts     | Toxic 21,744                           | Non-Toxic 77,272                             |\n| # Tokens      | Prompts 11.7 4 glyph[triangleright] 2  | Continuations 12.0 4 glyph[triangleright] 2  |\n| Avg. Toxicity | Prompts 0.29 0 glyph[triangleright] 27 | Continuations 0.38 0 glyph[triangleright] 31 |\n\nTable 2: Toxicity of generations conditioned on REALTOXICITYPROMPTS. Left: Expected maximum toxicity (with standard deviations as subscripts) over 25 generations. Right: The empirical probability of generating toxic text at least once over 25 generations.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 20
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-21",
      "content": "deviations as subscripts) over 25 generations. Right: The empirical probability of generating toxic text at least once over 25 generations. \n\n|        | Exp. Max. Toxicity             | Exp. Max. Toxicity             | Toxicity Prob.   | Toxicity Prob.   |\n|--------|--------------------------------|--------------------------------|------------------|------------------|\n| Model  | Toxic                          | Non-Toxic                      | Toxic            | Non-Toxic        |\n| GPT-1  | 0.78 0 glyph[triangleright] 18 | 0.58 0 glyph[triangleright] 22 | 0.90             | 0.60             |\n| GPT-2  | 0.75 0 glyph[triangleright] 19 | 0.51 0 glyph[triangleright] 22 | 0.88             | 0.48             |\n| GPT-3  | 0.75 0 glyph[triangleright] 20 | 0.52 0 glyph[triangleright] 23 | 0.87             | 0.50             |\n| CTRL   | 0.73 0 glyph[triangleright] 20 | 0.52 0 glyph[triangleright] 21 | 0.85             | 0.50             |\n| CTRL-W | 0.71 0 glyph[triangleright] 20 | 0.49 0 glyph[triangleright] 21 | 0.82             | 0.44             |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 21
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-22",
      "content": "0 glyph[triangleright] 21 | 0.85 | 0.50 | | CTRL-W | 0.71 0 glyph[triangleright] 20 | 0.49 0 glyph[triangleright] 21 | 0.82 | 0.44 | \n\noutbound URLs from Reddit, for which we extract TOXICITY scores with PERSPECTIVE API. To obtain a stratified range of prompt toxicity, 10 we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1]), for a total of 100K sentences. We then split sentences in half, yielding a prompt and a continuation , both of which we also score for toxicity. We include further preprocessing details in Appendix §A.\n\nOur final dataset includes 100K naturally occurring prompts, which average 11.7 ◦ 4.2 tokens in length (Table 1). REALTOXICITYPROMPTS contains 22K prompts with TOXICITY ≥ 0.5 (i.e., toxic prompts ). We find that prompt and continuation toxicity are slightly anti-correlated ( r = -0.08, p ≤ 0.001), indicating that, in our documents, toxicity as measured by PERSPECTIVE API is usually confined to one half of the sentence.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 22
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-23",
      "content": "toxicity are slightly anti-correlated ( r = -0.08, p ≤ 0.001), indicating that, in our documents, toxicity as measured by PERSPECTIVE API is usually confined to one half of the sentence. \n\n## 4.2 Prompted Toxicity in Neural Models\n\nUsing REALTOXICITYPROMPTS and the same generation procedures outlined in §3, we measure toxic degeneration in out-of-the-box neural language models. We characterize toxicity in prompted generations with two metrics: 1) the expected maxi-\n\n10 Oversampling toxicity is necessary since it is a relatively rare phenomenon online (Founta et al., 2018).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 23
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-24",
      "content": "Oversampling toxicity is necessary since it is a relatively rare phenomenon online (Founta et al., 2018). \n\nmum toxicity over k = 25 generations, which we estimate with a mean and standard deviation; and 2) the empirical probability of generating a span with TOXICITY ≥ 0.5 at least once over k = 25 generations. These metrics characterize toxic generations along two axes: the higher the expected maximum toxicity, the more toxic we expect the worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 24
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-25",
      "content": "worst-case generations to be, and the higher the toxicity probability, the more frequently the model generates toxicity. \n\nOur results show that while toxic prompts unsurprisingly yield higher toxicity in generations, nontoxic prompts still can still cause toxic generations at non-trivial rates (Table 2). Specifically, all five models have a toxicity probability near or above 0.5 for non-toxic prompts. This shows that even in innocuous contexts these models can still generate toxic content (as illustrated in Table 17 and 18 in Appendix §E), suggesting the need for models to 'unlearn' toxicity. Surprisingly, even CTRL-WIKI has similar generation toxicity to other models in prompted settings, even though it was trained on just Wikipedia. These results suggest that like the provenance of pretraining data (§3.1), prompt context can heavily influence generation toxicity, and that steering generations after pretraining is crucial to prevent toxic behavior in language models. In the following section, we explore the effectiveness of a variety of such methods to avoid toxicity.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 25
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-26",
      "content": "that steering generations after pretraining is crucial to prevent toxic behavior in language models. In the following section, we explore the effectiveness of a variety of such methods to avoid toxicity. \n\n## 5 Detoxifying Generations\n\nWe investigate the effectiveness of recent controllable generation methods at steering away from toxicity using REALTOXICITYPROMPTS. Specifically, we focus on GPT-2 as a base model for two detoxification techniques: data-based , where we pretrain the language model further, and decoding-based where we only change the generation strategy without changing model parameters. 11 As described in §4.2, we sample 25 generations per prompt for each model. We describe hyperparameters and training details for all methods in Appendix §B.\n\n## 5.1 Data-Based Detoxification\n\nWe consider two types of data-based detoxification in which we continue pretraining on approximately 150K documents from OWTC. 12",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 26
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-27",
      "content": "in Appendix §B. ## 5.1 Data-Based Detoxification We consider two types of data-based detoxification in which we continue pretraining on approximately 150K documents from OWTC. 12 \n\n11 We confirm that our detoxified models are still reasonable language models in terms of perplexity in Table 10, Appendix §B.6.\n\n12 Described in Appendix §B.3, our training corpora are fully disjoint from the prompts data.\n\nTable 3: Left: Average maximum toxicity (with standard deviations as subscripts) over 25 generations. Right: The empirical probability of generating toxic text at least once over 25 generations. The best performing detoxification method yielding the lowest toxicity per-category, is bolded. We display DAPT (Toxic) as a reference for the effectiveness of DAPT as a method of controlling LM behavior. All models are evaluated on a full dataset of 100K prompts, except PPLM, which is evaluated on a dataset of 10K prompts, due to computational budget.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 27
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-28",
      "content": "LM behavior. All models are evaluated on a full dataset of 100K prompts, except PPLM, which is evaluated on a dataset of 10K prompts, due to computational budget. \n\n|                |                                     | Exp. Max. Toxicity                                                                           | Exp. Max. Toxicity                                                                           | Exp. Max. Toxicity                                                                           | Toxicity Prob.   | Toxicity Prob.   | Toxicity Prob.   |\n|----------------|-------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------|------------------|------------------|\n| Category       | Model                               | Unprompted                                                                                   | Toxic                                                                                        | Non-Toxic                                                                                    | Unprompted       | Toxic            | Non-Toxic        |\n| Baseline       | GPT-2                               | 0.44 0 glyph[triangleright] 17                                                               | 0.75 0 glyph[triangleright] 19                                                               | 0.51 0 glyph[triangleright] 22                                                               | 0.33             | 0.88             | 0.48             |\n| Data-based     | DAPT (Non-Toxic) DAPT (Toxic) ATCON | 0.30 0 glyph[triangleright] 13 0.80 0 glyph[triangleright] 16 0.42 0 glyph[triangleright] 17 | 0.57 0 glyph[triangleright] 23 0.85 0 glyph[triangleright] 15 0.73 0 glyph[triangleright] 20 | 0.37 0 glyph[triangleright] 19 0.69 0 glyph[triangleright] 23 0.49 0 glyph[triangleright] 22 | 0.09 0.93 0.26   | 0.59 0.96 0.84   | 0.23 0.77 0.44   |\n| Decoding-based | VOCAB-SHIFT PPLM WORD FILTER        | 0.43 0 glyph[triangleright] 18 0.28 0 glyph[triangleright] 11 0.42 0 glyph[triangleright] 16 | 0.70 0 glyph[triangleright] 21 0.52 0 glyph[triangleright] 26 0.68 0 glyph[triangleright] 19 | 0.46 0 glyph[triangleright] 22 0.32 0 glyph[triangleright] 19 0.48 0 glyph[triangleright] 20 | 0.31 0.05 0.27   | 0.80 0.49 0.81   | 0.39 0.17 0.43   |",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 28
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-29",
      "content": "0 glyph[triangleright] 16 | 0.70 0 glyph[triangleright] 21 0.52 0 glyph[triangleright] 26 0.68 0 glyph[triangleright] 19 | 0.46 0 glyph[triangleright] 22 0.32 0 glyph[triangleright] 19 0.48 0 glyph[triangleright] 20 | 0.31 0.05 0.27 | 0.80 0.49 0.81 | 0.39 0.17 0.43 | \n\nDomain-Adaptive Pretraining (DAPT) Using the framework outlined in Gururangan et al. (2020), we perform an additional phase of pretraining on the non-toxic subset of a balanced corpus with GPT-2. For comparison, we also perform the experiment using the toxic subset.\n\nAttribute Conditioning (ATCON) Inspired by Ficler and Goldberg (2017) and Keskar et al. (2019), we prepend a corresponding toxicity attribute token ( &lt;|toxic|&gt; , &lt;|nontoxic|&gt; ) to a random sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the &lt;|nontoxic|&gt; token to our prompts.\n\n## 5.2 Decoding-Based Detoxification",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 29
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-30",
      "content": "sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the &lt;|nontoxic|&gt; token to our prompts. ## 5.2 Decoding-Based Detoxification \n\nNoting the additional cost of training language models further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by many practitioners.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 30
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-31",
      "content": "algorithm and are therefore more readily usable by many practitioners. \n\nVocabulary Shifting (VOCAB-SHIFT) Inspired by Eisenstein et al. (2011) and Ghosh et al. (2017), we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2's vocabulary, which we then use to boost the likelihood of non-toxic tokens. Given the language model's unnormalized probability (logits) over the vocabulary, we add the term β W · t , where t ∈ R 2 encodes (non-)toxicity, and W ∈ R V represents the associations between each token and (non-)toxicity, and β is the boosting strength. We set β = 3 for all experiments. We learn this representation using the toxicity labels on the balanced corpus described in §5.1 (See Appendix §B.3 for more details).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 31
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-32",
      "content": "for all experiments. We learn this representation using the toxicity labels on the balanced corpus described in §5.1 (See Appendix §B.3 for more details). \n\nWord Filtering (WORD FILTER) We also implement a language model blocklist, disallowing a set of words from being generated by GPT-2. We set the probability of generating any word from a list 13 of profanity, slurs, and swearwords to zero.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 32
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-33",
      "content": "word from a list 13 of profanity, slurs, and swearwords to zero. \n\nPPLM We use the recently released PPLM (Dathathri et al., 2020). This decoding method operates on GPT-2 by altering the past and present hidden representations to better reflect the desired attributes, using gradients from a discriminator (see Dathathri et al., 2020, for further details). In our experiments, we steer generations using the toxicity classifier released by the authors and the Hugging Face implementation. For PPLM, we only sample 10 generations per prompt, and evaluate with 10K prompts total, due to this decoding strategy being extremely computationally intensive (14 sec/generation, vs. 0.2 sec for GPT-2).\n\n## 5.3 Effect of Controllable Solutions on Generation Toxicity",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 33
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-34",
      "content": "decoding strategy being extremely computationally intensive (14 sec/generation, vs. 0.2 sec for GPT-2). ## 5.3 Effect of Controllable Solutions on Generation Toxicity \n\nWe investigate the effectiveness of our detoxification methods under REALTOXICITYPROMPTS, following the same generation procedures and experimental setups outlined in §4. Listed in Table 3, our results show that steering does not completely solve neural toxic degeneration, though all proposed techniques do reduce toxic behavior in GPT-2. Of all methods, DAPT (Non-Toxic), vocabulary shifting, and PPLM yield the lowest toxicity in generation. Despite its simplicity, DAPT (Non-Toxic) is one of the most effective methods for steering away from\n\n13 List of Dirty, Naughty, Obscene, and Otherwise Bad Words , downloaded from https://github.com/ LDNOOBW/List-of-Dirty-Naughty-Obsceneand-Otherwise-Bad-Words .\n\ntoxicity, highlighting the importance of pretraining data in neural toxic degeneration.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 34
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-35",
      "content": "Dirty, Naughty, Obscene, and Otherwise Bad Words , downloaded from https://github.com/ LDNOOBW/List-of-Dirty-Naughty-Obsceneand-Otherwise-Bad-Words . toxicity, highlighting the importance of pretraining data in neural toxic degeneration. \n\nPrompts That Challenge All Models We find that certain prompts consistently cause all models to generate toxicity (e.g., the four prompts in Figure 1). Specifically, there are 327 prompts that yielded at least one generation with 0.9 TOXICITY from all models, and 1,225 prompts when considering only the out-of-the-box language models (i.e., GPT-1, GPT-2, GPT-3, CTRL, CTRL-WIKI). 14 From qualitative investigations, these prompts tended to either be toxic themselves, or if innocuous, they contain opening quotes or prefixes of multiword expressions such as 'full of-' (Figure 1). Additionally, we find that at least 10% of those 1.2K come from factually unreliable news sources or appear in banned or quarantined subreddits.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 35
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-36",
      "content": "of-' (Figure 1). Additionally, we find that at least 10% of those 1.2K come from factually unreliable news sources or appear in banned or quarantined subreddits. \n\n## 6 Analyzing Toxicity in Web Text\n\nTo further investigate the phenomenon of neural toxic degeneration, and partially motivated by the surprising effectiveness of domain-adaptive pretraining on non-toxic data, we turn our focus to two corpora used to pretrain several language models. Specifically, we quantify the toxicity in OPENAIWT (GPT-2's training data; Radford et al., 2019) and its open-source replica OWTC (Gokaslan and Cohen, 2019), inspired by previous work in analyzing social biases in large text corpora (Fast et al., 2016). Then, we investigate the provenance of the data in these corpora, quantifying how many documents come from factually unreliable news sites or were shared on quarantined or banned subreddits.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 36
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-37",
      "content": "the provenance of the data in these corpora, quantifying how many documents come from factually unreliable news sites or were shared on quarantined or banned subreddits. \n\nOWTC is a large corpus of English web text scraped from outbound URLs in submissions on Reddit communities ( subreddits ). In the creation of OWTC,only links included in posts with a 'karma' (i.e., popularity) score of 3 or more were considered. Following the links, only English documents longer than 128 tokens are included in this corpus, amounting to 38 GB of text from about 8M documents. To allow for further analyses, we parse the URLs given with OWTC documents to extract the domain (often a news website, Figure 5 in Appendix §D; Sharoff, 2020), which we crossreference with news factuality ratings by Baly et al. (2018). We additionally cross-reference publicly\n\n14 When releasing REALTOXICITYPROMPTS, we will include a flag for prompts belong to this challenging subset.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 37
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-38",
      "content": "with news factuality ratings by Baly et al. (2018). We additionally cross-reference publicly 14 When releasing REALTOXICITYPROMPTS, we will include a flag for prompts belong to this challenging subset. \n\nFigure 3: TOXICITY scores of documents in OWTC (top) and OPENAI-WT (bottom). y -axis is in log-scale, and color gradient follows magnitude in x -axis. We consider a document toxic if its TOXICITY is ≥ 0.5. We additionally display the estimated total % of toxic documents in each corpus above each subplot.\n\n<!-- image -->\n\navailable Reddit dumps 15 to identify which subreddits the URLs were submitted to. We include further details on OWTC and metadata linking in Appendix §D.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 38
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-39",
      "content": "to identify which subreddits the URLs were submitted to. We include further details on OWTC and metadata linking in Appendix §D. \n\nOPENAI-WT is the pretraining corpus for GPT2 (Radford et al., 2019), also containing about 8M documents. Following OWTC, authors gathered URLs from Reddit, though from a different (but overlapping) timespan. Additionally, authors filtered content using a blocklist of sexually-explicit and otherwise offensive subreddits. 16 This corpus does not come paired with URL metadata.\n\nOverlap We find about 29% overlap between the two corpora, using a large-scale similarity search with locality-sensitive hashing (Rajaraman and Ullman, 2011, see Appendix D for details). We find\n\n15\n\nhttps://pushshift.io\n\n16 https://github.com/openai/gpt2/blob/master/model\\_card.md\n\nFigure 4: Top: Factual reliability in news sites that make up OWTC. Bottom: Unreliable news sources in OWTC have a much higher proportion of toxic content.\n\n<!-- image -->",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 39
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-40",
      "content": "Factual reliability in news sites that make up OWTC. Bottom: Unreliable news sources in OWTC have a much higher proportion of toxic content. <!-- image --> \n\nthat at least 2.3M documents in OPENAI-WT also appear in OWTC.\n\n## 6.1 Toxicity in Web Text\n\nShown in Figure 3, we find that both corpora contain non-negligible amounts of toxicity, with 2.1% of OWTC having TOXICITY ≥ 0.5, and 4.3% of OPENAI-WT. These rates are in line with Founta et al. (2018), who find that the prevalence of abusive or toxic content online roughly ranges between 0.1% and 3%, and suggest that these corpora merely reflect the 'natural' rates of toxicity. We note that, despite Radford et al. (2019) employing a blocklist of subreddits and 'bad' words, the toxicity in OPENAI-WT is twice the amount in OWTC. We show similar rates of toxicity using alternative PERSPECTIVE API labels on these corpora in Table 12 in Appendix §D.\n\n## 6.2 Sources of Toxic Content in Web Text",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 40
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-41",
      "content": "in OWTC. We show similar rates of toxicity using alternative PERSPECTIVE API labels on these corpora in Table 12 in Appendix §D. ## 6.2 Sources of Toxic Content in Web Text \n\nSince Reddit is known to have hosted communities that endorse hateful norms and conspiracy theories (Romano, 2017), we investigate the provenance of data in our web text corpora. Specifically, we quantify the variation of a document's toxicity with respect to the reliability of its host news site and\n\nTable 4: Examples of (purposefully uncensored) toxic documents that appear in GPT-2's training corpus, that were also submitted to quarantined or banned subreddits. We highlight spans that contribute to the overall toxicity of the document, which we identify manually.\n\n<!-- image -->\n\nthe nature of the subreddits to which it was posted.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 41
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-42",
      "content": "contribute to the overall toxicity of the document, which we identify manually. <!-- image --> the nature of the subreddits to which it was posted. \n\nToxicity from Unreliable News Sites Gathering all documents in OWTC associated with a news site, and cross-referencing reliability ratings from Baly et al. (2018), we find that news reliability correlates negatively with the proportion of documents that are toxic (Spearman ρ = -0.35). As shown in Figure 4, while low reliability news sites are less prevalent in OWTC, they contain more toxic documents compared to higher reliability news sites. Additionally, we find that at least 12% (272K) of the overlapping OPENAI-WT and OWTC documents with news reliability ratings come from low or mixed reliability news sites.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 42
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-43",
      "content": "at least 12% (272K) of the overlapping OPENAI-WT and OWTC documents with news reliability ratings come from low or mixed reliability news sites. \n\nToxicity from Quarantined or Banned Subreddits Our analyses show that a non-trivial portion of OWTC documents (at least 3%, 212K) come from links shared on banned or quarantined subreddits. 17 Unsurprisingly, documents shared on those subreddits contain substantially more toxicity than those from standard subreddits (see Figure 10 in Appendix §D), confirming Reddit users' propensity to share oppressive and abusive content (Massa-\n\n17 Quarantined subreddits are special-access only and easily scraped, whereas banned subreddits are inaccessible via the website and only available in data dumps. For more details, see https://en.wikipedia.org/ wiki/Controversial\\_Reddit\\_communities .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 43
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-44",
      "content": "scraped, whereas banned subreddits are inaccessible via the website and only available in data dumps. For more details, see https://en.wikipedia.org/ wiki/Controversial\\_Reddit\\_communities . \n\nnari, 2017; Mohan et al., 2017; Rajadesingan et al., 2020; Aran et al., 2020). From the overlapping OPENAI-WT and OWTC documents, we find that at least 63K documents were shared on banned or quarantined subreddits. With two example documents shown in Table 4, GPT-2 was pretrained on at least 40K documents from the quarantined /r/The Donald , and 4K documents from the banned /r/WhiteRights .\n\n## 7 Discussion and Recommendations",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 44
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-45",
      "content": "quarantined /r/The Donald , and 4K documents from the banned /r/WhiteRights . ## 7 Discussion and Recommendations \n\nOverall, our investigations demonstrate that toxicity is a prevalent issue in both neural language generation and web text corpora. Although they show some reduction in toxicity, steering methods do not fully protect neural models from toxic degeneration (§5). Additionally, the corpora that language models are pretrained on contain non-negligible amounts of toxic, abusive, and untrustworthy content (§6). Some implications of our findings are discussed below.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 45
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-46",
      "content": "of toxic, abusive, and untrustworthy content (§6). Some implications of our findings are discussed below. \n\nEffectiveness of 'Forgetting' Toxicity Our findings on data-based steering methods show that adaptive pretraining lowers a model's propensity to unpromptedly generate toxic language, but that its prompted generations can still be toxic. This raises the question: can language models ever fully 'forget' toxic pretraining data through further adaptation (Kirkpatrick et al., 2017; Gururangan et al., 2020)? The non-trivial amounts of toxicity generated by DAPT suggest that perhaps language models may be 'memorizing' the toxicity in pretraining data (Carlini et al., 2019) or that toxic examples may be more salient for the model and hence harder to unlearn (Koh and Liang, 2017). Future work could explore whether some variants of toxicity are harder to forget than others, or whether the biases of models used to select training data for steering introduce unwanted side effects in language model behavior after adaptation.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 46
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-47",
      "content": "toxicity are harder to forget than others, or whether the biases of models used to select training data for steering introduce unwanted side effects in language model behavior after adaptation. \n\nDecoding with a Purpose Our analyses also highlight the promise of certain decoding methods, such as PPLM (Dathathri et al., 2020), which is among the most effective methods we tested at avoiding toxicity with toxic prompts. In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as 'negative examples' to avoid toxicity in generation. Future work could also investigate infusing models with more sophisticated or nuanced representations of social biases (Ma et al., 2020).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 47
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-48",
      "content": "generation. Future work could also investigate infusing models with more sophisticated or nuanced representations of social biases (Ma et al., 2020). \n\nChoice of Pretraining Data As pretrained language models grow in size (Brown et al., 2020), so does their need for larger corpora, often drawn from easily accessible and abundant web text. However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 48
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-49",
      "content": "models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns. \n\nFirst, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models. Therefore, echoing calls for transparency in NLP research (Bender and Friedman, 2018; Mitchell et al., 2019; Dodge et al., 2019), we recommend researchers publicly release all relevant information during data collection (e.g., original text, source URLs, timestamps, platform-specific metadata) when building pretraining corpora.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 49
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-50",
      "content": "during data collection (e.g., original text, source URLs, timestamps, platform-specific metadata) when building pretraining corpora. \n\nSecond, using Reddit popularity as a curation heuristic introduces representational harm (Barocas et al., 2017) by biasing the populations whose language and perspectives are included in pretraining (e.g., Reddit users skew male; Barthel et al., 2016). This raises the question of who decides whose voices are going to be learned by the language model, and whose voices are excluded. Following Blodgett et al. (2020), we recommend a reexamination of the relationship between NLP systems and their end users, using methods from humancentered design, such as value-sensitive (Friedman et al., 2008) or participatory design (Sanders, 2002; DiSalvo et al., 2012; Denton et al., 2020), and archival data collection (Jo and Gebru, 2020). Given the potential for misuse and harm, we also echo calls for improving policy around public release of large language models (Zellers et al., 2019; McGuffie and Newhouse, 2020).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 50
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-51",
      "content": "2020). Given the potential for misuse and harm, we also echo calls for improving policy around public release of large language models (Zellers et al., 2019; McGuffie and Newhouse, 2020). \n\nIn general, the potential mismatch between the intent of curating pretraining data and its operationalization (e.g., karma thresholding, filtering out specific slurs and swearwords) biases the language model's pretraining data and behavior (Jacobs and Wallach, 2019). For example, filtering data based on PERSPECTIVE API could lead to a decrease in text by African American authors in pretraining data due to well-documented racial bias (Sap et al., 2019), which could lead to decreased performance on text written by non-White users. To avoid harm, researchers should be mindful and explicit about these decisions and engage with the end users of\n\nthe technology during these design phases.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 51
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-52",
      "content": "users. To avoid harm, researchers should be mindful and explicit about these decisions and engage with the end users of the technology during these design phases. \n\nImproving Toxicity Detection With the release of REALTOXICITYPROMPTS, we hope to encourage large-scale, systematic evaluations of detoxification techniques for language models. However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity (§2.2). To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al., 2013), hate speech (Golbeck et al., 2017), or microaggressions (Breitfeller et al., 2019). Additionally, measures of bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content).",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 52
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-53",
      "content": "bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). \n\nLimitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because OPENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora.\n\n## 8 Related Work",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 53
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-54",
      "content": "the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. ## 8 Related Work \n\nA wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al., 2020; Welleck et al., 2019).\n\nSimilar in spirit to REALTOXICITYPROMPTS, Wallace et al. (2019) find universal adversarial triggers , nonsensical prompts that trigger toxic generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 54
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-55",
      "content": "generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models. \n\nMost closely related to this work, Sheng et al. (2019) use a set of 60 templated prompts that mention majority or minority identities to study the social biases in generations by out-of-the-box pretrained language models. In our work, we study toxic degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1). Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model's generation towards demographic identities.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 55
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-56",
      "content": "phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model's generation towards demographic identities. \n\nThe creation of REALTOXICITYPROMPTS was partly inspired by work in detecting conversational patterns that can cause derailment into antisocial behavior in online conversations (Zhang et al., 2018; Stoop et al., 2019; Karan and ˇ Snajder, 2019). Our work also draws from a strong line of research into controlling the outputs of language models (Dathathri et al., 2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia ).\n\n## 9 Conclusion",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 56
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-57",
      "content": "2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia ). ## 9 Conclusion \n\nWe introduce REALTOXICITYPROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations. Finally, we provide recommendations for gathering pretraining data. The data, code, and interactive visualizations for this paper can be found at https://toxicdegeneration.allenai.org/ .\n\n## Acknowledgments",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 57
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-58",
      "content": "gathering pretraining data. The data, code, and interactive visualizations for this paper can be found at https://toxicdegeneration.allenai.org/ . ## Acknowledgments \n\nWe thank colleagues at UW NLP and AI2 for their helpful comments and feedback. We also thank Jonathan Borchardt, Carissa Schoenick, and Sam Skjonsberg for helping us develop the demo website. We thank OpenAI, specifically Bianca Martin and Miles Brundage, for providing access to GPT-3 through the OpenAI API Academic Access Program. This research was supported in part by NSF (IIS-1524371, IIS-1714566), DARPA under the CwC program through the ARO (W911NF15-1-0543), and DARPA under the MCS program through NIWC Pacific (N66001-19-2-4031).\n\n## References\n\nXavier Ferrer Aran, T. V. Nuenen, J. M. Such, and N. Criado. 2020. Discovering and categorising language biases in Reddit.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 58
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-59",
      "content": "(N66001-19-2-4031). ## References Xavier Ferrer Aran, T. V. Nuenen, J. M. Such, and N. Criado. 2020. Discovering and categorising language biases in Reddit. \n\nRamy Baly, Georgi Karadzhov, Dimitar Alexandrov, James Glass, and Preslav Nakov. 2018. Predicting factuality of reporting and bias of news media sources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3528-3539, Brussels, Belgium. Association for Computational Linguistics.\n\nSolon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The problem with bias: Allocative versus representational harms in machine learning. In SIGCIS .\n\nMichael Barthel, Galen Stocking, Jesse Holcomb, and Amy Mitchell. 2016. Seven-in-Ten Reddit users get news on the site. Accessed: 2020-6-2.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 59
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-60",
      "content": ". Michael Barthel, Galen Stocking, Jesse Holcomb, and Amy Mitchell. 2016. Seven-in-Ten Reddit users get news on the site. Accessed: 2020-6-2. \n\nChristine Basta, Marta R. Costa-juss` a, and Noe Casas. 2019. Evaluating the underlying gender bias in contextualized word embeddings. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing , pages 33-39, Florence, Italy. Association for Computational Linguistics.\n\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics , 6:587-604.\n\nSu Lin Blodgett, Solon Barocas, Hal Daum´ e III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of 'bias' in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 54545476, Online. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 60
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-61",
      "content": "survey of 'bias' in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 54545476, Online. Association for Computational Linguistics. \n\nSu Lin Blodgett, Lisa Green, and Brendan O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. In EMNLP .\n\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606 .\n\nLuke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 1664-1674, Hong Kong, China. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 61
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-62",
      "content": "in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 1664-1674, Hong Kong, China. Association for Computational Linguistics. \n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are Few-Shot learners.\n\nNicholas Carlini, Chang Liu, ´ Ulfar Erlingsson, Jernej Kos, and Dawn Xiaodong Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 62
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-63",
      "content": "Erlingsson, Jernej Kos, and Dawn Xiaodong Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium . \n\nMia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin Y. Lu, Jackie Tsay, Yinan Wang, Andrew M. Dai, Zhifeng Chen, Timothy Sohn, and Yonghui Wu. 2019. Gmail smart compose: Real-time assisted writing. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining .\n\nAnna Chung. 2019. How automated tools discriminate against black language. Accessed: 2019-03-02.\n\nGloria Cowan and D´ esir´ ee Khatchadourian. 2003. Empathy, ways of knowing, and interdependence as mediators of gender differences in attitudes toward hate speech and freedom of speech. Psychology of women quarterly , 27(4):300-308.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 63
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-64",
      "content": "knowing, and interdependence as mediators of gender differences in attitudes toward hate speech and freedom of speech. Psychology of women quarterly , 27(4):300-308. \n\nCristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013. A computational approach to politeness with application to social factors. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 250-259, Sofia, Bulgaria. Association for Computational Linguistics.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 64
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-65",
      "content": "2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations . \n\nThomas Davidson, Debasmita Bhattacharya, and Ingmar Weber. 2019. Racial bias in hate speech and abusive language detection datasets. In Proceedings of the Third Workshop on Abusive Language Online , pages 25-35, Florence, Italy. Association for Computational Linguistics.\n\nEmily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, and Morgan Klaus Scheuerman. 2020. Bringing the people back in: Contesting benchmark machine learning datasets. In ICML Workshop on Participatory Approaches to Machine Learning .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 65
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-66",
      "content": "back in: Contesting benchmark machine learning datasets. In ICML Workshop on Participatory Approaches to Machine Learning . \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nEmily Dinan, A. Fan, Ledell Yu Wu, J. Weston, Douwe Kiela, and Adina Williams. 2020. Multidimensional gender bias classification.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 66
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-67",
      "content": "A. Fan, Ledell Yu Wu, J. Weston, Douwe Kiela, and Adina Williams. 2020. Multidimensional gender bias classification. \n\nEmily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it break it fix it for dialogue safety: Robustness from adversarial human attack. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 4537-4546, Hong Kong, China. Association for Computational Linguistics.\n\nCarl DiSalvo, Andrew Clement, and Volkmar Pipek. 2012. Communities: Participatory design for, with and by communities.\n\nLucas Dixon, John Li, Jeffrey Scott Sorensen, Nithum Thain, and Lucy Vasserman. 2018. Measuring and mitigating unintended bias in text classification. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 67
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-68",
      "content": "and Lucy Vasserman. 2018. Measuring and mitigating unintended bias in text classification. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society . \n\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. 2019. Show your work: Improved reporting of experimental results. In EMNLP , pages 2185-2194, Hong Kong, China. Association for Computational Linguistics.\n\nJacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse additive generative models of text. In Proceedings of the 28th International Conference on International Conference on Machine Learning , ICML'11, page 1041-1048, Madison, WI, USA. Omnipress.\n\nEthan Fast, Tina Vachovsky, and Michael S. Bernstein. 2016. Shirtless and dangerous: Quantifying linguistic signals of gender bias in an online fiction writing community.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 68
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-69",
      "content": "Tina Vachovsky, and Michael S. Bernstein. 2016. Shirtless and dangerous: Quantifying linguistic signals of gender bias in an online fiction writing community. \n\nJessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. In Proceedings of the Workshop on Stylistic Variation , pages 94-104, Copenhagen, Denmark. Association for Computational Linguistics.\n\nAntigoni-Maria Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018. Large scale crowdsourcing and characterization of Twitter abusive behavior. In ICWSM .\n\nBatya Friedman, Peter H Kahn, and Alan Borning. 2008. Value sensitive design and information systems. The handbook of information and computer ethics , pages 69-101.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 69
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-70",
      "content": "Kahn, and Alan Borning. 2008. Value sensitive design and information systems. The handbook of information and computer ethics , pages 69-101. \n\nSayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, and Stefan Scherer. 2017. Affect-LM: A neural language model for customizable affective text generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 634-642, Vancouver, Canada. Association for Computational Linguistics.\n\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 70
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-71",
      "content": "Vancouver, Canada. Association for Computational Linguistics. Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. \n\nJennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo, Alexandra Berlinger, Siddharth Bhagwan, Cody Buntain, Paul Cheakalos, Alicia A. Geller, Quint Gergory, Rajesh Kumar Gnanasekaran, Raja Rajan Gunasekaran, Kelly M. Hoffman, Jenny Hottle, Vichita Jienjitlert, Shivika Khare, Ryan Lau, Marianna J. Martindale, Shalmali Naik, Heather L. Nixon, Piyush Ramachandran, Kristine M. Rogers, Lisa Rogers, Meghna Sardana Sarin, Gaurav Shahane, Jayanee Thanki, Priyanka Vengataraman, Zijian Wan, and Derek Michael Wu. 2017. A large labeled corpus for online harassment research. In Proceedings of the 2017 ACM on Web Science Conference , WebSci '17, page 229-233, New York, NY, USA. Association for Computing Machinery.\n\nLisa Green. 2002. African American English: A Linguistic Introduction , 8.3.2002 edition edition. Cambridge University Press.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 71
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-72",
      "content": "229-233, New York, NY, USA. Association for Computing Machinery. Lisa Green. 2002. African American English: A Linguistic Introduction , 8.3.2002 edition edition. Cambridge University Press. \n\nSuchin Gururangan, Ana Marasovi´ c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342-8360, Online. Association for Computational Linguistics.\n\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1638-1649, Melbourne, Australia. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 72
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-73",
      "content": "the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1638-1649, Melbourne, Australia. Association for Computational Linguistics. \n\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. International Conference on Learning Representations .\n\nMatthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.\n\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social biases in NLP models as barriers for persons with disabilities. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5491-5501, Online. Association for Computational Linguistics.\n\nAbigail Z. Jacobs and Hanna M. Wallach. 2019. Measurement and fairness.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 73
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-74",
      "content": "the Association for Computational Linguistics , pages 5491-5501, Online. Association for Computational Linguistics. Abigail Z. Jacobs and Hanna M. Wallach. 2019. Measurement and fairness. \n\nEun Seo Jo and Timnit Gebru. 2020. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency , FAT* '20, page 306-316, New York, NY, USA. Association for Computing Machinery.\n\nMladen Karan and Jan ˇ Snajder. 2019. Preemptive toxic language detection in Wikipedia comments using thread-level context. In Proceedings of the Third Workshop on Abusive Language Online , pages 129134, Florence, Italy. Association for Computational Linguistics.\n\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional Transformer language model for controllable generation.\n\nAdam King. 2019. Talk to Transformer. Accessed 0602-2020.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 74
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-75",
      "content": "R. Varshney, Caiming Xiong, and Richard Socher. 2019. CTRL: A conditional Transformer language model for controllable generation. Adam King. 2019. Talk to Transformer. Accessed 0602-2020. \n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences , 114(13):3521-3526.\n\nPang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML'17, page 1885-1894. JMLR.org.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 75
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-76",
      "content": "functions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML'17, page 1885-1894. JMLR.org. \n\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019. Measuring bias in contextualized word representations. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing , pages 166-172, Florence, Italy. Association for Computational Linguistics.\n\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278-2324.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach.\n\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020. PowerTransformer: Unsupervised controllable revision for biased language correction. In EMNLP .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 76
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-77",
      "content": "optimized bert pretraining approach. Xinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020. PowerTransformer: Unsupervised controllable revision for biased language correction. In EMNLP . \n\nAdrienne Massanari. 2017. #gamergate and the fappening: How Reddit's algorithm, governance, and culture support toxic technocultures. New Media &amp; Society , 19(3):329-346.\n\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 622-628, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nKris McGuffie and Alex Newhouse. 2020. The radicalization risks of GPT-3 and advanced neural language models.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 77
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-78",
      "content": "Minneapolis, Minnesota. Association for Computational Linguistics. Kris McGuffie and Alex Newhouse. 2020. The radicalization risks of GPT-3 and advanced neural language models. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency , FAT* '19, page 220-229, New York, NY, USA. Association for Computing Machinery.\n\nShruthi Mohan, Apala Guha, Michael Harris, Fred Popowich, Ashley Schuster, and Chris Priebe. 2017. The impact of toxic language on the health of Reddit communities. In Canadian Conference on AI .\n\nJi Ho Park and Pascale Fung. 2017. One-step and twostep classification for abusive language detection on Twitter. In Proceedings of the Workshop on Abusive Language Online .",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 78
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-79",
      "content": "Park and Pascale Fung. 2017. One-step and twostep classification for abusive language detection on Twitter. In Proceedings of the Workshop on Abusive Language Online . \n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d ´ Alch´ e Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 8024-8035. Curran Associates, Inc.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 79
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-80",
      "content": "Systems 32 , pages 8024-8035. Curran Associates, Inc. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. \n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text Transformer.\n\nAshwin Rajadesingan, Paul Resnick, and Ceren Budak. 2020. Quick, community-specific learning: How distinctive toxicity norms are maintained in political subreddits. Proceedings of the International AAAI Conference on Web and Social Media , 14(1):557568.\n\nAnand Rajaraman and Jeffrey David Ullman. 2011. Mining of massive datasets . Cambridge University Press.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 80
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-81",
      "content": "AAAI Conference on Web and Social Media , 14(1):557568. Anand Rajaraman and Jeffrey David Ullman. 2011. Mining of massive datasets . Cambridge University Press. \n\nAja Romano. 2017. Reddit just banned one of its most toxic forums. but it won't touch The Donald. Accessed: 2020-02-23.\n\nBj¨ orn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, and Michael Wojatzki. 2017. Measuring the reliability of hate speech annotations: the case of the european refugee crisis. In NLP 4 CMC Workshop .\n\nElizabeth Sanders. 2002. From user-centered to participatory design approaches , pages 1-7.\n\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1668-1678, Florence, Italy. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 81
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-82",
      "content": "in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1668-1678, Florence, Italy. Association for Computational Linguistics. \n\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5477-5490, Online. Association for Computational Linguistics.\n\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 17151725, Berlin, Germany. Association for Computational Linguistics.\n\nSerge Sharoff. 2020. Know thy corpus! robust methods for digital curation of web corpora.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 82
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-83",
      "content": "Long Papers) , pages 17151725, Berlin, Germany. Association for Computational Linguistics. Serge Sharoff. 2020. Know thy corpus! robust methods for digital curation of web corpora. \n\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 34073412, Hong Kong, China. Association for Computational Linguistics.\n\nWessel Stoop, Florian Kunneman, Antal van den Bosch, and Ben Miller. 2019. Detecting harassment in real-time as conversations develop. In Proceedings of the Third Workshop on Abusive Language Online , pages 19-24, Florence, Italy. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 83
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-84",
      "content": "as conversations develop. In Proceedings of the Third Workshop on Abusive Language Online , pages 19-24, Florence, Italy. Association for Computational Linguistics. \n\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. 'transforming' delete, retrieve, generate approach for controlled text style transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 32693279, Hong Kong, China. Association for Computational Linguistics.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS'17, page 6000-6010, Red Hook, NY, USA. Curran Associates Inc.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 84
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-85",
      "content": "In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS'17, page 6000-6010, Red Hook, NY, USA. Curran Associates Inc. \n\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 2153-2162, Hong Kong, China. Association for Computational Linguistics.\n\nAlex Wang and Kyunghyun Cho. 2019. Bert has a mouth, and it must speak: Bert as a markov random field language model.\n\nZeerak Waseem. 2016. Are you a racist or am I seeing things? annotator influence on hate speech detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social Science , pages 138142, Austin, Texas. Association for Computational Linguistics.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 85
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-86",
      "content": "on hate speech detection on Twitter. In Proceedings of the First Workshop on NLP and Computational Social Science , pages 138142, Austin, Texas. Association for Computational Linguistics. \n\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019. Neural text generation with unlikelihood training.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´ emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art natural language processing.\n\nBianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD '02, page 694-699, New York, NY, USA. Association for Computing Machinery.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 86
      }
    },
    {
      "id": "unpaywall-10.18653_v1_2020.findings-emnlp.301-chunk-87",
      "content": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD '02, page 694-699, New York, NY, USA. Association for Computing Machinery. \n\n- Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d ´ Alch´ e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 , pages 9054-9065. Curran Associates, Inc.\n- Justine Zhang, Jonathan Chang, Cristian DanescuNiculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario Taraborelli, and Nithum Thain. 2018. Conversations gone awry: Detecting early signs of conversational failure. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1350-1361, Melbourne, Australia. Association for Computational Linguistics.\n- Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender bias in contextualized word embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 629-634, Minneapolis, Minnesota. Association for Computational Linguistics.\n- Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. 2015 IEEE International Conference on Computer Vision (ICCV) .\n- Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.",
      "metadata": {
        "source": "unpaywall:10.18653/v1/2020.findings-emnlp.301",
        "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration",
        "category": "Safety",
        "year": 2020,
        "authors": "undefined undefined, undefined undefined, undefined undefined, undefined undefined, undefined undefined",
        "chunkIndex": 87
      }
    }
  ],
  "tables": [
    "|               | REALTOXICITYPROMPTS                    | REALTOXICITYPROMPTS                          |\n|---------------|----------------------------------------|----------------------------------------------|\n| # Prompts     | Toxic 21,744                           | Non-Toxic 77,272                             |\n| # Tokens      | Prompts 11.7 4 glyph[triangleright] 2  | Continuations 12.0 4 glyph[triangleright] 2  |\n| Avg. Toxicity | Prompts 0.29 0 glyph[triangleright] 27 | Continuations 0.38 0 glyph[triangleright] 31 |",
    "|        | Exp. Max. Toxicity             | Exp. Max. Toxicity             | Toxicity Prob.   | Toxicity Prob.   |\n|--------|--------------------------------|--------------------------------|------------------|------------------|\n| Model  | Toxic                          | Non-Toxic                      | Toxic            | Non-Toxic        |\n| GPT-1  | 0.78 0 glyph[triangleright] 18 | 0.58 0 glyph[triangleright] 22 | 0.90             | 0.60             |\n| GPT-2  | 0.75 0 glyph[triangleright] 19 | 0.51 0 glyph[triangleright] 22 | 0.88             | 0.48             |\n| GPT-3  | 0.75 0 glyph[triangleright] 20 | 0.52 0 glyph[triangleright] 23 | 0.87             | 0.50             |\n| CTRL   | 0.73 0 glyph[triangleright] 20 | 0.52 0 glyph[triangleright] 21 | 0.85             | 0.50             |\n| CTRL-W | 0.71 0 glyph[triangleright] 20 | 0.49 0 glyph[triangleright] 21 | 0.82             | 0.44             |",
    "|                |                                     | Exp. Max. Toxicity                                                                           | Exp. Max. Toxicity                                                                           | Exp. Max. Toxicity                                                                           | Toxicity Prob.   | Toxicity Prob.   | Toxicity Prob.   |\n|----------------|-------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------|------------------|------------------|\n| Category       | Model                               | Unprompted                                                                                   | Toxic                                                                                        | Non-Toxic                                                                                    | Unprompted       | Toxic            | Non-Toxic        |\n| Baseline       | GPT-2                               | 0.44 0 glyph[triangleright] 17                                                               | 0.75 0 glyph[triangleright] 19                                                               | 0.51 0 glyph[triangleright] 22                                                               | 0.33             | 0.88             | 0.48             |\n| Data-based     | DAPT (Non-Toxic) DAPT (Toxic) ATCON | 0.30 0 glyph[triangleright] 13 0.80 0 glyph[triangleright] 16 0.42 0 glyph[triangleright] 17 | 0.57 0 glyph[triangleright] 23 0.85 0 glyph[triangleright] 15 0.73 0 glyph[triangleright] 20 | 0.37 0 glyph[triangleright] 19 0.69 0 glyph[triangleright] 23 0.49 0 glyph[triangleright] 22 | 0.09 0.93 0.26   | 0.59 0.96 0.84   | 0.23 0.77 0.44   |\n| Decoding-based | VOCAB-SHIFT PPLM WORD FILTER        | 0.43 0 glyph[triangleright] 18 0.28 0 glyph[triangleright] 11 0.42 0 glyph[triangleright] 16 | 0.70 0 glyph[triangleright] 21 0.52 0 glyph[triangleright] 26 0.68 0 glyph[triangleright] 19 | 0.46 0 glyph[triangleright] 22 0.32 0 glyph[triangleright] 19 0.48 0 glyph[triangleright] 20 | 0.31 0.05 0.27   | 0.80 0.49 0.81   | 0.39 0.17 0.43   |"
  ],
  "stats": {
    "totalCharacters": 60923,
    "chunkCount": 88,
    "tableCount": 3,
    "oaStatus": "gold",
    "pdfUrl": "https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf"
  }
}