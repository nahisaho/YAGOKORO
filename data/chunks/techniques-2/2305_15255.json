{
  "paper": {
    "id": "2305.15255v4",
    "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
    "abstract": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
    "authors": [
      "Eliya Nachmani",
      "Alon Levkovitch",
      "Roy Hirsch",
      "Julian Salazar",
      "Chulayuth Asawaroengchai",
      "Soroosh Mariooryad",
      "Ehud Rivlin",
      "RJ Skerry-Ryan",
      "Michelle Tadmor Ramanovich"
    ],
    "published": "2023-05-24T15:39:43.000Z",
    "updated": "2024-05-31T01:29:27.000Z",
    "primaryCategory": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2305.15255v4",
    "absUrl": "https://arxiv.org/abs/2305.15255v4"
  },
  "chunks": [
    {
      "id": "2305.15255v4-chunk-0",
      "content": "Eliya Nachmani 1 , ∗ , Alon Levkovitch 1 , 3 , ∗ , † , Roy Hirsch 2 , Julian Salazar 1 , Chulayuth Asawaroengchai 1 , Soroosh Mariooryad 1 , Ehud Rivlin 2 , RJ Skerry-Ryan 1 , Michelle Tadmor Ramanovich 1 1 Google Research, 2 Verily AI, 3 Tel-Aviv Univeristy {eliyn, alevkovitch, royhirsch}@google.com",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "SPOKEN QUESTION ANSWERING AND SPEECH CONTINUATION USING SPECTROGRAM-POWERED LLM",
        "chunkIndex": 0,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-1",
      "content": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained endto-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a 'cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples and spoken QA dataset via our website. 1",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "ABSTRACT",
        "chunkIndex": 1,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-2",
      "content": "The goal of natural language processing (NLP) is to develop computational models that can understand and generate human language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Combined with the Transformer model architecture (Vaswani et al., 2017), large language models (LLMs) trained on web-scale amounts of text, with proportionate compute and size, have demonstrated remarkable success in NLP tasks (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a; Scao et al., 2022; Zeng et al., 2023). However, transferring these abilities to spoken human language remains a challenging frontier. Spoken dialog systems remain a cascade of separately trained automatic speech recognition (ASR), natural language understanding (NLU) and generation (NLG), and text-to-speech (TTS) systems (Gorin et al., 1997; Jokinen &amp; McTear, 2009), with LLMs now",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 2,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-3",
      "content": "trained automatic speech recognition (ASR), natural language understanding (NLU) and generation (NLG), and text-to-speech (TTS) systems (Gorin et al., 1997; Jokinen &amp; McTear, 2009), with LLMs now playing the role of a combined NLU and NLG system. However, such cascades introduce latency and additional mechanisms for propagating and rendering non-verbal cues like speaker identity and prosody. Recently, spoken language models (Lakhotia et al., 2021; Kharitonov et al., 2022) and other generative audio models (Dhariwal et al., 2020; Hawthorne et al., 2022; Borsos et al., 2023; Agostinelli et al., 2023) have emerged as a promising avenue for generative speech modeling. These works quantize audio representations (Hsu et al., 2021; Chung et al., 2021; Zeghidour et al., 2022; Défossez et al., 2022) into learned discrete tokens compatible with the same next-token cross-entropy objective as text LLMs, a step that (Nguyen et al., 2022) argued as necessary for generative quality.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 3,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-4",
      "content": "ossez et al., 2022) into learned discrete tokens compatible with the same next-token cross-entropy objective as text LLMs, a step that (Nguyen et al., 2022) argued as necessary for generative quality. In this paper, we introduce Spectron, a novel spoken language model that:\n\n- Directly process spectrograms as both input and output. Spectron leverages the audio capabilities of a pre-trained speech encoder through the use of intermediate projection layers.\n\n∗ Equal contribution.\n\n† Work done during an internship at Google.\n\n1 https://michelleramanovich.github.io/spectron/spectron\n\nFigure 1: Spectron connects the encoder of a speech recognition model with a pre-trained Transformer decoder language model. At training time, we take speech utterances and split their audio into a prompt and its continuation .",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 4,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-5",
      "content": "the encoder of a speech recognition model with a pre-trained Transformer decoder language model. At training time, we take speech utterances and split their audio into a prompt and its continuation . From the prompt speech features, the full (prompt and continuation's) transcript must be reconstructed, as well as the continuation's speech features via newly introduced pre- and post-net speech modules. At inference time, only a prompt is provided; the prompt's transcription, text continuation, and speech continuations are all generated by the model.\n\n<!-- image -->\n\n- Demonstrably transfer generative ability from a pre-trained LLM, as shown by competitive performance in semantic coherence and spoken question answering over other end-to-end spoken language models.\n\nTo quantify this transfer of knowledge, we also introduce two benchmarks for the nascent spoken QA task, which we synthesize from Web Questions (Berant et al., 2013) and generations from LLaMA (Touvron et al., 2023).",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 5,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-6",
      "content": "transfer of knowledge, we also introduce two benchmarks for the nascent spoken QA task, which we synthesize from Web Questions (Berant et al., 2013) and generations from LLaMA (Touvron et al., 2023). Audio samples and our LLaMA dataset can be found on the project website given on the first page.\n\nOur work shows that the inductive biases from a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective which implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. The language model transcribes and generates text continuations, acting as an 'intermediate scratchpad' (Nye et al., 2021; Wei et al., 2022) to be conditioned on for audio generation.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 6,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-7",
      "content": "a joint manner. The language model transcribes and generates text continuations, acting as an 'intermediate scratchpad' (Nye et al., 2021; Wei et al., 2022) to be conditioned on for audio generation. A novel spectrogram regression loss also supervises the model to match the higher-order temporal and feature deltas of the ground truth, based on the idea that the derivatives of the ground truth express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in Figure 1 and described in the rest of this work.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "1 INTRODUCTION",
        "chunkIndex": 7,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-8",
      "content": "The dominant approach to spoken language modeling is to use compact discrete speech representations. This allows the application of text-based language models to speech data. Typically, these representations are created by clustering the outputs of a speech encoder using K-means and taking the centroids as tokens. The resulting discrete sequences can be easily modeled using Transformer architectures (Vaswani et al., 2017). Below are notable examples of works using this approach; a comparison table is also presented in Appendix A.2.\n\nGenerative Spoken Language Modeling (GSLM; Lakhotia et al., 2021) offers a baseline system that operate on units quantized from pre-trained audio representations, such as HuBERT (Hsu et al., 2021). The quantized units are processed by a Transformer-based model. An additional unit-to-speech",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 8,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-9",
      "content": "that operate on units quantized from pre-trained audio representations, such as HuBERT (Hsu et al., 2021). The quantized units are processed by a Transformer-based model. An additional unit-to-speech\n\ndecoder converts the generated units into spectrograms. Spectron's approach is more simple and explicit, where a single model is input with spectrograms and outputs raw spectrograms.\n\nTWIST (Hassid et al., 2023) uses the same unit-to-speech and speech-to-unit systems as GSLM, but warm-starts the spoken language model from a text-based language model. They show that this warm-start improves overall metrics and convergence speed, with reasonable performance on StoryCloze tasks (though notably degraded from the text model). For the textual language model, they use the state-of-the-art and open-weight OPT (Zhang et al., 2022a) and LLaMA models up to 7B (13B at camera-ready time) and show that spoken language model improvements scale.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 9,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-10",
      "content": "ual language model, they use the state-of-the-art and open-weight OPT (Zhang et al., 2022a) and LLaMA models up to 7B (13B at camera-ready time) and show that spoken language model improvements scale.\n\nAudioLM (Borsos et al., 2023) utilizes two kinds of quantized representations: w2v-BERT (Chung et al., 2021) as semantic tokens, and SoundStream (Zeghidour et al., 2022) as acoustic tokens. SoundStream embeddings undergo discretization using residual vector quantization (RVQ), resulting in a hierarchy of vector quantizers. AudioLM utilizes three transformer models, each corresponding to a different layer of token generation. As Spectron does not involve any quantization, our method naturally preserves the input's semantic and acoustic characteristics. Furthermore, Spectron offers a single model trained with a unified objective, as opposed to the multiple components of AudioLM.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 10,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-11",
      "content": "d naturally preserves the input's semantic and acoustic characteristics. Furthermore, Spectron offers a single model trained with a unified objective, as opposed to the multiple components of AudioLM.\n\nSpeechGPT (Zhang et al., 2023a) adapts LLaMA-7B to perform speech tasks by using both discrete speech representations and text. They introduce the SpeechInstruct dataset which they use for instruction tuning. SpeechGPT is trained in 3 different steps: modality adaptation, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning (using LoRA; Hu et al., 2022). The obtained model is capable of generating both speech and text, as well as following instructions in both modalities. Our method, in comparison, is trained using a single reconstruction step and uses only public datasets for integration.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 11,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-12",
      "content": "ting both speech and text, as well as following instructions in both modalities. Our method, in comparison, is trained using a single reconstruction step and uses only public datasets for integration. Despite not using a curated dataset or specialized prompts, we demonstrate competitive performance on spoken question answering and superior results for speech continuation.\n\nAs for related works in other tasks: Spoken language understanding (SLU) : A number of recent studies explore the usage of pre-trained language models (LMs) for different SLU tasks. Gong et al. (2023), Zhao et al. (2023), and Liu et al. (2023a) fine-tuned LMs on audio data to perform speech-to-text question answering tasks, that is, answer textual questions about directly-input audio. Fathullah et al. (2023) showed that adding an audio encoder to an LM and training with LoRA enables the LM to perform automatic speech recognition (ASR). Zhang et al.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 12,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-13",
      "content": "tions about directly-input audio. Fathullah et al. (2023) showed that adding an audio encoder to an LM and training with LoRA enables the LM to perform automatic speech recognition (ASR). Zhang et al. (2022b) aligned text and audio tokens to perform a large number of SLU tasks. Peng et al. (2023) showed that LMs can be used to answer text questions about spoken properties of language. Spectron, in comparison, produces both textual outputs and spectrograms using a single autoregressive decoder. Multi-modal text-speech training : Ao et al. (2022) performed joint training on speech and text data to perform multiple tasks, such as text-to-speech (TTS) and ASR. Ren et al. (2019) used unsupervised pre-training on text and speech data to perform TTS for low-resource languages. Textual-guided audio generation : Liu et al. (2023b) used LMs to generate audio scripts and interacting with audio-creation APIs. Huang et al.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 13,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-14",
      "content": "nd speech data to perform TTS for low-resource languages. Textual-guided audio generation : Liu et al. (2023b) used LMs to generate audio scripts and interacting with audio-creation APIs. Huang et al. (2023) augmented the ChatGPT input/output interface to invoke ASR / TTS tools.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "2 RELATED WORK",
        "chunkIndex": 14,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-15",
      "content": "We propose a novel architecture for direct speech continuation. The architecture is initialized with a pre-trained speech encoder denoted as E and a pre-trained language decoder denoted as LM. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features are fed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a cross-entropy loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1 ARCHITECTURE",
        "chunkIndex": 15,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-16",
      "content": "During training, the proposed model uses supervised speech utterances, which are pairs of speech x and transcripts y for training. The speech input, denoted as x , is a spectrogram that is split into two\n\nsegments at position s :",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.1 INPUT PRE-PROCESSING",
        "chunkIndex": 16,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-17",
      "content": "We use prefix decoder language models with 350M or 1B parameters trained in the manner of PaLM 2 (Google, 2023), which we denote as LM ( -) . The LM receives the encoded features of the prompt x lm p as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; i.e., there is no cross-attention between the encoder and the decoder. This late-stage integration is consistent with work in ASR, which found that joint fine-tuning of a pre-trained speech encoder and a pre-trained LM decoder into a sequence-to-sequence model can improve performance, even if the integration occurs as a single final layer (Deng et al., 2021); more layers did not improve performance, which they attribute to having sufficiently powerful text representations. During training, the decoder is teacher-forced to predict the text transcription y p , text continuation y c , and speech embeddings x p c .",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.3 LANGUAGE MODEL",
        "chunkIndex": 17,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-18",
      "content": "ibute to having sufficiently powerful text representations. During training, the decoder is teacher-forced to predict the text transcription y p , text continuation y c , and speech embeddings x p c . To convert the speech embeddings to and from spectrograms, we introduce lightweight modules h pre and h post , described in the next section. In all, we get next-step predictions for the concatenation of these text tokens and embeddings:\n\n<!-- formula-not-decoded -->\n\nBy having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, we benefit from the pre-training of the LM in the text domain to continue the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate scratchpads (Nye et al., 2021) or chain-of-thought (CoT; Wei et al., 2022).",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.3 LANGUAGE MODEL",
        "chunkIndex": 18,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-19",
      "content": "ty of the synthesized speech, analogous to improvements in text-based language models when using intermediate scratchpads (Nye et al., 2021) or chain-of-thought (CoT; Wei et al., 2022).",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.3 LANGUAGE MODEL",
        "chunkIndex": 19,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-20",
      "content": "To enable the language model decoder to model speech features, we employ a multi-layer perceptron (MLP), the pre-net h pre to project the ground truth spectrogram speech continuations x c to the language model dimension x p c = h pre ( x c ) . This pre-net h pre compresses the spectrogram input x c into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process, as demonstrated in previous work (Shen et al., 2018). To project ˆ x p c from the language model dimension to the spectrogram dimension, the model employs a post-net h post , which is also an MLP. This projection is represented by ˆ x c = h post (ˆ x p c ) .\n\nBoth h pre and h post are two-layer MLPs. Additionally, the input text sequence [ y p , y c ] is padded at the beginning with a 'start of sequence' (sos) token, while the output sequence is padded with an 'end of sequence' (eos) token at the f",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.4 ACOUSTIC PROJECTION LAYERS",
        "chunkIndex": 20,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-21",
      "content": "dditionally, the input text sequence [ y p , y c ] is padded at the beginning with a 'start of sequence' (sos) token, while the output sequence is padded with an 'end of sequence' (eos) token at the final position.\n\n<!-- formula-not-decoded -->\n\nThe first segment x p (which we call the prompt ) is fed into the speech encoder E to give continuous representations that condition the LM. The second segment x c (the continuation ) is used later for a spectrogram reconstruction loss. SpecAugment (Park et al., 2019) is applied for data augmentation. The corresponding transcripts y can be also split at position ϕ ( s ) :\n\n<!-- formula-not-decoded -->\n\nwhere ϕ ( s ) maps the feature index s in x to its text token index in y . Note that ϕ ( s ) is not needed for our training losses.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.4 ACOUSTIC PROJECTION LAYERS",
        "chunkIndex": 21,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-22",
      "content": "The speech encoder E is a 600M-parameter Conformer encoder (Gulati et al., 2020) pre-trained on web-scale data (12M hours; Zhang et al., 2023b). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of Conformer blocks. Each Conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs of the total encoder E are passed through a layer P that projects the hidden representations into the embedding dimension of the language model. We denote these final embeddings\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.1.2 SPEECH ENCODER",
        "chunkIndex": 22,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-23",
      "content": "The training methodology of the proposed approach is depicted in Figure 1. It uses two distinct loss functions: (1) cross-entropy loss, employed for both speech recognition and transcript continuation, and (2) regression loss, employed for speech continuation. During training, all parameters are updated (speech encoder E , projection layer P s , language model LM, pre-net h pre , and post-net h post ).",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2 TRAINING OBJECTIVE",
        "chunkIndex": 23,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-24",
      "content": "The first loss term is a combination of a speech recognition loss L ASR and a transcript continuation loss L LM, which are given by:\n\n<!-- formula-not-decoded -->\n\nwhere CE denotes cross-entropy, which quantifies the dissimilarity between the predicted distribution over ˆ y p , ˆ y c , and the corresponding ground truth distribution over y p , y c . This objective increases the likelihood of the text [ y p , y c ] under the conditional distribution modeled by the LM.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2.1 SPEECH RECOGNITION AND TRANSCRIPT CONTINUATION",
        "chunkIndex": 24,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-25",
      "content": "The speech continuation objective is formulated as a regression task, predicting each frame's spectrogram channels independently given previous frame spectrogram predictions and the ASR and LMcontext. To promote convergence and improve modeling power, we apply ℓ 1 and ℓ 2 regression losses on the spectrogram (Shen et al., 2020). These losses are applied to the feature-deltas of the spectrogram, and to the time-deltas of the spectrogram up to order K , giving '(discrete) derivative loss' terms. That is, for a tensor z of dimension T × F we define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor a ground truth spectrogram x c and the predicted spectrogram ˆ x c , the speech continuation loss is a combination of three objectives:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe overall speech continuation loss is thus given by:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2.2 SPEECH CONTINUATION",
        "chunkIndex": 25,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-26",
      "content": "Using the above notation, our objective is:\n\n<!-- formula-not-decoded -->\n\nSince L ASR and L LM are cross-entropy losses and since y = [ y p , y c ] (Eq.2), the overall speech recognition and transcript continuation loss can be written as:\n\n<!-- formula-not-decoded -->\n\nwhere ˆ y is the concatenation of ˆ y p and ˆ y c . This simplifies the overall loss to:\n\n<!-- formula-not-decoded -->\n\nwhere λ r is a weighting coefficient. This simplification eliminates the necessity of the text-speech time alignment ϕ ( s ) . Our approach can be seen as jointly optimizing three capabilities:\n\nSpeech recognition ( L ASR): The combined model learns to transcribe speech audio into text. As we use a pre-trained speech encoder and a pre-trained language model, this objective encourages the alignment and integration of each model's functionality.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2.3 OVERALL LOSS",
        "chunkIndex": 26,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-27",
      "content": "ns to transcribe speech audio into text. As we use a pre-trained speech encoder and a pre-trained language model, this objective encourages the alignment and integration of each model's functionality.\n\nTranscript continuation ( L LM): This reuses, maintains, and leverages the language model's ability to generate natural text as learned from its training scheme, for example, dialogue for a chat-optimized LM. Depending on the utterance, the decoder may further learn to use paralinguistic cues from the prompt speech to favor certain completions.\n\nConditional speech synthesis ( L Recon.): We reuse the language model's autoregressive generation ability and direct it toward spectrogram reconstruction. As the teacher-forced transcript is available and the most 'accessible' feature, the decoder learns to perform text-to-speech. In this way, the model can synthesize the LM's arbitrary textual continuations at inference time, including words not found in training.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2.3 OVERALL LOSS",
        "chunkIndex": 27,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-28",
      "content": "essible' feature, the decoder learns to perform text-to-speech. In this way, the model can synthesize the LM's arbitrary textual continuations at inference time, including words not found in training. Finally, we expect that good spectrogram-level continuations require the preservation of speaker, prosody, and channel effects from the original speech prompt.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.2.3 OVERALL LOSS",
        "chunkIndex": 28,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-29",
      "content": "In inference, the speech prompt x p is encoded by the speech encoder E , then projected by P s to the LM's dimension to give x lm p (Eq. 3). Utilizing x lm p and the start-of-sentence (sos) token, the language model decodes text in an autoregressive manner: ˆ y = LM ([ x lm p , sos ]) until eos is emitted, where ˆ y is a concatenation of the predicted transcript and continuation [ˆ y p , ˆ y c ] . Following this, the language model decodes a spectrogram in an autoregressive manner. It predicts the next spectrogram feature estimate ˆ x c ( t ) using prompt features x lm p , text prediction ˆ y and past estimated spectrogram features ˆ x c ( ≤ t -1) . Past spectrogram estimates ˆ x c ( ≤ t -1) are projected to the language model dimension: ˆ x p c ( ≤ t -1) = h pre (ˆ x c ( ≤ t -1)) . Then, ˆ x p c ( t ) is predicted at step t : ˆ x p c ( t ) = LM ([ x lm p , sos , ˆ y, ˆ x p c ( ≤ t -1)]) The decoded output ˆ x p c ( t ) is then projected to the spectrogram domain using h post : ˆ x c (",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.3 INFERENCE",
        "chunkIndex": 29,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-30",
      "content": "ˆ x p c ( t ) is predicted at step t : ˆ x p c ( t ) = LM ([ x lm p , sos , ˆ y, ˆ x p c ( ≤ t -1)]) The decoded output ˆ x p c ( t ) is then projected to the spectrogram domain using h post : ˆ x c ( t ) = h post (ˆ x p c ( t )) . Finally, a vocoder converts the predicted spectrogram ˆ x c into a waveform signal.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "3.3 INFERENCE",
        "chunkIndex": 30,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-31",
      "content": "To empirically evaluate the performance of the proposed approach, we conducted experiments on the Libri-Light dataset (Kahn et al., 2020). Libri-Light is a 60k hour English dataset consisting of unlabelled read speech from LibriVox audiobooks. For our training objective, the dataset was transcribed using a NST (Park et al., 2020) model trained on LibriSpeech (960 hours). We used a frozen neural vocoder, WaveFit (Koizumi et al., 2022), with its default hyperparameters to convert the predicted spectrograms into raw audio. Our proposed model was trained using 64 TPUv4 chips (Jouppi et al., 2023), over a duration of 48 hours. We give a comprehensive table of hyperparameters in Appendix A.1. We consider a predetermined set of 3-second prefixes denoted as s = 3 sec. During training utterances with a length of less than 3 seconds are discarded. For Libri-Light, only 0 . 04% of utterances are less than 3 seconds.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.1 DATA AND PRE-PROCESSING",
        "chunkIndex": 31,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-32",
      "content": "mined set of 3-second prefixes denoted as s = 3 sec. During training utterances with a length of less than 3 seconds are discarded. For Libri-Light, only 0 . 04% of utterances are less than 3 seconds. To evaluate our model and the baseline models during testing, we utilize the test-clean test set from LibriSpeech (Panayotov et al., 2015). We employ the first 3 seconds of each utterance in the test set as a prompt to the models, excluding the ground truth transcripts. For semantic and acoustic quality, Spectron was trained with a LM of 350 million parameters, while for the question answering task, Spectron was trained with an LM of 1 billion parameters. The two models are identical except for the LM. In Sections 4.2.1 and 4.2.2 a 350M LM was used; in Section 4.2.3 a 1B LM was used.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.1 DATA AND PRE-PROCESSING",
        "chunkIndex": 32,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-33",
      "content": "We compare our method against existing spoken language models:\n\nGSLM: We evaluate their best model, the HuBERT-L6 configuration with 200 token units, for conditional speech continuation. The model was trained on a filtered subset of Libri-Light (Rivière &amp; Dupoux, 2021). AudioLM: We utilize the Libri-Light trained model described in their work. The two AudioLM models we compare against differ in the number of SoundStream residual vector quantizer (RVQ) layers they generate. One model generates the top 3 layers ( 3-RVQ ), while the other model generates all 12 layers ( 12-RVQ ). TWIST: We evaluate both the OPT-1.3B and LLaMA-7Binitialized versions of their models, which were trained towards quantized HuBERT representations.\n\nTheir models were trained on Libri-Light, Spotify podcasts (Clifton et al., 2020), The People's Speech (Galvez et al., 2021) and V oxPopuli (Wang et al., 2021).",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2 BASELINES",
        "chunkIndex": 33,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-34",
      "content": "ards quantized HuBERT representations.\n\nTheir models were trained on Libri-Light, Spotify podcasts (Clifton et al., 2020), The People's Speech (Galvez et al., 2021) and V oxPopuli (Wang et al., 2021). SpeechGPT: We evaluate their open-sourced model, which is based upon the LLaMA-7B model with HuBERT speech representations. This model is termed SpeechGPT-7B-com, and was trained using all 3 training stages in SpeechGPT. The model was trained using the Libri-Light and SpeechInstruct datasets.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2 BASELINES",
        "chunkIndex": 34,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-35",
      "content": "We employ the log-perplexity metric to evaluate the semantic quality of the speech output from the models. We use a state-of-the-art Conformer ASR system (Zhang et al., 2023b) trained on a proprietary English-only dataset to transcribe the speech continuation. Subsequently, we compute the log-perplexity of the predicted transcripts using GPT-2 medium (Radford et al., 2019) via the opensource transformers library (Wolf et al., 2020). The results presented in Table 1 demonstrate the performance gains of our method compared to previous approaches such as GSLM, where our method achieves an improvement of 170 . 91 in log-perplexity. Furthermore, when compared to the state-of-the-art AudioLM method, our approach outperforms both the 3-RVQ and 12-RVQ variants, exhibiting enhancements of 12 . 88 and 14 . 20 respectively. Moreover, the results in Table 1 reveal that our method exhibits improved performance compared to existing cascade methods.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.1 SEMANTIC QUALITY",
        "chunkIndex": 35,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-36",
      "content": "We consider two metrics to capture acoustic quality and speaker consistency, respectively: Naturalness Mean Opinion Score ( N-MOS ; Nguyen et al., 2023) : This is reported solely for speech continuations. Human evaluators are tasked with assigning a rating on a five-point scale to denote the perceived naturalness of a given speech utterance, spanning from 1 (indicative of poor quality) to 5 (indicative of excellent quality). Tests were conducted using 20 randomly sampled utterances from the LibriSpeech test-clean test set. 30 raters participated in the tests. The prompts were not available to the raters. Avg. speaker similarity: We compute the speaker similarity between the input prompt and its generated continuation using the speaker encoder of the PnG-NAT TTS model (Morioka et al., 2022). We compute the speaker embeddings of both and measure the cosine similarity between each pair of embeddings. We report the average across the entire test set.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.2 ACOUSTIC QUALITY",
        "chunkIndex": 36,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-37",
      "content": "PnG-NAT TTS model (Morioka et al., 2022). We compute the speaker embeddings of both and measure the cosine similarity between each pair of embeddings. We report the average across the entire test set.\n\nAs seen in Table 2, our approach performs better than GSLM in terms of N-MOS with an improvement of 0 . 55 absolute. When compared to AudioLM, our approach is comparable to the 3-RVQ version and slightly inferior to the 12-RVQ version, with a decrease of 0 . 19 in N-MOS. One can see in Table 2 that the results of TWIST are similar to those of GSLM, and that Spectron outperforms the 1.3B and 7B versions by 0 . 4 and 0 . 65 respectively. SpeechGPT performs slightly inferior to Spectron, which outperforms it by a score of 0 . 3 . Table 3 presents the results for average speaker similarity. Our method demonstrates a significant improvement of 0 . 31 over the GSLM method. When compared to AudioLM, our method outperforms both the 3-RVQ and 12-RVQ versions, with increases of 0 . 05 and 0 .",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.2 ACOUSTIC QUALITY",
        "chunkIndex": 37,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-38",
      "content": "Our method demonstrates a significant improvement of 0 . 31 over the GSLM method. When compared to AudioLM, our method outperforms both the 3-RVQ and 12-RVQ versions, with increases of 0 . 05 and 0 . 07 in average speaker similarity, respectively. Moreover, comparing to TWIST 1.3B and 7B, the proposed method improve the average speaker similarity by 0 . 18 and 0 . 19 , respectively. These results indicate that comparable acoustic quality can be achieved with Spectron's simpler approach. Our model is trained end-to-end and utilizes the universal speech representation encoded by spectrograms. Note that SpeechGPT does not intend to preserve speaker identity, which is why its average speaker similarity is lower.\n\nTable 1: Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.2 ACOUSTIC QUALITY",
        "chunkIndex": 38,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-39",
      "content": "intend to preserve speaker identity, which is why its average speaker similarity is lower.\n\nTable 1: Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.\n\n| Method           |   Log-perplexity ( ↓ ) |\n|------------------|------------------------|\n| GSLM             |                 296.99 |\n| AudioLM (3-RVQ)  |                 138.96 |\n| AudioLM (12-RVQ) |                 140.28 |\n| TWIST (1.3B)     |                 229.53 |\n| TWIST (7B)       |                 170.81 |\n| SpeechGPT        |                 136.42 |\n| Spectron (350M)  |                 126.08 |\n\nTable 2: Naturalness Mean Opinion Score (N-MOS; Mean ± SE) for completions of LibriSpeech utterances.\n\n| Method           | N-MOS ( ↑ )   |\n|------------------|---------------|\n| GSLM             | 3.13 ± 0.32   |\n| AudioLM (3-RVQ)  | 3.61 ± 0.29   |\n| AudioLM (12-RVQ) | 3.87 ± 0.32   |\n| TWIST (1.3B)     | 3.28 ± 0.24   |\n| TWIST (7B)       | 3.03 ± 0.22   |\n| SpeechGPT        |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.2 ACOUSTIC QUALITY",
        "chunkIndex": 39,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-40",
      "content": "| 3.13 ± 0.32   |\n| AudioLM (3-RVQ)  | 3.61 ± 0.29   |\n| AudioLM (12-RVQ) | 3.87 ± 0.32   |\n| TWIST (1.3B)     | 3.28 ± 0.24   |\n| TWIST (7B)       | 3.03 ± 0.22   |\n| SpeechGPT        | 3.38 ± 0.30   |\n| Spectron (350M)  | 3.68 ± 0.29   |\n| Ground Truth     | 4.23 ± 0.33   |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.2 ACOUSTIC QUALITY",
        "chunkIndex": 40,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-41",
      "content": "We propose examining whether the models can continue spoken sentences or questions with the appropriate answer. This can can be viewed as spoken generative QA; the correct answer must be produced out of infinite possible speech continuations. Note that except for SpeechGPT, all other methods (including ours) have not seen instruction data and are thus evaluated in a zero-shot fashion for spoken question answering. Given that the various spoken language models are evaluated with 3-second input contexts, we use TTS (via the publicly-available Google Cloud TTS service, voice en-US-Neural2-C) to synthesize questions that fit within this duration. The questions are drawn from an existing set and a new test set which we name LLaMA-Questions. WebQuestions (Berant et al., 2013) is an open-ended text QA NLP dataset. The dataset contains open-ended questions that are answerable via the Freebase database and are centered around a single named entity.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.3 QUESTION ANSWERING",
        "chunkIndex": 41,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-42",
      "content": "tions (Berant et al., 2013) is an open-ended text QA NLP dataset. The dataset contains open-ended questions that are answerable via the Freebase database and are centered around a single named entity. LLaMAQuestions is an open-domain world knowledge QA dataset that we synthesized using LLaMA-7B. We prompted the model to provide questions and short answers regarding various topics. Overall, we gathered 300 questions in this manner and generally verified the answers. Answer accuracy: We use a Conformer ASR system (Zhang et al., 2023b) to transcribe the answers of the models. If the text answer is contained in the transcript, we count the answer as being correct (as zero-shot models are merely continuing the prefix audio).\n\nThe results presented in Table 4 demonstrate the performance of the proposed model in comparison to other existing models. Specifically, the proposed model exhibits an accuracy of 22 . 9% on LLaMAQuestions, while SpeechGPT achieves a comparable accuracy of 21 . 9% .",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.3 QUESTION ANSWERING",
        "chunkIndex": 42,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-43",
      "content": "e proposed model in comparison to other existing models. Specifically, the proposed model exhibits an accuracy of 22 . 9% on LLaMAQuestions, while SpeechGPT achieves a comparable accuracy of 21 . 9% . Note that in contrast to SpeechGPT's utilization of a larger model architecture comprising 7 billion parameters, our proposed method uses a more modest 1 billion parameter LM for comparable results. In contrast, TWIST models with 1.3 billion and 7 billion parameters demonstrate lower accuracies of 1% and 0 . 5% respectively. Upon careful examination, it becomes evident that these models predominantly generate completions of input questions rather than providing substantive answers. AudioLM 3-RVQ, AudioLM 12-RVQ and GSLM achieved accuracy of 7% , 6 . 7% and 4% , respectability, which is likely due to the fact that the underlying Transformer architecture is not pre-trained on a large language model. Similarly, on the Web Questions test set, the proposed model attains an accuracy of 6 .",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.3 QUESTION ANSWERING",
        "chunkIndex": 43,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-44",
      "content": "ikely due to the fact that the underlying Transformer architecture is not pre-trained on a large language model. Similarly, on the Web Questions test set, the proposed model attains an accuracy of 6 . 1% , while SpeechGPT yields a comparable accuracy of 6 . 5% . Again, TWIST models with 1.3 billion and 7 billion parameters achieve accuracies of 0 . 7% and 1 . 1% respectively, further reinforcing the observed trend of completion-centric behavior rather than direct question answering. Additionally, models such as AudioLM 3-RVQ, AudioLM 12-RVQ, and GSLM exhibit accuracies of 2 . 3% , 2 . 3% , and 1 . 5% respectively, which can likely be attributed to the absence of pre-training on a large-scale language model within the underlying Transformer architecture.\n\nAudio samples and our spoken QA dataset can be found on the project website.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.3 QUESTION ANSWERING",
        "chunkIndex": 44,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-45",
      "content": "To understand the individual impacts of various components within the proposed approach, an ablation study was conducted. We measure the log-perplexity over the test-clean test set of the LibriSpeech\n\nTable 3: Average Speaker Similarity metric for completions of LibriSpeech utterances.\n\n| Method           |   Speaker Sim. ( ↑ ) |\n|------------------|----------------------|\n| GSLM             |                 0.11 |\n| AudioLM (3-RVQ)  |                 0.37 |\n| AudioLM (12-RVQ) |                 0.35 |\n| TWIST (1.3B)     |                 0.24 |\n| TWIST (7B)       |                 0.23 |\n| SpeechGPT        |                 0.05 |\n| Spectron (350M)  |                 0.42 |\n\nTable 4: Accuracy ( % ) on spoken question answering datasets.\n\n| Method           |   Web Questions ( ↑ ) |   LLaMA-Questions ( ↑ ) | Zero-Shot   |\n|------------------|-----------------------|-------------------------|-------------|\n| GSLM             |                   1.5 |                     4   | ✓",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.4 ABLATION ANALYSIS",
        "chunkIndex": 45,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-46",
      "content": "A-Questions ( ↑ ) | Zero-Shot   |\n|------------------|-----------------------|-------------------------|-------------|\n| GSLM             |                   1.5 |                     4   | ✓           |\n| AudioLM (3-RVQ)  |                   2.3 |                     7   | ✓           |\n| AudioLM (12-RVQ) |                   2.3 |                     6.7 | ✓           |\n| TWIST (1.3B)     |                   0.7 |                     1   | ✓           |\n| TWIST (7B)       |                   1.1 |                     0.5 | ✓           |\n| SpeechGPT (7B)   |                   6.5 |                    21.9 | ×           |\n| Spectron (1B)    |                   6.1 |                    22.9 | ✓           |\n\ndataset (Panayotov et al., 2015). This study involved removing each specific component in isolation.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.4 ABLATION ANALYSIS",
        "chunkIndex": 46,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-47",
      "content": "×           |\n| Spectron (1B)    |                   6.1 |                    22.9 | ✓           |\n\ndataset (Panayotov et al., 2015). This study involved removing each specific component in isolation. (i) Disabled intermediate loss on text ('L CE') (ii) removed spectrogram derivative loss ('-( L f + L t)') (iii) removed pre-training of the language model LM, letting it train from scratch (iv) removed pre-training of the speech encoder E and training it from scratch (v) removed pre-training of both the speech encoder E and language model LM, training the entire model from scratch. The findings are summarized in Table 5. The results demonstrate that each of the aforementioned components contributes to the overall performance enhancement of the proposed approach. Notably, the ASR &amp; LM cross-entropy loss L CE and the spectrogram derivative loss L f + L t have the most significant impact, leading to a degradation of 661 . 81 and 588 . 35 in the log-perplexity score, respectively.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.4 ABLATION ANALYSIS",
        "chunkIndex": 47,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-48",
      "content": "LM cross-entropy loss L CE and the spectrogram derivative loss L f + L t have the most significant impact, leading to a degradation of 661 . 81 and 588 . 35 in the log-perplexity score, respectively. Furthermore, the incorporation of the pre-trained speech encoder and pre-trained language model exhibits a discernible decline in performance, resulting in a degradation of 87 . 17 and 75 . 63 in the log-perplexity score, respectively. Notably, when both the speech encoder and pre-trained language model are removed, a degradation of 118 . 31 in the log-perplexity score is observed.\n\nTable 5: Ablation analysis.\n\n| Model                            |   Log-perplexity ( ↓ ) |\n|----------------------------------|------------------------|\n| Proposed Spectron (350M)         |                 126.08 |\n| -L CE                            |                 714.43 |\n| - ( L f + L t )                  |                 787.89 |\n| - Pre-trained LM                 |                 201.71 |\n| - Pre-trai",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.4 ABLATION ANALYSIS",
        "chunkIndex": 48,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-49",
      "content": "|\n| -L CE                            |                 714.43 |\n| - ( L f + L t )                  |                 787.89 |\n| - Pre-trained LM                 |                 201.71 |\n| - Pre-trained speech encoder     |                 213.25 |\n| - Pre-trained LM &speech encoder |                 244.39 |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "4.2.4 ABLATION ANALYSIS",
        "chunkIndex": 49,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-50",
      "content": "The limitation of our work is the high time and space complexity of generating spectrogram frames. Since spectrogram frames are computed with a rate of 12.5 ms, generation of long speech utterances is not possible. We hypothesize that potential solutions include generating multiple spectrogram frames from each hidden representation. Another limitation is that text and spectrogram decoding processes are not parallelizable. This hinders the ability to use Spectron in streaming scenarios and introduces a small latency between audio input and output. We leave the development of a parallelized decoding algorithm for future work. We further recognize that biases in the pre-trained language model may be sustained in our model, we refer to Google (2023) for a detailed discussion of ethical considerations for text-based language models.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "5 LIMITATIONS AND FUTURE WORK",
        "chunkIndex": 50,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-51",
      "content": "We proposed Spectron, a neural direct speech continuation model that can be trained end-to-end and operates in the spectrogram domain. We showed that a pre-trained language model can be given speech recognition and generation capabilities post-hoc, by fine-tuning on continuation tasks using a pre-trained speech encoder and a novel training objective. The result is a model that benefits from the pre-training of both models and outperforms previous spoken language models on various metrics.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "6 CONCLUSION",
        "chunkIndex": 51,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-52",
      "content": "The authors would like to thank Heiga Zen, Neil Zeghidour, Eugene Kharitonov, Tal Schuster, Bryan Richter, Christian Frank, Marco Tagliasacchi, Nadav Bar, and the rest of the Google Research team for helpful discussions and previous work on data preparation. The contribution of Alon Levkovitch is part of his Ph.D. thesis research conducted at Tel-Aviv University.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "ACKNOWLEDGMENTS",
        "chunkIndex": 52,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-53",
      "content": "- Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse H. Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and Christian Havnø Frank. MusicLM: Generating music from text. CoRR , abs/2301.11325, 2023.\n- Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. SpeechT5: Unified-modal encoderdecoder pre-training for spoken language processing. In ACL (1) , pp. 5723-5738. Association for Computational Linguistics, 2022.\n- Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice: A massivelymultilingual speech corpus. In LREC , pp. 4218-4222. European Language Resources Association, 2020.\n- Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 53,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-54",
      "content": "Gregor Weber. Common Voice: A massivelymultilingual speech corpus. In LREC , pp. 4218-4222. European Language Resources Association, 2020.\n- Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In EMNLP , pp. 1533-1544. ACL, 2013.\n- Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. AudioLM: A language modeling approach to audio generation. IEEE ACM Trans. Audio Speech Lang. Process. , 31:2523-2533, 2023.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 54,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-55",
      "content": "d Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS , 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant M",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 55,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-56",
      "content": "n Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. CoRR , abs/2204.02311, 2022.\n- Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In ASRU , pp.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 56,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-57",
      "content": "Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In ASRU , pp. 244-250. IEEE, 2021.\n- Ann Clifton, Aasish Pappu, Sravana Reddy, Yongze Yu, Jussi Karlgren, Ben Carterette, and Rosie Jones. The Spotify Podcasts dataset. arXiv preprint arXiv:2004.04270 , 2020.\n- Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. CoRR , abs/2210.13438, 2022.\n\n- Keqi Deng, Songjun Cao, Yike Zhang, and Long Ma. Improving hybrid CTC/attention end-to-end speech recognition with pretrained acoustic and language models. In ASRU , pp. 76-82. IEEE, 2021.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1) , pp. 4171-4186.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 57,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-58",
      "content": "-82. IEEE, 2021.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1) , pp. 4171-4186. Association for Computational Linguistics, 2019.\n- Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. CoRR , abs/2005.00341, 2020.\n- Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with speech recognition abilities. arXiv preprint arXiv:2307.11795 , 2023.\n- Daniel Galvez, Greg Diamos, Juan Torres, Keith Achorn, Juan Felipe Cerón, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi. The People's Speech: A largescale diverse english speech recognition dataset for commercial usage.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 58,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-59",
      "content": "Achorn, Juan Felipe Cerón, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi. The People's Speech: A largescale diverse english speech recognition dataset for commercial usage. In NeurIPS Datasets and Benchmarks , 2021.\n- Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. arXiv preprint arXiv:2305.10790 , 2023.\n- Google. PaLM 2 technical report, 2023. https://ai.google/static/documents/ palm2techreport.pdf .\n- Allen L. Gorin, Giuseppe Riccardi, and Jeremy H. Wright. How may I help you? Speech Commun. , 23(1-2):113-127, 1997.\n- Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. In INTERSPEECH , pp. 5036-5040.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 59,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-60",
      "content": "ki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. In INTERSPEECH , pp. 5036-5040. ISCA, 2020.\n- Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. Textually pretrained speech language models. In NeurIPS , 2023.\n- Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew M. Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, João Carreira, and Jesse H. Engel. General-purpose, long-context autoregressive modeling with perceiver AR. In ICML , volume 162 of Proceedings of Machine Learning Research , pp. 8535-8558.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 60,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-61",
      "content": "e Alayrac, João Carreira, and Jesse H. Engel. General-purpose, long-context autoregressive modeling with perceiver AR. In ICML , volume 162 of Proceedings of Machine Learning Research , pp. 8535-8558. PMLR, 2022.\n- Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process. , 29:3451-3460, 2021.\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR . OpenReview.net, 2022.\n- Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding and generating speech, music, sound, and talking head. arXiv preprint arXiv:2304.12995 , 2023.\n- Kristiina Jokinen and Michael McTear.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 61,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-62",
      "content": "g Hong, Jiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding and generating speech, music, sound, and talking head. arXiv preprint arXiv:2304.12995 , 2023.\n- Kristiina Jokinen and Michael McTear. Spoken dialogue systems. Synthesis Lectures on Human Language Technologies , 2(1):1-151, 2009.\n- Norman P. Jouppi, George Kurian, Sheng Li, Peter C. Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David A. Patterson. TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In ISCA , pp. 82:1-82:14. ACM, 2023.\n\n- Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, and Emmanuel Dupoux. LibriLight: A benchmark for ASR with limited or no supervision.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 62,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-63",
      "content": "sky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, and Emmanuel Dupoux. LibriLight: A benchmark for ASR with limited or no supervision. In ICASSP , pp. 7669-7673. IEEE, 2020.\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. Textfree prosody-aware generative spoken language modeling. In ACL (1) , pp. 8666-8681. Association for Computational Linguistics, 2022.\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n- Yuma Koizumi, Kohei Yatabe, Heiga Zen, and Michiel Bacchiani.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 63,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-64",
      "content": "Computational Linguistics, 2022.\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n- Yuma Koizumi, Kohei Yatabe, Heiga Zen, and Michiel Bacchiani. WaveFit: an iterative and nonautoregressive neural vocoder based on fixed-point iteration. In SLT , pp. 884-891. IEEE, 2022.\n- Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Proc. NeurIPS , 33:17022-17033, 2020.\n- Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics , 9:1336-1354, 2021. doi: 10.1162/tacl\\_a\\_00430.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 64,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-65",
      "content": "man Mohamed, and Emmanuel Dupoux. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics , 9:1336-1354, 2021. doi: 10.1162/tacl\\_a\\_00430. URL https: //aclanthology.org/2021.tacl-1.79 .\n- Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Miguel Pino, Jiatao Gu, and Wei-Ning Hsu. Textless speechto-speech translation on real data. In NAACL-HLT , pp. 860-872. Association for Computational Linguistics, 2022.\n- Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning. arXiv preprint arXiv:2308.11276 , 2023a.\n- Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, et al. WavJourney: Compositional audio creation with large language models.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 65,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-66",
      "content": "- Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, et al. WavJourney: Compositional audio creation with large language models. arXiv preprint arXiv:2307.14335 , 2023b.\n- Nobuyuki Morioka, Heiga Zen, Nanxin Chen, Yu Zhang, and Yifan Ding. Residual adapters for few-shot text-to-speech speaker adaptation. arXiv preprint arXiv:2210.15868 , 2022.\n- Tu Anh Nguyen, Benoît Sagot, and Emmanuel Dupoux. Are discrete units necessary for spoken language modeling? IEEE J. Sel. Top. Signal Process. , 16(6):1415-1423, 2022.\n- Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics , 11: 250-266, 2023.\n- Maxwell I.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 66,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-67",
      "content": "Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics , 11: 250-266, 2023.\n- Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. CoRR , abs/2112.00114, 2021.\n- Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on , pp. 5206-5210. IEEE, 2015.\n\n- Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. SpecAugment: A simple data augmentation method for automatic speech recognition.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 67,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-68",
      "content": "5210. IEEE, 2015.\n\n- Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. SpecAugment: A simple data augmentation method for automatic speech recognition. In INTERSPEECH , 2019.\n- Daniel S Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V Le. Improved noisy student training for automatic speech recognition. arXiv preprint arXiv:2005.09629 , 2020.\n- Linkai Peng, Baorian Nuchged, and Yingming Gao. Spoken language intelligence of large language models for language learning. arXiv preprint arXiv:2308.14536 , 2023.\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n- Yi Ren, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Almost unsupervised text to speech and automatic speech recognition. In International conference on machine learning , pp. 5410-5419.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 68,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-69",
      "content": "- Yi Ren, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Almost unsupervised text to speech and automatic speech recognition. In International conference on machine learning , pp. 5410-5419. PMLR, 2019.\n- Morgane Rivière and Emmanuel Dupoux. Towards unsupervised learning of speech features in the wild. In 2021 IEEE Spoken Language Technology Workshop (SLT) , pp. 156-163. IEEE, 2021.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor So",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 69,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-70",
      "content": "z Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176Bparameter open-access multilingual language model. CoRR , abs/2211.05100, 2022.\n- Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural TTS synthesis by conditioning wavenet on Mel spectrogram predictions. In ICASSP , pp. 4779-4783. IEEE, 2018.\n- Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, and Yonghui Wu.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 70,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-71",
      "content": "TS synthesis by conditioning wavenet on Mel spectrogram predictions. In ICASSP , pp. 4779-4783. IEEE, 2018.\n- Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, and Yonghui Wu. Nonattentive Tacotron: Robust and controllable neural tts synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301 , 2020.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , pp. 5998-6008, 2017.\n- Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 71,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-72",
      "content": "ukhin. Attention is all you need. In NIPS , pp. 5998-6008, 2017.\n- Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In ACL 2021-59th Annual Meeting of the Association for Computational Linguistics , 2021.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS , 2022.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 72,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-73",
      "content": ", Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP (Demos) , pp. 38-45. Association for Computational Linguistics, 2020.\n\n- Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-yi Lee. SUPERB: speech processing universal performance benchmark. In Interspeech , pp. 1194-1198. ISCA, 2021.\n- Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 73,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-74",
      "content": "1194-1198. ISCA, 2021.\n- Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process. , 30: 495-507, 2022.\n- Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In ICLR . OpenReview.net, 2023.\n- Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In EMNLP (Findings) , pp. 15757-15773. Association for Computational Linguistics, 2023a.\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 74,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-75",
      "content": "In EMNLP (Findings) , pp. 15757-15773. Association for Computational Linguistics, 2023a.\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. CoRR , abs/2205.01068, 2022a.\n- Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara N. Sainath, Pedro J. Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Françoise Beaufays, and Yonghui Wu. Google USM: Scaling automatic speech recognition beyond 100 languages.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 75,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-76",
      "content": "an, Bhuvana Ramabhadran, Tara N. Sainath, Pedro J. Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Françoise Beaufays, and Yonghui Wu. Google USM: Scaling automatic speech recognition beyond 100 languages. CoRR , abs/2303.01037, 2023b.\n- Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, et al. SpeechLM: Enhanced speech pre-training with unpaired textual data. arXiv preprint arXiv:2209.15329 , 2022b.\n- Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang. LibriSQA: Pioneering free-form and open-ended spoken question answering with a novel dataset and framework. arXiv preprint arXiv:2308.10390 , 2023.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "REFERENCES",
        "chunkIndex": 76,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-77",
      "content": "Table 6: Model hyper-parameters used in the experiments. (' × n ': n layers)\n\n| Input &Output                              |                         |\n|--------------------------------------------|-------------------------|\n| Sample rate (Hz)                           | 16,000                  |\n| Mel channels                               | 128                     |\n| Mel lower band (Hz)                        | 20                      |\n| Mel upper band (Hz)                        | 8,000                   |\n| Frame size (ms)                            | 50.0                    |\n| Frame step (ms)                            | 12.5                    |\n| SpecAugment                                |                         |\n| Freq blocks                                | 2                       |\n| Time blocks                                | 10                      |\n| Freq mask max bins                         | 27                      |\n| Time mask max frames                       |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.1 TABLE OF HYPER-PARAMETERS",
        "chunkIndex": 77,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-78",
      "content": "|\n| Time blocks                                | 10                      |\n| Freq mask max bins                         | 27                      |\n| Time mask max frames                       | 40                      |\n| Time block max length ratio                | 0.05                    |\n| Speech Encoder                             |                         |\n| Conformer dims                             | 1024                    |\n| Attention heads                            | 8                       |\n| Conv kernal size                           | (3, 3)                  |\n| Conv stride size                           | (2, 2)                  |\n| Language Model                             |                         |\n| Transformer (dim × layers)                 | 1024                    |\n| Dim per head                               | 64                      |\n| Hidden dims                                | 4096                    |\n| Num heads",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.1 TABLE OF HYPER-PARAMETERS",
        "chunkIndex": 78,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-79",
      "content": "|\n| Dim per head                               | 64                      |\n| Hidden dims                                | 4096                    |\n| Num heads                                  | 16                      |\n| Vocab size                                 | 256,000                 |\n| WaveFit vocoder                            |                         |\n| Iterations                                 | 5                       |\n| UBlock upsampling factors                  | [5, 5, 2, 2, 2]         |\n| STFT loss resolutions                      | 3                       |\n| Hann win size, frame shift, FFT size res 1 | [160, 32, 512]          |\n| Hann win size, frame shift, FFT size res 2 | [400, 80, 1024]         |\n| Hann win size, frame shift, FFT size res 3 | [800, 160, 2048]        |\n| Multi-period discriminator                 | Kong et al.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.1 TABLE OF HYPER-PARAMETERS",
        "chunkIndex": 79,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-80",
      "content": "nn win size, frame shift, FFT size res 2 | [400, 80, 1024]         |\n| Hann win size, frame shift, FFT size res 3 | [800, 160, 2048]        |\n| Multi-period discriminator                 | Kong et al. (2020)      |\n| Multi-period discriminator loss weight     | 1.0                     |\n| Training                                   |                         |\n| Optimizer                                  | Adam (Kingma &Ba, 2015) |\n| Learning rate schedule                     | Vaswani et al. (2017)   |\n| Learning rate (peak)                       | 3 . 5 × 10 - 4          |\n| Warm-up steps Batch size                   | 8K 128                  |\n| Continuation loss weight λ r               | 0.1                     |\n| Derivative loss order K                    | 3                       |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.1 TABLE OF HYPER-PARAMETERS",
        "chunkIndex": 80,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-81",
      "content": "Given the variation in pre-trained models across the methods discussed in this paper, we find it fit to conduct a more comprehensive comparison of their performance. This detailed comparison is presented in Table 7. Regarding the Word Error Rate (WER) assessment for ASR systems, the WER scores are sourced from the SUPERB benchmark paper (Yang et al., 2021). WERs are reported on the LibriSpeech test-clean test set. It's important to note that these scores rely solely on the speech encoder type due to limited data availability for all utilized models. For instance, models such as mHuBERT (Lee et al., 2022) employed in SpeechGPT and the New Frequency HuBert adapted and trained within TWIST exist solely as tokenization models and lack dedicated ASR model forms.\n\nThe performance comparison of speech encoders referenced in various methods within this paper is depicted in Table 7. Notably, the performance of the speech encoders on the LibriSpeech test set is comparable.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.2 EXTENDED COMPARISON TO PREVIOUS METHODS",
        "chunkIndex": 81,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-82",
      "content": "ormance comparison of speech encoders referenced in various methods within this paper is depicted in Table 7. Notably, the performance of the speech encoders on the LibriSpeech test set is comparable. However, concerning the language models (LMs) utilized, more variation is evident among the methods. LMs span a spectrum, ranging from larger models such as SpeechGPT and TWIST employing 7B LMs, to intermediate-sized models like Spectron and AudioLM employing approximately 1B LMs, and finally, GSLM utilizing a smaller 200M parameter LM. It is widely acknowledged that LM performance is significantly influenced by model size (Kaplan et al., 2020). Moreover, diverse datasets have been employed across these systems, including LibriSpeech, LibriLight, SpeechInstruct, VoxPopuli (Wang et al., 2021), Common-Voice (Ardila et al., 2020), Spotify (Clifton et al., 2020), and The People's Speech (People; Galvez et al., 2021).\n\nTable 7: Comparison of different models mentioned in this paper.",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.2 EXTENDED COMPARISON TO PREVIOUS METHODS",
        "chunkIndex": 82,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-83",
      "content": "al., 2021), Common-Voice (Ardila et al., 2020), Spotify (Clifton et al., 2020), and The People's Speech (People; Galvez et al., 2021).\n\nTable 7: Comparison of different models mentioned in this paper.\n\n| Detail                 | Spectron              | SpeechGPT                   | TWIST                                            | GSLM                    | AudioLM     |\n|------------------------|-----------------------|-----------------------------|--------------------------------------------------|-------------------------|-------------|\n| LM #Params             | 1B / 350M             | 7B                          | 1B/7B                                            | ∼ 200M                  | 0.9B        |\n| LM Type                | PaLM-2                | LLaMA                       | OPT/LLaMA                                        | None                    | None        |\n| Speech Encoder         | USM                   | mHuBERT                     | T-HuBERT",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.2 EXTENDED COMPARISON TO PREVIOUS METHODS",
        "chunkIndex": 83,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-84",
      "content": "PT/LLaMA                                        | None                    | None        |\n| Speech Encoder         | USM                   | mHuBERT                     | T-HuBERT                                         | HuBERT                  | Wav2Vec     |\n| Speech Encoder #Params | 600M                  | 317M                        | 317M                                             | 317M                    | 600M        |\n| WER of Speech Encoder  | 3.1                   | 2.94                        | 2.94                                             | 2.94                    | 3.1         |\n| Speech Encoder Dataset | Web-scale LibriSpeech | VoxPopuli 100k              | LibriSpeech VoxPopuli CommonVoice Spotify Fisher | LibriSpeech Libri-Light | Libri-Light |\n| Training Dataset       | Libri-Light           | LibriSpeech Speech Instruct | LibriSpeech Spotify People VoxPopuli             | Libri-Light             | Libri-Light |\n| #Training Examples     | 60k                   |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.2 EXTENDED COMPARISON TO PREVIOUS METHODS",
        "chunkIndex": 84,
        "totalChunks": 86
      }
    },
    {
      "id": "2305.15255v4-chunk-85",
      "content": "| Libri-Light           | LibriSpeech Speech Instruct | LibriSpeech Spotify People VoxPopuli             | Libri-Light             | Libri-Light |\n| #Training Examples     | 60k                   | 60k + 38k                   | 150k                                             | 60k                     | 60k         |",
      "metadata": {
        "source": "arxiv:2305.15255v4",
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
          "Eliya Nachmani",
          "Alon Levkovitch",
          "Roy Hirsch",
          "Julian Salazar",
          "Chulayuth Asawaroengchai",
          "Soroosh Mariooryad",
          "Ehud Rivlin",
          "RJ Skerry-Ryan",
          "Michelle Tadmor Ramanovich"
        ],
        "section": "A.2 EXTENDED COMPARISON TO PREVIOUS METHODS",
        "chunkIndex": 85,
        "totalChunks": 86
      }
    }
  ],
  "fullText": "## SPOKEN QUESTION ANSWERING AND SPEECH CONTINUATION USING SPECTROGRAM-POWERED LLM\n\nEliya Nachmani 1 , ∗ , Alon Levkovitch 1 , 3 , ∗ , † , Roy Hirsch 2 , Julian Salazar 1 , Chulayuth Asawaroengchai 1 , Soroosh Mariooryad 1 , Ehud Rivlin 2 , RJ Skerry-Ryan 1 , Michelle Tadmor Ramanovich 1 1 Google Research, 2 Verily AI, 3 Tel-Aviv Univeristy {eliyn, alevkovitch, royhirsch}@google.com\n\n## ABSTRACT\n\nWe present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained endto-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a 'cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples and spoken QA dataset via our website. 1\n\n## 1 INTRODUCTION\n\nThe goal of natural language processing (NLP) is to develop computational models that can understand and generate human language. By capturing the statistical patterns and structures of text-based natural language, language models can predict and generate coherent and meaningful sequences of words. Combined with the Transformer model architecture (Vaswani et al., 2017), large language models (LLMs) trained on web-scale amounts of text, with proportionate compute and size, have demonstrated remarkable success in NLP tasks (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022a; Scao et al., 2022; Zeng et al., 2023). However, transferring these abilities to spoken human language remains a challenging frontier. Spoken dialog systems remain a cascade of separately trained automatic speech recognition (ASR), natural language understanding (NLU) and generation (NLG), and text-to-speech (TTS) systems (Gorin et al., 1997; Jokinen &amp; McTear, 2009), with LLMs now playing the role of a combined NLU and NLG system. However, such cascades introduce latency and additional mechanisms for propagating and rendering non-verbal cues like speaker identity and prosody. Recently, spoken language models (Lakhotia et al., 2021; Kharitonov et al., 2022) and other generative audio models (Dhariwal et al., 2020; Hawthorne et al., 2022; Borsos et al., 2023; Agostinelli et al., 2023) have emerged as a promising avenue for generative speech modeling. These works quantize audio representations (Hsu et al., 2021; Chung et al., 2021; Zeghidour et al., 2022; Défossez et al., 2022) into learned discrete tokens compatible with the same next-token cross-entropy objective as text LLMs, a step that (Nguyen et al., 2022) argued as necessary for generative quality. In this paper, we introduce Spectron, a novel spoken language model that:\n\n- Directly process spectrograms as both input and output. Spectron leverages the audio capabilities of a pre-trained speech encoder through the use of intermediate projection layers.\n\n∗ Equal contribution.\n\n† Work done during an internship at Google.\n\n1 https://michelleramanovich.github.io/spectron/spectron\n\nFigure 1: Spectron connects the encoder of a speech recognition model with a pre-trained Transformer decoder language model. At training time, we take speech utterances and split their audio into a prompt and its continuation . From the prompt speech features, the full (prompt and continuation's) transcript must be reconstructed, as well as the continuation's speech features via newly introduced pre- and post-net speech modules. At inference time, only a prompt is provided; the prompt's transcription, text continuation, and speech continuations are all generated by the model.\n\n<!-- image -->\n\n- Demonstrably transfer generative ability from a pre-trained LLM, as shown by competitive performance in semantic coherence and spoken question answering over other end-to-end spoken language models.\n\nTo quantify this transfer of knowledge, we also introduce two benchmarks for the nascent spoken QA task, which we synthesize from Web Questions (Berant et al., 2013) and generations from LLaMA (Touvron et al., 2023). Audio samples and our LLaMA dataset can be found on the project website given on the first page.\n\nOur work shows that the inductive biases from a pre-trained speech encoder and a language model decoder enable end-to-end training and state-of-the-art performance without sacrificing representational fidelity. Key to this is a novel end-to-end training objective which implicitly supervises speech recognition, text continuation, and conditional speech synthesis in a joint manner. The language model transcribes and generates text continuations, acting as an 'intermediate scratchpad' (Nye et al., 2021; Wei et al., 2022) to be conditioned on for audio generation. A novel spectrogram regression loss also supervises the model to match the higher-order temporal and feature deltas of the ground truth, based on the idea that the derivatives of the ground truth express rich, longer-range information about the shape of the signal. Our overall scheme is summarized in Figure 1 and described in the rest of this work.\n\n## 2 RELATED WORK\n\nThe dominant approach to spoken language modeling is to use compact discrete speech representations. This allows the application of text-based language models to speech data. Typically, these representations are created by clustering the outputs of a speech encoder using K-means and taking the centroids as tokens. The resulting discrete sequences can be easily modeled using Transformer architectures (Vaswani et al., 2017). Below are notable examples of works using this approach; a comparison table is also presented in Appendix A.2.\n\nGenerative Spoken Language Modeling (GSLM; Lakhotia et al., 2021) offers a baseline system that operate on units quantized from pre-trained audio representations, such as HuBERT (Hsu et al., 2021). The quantized units are processed by a Transformer-based model. An additional unit-to-speech\n\ndecoder converts the generated units into spectrograms. Spectron's approach is more simple and explicit, where a single model is input with spectrograms and outputs raw spectrograms.\n\nTWIST (Hassid et al., 2023) uses the same unit-to-speech and speech-to-unit systems as GSLM, but warm-starts the spoken language model from a text-based language model. They show that this warm-start improves overall metrics and convergence speed, with reasonable performance on StoryCloze tasks (though notably degraded from the text model). For the textual language model, they use the state-of-the-art and open-weight OPT (Zhang et al., 2022a) and LLaMA models up to 7B (13B at camera-ready time) and show that spoken language model improvements scale.\n\nAudioLM (Borsos et al., 2023) utilizes two kinds of quantized representations: w2v-BERT (Chung et al., 2021) as semantic tokens, and SoundStream (Zeghidour et al., 2022) as acoustic tokens. SoundStream embeddings undergo discretization using residual vector quantization (RVQ), resulting in a hierarchy of vector quantizers. AudioLM utilizes three transformer models, each corresponding to a different layer of token generation. As Spectron does not involve any quantization, our method naturally preserves the input's semantic and acoustic characteristics. Furthermore, Spectron offers a single model trained with a unified objective, as opposed to the multiple components of AudioLM.\n\nSpeechGPT (Zhang et al., 2023a) adapts LLaMA-7B to perform speech tasks by using both discrete speech representations and text. They introduce the SpeechInstruct dataset which they use for instruction tuning. SpeechGPT is trained in 3 different steps: modality adaptation, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning (using LoRA; Hu et al., 2022). The obtained model is capable of generating both speech and text, as well as following instructions in both modalities. Our method, in comparison, is trained using a single reconstruction step and uses only public datasets for integration. Despite not using a curated dataset or specialized prompts, we demonstrate competitive performance on spoken question answering and superior results for speech continuation.\n\nAs for related works in other tasks: Spoken language understanding (SLU) : A number of recent studies explore the usage of pre-trained language models (LMs) for different SLU tasks. Gong et al. (2023), Zhao et al. (2023), and Liu et al. (2023a) fine-tuned LMs on audio data to perform speech-to-text question answering tasks, that is, answer textual questions about directly-input audio. Fathullah et al. (2023) showed that adding an audio encoder to an LM and training with LoRA enables the LM to perform automatic speech recognition (ASR). Zhang et al. (2022b) aligned text and audio tokens to perform a large number of SLU tasks. Peng et al. (2023) showed that LMs can be used to answer text questions about spoken properties of language. Spectron, in comparison, produces both textual outputs and spectrograms using a single autoregressive decoder. Multi-modal text-speech training : Ao et al. (2022) performed joint training on speech and text data to perform multiple tasks, such as text-to-speech (TTS) and ASR. Ren et al. (2019) used unsupervised pre-training on text and speech data to perform TTS for low-resource languages. Textual-guided audio generation : Liu et al. (2023b) used LMs to generate audio scripts and interacting with audio-creation APIs. Huang et al. (2023) augmented the ChatGPT input/output interface to invoke ASR / TTS tools.\n\n## 3 APPROACH\n\n## 3.1 ARCHITECTURE\n\nWe propose a novel architecture for direct speech continuation. The architecture is initialized with a pre-trained speech encoder denoted as E and a pre-trained language decoder denoted as LM. The encoder is prompted with a speech utterance as input, which it encodes into continuous linguistic features. These features are fed into the decoder as a prefix, and the whole encoder-decoder is optimized to jointly minimize a cross-entropy loss (for speech recognition and transcript continuation) and a novel reconstruction loss (for speech continuation). During inference, one provides a spoken speech prompt, which is encoded and then decoded to give both text and speech continuations.\n\n## 3.1.1 INPUT PRE-PROCESSING\n\nDuring training, the proposed model uses supervised speech utterances, which are pairs of speech x and transcripts y for training. The speech input, denoted as x , is a spectrogram that is split into two\n\nsegments at position s :\n\n## 3.1.3 LANGUAGE MODEL\n\nWe use prefix decoder language models with 350M or 1B parameters trained in the manner of PaLM 2 (Google, 2023), which we denote as LM ( -) . The LM receives the encoded features of the prompt x lm p as a prefix. Note that this is the only connection between the speech encoder and the LM decoder; i.e., there is no cross-attention between the encoder and the decoder. This late-stage integration is consistent with work in ASR, which found that joint fine-tuning of a pre-trained speech encoder and a pre-trained LM decoder into a sequence-to-sequence model can improve performance, even if the integration occurs as a single final layer (Deng et al., 2021); more layers did not improve performance, which they attribute to having sufficiently powerful text representations. During training, the decoder is teacher-forced to predict the text transcription y p , text continuation y c , and speech embeddings x p c . To convert the speech embeddings to and from spectrograms, we introduce lightweight modules h pre and h post , described in the next section. In all, we get next-step predictions for the concatenation of these text tokens and embeddings:\n\n<!-- formula-not-decoded -->\n\nBy having the same architecture decode the intermediate text and the spectrograms, we gain two benefits. First, we benefit from the pre-training of the LM in the text domain to continue the prompt in the text domain before synthesizing the speech. Secondly, the predicted text serves as intermediate reasoning, enhancing the quality of the synthesized speech, analogous to improvements in text-based language models when using intermediate scratchpads (Nye et al., 2021) or chain-of-thought (CoT; Wei et al., 2022).\n\n## 3.1.4 ACOUSTIC PROJECTION LAYERS\n\nTo enable the language model decoder to model speech features, we employ a multi-layer perceptron (MLP), the pre-net h pre to project the ground truth spectrogram speech continuations x c to the language model dimension x p c = h pre ( x c ) . This pre-net h pre compresses the spectrogram input x c into a lower dimension, creating a bottleneck that aids the decoding process. This bottleneck mechanism prevents the model from repetitively generating the same prediction in the decoding process, as demonstrated in previous work (Shen et al., 2018). To project ˆ x p c from the language model dimension to the spectrogram dimension, the model employs a post-net h post , which is also an MLP. This projection is represented by ˆ x c = h post (ˆ x p c ) .\n\nBoth h pre and h post are two-layer MLPs. Additionally, the input text sequence [ y p , y c ] is padded at the beginning with a 'start of sequence' (sos) token, while the output sequence is padded with an 'end of sequence' (eos) token at the final position.\n\n<!-- formula-not-decoded -->\n\nThe first segment x p (which we call the prompt ) is fed into the speech encoder E to give continuous representations that condition the LM. The second segment x c (the continuation ) is used later for a spectrogram reconstruction loss. SpecAugment (Park et al., 2019) is applied for data augmentation. The corresponding transcripts y can be also split at position ϕ ( s ) :\n\n<!-- formula-not-decoded -->\n\nwhere ϕ ( s ) maps the feature index s in x to its text token index in y . Note that ϕ ( s ) is not needed for our training losses.\n\n## 3.1.2 SPEECH ENCODER\n\nThe speech encoder E is a 600M-parameter Conformer encoder (Gulati et al., 2020) pre-trained on web-scale data (12M hours; Zhang et al., 2023b). It takes the spectrogram of the source speech as input, generating a hidden representation that incorporates both linguistic and acoustic information. The input spectrogram is first subsampled using a convolutional layer and then processed by a series of Conformer blocks. Each Conformer block consists of a feed-forward layer, a self-attention layer, a convolution layer, and a second feed-forward layer. The outputs of the total encoder E are passed through a layer P that projects the hidden representations into the embedding dimension of the language model. We denote these final embeddings\n\n<!-- formula-not-decoded -->\n\n## 3.2 TRAINING OBJECTIVE\n\nThe training methodology of the proposed approach is depicted in Figure 1. It uses two distinct loss functions: (1) cross-entropy loss, employed for both speech recognition and transcript continuation, and (2) regression loss, employed for speech continuation. During training, all parameters are updated (speech encoder E , projection layer P s , language model LM, pre-net h pre , and post-net h post ).\n\n## 3.2.1 SPEECH RECOGNITION AND TRANSCRIPT CONTINUATION\n\nThe first loss term is a combination of a speech recognition loss L ASR and a transcript continuation loss L LM, which are given by:\n\n<!-- formula-not-decoded -->\n\nwhere CE denotes cross-entropy, which quantifies the dissimilarity between the predicted distribution over ˆ y p , ˆ y c , and the corresponding ground truth distribution over y p , y c . This objective increases the likelihood of the text [ y p , y c ] under the conditional distribution modeled by the LM.\n\n## 3.2.2 SPEECH CONTINUATION\n\nThe speech continuation objective is formulated as a regression task, predicting each frame's spectrogram channels independently given previous frame spectrogram predictions and the ASR and LMcontext. To promote convergence and improve modeling power, we apply ℓ 1 and ℓ 2 regression losses on the spectrogram (Shen et al., 2020). These losses are applied to the feature-deltas of the spectrogram, and to the time-deltas of the spectrogram up to order K , giving '(discrete) derivative loss' terms. That is, for a tensor z of dimension T × F we define:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor a ground truth spectrogram x c and the predicted spectrogram ˆ x c , the speech continuation loss is a combination of three objectives:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe overall speech continuation loss is thus given by:\n\n<!-- formula-not-decoded -->\n\n## 3.2.3 OVERALL LOSS\n\nUsing the above notation, our objective is:\n\n<!-- formula-not-decoded -->\n\nSince L ASR and L LM are cross-entropy losses and since y = [ y p , y c ] (Eq.2), the overall speech recognition and transcript continuation loss can be written as:\n\n<!-- formula-not-decoded -->\n\nwhere ˆ y is the concatenation of ˆ y p and ˆ y c . This simplifies the overall loss to:\n\n<!-- formula-not-decoded -->\n\nwhere λ r is a weighting coefficient. This simplification eliminates the necessity of the text-speech time alignment ϕ ( s ) . Our approach can be seen as jointly optimizing three capabilities:\n\nSpeech recognition ( L ASR): The combined model learns to transcribe speech audio into text. As we use a pre-trained speech encoder and a pre-trained language model, this objective encourages the alignment and integration of each model's functionality.\n\nTranscript continuation ( L LM): This reuses, maintains, and leverages the language model's ability to generate natural text as learned from its training scheme, for example, dialogue for a chat-optimized LM. Depending on the utterance, the decoder may further learn to use paralinguistic cues from the prompt speech to favor certain completions.\n\nConditional speech synthesis ( L Recon.): We reuse the language model's autoregressive generation ability and direct it toward spectrogram reconstruction. As the teacher-forced transcript is available and the most 'accessible' feature, the decoder learns to perform text-to-speech. In this way, the model can synthesize the LM's arbitrary textual continuations at inference time, including words not found in training. Finally, we expect that good spectrogram-level continuations require the preservation of speaker, prosody, and channel effects from the original speech prompt.\n\n## 3.3 INFERENCE\n\nIn inference, the speech prompt x p is encoded by the speech encoder E , then projected by P s to the LM's dimension to give x lm p (Eq. 3). Utilizing x lm p and the start-of-sentence (sos) token, the language model decodes text in an autoregressive manner: ˆ y = LM ([ x lm p , sos ]) until eos is emitted, where ˆ y is a concatenation of the predicted transcript and continuation [ˆ y p , ˆ y c ] . Following this, the language model decodes a spectrogram in an autoregressive manner. It predicts the next spectrogram feature estimate ˆ x c ( t ) using prompt features x lm p , text prediction ˆ y and past estimated spectrogram features ˆ x c ( ≤ t -1) . Past spectrogram estimates ˆ x c ( ≤ t -1) are projected to the language model dimension: ˆ x p c ( ≤ t -1) = h pre (ˆ x c ( ≤ t -1)) . Then, ˆ x p c ( t ) is predicted at step t : ˆ x p c ( t ) = LM ([ x lm p , sos , ˆ y, ˆ x p c ( ≤ t -1)]) The decoded output ˆ x p c ( t ) is then projected to the spectrogram domain using h post : ˆ x c ( t ) = h post (ˆ x p c ( t )) . Finally, a vocoder converts the predicted spectrogram ˆ x c into a waveform signal.\n\n## 4 EXPERIMENTS AND RESULTS\n\n## 4.1 DATA AND PRE-PROCESSING\n\nTo empirically evaluate the performance of the proposed approach, we conducted experiments on the Libri-Light dataset (Kahn et al., 2020). Libri-Light is a 60k hour English dataset consisting of unlabelled read speech from LibriVox audiobooks. For our training objective, the dataset was transcribed using a NST (Park et al., 2020) model trained on LibriSpeech (960 hours). We used a frozen neural vocoder, WaveFit (Koizumi et al., 2022), with its default hyperparameters to convert the predicted spectrograms into raw audio. Our proposed model was trained using 64 TPUv4 chips (Jouppi et al., 2023), over a duration of 48 hours. We give a comprehensive table of hyperparameters in Appendix A.1. We consider a predetermined set of 3-second prefixes denoted as s = 3 sec. During training utterances with a length of less than 3 seconds are discarded. For Libri-Light, only 0 . 04% of utterances are less than 3 seconds. To evaluate our model and the baseline models during testing, we utilize the test-clean test set from LibriSpeech (Panayotov et al., 2015). We employ the first 3 seconds of each utterance in the test set as a prompt to the models, excluding the ground truth transcripts. For semantic and acoustic quality, Spectron was trained with a LM of 350 million parameters, while for the question answering task, Spectron was trained with an LM of 1 billion parameters. The two models are identical except for the LM. In Sections 4.2.1 and 4.2.2 a 350M LM was used; in Section 4.2.3 a 1B LM was used.\n\n## 4.2 BASELINES\n\nWe compare our method against existing spoken language models:\n\nGSLM: We evaluate their best model, the HuBERT-L6 configuration with 200 token units, for conditional speech continuation. The model was trained on a filtered subset of Libri-Light (Rivière &amp; Dupoux, 2021). AudioLM: We utilize the Libri-Light trained model described in their work. The two AudioLM models we compare against differ in the number of SoundStream residual vector quantizer (RVQ) layers they generate. One model generates the top 3 layers ( 3-RVQ ), while the other model generates all 12 layers ( 12-RVQ ). TWIST: We evaluate both the OPT-1.3B and LLaMA-7Binitialized versions of their models, which were trained towards quantized HuBERT representations.\n\nTheir models were trained on Libri-Light, Spotify podcasts (Clifton et al., 2020), The People's Speech (Galvez et al., 2021) and V oxPopuli (Wang et al., 2021). SpeechGPT: We evaluate their open-sourced model, which is based upon the LLaMA-7B model with HuBERT speech representations. This model is termed SpeechGPT-7B-com, and was trained using all 3 training stages in SpeechGPT. The model was trained using the Libri-Light and SpeechInstruct datasets.\n\n## 4.2.1 SEMANTIC QUALITY\n\nWe employ the log-perplexity metric to evaluate the semantic quality of the speech output from the models. We use a state-of-the-art Conformer ASR system (Zhang et al., 2023b) trained on a proprietary English-only dataset to transcribe the speech continuation. Subsequently, we compute the log-perplexity of the predicted transcripts using GPT-2 medium (Radford et al., 2019) via the opensource transformers library (Wolf et al., 2020). The results presented in Table 1 demonstrate the performance gains of our method compared to previous approaches such as GSLM, where our method achieves an improvement of 170 . 91 in log-perplexity. Furthermore, when compared to the state-of-the-art AudioLM method, our approach outperforms both the 3-RVQ and 12-RVQ variants, exhibiting enhancements of 12 . 88 and 14 . 20 respectively. Moreover, the results in Table 1 reveal that our method exhibits improved performance compared to existing cascade methods.\n\n## 4.2.2 ACOUSTIC QUALITY\n\nWe consider two metrics to capture acoustic quality and speaker consistency, respectively: Naturalness Mean Opinion Score ( N-MOS ; Nguyen et al., 2023) : This is reported solely for speech continuations. Human evaluators are tasked with assigning a rating on a five-point scale to denote the perceived naturalness of a given speech utterance, spanning from 1 (indicative of poor quality) to 5 (indicative of excellent quality). Tests were conducted using 20 randomly sampled utterances from the LibriSpeech test-clean test set. 30 raters participated in the tests. The prompts were not available to the raters. Avg. speaker similarity: We compute the speaker similarity between the input prompt and its generated continuation using the speaker encoder of the PnG-NAT TTS model (Morioka et al., 2022). We compute the speaker embeddings of both and measure the cosine similarity between each pair of embeddings. We report the average across the entire test set.\n\nAs seen in Table 2, our approach performs better than GSLM in terms of N-MOS with an improvement of 0 . 55 absolute. When compared to AudioLM, our approach is comparable to the 3-RVQ version and slightly inferior to the 12-RVQ version, with a decrease of 0 . 19 in N-MOS. One can see in Table 2 that the results of TWIST are similar to those of GSLM, and that Spectron outperforms the 1.3B and 7B versions by 0 . 4 and 0 . 65 respectively. SpeechGPT performs slightly inferior to Spectron, which outperforms it by a score of 0 . 3 . Table 3 presents the results for average speaker similarity. Our method demonstrates a significant improvement of 0 . 31 over the GSLM method. When compared to AudioLM, our method outperforms both the 3-RVQ and 12-RVQ versions, with increases of 0 . 05 and 0 . 07 in average speaker similarity, respectively. Moreover, comparing to TWIST 1.3B and 7B, the proposed method improve the average speaker similarity by 0 . 18 and 0 . 19 , respectively. These results indicate that comparable acoustic quality can be achieved with Spectron's simpler approach. Our model is trained end-to-end and utilizes the universal speech representation encoded by spectrograms. Note that SpeechGPT does not intend to preserve speaker identity, which is why its average speaker similarity is lower.\n\nTable 1: Log-perplexity for completions of LibriSpeech utterances given a 3-second prompt. Lower is better.\n\n| Method           |   Log-perplexity ( ↓ ) |\n|------------------|------------------------|\n| GSLM             |                 296.99 |\n| AudioLM (3-RVQ)  |                 138.96 |\n| AudioLM (12-RVQ) |                 140.28 |\n| TWIST (1.3B)     |                 229.53 |\n| TWIST (7B)       |                 170.81 |\n| SpeechGPT        |                 136.42 |\n| Spectron (350M)  |                 126.08 |\n\nTable 2: Naturalness Mean Opinion Score (N-MOS; Mean ± SE) for completions of LibriSpeech utterances.\n\n| Method           | N-MOS ( ↑ )   |\n|------------------|---------------|\n| GSLM             | 3.13 ± 0.32   |\n| AudioLM (3-RVQ)  | 3.61 ± 0.29   |\n| AudioLM (12-RVQ) | 3.87 ± 0.32   |\n| TWIST (1.3B)     | 3.28 ± 0.24   |\n| TWIST (7B)       | 3.03 ± 0.22   |\n| SpeechGPT        | 3.38 ± 0.30   |\n| Spectron (350M)  | 3.68 ± 0.29   |\n| Ground Truth     | 4.23 ± 0.33   |\n\n## 4.2.3 QUESTION ANSWERING\n\nWe propose examining whether the models can continue spoken sentences or questions with the appropriate answer. This can can be viewed as spoken generative QA; the correct answer must be produced out of infinite possible speech continuations. Note that except for SpeechGPT, all other methods (including ours) have not seen instruction data and are thus evaluated in a zero-shot fashion for spoken question answering. Given that the various spoken language models are evaluated with 3-second input contexts, we use TTS (via the publicly-available Google Cloud TTS service, voice en-US-Neural2-C) to synthesize questions that fit within this duration. The questions are drawn from an existing set and a new test set which we name LLaMA-Questions. WebQuestions (Berant et al., 2013) is an open-ended text QA NLP dataset. The dataset contains open-ended questions that are answerable via the Freebase database and are centered around a single named entity. LLaMAQuestions is an open-domain world knowledge QA dataset that we synthesized using LLaMA-7B. We prompted the model to provide questions and short answers regarding various topics. Overall, we gathered 300 questions in this manner and generally verified the answers. Answer accuracy: We use a Conformer ASR system (Zhang et al., 2023b) to transcribe the answers of the models. If the text answer is contained in the transcript, we count the answer as being correct (as zero-shot models are merely continuing the prefix audio).\n\nThe results presented in Table 4 demonstrate the performance of the proposed model in comparison to other existing models. Specifically, the proposed model exhibits an accuracy of 22 . 9% on LLaMAQuestions, while SpeechGPT achieves a comparable accuracy of 21 . 9% . Note that in contrast to SpeechGPT's utilization of a larger model architecture comprising 7 billion parameters, our proposed method uses a more modest 1 billion parameter LM for comparable results. In contrast, TWIST models with 1.3 billion and 7 billion parameters demonstrate lower accuracies of 1% and 0 . 5% respectively. Upon careful examination, it becomes evident that these models predominantly generate completions of input questions rather than providing substantive answers. AudioLM 3-RVQ, AudioLM 12-RVQ and GSLM achieved accuracy of 7% , 6 . 7% and 4% , respectability, which is likely due to the fact that the underlying Transformer architecture is not pre-trained on a large language model. Similarly, on the Web Questions test set, the proposed model attains an accuracy of 6 . 1% , while SpeechGPT yields a comparable accuracy of 6 . 5% . Again, TWIST models with 1.3 billion and 7 billion parameters achieve accuracies of 0 . 7% and 1 . 1% respectively, further reinforcing the observed trend of completion-centric behavior rather than direct question answering. Additionally, models such as AudioLM 3-RVQ, AudioLM 12-RVQ, and GSLM exhibit accuracies of 2 . 3% , 2 . 3% , and 1 . 5% respectively, which can likely be attributed to the absence of pre-training on a large-scale language model within the underlying Transformer architecture.\n\nAudio samples and our spoken QA dataset can be found on the project website.\n\n## 4.2.4 ABLATION ANALYSIS\n\nTo understand the individual impacts of various components within the proposed approach, an ablation study was conducted. We measure the log-perplexity over the test-clean test set of the LibriSpeech\n\nTable 3: Average Speaker Similarity metric for completions of LibriSpeech utterances.\n\n| Method           |   Speaker Sim. ( ↑ ) |\n|------------------|----------------------|\n| GSLM             |                 0.11 |\n| AudioLM (3-RVQ)  |                 0.37 |\n| AudioLM (12-RVQ) |                 0.35 |\n| TWIST (1.3B)     |                 0.24 |\n| TWIST (7B)       |                 0.23 |\n| SpeechGPT        |                 0.05 |\n| Spectron (350M)  |                 0.42 |\n\nTable 4: Accuracy ( % ) on spoken question answering datasets.\n\n| Method           |   Web Questions ( ↑ ) |   LLaMA-Questions ( ↑ ) | Zero-Shot   |\n|------------------|-----------------------|-------------------------|-------------|\n| GSLM             |                   1.5 |                     4   | ✓           |\n| AudioLM (3-RVQ)  |                   2.3 |                     7   | ✓           |\n| AudioLM (12-RVQ) |                   2.3 |                     6.7 | ✓           |\n| TWIST (1.3B)     |                   0.7 |                     1   | ✓           |\n| TWIST (7B)       |                   1.1 |                     0.5 | ✓           |\n| SpeechGPT (7B)   |                   6.5 |                    21.9 | ×           |\n| Spectron (1B)    |                   6.1 |                    22.9 | ✓           |\n\ndataset (Panayotov et al., 2015). This study involved removing each specific component in isolation. (i) Disabled intermediate loss on text ('L CE') (ii) removed spectrogram derivative loss ('-( L f + L t)') (iii) removed pre-training of the language model LM, letting it train from scratch (iv) removed pre-training of the speech encoder E and training it from scratch (v) removed pre-training of both the speech encoder E and language model LM, training the entire model from scratch. The findings are summarized in Table 5. The results demonstrate that each of the aforementioned components contributes to the overall performance enhancement of the proposed approach. Notably, the ASR &amp; LM cross-entropy loss L CE and the spectrogram derivative loss L f + L t have the most significant impact, leading to a degradation of 661 . 81 and 588 . 35 in the log-perplexity score, respectively. Furthermore, the incorporation of the pre-trained speech encoder and pre-trained language model exhibits a discernible decline in performance, resulting in a degradation of 87 . 17 and 75 . 63 in the log-perplexity score, respectively. Notably, when both the speech encoder and pre-trained language model are removed, a degradation of 118 . 31 in the log-perplexity score is observed.\n\nTable 5: Ablation analysis.\n\n| Model                            |   Log-perplexity ( ↓ ) |\n|----------------------------------|------------------------|\n| Proposed Spectron (350M)         |                 126.08 |\n| -L CE                            |                 714.43 |\n| - ( L f + L t )                  |                 787.89 |\n| - Pre-trained LM                 |                 201.71 |\n| - Pre-trained speech encoder     |                 213.25 |\n| - Pre-trained LM &speech encoder |                 244.39 |\n\n## 5 LIMITATIONS AND FUTURE WORK\n\nThe limitation of our work is the high time and space complexity of generating spectrogram frames. Since spectrogram frames are computed with a rate of 12.5 ms, generation of long speech utterances is not possible. We hypothesize that potential solutions include generating multiple spectrogram frames from each hidden representation. Another limitation is that text and spectrogram decoding processes are not parallelizable. This hinders the ability to use Spectron in streaming scenarios and introduces a small latency between audio input and output. We leave the development of a parallelized decoding algorithm for future work. We further recognize that biases in the pre-trained language model may be sustained in our model, we refer to Google (2023) for a detailed discussion of ethical considerations for text-based language models.\n\n## 6 CONCLUSION\n\nWe proposed Spectron, a neural direct speech continuation model that can be trained end-to-end and operates in the spectrogram domain. We showed that a pre-trained language model can be given speech recognition and generation capabilities post-hoc, by fine-tuning on continuation tasks using a pre-trained speech encoder and a novel training objective. The result is a model that benefits from the pre-training of both models and outperforms previous spoken language models on various metrics.\n\n## ACKNOWLEDGMENTS\n\nThe authors would like to thank Heiga Zen, Neil Zeghidour, Eugene Kharitonov, Tal Schuster, Bryan Richter, Christian Frank, Marco Tagliasacchi, Nadav Bar, and the rest of the Google Research team for helpful discussions and previous work on data preparation. The contribution of Alon Levkovitch is part of his Ph.D. thesis research conducted at Tel-Aviv University.\n\n## REFERENCES\n\n- Andrea Agostinelli, Timo I. Denk, Zalán Borsos, Jesse H. Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matthew Sharifi, Neil Zeghidour, and Christian Havnø Frank. MusicLM: Generating music from text. CoRR , abs/2301.11325, 2023.\n- Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. SpeechT5: Unified-modal encoderdecoder pre-training for spoken language processing. In ACL (1) , pp. 5723-5738. Association for Computational Linguistics, 2022.\n- Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common Voice: A massivelymultilingual speech corpus. In LREC , pp. 4218-4222. European Language Resources Association, 2020.\n- Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In EMNLP , pp. 1533-1544. ACL, 2013.\n- Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matthew Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. AudioLM: A language modeling approach to audio generation. IEEE ACM Trans. Audio Speech Lang. Process. , 31:2523-2533, 2023.\n- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS , 2020.\n- Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways. CoRR , abs/2204.02311, 2022.\n- Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In ASRU , pp. 244-250. IEEE, 2021.\n- Ann Clifton, Aasish Pappu, Sravana Reddy, Yongze Yu, Jussi Karlgren, Ben Carterette, and Rosie Jones. The Spotify Podcasts dataset. arXiv preprint arXiv:2004.04270 , 2020.\n- Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. CoRR , abs/2210.13438, 2022.\n\n- Keqi Deng, Songjun Cao, Yike Zhang, and Long Ma. Improving hybrid CTC/attention end-to-end speech recognition with pretrained acoustic and language models. In ASRU , pp. 76-82. IEEE, 2021.\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1) , pp. 4171-4186. Association for Computational Linguistics, 2019.\n- Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. CoRR , abs/2005.00341, 2020.\n- Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with speech recognition abilities. arXiv preprint arXiv:2307.11795 , 2023.\n- Daniel Galvez, Greg Diamos, Juan Torres, Keith Achorn, Juan Felipe Cerón, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi. The People's Speech: A largescale diverse english speech recognition dataset for commercial usage. In NeurIPS Datasets and Benchmarks , 2021.\n- Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. arXiv preprint arXiv:2305.10790 , 2023.\n- Google. PaLM 2 technical report, 2023. https://ai.google/static/documents/ palm2techreport.pdf .\n- Allen L. Gorin, Giuseppe Riccardi, and Jeremy H. Wright. How may I help you? Speech Commun. , 23(1-2):113-127, 1997.\n- Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented transformer for speech recognition. In INTERSPEECH , pp. 5036-5040. ISCA, 2020.\n- Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. Textually pretrained speech language models. In NeurIPS , 2023.\n- Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew M. Botvinick, Ian Simon, Hannah Sheahan, Neil Zeghidour, Jean-Baptiste Alayrac, João Carreira, and Jesse H. Engel. General-purpose, long-context autoregressive modeling with perceiver AR. In ICML , volume 162 of Proceedings of Machine Learning Research , pp. 8535-8558. PMLR, 2022.\n- Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process. , 29:3451-3460, 2021.\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR . OpenReview.net, 2022.\n- Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding and generating speech, music, sound, and talking head. arXiv preprint arXiv:2304.12995 , 2023.\n- Kristiina Jokinen and Michael McTear. Spoken dialogue systems. Synthesis Lectures on Human Language Technologies , 2(1):1-151, 2009.\n- Norman P. Jouppi, George Kurian, Sheng Li, Peter C. Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, and David A. Patterson. TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In ISCA , pp. 82:1-82:14. ACM, 2023.\n\n- Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, and Emmanuel Dupoux. LibriLight: A benchmark for ASR with limited or no supervision. In ICASSP , pp. 7669-7673. IEEE, 2020.\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. Textfree prosody-aware generative spoken language modeling. In ACL (1) , pp. 8666-8681. Association for Computational Linguistics, 2022.\n- Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster) , 2015.\n- Yuma Koizumi, Kohei Yatabe, Heiga Zen, and Michiel Bacchiani. WaveFit: an iterative and nonautoregressive neural vocoder based on fixed-point iteration. In SLT , pp. 884-891. IEEE, 2022.\n- Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Proc. NeurIPS , 33:17022-17033, 2020.\n- Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics , 9:1336-1354, 2021. doi: 10.1162/tacl\\_a\\_00430. URL https: //aclanthology.org/2021.tacl-1.79 .\n- Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Miguel Pino, Jiatao Gu, and Wei-Ning Hsu. Textless speechto-speech translation on real data. In NAACL-HLT , pp. 860-872. Association for Computational Linguistics, 2022.\n- Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning. arXiv preprint arXiv:2308.11276 , 2023a.\n- Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, et al. WavJourney: Compositional audio creation with large language models. arXiv preprint arXiv:2307.14335 , 2023b.\n- Nobuyuki Morioka, Heiga Zen, Nanxin Chen, Yu Zhang, and Yifan Ding. Residual adapters for few-shot text-to-speech speaker adaptation. arXiv preprint arXiv:2210.15868 , 2022.\n- Tu Anh Nguyen, Benoît Sagot, and Emmanuel Dupoux. Are discrete units necessary for spoken language modeling? IEEE J. Sel. Top. Signal Process. , 16(6):1415-1423, 2022.\n- Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics , 11: 250-266, 2023.\n- Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. CoRR , abs/2112.00114, 2021.\n- Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on , pp. 5206-5210. IEEE, 2015.\n\n- Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. SpecAugment: A simple data augmentation method for automatic speech recognition. In INTERSPEECH , 2019.\n- Daniel S Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, and Quoc V Le. Improved noisy student training for automatic speech recognition. arXiv preprint arXiv:2005.09629 , 2020.\n- Linkai Peng, Baorian Nuchged, and Yingming Gao. Spoken language intelligence of large language models for language learning. arXiv preprint arXiv:2308.14536 , 2023.\n- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n- Yi Ren, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Almost unsupervised text to speech and automatic speech recognition. In International conference on machine learning , pp. 5410-5419. PMLR, 2019.\n- Morgane Rivière and Emmanuel Dupoux. Towards unsupervised learning of speech features in the wild. In 2021 IEEE Spoken Language Technology Workshop (SLT) , pp. 156-163. IEEE, 2021.\n- Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176Bparameter open-access multilingual language model. CoRR , abs/2211.05100, 2022.\n- Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural TTS synthesis by conditioning wavenet on Mel spectrogram predictions. In ICASSP , pp. 4779-4783. IEEE, 2018.\n- Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang, Isaac Elias, Heiga Zen, and Yonghui Wu. Nonattentive Tacotron: Robust and controllable neural tts synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301 , 2020.\n- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , pp. 5998-6008, 2017.\n- Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In ACL 2021-59th Annual Meeting of the Association for Computational Linguistics , 2021.\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS , 2022.\n- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP (Demos) , pp. 38-45. Association for Computational Linguistics, 2020.\n\n- Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-yi Lee. SUPERB: speech processing universal performance benchmark. In Interspeech , pp. 1194-1198. ISCA, 2021.\n- Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process. , 30: 495-507, 2022.\n- Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In ICLR . OpenReview.net, 2023.\n- Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In EMNLP (Findings) , pp. 15757-15773. Association for Computational Linguistics, 2023a.\n- Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: Open pre-trained transformer language models. CoRR , abs/2205.01068, 2022a.\n- Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara N. Sainath, Pedro J. Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Françoise Beaufays, and Yonghui Wu. Google USM: Scaling automatic speech recognition beyond 100 languages. CoRR , abs/2303.01037, 2023b.\n- Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, et al. SpeechLM: Enhanced speech pre-training with unpaired textual data. arXiv preprint arXiv:2209.15329 , 2022b.\n- Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, and Yu Wang. LibriSQA: Pioneering free-form and open-ended spoken question answering with a novel dataset and framework. arXiv preprint arXiv:2308.10390 , 2023.\n\n## A APPENDIX\n\n## A.1 TABLE OF HYPER-PARAMETERS\n\nTable 6: Model hyper-parameters used in the experiments. (' × n ': n layers)\n\n| Input &Output                              |                         |\n|--------------------------------------------|-------------------------|\n| Sample rate (Hz)                           | 16,000                  |\n| Mel channels                               | 128                     |\n| Mel lower band (Hz)                        | 20                      |\n| Mel upper band (Hz)                        | 8,000                   |\n| Frame size (ms)                            | 50.0                    |\n| Frame step (ms)                            | 12.5                    |\n| SpecAugment                                |                         |\n| Freq blocks                                | 2                       |\n| Time blocks                                | 10                      |\n| Freq mask max bins                         | 27                      |\n| Time mask max frames                       | 40                      |\n| Time block max length ratio                | 0.05                    |\n| Speech Encoder                             |                         |\n| Conformer dims                             | 1024                    |\n| Attention heads                            | 8                       |\n| Conv kernal size                           | (3, 3)                  |\n| Conv stride size                           | (2, 2)                  |\n| Language Model                             |                         |\n| Transformer (dim × layers)                 | 1024                    |\n| Dim per head                               | 64                      |\n| Hidden dims                                | 4096                    |\n| Num heads                                  | 16                      |\n| Vocab size                                 | 256,000                 |\n| WaveFit vocoder                            |                         |\n| Iterations                                 | 5                       |\n| UBlock upsampling factors                  | [5, 5, 2, 2, 2]         |\n| STFT loss resolutions                      | 3                       |\n| Hann win size, frame shift, FFT size res 1 | [160, 32, 512]          |\n| Hann win size, frame shift, FFT size res 2 | [400, 80, 1024]         |\n| Hann win size, frame shift, FFT size res 3 | [800, 160, 2048]        |\n| Multi-period discriminator                 | Kong et al. (2020)      |\n| Multi-period discriminator loss weight     | 1.0                     |\n| Training                                   |                         |\n| Optimizer                                  | Adam (Kingma &Ba, 2015) |\n| Learning rate schedule                     | Vaswani et al. (2017)   |\n| Learning rate (peak)                       | 3 . 5 × 10 - 4          |\n| Warm-up steps Batch size                   | 8K 128                  |\n| Continuation loss weight λ r               | 0.1                     |\n| Derivative loss order K                    | 3                       |\n\n## A.2 EXTENDED COMPARISON TO PREVIOUS METHODS\n\nGiven the variation in pre-trained models across the methods discussed in this paper, we find it fit to conduct a more comprehensive comparison of their performance. This detailed comparison is presented in Table 7. Regarding the Word Error Rate (WER) assessment for ASR systems, the WER scores are sourced from the SUPERB benchmark paper (Yang et al., 2021). WERs are reported on the LibriSpeech test-clean test set. It's important to note that these scores rely solely on the speech encoder type due to limited data availability for all utilized models. For instance, models such as mHuBERT (Lee et al., 2022) employed in SpeechGPT and the New Frequency HuBert adapted and trained within TWIST exist solely as tokenization models and lack dedicated ASR model forms.\n\nThe performance comparison of speech encoders referenced in various methods within this paper is depicted in Table 7. Notably, the performance of the speech encoders on the LibriSpeech test set is comparable. However, concerning the language models (LMs) utilized, more variation is evident among the methods. LMs span a spectrum, ranging from larger models such as SpeechGPT and TWIST employing 7B LMs, to intermediate-sized models like Spectron and AudioLM employing approximately 1B LMs, and finally, GSLM utilizing a smaller 200M parameter LM. It is widely acknowledged that LM performance is significantly influenced by model size (Kaplan et al., 2020). Moreover, diverse datasets have been employed across these systems, including LibriSpeech, LibriLight, SpeechInstruct, VoxPopuli (Wang et al., 2021), Common-Voice (Ardila et al., 2020), Spotify (Clifton et al., 2020), and The People's Speech (People; Galvez et al., 2021).\n\nTable 7: Comparison of different models mentioned in this paper.\n\n| Detail                 | Spectron              | SpeechGPT                   | TWIST                                            | GSLM                    | AudioLM     |\n|------------------------|-----------------------|-----------------------------|--------------------------------------------------|-------------------------|-------------|\n| LM #Params             | 1B / 350M             | 7B                          | 1B/7B                                            | ∼ 200M                  | 0.9B        |\n| LM Type                | PaLM-2                | LLaMA                       | OPT/LLaMA                                        | None                    | None        |\n| Speech Encoder         | USM                   | mHuBERT                     | T-HuBERT                                         | HuBERT                  | Wav2Vec     |\n| Speech Encoder #Params | 600M                  | 317M                        | 317M                                             | 317M                    | 600M        |\n| WER of Speech Encoder  | 3.1                   | 2.94                        | 2.94                                             | 2.94                    | 3.1         |\n| Speech Encoder Dataset | Web-scale LibriSpeech | VoxPopuli 100k              | LibriSpeech VoxPopuli CommonVoice Spotify Fisher | LibriSpeech Libri-Light | Libri-Light |\n| Training Dataset       | Libri-Light           | LibriSpeech Speech Instruct | LibriSpeech Spotify People VoxPopuli             | Libri-Light             | Libri-Light |\n| #Training Examples     | 60k                   | 60k + 38k                   | 150k                                             | 60k                     | 60k         |",
  "tables": [
    {
      "index": 0,
      "markdown": "| Method           |   Log-perplexity ( ↓ ) |\n|------------------|------------------------|\n| GSLM             |                 296.99 |\n| AudioLM (3-RVQ)  |                 138.96 |\n| AudioLM (12-RVQ) |                 140.28 |\n| TWIST (1.3B)     |                 229.53 |\n| TWIST (7B)       |                 170.81 |\n| SpeechGPT        |                 136.42 |\n| Spectron (350M)  |                 126.08 |"
    },
    {
      "index": 1,
      "markdown": "| Method           | N-MOS ( ↑ )   |\n|------------------|---------------|\n| GSLM             | 3.13 ± 0.32   |\n| AudioLM (3-RVQ)  | 3.61 ± 0.29   |\n| AudioLM (12-RVQ) | 3.87 ± 0.32   |\n| TWIST (1.3B)     | 3.28 ± 0.24   |\n| TWIST (7B)       | 3.03 ± 0.22   |\n| SpeechGPT        | 3.38 ± 0.30   |\n| Spectron (350M)  | 3.68 ± 0.29   |\n| Ground Truth     | 4.23 ± 0.33   |"
    },
    {
      "index": 2,
      "markdown": "| Method           |   Speaker Sim. ( ↑ ) |\n|------------------|----------------------|\n| GSLM             |                 0.11 |\n| AudioLM (3-RVQ)  |                 0.37 |\n| AudioLM (12-RVQ) |                 0.35 |\n| TWIST (1.3B)     |                 0.24 |\n| TWIST (7B)       |                 0.23 |\n| SpeechGPT        |                 0.05 |\n| Spectron (350M)  |                 0.42 |"
    },
    {
      "index": 3,
      "markdown": "| Method           |   Web Questions ( ↑ ) |   LLaMA-Questions ( ↑ ) | Zero-Shot   |\n|------------------|-----------------------|-------------------------|-------------|\n| GSLM             |                   1.5 |                     4   | ✓           |\n| AudioLM (3-RVQ)  |                   2.3 |                     7   | ✓           |\n| AudioLM (12-RVQ) |                   2.3 |                     6.7 | ✓           |\n| TWIST (1.3B)     |                   0.7 |                     1   | ✓           |\n| TWIST (7B)       |                   1.1 |                     0.5 | ✓           |\n| SpeechGPT (7B)   |                   6.5 |                    21.9 | ×           |\n| Spectron (1B)    |                   6.1 |                    22.9 | ✓           |"
    },
    {
      "index": 4,
      "markdown": "| Model                            |   Log-perplexity ( ↓ ) |\n|----------------------------------|------------------------|\n| Proposed Spectron (350M)         |                 126.08 |\n| -L CE                            |                 714.43 |\n| - ( L f + L t )                  |                 787.89 |\n| - Pre-trained LM                 |                 201.71 |\n| - Pre-trained speech encoder     |                 213.25 |\n| - Pre-trained LM &speech encoder |                 244.39 |"
    },
    {
      "index": 5,
      "markdown": "| Input &Output                              |                         |\n|--------------------------------------------|-------------------------|\n| Sample rate (Hz)                           | 16,000                  |\n| Mel channels                               | 128                     |\n| Mel lower band (Hz)                        | 20                      |\n| Mel upper band (Hz)                        | 8,000                   |\n| Frame size (ms)                            | 50.0                    |\n| Frame step (ms)                            | 12.5                    |\n| SpecAugment                                |                         |\n| Freq blocks                                | 2                       |\n| Time blocks                                | 10                      |\n| Freq mask max bins                         | 27                      |\n| Time mask max frames                       | 40                      |\n| Time block max length ratio                | 0.05                    |\n| Speech Encoder                             |                         |\n| Conformer dims                             | 1024                    |\n| Attention heads                            | 8                       |\n| Conv kernal size                           | (3, 3)                  |\n| Conv stride size                           | (2, 2)                  |\n| Language Model                             |                         |\n| Transformer (dim × layers)                 | 1024                    |\n| Dim per head                               | 64                      |\n| Hidden dims                                | 4096                    |\n| Num heads                                  | 16                      |\n| Vocab size                                 | 256,000                 |\n| WaveFit vocoder                            |                         |\n| Iterations                                 | 5                       |\n| UBlock upsampling factors                  | [5, 5, 2, 2, 2]         |\n| STFT loss resolutions                      | 3                       |\n| Hann win size, frame shift, FFT size res 1 | [160, 32, 512]          |\n| Hann win size, frame shift, FFT size res 2 | [400, 80, 1024]         |\n| Hann win size, frame shift, FFT size res 3 | [800, 160, 2048]        |\n| Multi-period discriminator                 | Kong et al. (2020)      |\n| Multi-period discriminator loss weight     | 1.0                     |\n| Training                                   |                         |\n| Optimizer                                  | Adam (Kingma &Ba, 2015) |\n| Learning rate schedule                     | Vaswani et al. (2017)   |\n| Learning rate (peak)                       | 3 . 5 × 10 - 4          |\n| Warm-up steps Batch size                   | 8K 128                  |\n| Continuation loss weight λ r               | 0.1                     |\n| Derivative loss order K                    | 3                       |"
    },
    {
      "index": 6,
      "markdown": "| Detail                 | Spectron              | SpeechGPT                   | TWIST                                            | GSLM                    | AudioLM     |\n|------------------------|-----------------------|-----------------------------|--------------------------------------------------|-------------------------|-------------|\n| LM #Params             | 1B / 350M             | 7B                          | 1B/7B                                            | ∼ 200M                  | 0.9B        |\n| LM Type                | PaLM-2                | LLaMA                       | OPT/LLaMA                                        | None                    | None        |\n| Speech Encoder         | USM                   | mHuBERT                     | T-HuBERT                                         | HuBERT                  | Wav2Vec     |\n| Speech Encoder #Params | 600M                  | 317M                        | 317M                                             | 317M                    | 600M        |\n| WER of Speech Encoder  | 3.1                   | 2.94                        | 2.94                                             | 2.94                    | 3.1         |\n| Speech Encoder Dataset | Web-scale LibriSpeech | VoxPopuli 100k              | LibriSpeech VoxPopuli CommonVoice Spotify Fisher | LibriSpeech Libri-Light | Libri-Light |\n| Training Dataset       | Libri-Light           | LibriSpeech Speech Instruct | LibriSpeech Spotify People VoxPopuli             | Libri-Light             | Libri-Light |\n| #Training Examples     | 60k                   | 60k + 38k                   | 150k                                             | 60k                     | 60k         |"
    }
  ],
  "stats": {
    "pages": 16,
    "chunksCreated": 86,
    "totalCharacters": 59578,
    "totalWords": 8783,
    "numTables": 7,
    "processingTimeMs": 25423
  }
}