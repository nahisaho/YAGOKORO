{
  "paper": {
    "id": "2310.12036v2",
    "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
    "abstract": "The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation. In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $Ψ$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $Ψ$PO) and to identify their potential pitfalls. We then consider another special case for $Ψ$PO by setting $Ψ$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.",
    "authors": [
      "Mohammad Gheshlaghi Azar",
      "Mark Rowland",
      "Bilal Piot",
      "Daniel Guo",
      "Daniele Calandriello",
      "Michal Valko",
      "Rémi Munos"
    ],
    "published": "2023-10-18T15:21:28.000Z",
    "updated": "2023-11-22T00:02:49.000Z",
    "primaryCategory": "cs.AI",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "pdfUrl": "https://arxiv.org/pdf/2310.12036v2",
    "absUrl": "https://arxiv.org/abs/2310.12036v2"
  },
  "chunks": [
    {
      "id": "2310.12036v2-chunk-0",
      "content": "Mohammad Gheshlaghi Azar\n\nDaniel Guo\n\nMark Rowland\n\nDaniele Calandriello\n\nMichal Valko\n\nGoogle DeepMind",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "chunkIndex": 0,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-1",
      "content": "The prevalent deployment of learning from human preferences through reinforcement learning ( RLHF ) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation ( DPO ) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.\n\nIn this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called Ψ PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "Abstract",
        "chunkIndex": 1,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-2",
      "content": "gorithms. In particular we derive a new general objective called Ψ PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of Ψ PO ) and to identify their potential pitfalls. We then consider another special case for Ψ PO by setting Ψ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.\n\nUnder review.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "Abstract",
        "chunkIndex": 2,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-3",
      "content": "Learning from human preferences (Christiano et al., 2017) is a paradigm adopted in the natural language processing literature to better align pretrained (Radford et al., 2018; Ramachandran et al., 2016) and instruction-tuned (Wei et al., 2022) generative language models to human desiderata. It consists in first collecting large amounts of data where each datum is composed of a context, pairs of continuations of the context, also called generations, and a pairwise human preference that indicates which generation is the best. Then, a policy generating good generations given a context is learnt from the collected data. We frame the problem of learning from human preferences as an offline contextual bandit problem (Lu et al., 2010). The goal of this bandit problem is that given a context to choose an action (playing the role of the generation) which is most preferred by a human rater under the constraint that the resulting bandit policy should be close to some known reference policy.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 3,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-4",
      "content": "to choose an action (playing the role of the generation) which is most preferred by a human rater under the constraint that the resulting bandit policy should be close to some known reference policy. The constraint of staying close to a known reference policy can be satisfied e.g., by using KL regularisation (Geist et al., 2019) and its role is to avoid model drift (Lazaridou et al., 2020; Lu et al., 2020).\n\nA prominent approach to tackle the problem of learning from human preferences is through reinforcement learning from human feedback ( RLHF , Ouyang et al., 2022; Stiennon et al., 2020) in which first a reward model is trained in the form of a classifier of preferred and dispreferred actions. Then the bandit policy is trained through RL to maximize this learned reward model while minimizing the distance with the reference policy. Recently RLHF has been used successfully in solving the problem of aligning generative language models with human preferences (Ouyang et al., 2022).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 4,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-5",
      "content": "minimizing the distance with the reference policy. Recently RLHF has been used successfully in solving the problem of aligning generative language models with human preferences (Ouyang et al., 2022). Furthermore recent works such as direct preference optimisation ( DPO , Rafailov et al., 2023) and ( SLiC-HF , Zhao et al., 2023) have shown that it is possible to optimize the bandit policy directly from human preferences without learning a reward model. They also have shown that on a selection of standard language\n\nBilal Piot R´ emi Munos\n\ntasks they are competitive with the state of the art RLHF while they are simpler to implement and require less resources.\n\nDespite this practical success, little is known regarding theoretical foundations of these practical methods. Notable exceptions, that consider specific special cases, are (Wang et al., 2023; Chen et al., 2022) and prior work on preference-based (Busa-Fekete et al., 2014, 2013) and dueling bandits and RL (Novoseller et al., 2020;",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 5,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-6",
      "content": "at consider specific special cases, are (Wang et al., 2023; Chen et al., 2022) and prior work on preference-based (Busa-Fekete et al., 2014, 2013) and dueling bandits and RL (Novoseller et al., 2020; Pacchiano et al., 2023). However, these theoretical works focus on providing theoretical guarantees in terms of regret bounds in the standard bandit setting and they do not deal with the practical setting of RLHF , DPO and SLiC-HF .\n\nIn this work, our focus is on bridging the gap between theory and practice by introducing a simple and general theoretical representation of the practical algorithms for learning from human preferences. In particular, we show that it is possible to characterise the objective functions of RLHF and DPO as special cases of a more general objective exclusively expressed in terms of pairwise preferences. We call this objective Ψ-preference optimisation (Ψ PO ) objective, where Ψ is an arbitrary non-deceasing mapping.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 6,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-7",
      "content": "s of a more general objective exclusively expressed in terms of pairwise preferences. We call this objective Ψ-preference optimisation (Ψ PO ) objective, where Ψ is an arbitrary non-deceasing mapping. We then analyze this objective function in the special cases of RLHF and DPO and investigate its potential pitfalls. Our theoretical investigation of RLHF and DPO reveals that in principle they can be both vulnerable to overfitting. This is due to the fact that those methods rely on the strong assumption that pairwise preferences can be substituted with ELo-score (pointwise rewards) via a Bradley-Terry (BT) modelisation (Bradley and Terry, 1952). In particular, this assumption could be problematic when the (sampled) preferences are deterministic or nearly deterministic as it leads to over-fitting to the preference dataset at the expense of ignoring the KL-regularisation term (see Sec. 4.2).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 7,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-8",
      "content": "tic when the (sampled) preferences are deterministic or nearly deterministic as it leads to over-fitting to the preference dataset at the expense of ignoring the KL-regularisation term (see Sec. 4.2). We then present a simple solution to avoid the problem of overfitting, namely by setting Ψ to identity in the Ψ PO . This method is called IdentityPO ( IPO ) and by construction bypasses the BT modelisation assumption for preferences (see Sec. 5). Finally, we propose a practical solution, via a sampled loss function (see Sec. 5.2), to optimize this simplified version of Ψ PO empirically and, we compare its performance with DPO on simple bandit examples, providing empirical support for our theoretical findings (see Sec. 5.3 and Sec. 5.4).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "1 Introduction",
        "chunkIndex": 8,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-9",
      "content": "In the remaining, we build on the notations of DPO (Rafailov et al., 2023). Given a context x ∈ X , where X is the finite space of contexts, we assume a finite action space Y . A policy π ∈ ∆ X Y associates to each context x ∈ X a discrete probability distribution π ( . | x ) ∈ ∆ Y where ∆ Y is the set of discrete distributions over Y . We denote µ ∈ ∆ X Y the behavior policy. From a given context x , let y, y ′ ∼ µ ( x ) be two actions generated independently by the reference policy. These are then presented to human raters who express preferences for one of the generations, denoted as y w ≻ y l where y w and y l denote the preferred and dispreferred actions amongst { y, y ′ } respectively. We then write true human preference p ∗ ( y ≻ y ′ | x ) the probability of y being preferred to y ′ knowing the context x . The probability comes from the randomness of the choice of the human we ask for their preference.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "2 Notations",
        "chunkIndex": 9,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-10",
      "content": "man preference p ∗ ( y ≻ y ′ | x ) the probability of y being preferred to y ′ knowing the context x . The probability comes from the randomness of the choice of the human we ask for their preference. So p ∗ ( y ≻ y ′ | x ) = E h [ I { h prefers y to y ′ given x } ], where the expectation is over humans h . We also introduce the expected preference of a generation y over a distribution µ knowing x , noted p ∗ ( y ≻ µ | x ), via the following equation:\n\n<!-- formula-not-decoded -->\n\nFor any two policy π, µ ∈ ∆ X Y and a context distribution ρ we denote the total preference of policy π to µ as\n\n<!-- formula-not-decoded -->\n\nIn practice, we do not observe p ∗ directly, but samples I ( y, y ′ | x ) from a Bernoulli distribution with mean p ∗ ( y ≻ y ′ | x ) (i.e., I ( y, y ′ | x ) is 1 with probability p ∗ ( y ≻ y ′ | x ) and 0 otherwise).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "2 Notations",
        "chunkIndex": 10,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-11",
      "content": "o not observe p ∗ directly, but samples I ( y, y ′ | x ) from a Bernoulli distribution with mean p ∗ ( y ≻ y ′ | x ) (i.e., I ( y, y ′ | x ) is 1 with probability p ∗ ( y ≻ y ′ | x ) and 0 otherwise). In particular, we assume we have access to the preferences through a dataset of rated generations D = ( x i , y i , y ′ i ) N i =1 = ( x i , y w,i ≻ y l,i ) N i = i , where N is the dataset size. In addition, for a general finite set S , a discrete probability distribution η ∈ ∆ S and a real function f ∈ R S , we note the expectation of f under η as E s ∼ η [ f ( s )] = ∑ s ∈S f ( s ) η ( s ). For a finite dataset D = ( s i ) N i =1 , with s i ∈ S for each i , and a real function f ∈ R S , we denote the empirical expectation of f under D as E s ∼ D [ f ( s )] = 1 N ∑ N i =1 f ( s i ).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "2 Notations",
        "chunkIndex": 11,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-12",
      "content": "The standard RLHF paradigm (Christiano et al., 2017; Stiennon et al., 2020) consists of two main stages: (i) learning the reward model; (ii) policy optimisation using the learned reward. Here we provide a recap of these stages.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "3.1 Reinforcement Learning from Human Feedback ( RLHF )",
        "chunkIndex": 12,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-13",
      "content": "Learning a reward model consists in training a binary classifier to discriminate between the preferred and dispreferred actions using a logistic regression loss. For the classifier, a popular choice is Bradley-Terry model: for a given context x and action y , we denote the pointwise reward, which can also be interpreted as an Elo score, of y given x by r ( x, y ). The Bradley-Terry model represents the preference function p ( y ≻ y ′ | x ) (classifier) as a sigmoid of the difference of rewards:\n\n<!-- formula-not-decoded -->\n\nwhere σ ( · ) denotes the sigmoid function and plays the role of normalisation. Given the dataset D = ( x i , y w,i ≻ y l,i ) N i =1 one can learn the reward function by optimizing the following logistic regression loss\n\n<!-- formula-not-decoded -->\n\nAssuming that p ∗ ( y ≻ y ′ | x ) conforms to the BradleyTerry model, one can show that as the size of the dataset D grows, p ( y ≻ y ′ | x ) becomes a more and more accurate estimate of true p ∗ ( y ≻ y ′ | x ) and in",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "3.1.1 Learning the Reward Model",
        "chunkIndex": 13,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-14",
      "content": "( y ≻ y ′ | x ) conforms to the BradleyTerry model, one can show that as the size of the dataset D grows, p ( y ≻ y ′ | x ) becomes a more and more accurate estimate of true p ∗ ( y ≻ y ′ | x ) and in the limit converges to p ∗ ( y ≻ y ′ | x ).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "3.1.1 Learning the Reward Model",
        "chunkIndex": 14,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-15",
      "content": "Using the reward (Elo-score) r ( x, y ) the RLHF objective is simply to optimize for the policy π ∈ ∆ X Y that maximizes the expected reward while minimizing the distance between π and some reference policy π ref ∈ ∆ X Y through the following KL-regularized objective function:\n\n<!-- formula-not-decoded -->\n\nin which the context x is drawn from ρ and the action y is drawn from π ( . | x ). The divergence D KL ( π || π ref ) is defined as follows:\n\n<!-- formula-not-decoded -->\n\nwhere:\n\n<!-- formula-not-decoded -->\n\nThe objective in Equation (3) is essentially optimized by PPO (Schulman et al., 2017) or similar approaches.\n\nThe combination of RLHF +PPO has been used with great success in practice (e.g., InsturctGPT and GPT4 Ouyang et al., 2022; OpenAI, 2023).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "3.1.2 Policy Optimisation with the Learned Reward",
        "chunkIndex": 15,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-16",
      "content": "An alternative approach to the RL paradigm described above is direct preference optimisation ( DPO ; Rafailov et al., 2023), which avoids the training of a reward model altogether. The loss that DPO optimises, given an empirical dataset D , as a function of π , is given by\n\n<!-- formula-not-decoded -->\n\nIn its population form, the loss takes on the form\n\n<!-- formula-not-decoded -->\n\nRafailov et al. (2023) show that when (i) the BradleyTerry model in Equation (1) perfectly fits the preference data and (ii) the optimal reward function r is obtained from the loss in Equation (2), then the global optimisers of the RLHF objective in Equation (3) and the DPO objective in Equation (5) perfectly coincide. In fact, this correspondence is true more generally; see Proposition 4 in Appendix B.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "3.2 Direct Preference Optimisation",
        "chunkIndex": 16,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-17",
      "content": "A central conceptual contribution of the paper is to propose a general objective for RLHF , based on maximizing a non-linear function of preferences. To this end, we consider a general non-decreasing function Ψ : [0 , 1] → R , a reference policy π ref ∈ ∆ X Y , and a real positive regularisation parameter τ ∈ R ∗ + , and define the Ψ -preference optimisation objective (Ψ PO ) as\n\n<!-- formula-not-decoded -->\n\nThis objective balances the maximisation of a potentially non-linear function of preference probabilities with the KL regularisation term which encourages policies to be close to the reference π ref . This is motivated by the form of Equation (3), and we will see in the next subsection that it strictly generalises both RLHF and DPO , when the BT model holds.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4 A General Objective for Preference Optimisation",
        "chunkIndex": 17,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-18",
      "content": "In the remaining, we omit the dependency on x for the ease of notations. This is without losing generality and all the following results are true for all x ∈ Supp ( ρ ).\n\nWe first connect DPO and RLHF with the Ψ-preference objective in Equation (6), under the special choice of Ψ( q ) = log( q/ (1 -q )). More precisely, the following proposition establishes this connection.\n\nProposition 1. Suppose Ψ( q ) = log( q/ (1 -q )). When the Bradley-Terry model holds for p ∗ , that is, there exists r : Y → R such that\n\n<!-- formula-not-decoded -->\n\nthen the optimal policy for Equation (6), for the RLHF objective in Equation (3), and for the standard DPO objective in Equation (5) are identical.\n\nProof. Note that under the assumption that the Bradley-Terry model holds, we have\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.1 A Deeper Analysis of DPO and RLHF",
        "chunkIndex": 18,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-19",
      "content": "tive in Equation (3), and for the standard DPO objective in Equation (5) are identical.\n\nProof. Note that under the assumption that the Bradley-Terry model holds, we have\n\n<!-- formula-not-decoded -->\n\nThis is equal to the reward in Equation (3), up to an additive constant, and so it therefore follows that the optimal policy for Equation (6) and for optimizing the objective in Equation (3) are identical. Further, as shown by Rafailov et al. (2023), the optimal policy for the DPO objective in Equation (5) and the objective in Equation (3) are identical, which gives the statement of the proposition.\n\nApplying this proposition to the objective function of Equation (6), for which there exists an analytical solution, reveals that under the BT assumption the closedform solution to DPO and RLHF can be written as\n\n<!-- formula-not-decoded -->\n\nThe derivations leading to Equation 7 is a well known result and is provided in App. A.1 for completeness.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.1 A Deeper Analysis of DPO and RLHF",
        "chunkIndex": 19,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-20",
      "content": "an be written as\n\n<!-- formula-not-decoded -->\n\nThe derivations leading to Equation 7 is a well known result and is provided in App. A.1 for completeness.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.1 A Deeper Analysis of DPO and RLHF",
        "chunkIndex": 20,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-21",
      "content": "It is worth taking a step back, and asking what kinds of policies the above objective leads us to discover. This highly non-linear transformation of the preference probabilities means that small increases in preference probabilities already close to 1 are just as incentivized as larger increases in preference probabilities around 50%, which may be undesirable. The maximisation of logit-preferences, or Elo score in game-theoretic terminology, can also have counter-intuitive effects, even in transitive settings (Bertrand et al., 2023).\n\nConsider the simple example where we have two actions y and y ′ such that p ∗ ( y ≻ y ′ ) = 1, i.e., y is always preferred to y ′ . Then the Bradley-Terry model would require that ( r ( y ) -r ( y ′ )) → + ∞ to satisfy (1). If we plug this into the optimal policy (7) then we would get that π ∗ ( y ′ ) π ∗ ( y ) = 0 (i.e., π ∗ ( y ′ ) = 0) irrespective of what constant τ is used for the KL-regularisation.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.2 Weak Regularisation and Overfitting",
        "chunkIndex": 21,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-22",
      "content": "to satisfy (1). If we plug this into the optimal policy (7) then we would get that π ∗ ( y ′ ) π ∗ ( y ) = 0 (i.e., π ∗ ( y ′ ) = 0) irrespective of what constant τ is used for the KL-regularisation. Thus the strength of the KL-regularisation becomes weaker and weaker the more deterministic the preferences.\n\nThe weakness of the KL-regularisation becomes even more pronounced in the finite data regime, where we only have access to a sample estimate of the preference ˆ p ( y ≻ y ′ ). Even if the true preference is, e.g., p ∗ ( y ≻ y ′ ) = 0 . 8, empirically it can be very possible when we only have a few data points to estimate ˆ p ( y ≻ y ′ ) = 1, in which case the empirical optimal policy would make π ( y ′ ) = 0 for any τ . This means that overfitting can be a substantial empirical issue, especially when the context and action spaces are extremely large as it is for large language models.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.2 Weak Regularisation and Overfitting",
        "chunkIndex": 22,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-23",
      "content": "d make π ( y ′ ) = 0 for any τ . This means that overfitting can be a substantial empirical issue, especially when the context and action spaces are extremely large as it is for large language models.\n\nWhy may standard RLHF be more robust to this problem in practice? While a purported advantage of DPO is that it avoids the need to fit a reward function, we observe that in practice when empirical preference probabilities are in the set { 0 , 1 } , the reward function ends up being underfit . The optimal rewards in the presence of { 0 , 1 } preference probabilities are infinite, but these values are avoided, and indeed regularisation of the reward function has been observed to be an important aspect of RLHF training in practice (Christiano et al., 2017). This underfitting of the reward function is thus crucial in obtaining a final policy that is sufficiently regularised towards the reference policy π ref , and DPO , in avoiding the training of the reward function, loses the regularisatio",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.2 Weak Regularisation and Overfitting",
        "chunkIndex": 23,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-24",
      "content": "ion is thus crucial in obtaining a final policy that is sufficiently regularised towards the reference policy π ref , and DPO , in avoiding the training of the reward function, loses the regularisation of the policy that the underfitted reward function affords.\n\nWhile standard empirical practices such as earlystopping can still be used as an additional form of regularisation to curtail this kind of overfitting, in the next section, we will introduce a modification of the Ψ PO objective such that the optimal empirical policy can be close to π ref even when preferences are deterministic.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "4.2 Weak Regularisation and Overfitting",
        "chunkIndex": 24,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-25",
      "content": "We have observed in the previous section that DPO is prone to overfitting, and this stems from a combination of the unboundedness of Ψ, together with not training an explicit reward function. Not training a reward function directly is a clear advantage of DPO , but we would like to avoid the problems of overfitting as well.\n\nThis analysis of DPO motivates choices of Ψ which are bounded, ensuring that the KL regularisation in Equation 6 remains effective even in the regime of { 0 , 1 } -valued preferences, as it is often the case when working with empirical datasets. A particularly natural form of objective to consider is given by taking Ψ to be the identity mapping in Equation (6), leading to direct regularized optimisation of total preferences :\n\n<!-- formula-not-decoded -->\n\nThe standard approach to optimize the objective function of Equation (8) is through RLHF with the choice of reward r ( y ) = p ∗ ( y ≻ µ ).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5 IPO : Ψ PO with identity mapping",
        "chunkIndex": 25,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-26",
      "content": "sation of total preferences :\n\n<!-- formula-not-decoded -->\n\nThe standard approach to optimize the objective function of Equation (8) is through RLHF with the choice of reward r ( y ) = p ∗ ( y ≻ µ ). However both using RL and estimating the reward model r ( y ) can be costly. Inspired by DPO one would like to devise an empirical solution for the optimisation problem of Equation (8) which can directly learn from the preference dataset. Thus it would be able to avoid RL and reward modeling altogether.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5 IPO : Ψ PO with identity mapping",
        "chunkIndex": 26,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-27",
      "content": "As with DPO , it will be beneficial to re-express Equation (8) as an offline learning objective. To derive such an expression, we begin by following the derivation of Rafailov et al. (2023), manipulating the analytic expression for the optimal policy into a system of rootfinding problems. As in the previous section, we drop dependence on the context x from our notation, as all arguments can be applied on a per-context basis.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor any y, y ′ ∈ Supp ( π ref ), we therefore have\n\n<!-- formula-not-decoded -->\n\nBy letting\n\n<!-- formula-not-decoded -->\n\nand rearranging Equation (10), we obtain\n\n<!-- formula-not-decoded -->\n\nThe core idea now is to consider a policy π , define\n\n<!-- formula-not-decoded -->\n\nand aim to solve the equations:\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.1 Derivations and Computationally Efficient Algorithm",
        "chunkIndex": 27,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-28",
      "content": "quation (10), we obtain\n\n<!-- formula-not-decoded -->\n\nThe core idea now is to consider a policy π , define\n\n<!-- formula-not-decoded -->\n\nand aim to solve the equations:\n\n<!-- formula-not-decoded -->\n\nLoss for IPO . We now depart from the approach to the analysis employed by Rafailov et al. (2023), to obtain a novel offline formulation of Equation (6), in the specific case of Ψ as the identity function. In this case, Equation (12) reduces to\n\n<!-- formula-not-decoded -->\n\nWe begin by re-expressing these root-finding problems as a single optimisation problem L ( π ):\n\n<!-- formula-not-decoded -->\n\nOne can easily show that for the choice of π ∗ we have L ( π ∗ ) = 0. Thus π ∗ is a global minimizer of L ( π ). The following theorem establishes the uniqueness of this solution.\n\nTheorem 2 (Uniqueness of Global/Local Optima) . Assume that Supp ( µ ) = Supp ( π ref ) and define Π to be the set of policies π such that Supp ( π ) = Supp ( µ ).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.1 Derivations and Computationally Efficient Algorithm",
        "chunkIndex": 28,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-29",
      "content": "es the uniqueness of this solution.\n\nTheorem 2 (Uniqueness of Global/Local Optima) . Assume that Supp ( µ ) = Supp ( π ref ) and define Π to be the set of policies π such that Supp ( π ) = Supp ( µ ). Then π ↦→ L ( π ) has a unique local/global minimum in Π, which is π ∗ .\n\nProof. By assumption, π ∗ ∈ Π, and by definition ∀ π ∈ Π , L ( π ) ≥ 0 as L ( π ) is an expectation of squared terms. Further, from Equation (11), it follows immediately that L ( π ∗ ) = 0, and so we deduce that π ∗ is a global optimum for L . We now show that there are no other local/global minima for L in Π.\n\nWe write J = Supp ( µ ). We parametrise the set Π via vectors of logits s ∈ R J , setting π s ( y ) = exp( s ( y )) / ∑ y ′ ∈ J exp( s ( y ′ )) for y ∈ J , and π s ( y ) = 0 otherwise. Let us write L ( s ) = L ( π s ) for the objective as a function of the logits s .\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.1 Derivations and Computationally Efficient Algorithm",
        "chunkIndex": 29,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-30",
      "content": ") = exp( s ( y )) / ∑ y ′ ∈ J exp( s ( y ′ )) for y ∈ J , and π s ( y ) = 0 otherwise. Let us write L ( s ) = L ( π s ) for the objective as a function of the logits s .\n\n<!-- formula-not-decoded -->\n\nThe objective is quadratic as a function of the logits s . Further, by expanding the quadratic above, we see that the loss can be expressed as a sum of squares\n\n<!-- formula-not-decoded -->\n\nplus linear and constant terms. This is therefore a positive-semidefinite quadratic, and hence is convex. We thus deduce that all local minimisers of the loss L ( s ) are global minimisers as well (Boyd and Vandenberghe, 2004, Chap. 4). We now notice since π s is a surjective continuous mapping from s to π one can easily show from the definition of local minimum that every local minimiser π of L corresponds to a set of local minimisers S π of L . Thus all local minimums of L are also global minimums as well.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.1 Derivations and Computationally Efficient Algorithm",
        "chunkIndex": 30,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-31",
      "content": "an easily show from the definition of local minimum that every local minimiser π of L corresponds to a set of local minimisers S π of L . Thus all local minimums of L are also global minimums as well.\n\nFinally, the only direction s in which the quadratic in Equation (15) does not increase away from 0 is when all bracketed terms remain 0; that is, in the direction (1 , . . . , 1) ∈ R J . Thus, L ( s ) is strictly convex, except in the direction (1 , . . . , 1). (Boyd and Vandenberghe, 2004, Chap. 3). However, modifying logits in the direction e = (1 , . . . , 1) does not modify the resulting policy π s , since, for y ∈ J ,\n\n<!-- formula-not-decoded -->\n\nThe strict convexity combined with the fact that π ∗ is a global minima proves that π ∗ is the unique global/local minima in Π (Boyd and Vandenberghe, 2004, Chap. 4).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.1 Derivations and Computationally Efficient Algorithm",
        "chunkIndex": 31,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-32",
      "content": "In order to obtain the sampled loss for IPO we need to show that we can build an unbiased estimate of the right-hand side of the equation (13). To this end, we consider the Population IPO Loss :\n\n<!-- formula-not-decoded -->\n\nwhere I ( y, y ′ ) is drawn from a Bernoulli distribution with mean p ∗ ( y ≻ y ′ ), i.e., I ( y, y ′ ) is 1 if y is preferred to y ′ (which happens with probability p ∗ ( y ≻ y ′ )), and 0 otherwise. This straightforwardly yields a sample-based loss that can be used, by sampling a pair ( y, y ′ ) from the preference dataset, and consulting the recorded preference to obtain a sample from I ( y, y ′ ). The following proposition justifies the switch from Equation (13) to Equation (16), by demonstrating their equality.\n\nProposition 3. The expressions in Equation (13) and Equation (16) are equal, up to an additive constant independent of π .\n\nProof. This equivalence is not completely trivial, since in general the conditional expectation\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.2 Sampled Loss for IPO",
        "chunkIndex": 32,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-33",
      "content": "nd Equation (16) are equal, up to an additive constant independent of π .\n\nProof. This equivalence is not completely trivial, since in general the conditional expectation\n\n<!-- formula-not-decoded -->\n\nis not equal to the corresponding quantity appearing in Equation (13), namely\n\n<!-- formula-not-decoded -->\n\nWe instead need to exploit some symmetry between the distributions of y and y ′ , and use the fact that h π ( y, y ′ ) decomposes as an additive function of y and y ′ . To show this equality of losses, it is enough to focus on the 'cross-terms' obtained when expanding the quadratics in Equations (13) and (16); that is, to show\n\n<!-- formula-not-decoded -->\n\nNow, starting with the right-hand side, and using the shorthand π y = log( π ( y )), π R y = log( π ref ( y )), p y = p ∗ ( y ≻ µ ), and similarly for y ′ , we have\n\n<!-- formula-not-decoded -->\n\nwhere we have used iid-ness of y and y ′ , and E y ∼ µ [ p y ] = 1 / 2. Turning to the left-hand side, we have",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.2 Sampled Loss for IPO",
        "chunkIndex": 33,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-34",
      "content": "y )), p y = p ∗ ( y ≻ µ ), and similarly for y ′ , we have\n\n<!-- formula-not-decoded -->\n\nwhere we have used iid-ness of y and y ′ , and E y ∼ µ [ p y ] = 1 / 2. Turning to the left-hand side, we have\n\n<!-- formula-not-decoded -->\n\nwhere we use the fact that E y ′ ∼ µ I ( y, y ′ ) = p y and E y ∼ µ I ( y, y ′ ) = 1 -p y ′ . This demonstrates equality of the losses, as required.\n\nWe now discuss how to approximate the loss in Equation (16) with an empirical dataset. As in our earlier discussion, the empirical dataset D takes the form\n\n( y w,i , y l,i ) N i = i . Note that each datapoint ( y w , i , y l,i ) contributes two terms to an empirical approximation of Equation (16), with ( y, y ′ , I ( y, y ′ )) = ( y w,i , y l,i , 1), and also ( y, y ′ , I ( y, y ′ )) = ( y l,i , y w,i , 0). This symmetry is important to exploit, and leads to a reduction in the variance of the loss. The overall empirical loss is therefore given by\n\n<!-- formula-not-decoded -->\n\nwhich up to a constant equals:",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.2 Sampled Loss for IPO",
        "chunkIndex": 34,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-35",
      "content": "s symmetry is important to exploit, and leads to a reduction in the variance of the loss. The overall empirical loss is therefore given by\n\n<!-- formula-not-decoded -->\n\nwhich up to a constant equals:\n\n<!-- formula-not-decoded -->\n\nThis simplified form of the loss provides some valuable insights on the way in which IPO optimizes the policy π : IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios log( π ( y w ) /π ( y l )) and log( π ref ( y w ) /π ref ( y l )) to τ -1 2 . So the weaker the regularisation becomes, the higher would be the log-likelihood ratio of y w to y l . In other words IPO , unlike DPO , always regularizes its solution towards π ref by controlling the gap between the log-likelihood ratios log( π ( y w ) /π ( y l )) and log( π ref ( y w ) /π ref ( y l )), thus avoiding the over-fitting to the preference dataset. We summarize the sampled IPO in Algorithm 1:",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.2 Sampled Loss for IPO",
        "chunkIndex": 35,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-36",
      "content": "( y w ) /π ref ( y l )), thus avoiding the over-fitting to the preference dataset. We summarize the sampled IPO in Algorithm 1:",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.2 Sampled Loss for IPO",
        "chunkIndex": 36,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-37",
      "content": "Require: Dataset D of prompts, preferred and dispreferred generations x , y w and y l , respectively. A reference policy π ref\n\n- 1: Define\n\n<!-- formula-not-decoded -->\n\n- 2: Starting from π = π ref minimize\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "Algorithm 1 Sampled IPO",
        "chunkIndex": 37,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-38",
      "content": "To illustrate the qualitative difference between our algorithm and DPO we will consider a few simple cases. For simplicity we assume there is no context x , i.e., we are in the bandit setting.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.3 Illustrative Examples",
        "chunkIndex": 38,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-39",
      "content": "We first consider the simple case where we have 2 actions only, y 1 and y 2 , and a deterministic preference between them: p ∗ ( y 1 ≻ y 2 ) = 1. Suppose we start with a uniform π ref and µ . We know from Section 4.2 that DPO will converge to the deterministic policy π ∗ ( y 1 ) = 1, π ∗ ( y 2 ) = 0 regardless of the value of τ . Thus even when the regularisation coefficient τ is very large, this is very different from the uniform π ref .\n\nNow, let us derive the optimal policy for IPO. We have p ∗ ( y 1 ≻ µ ) = 3 / 4 and p ∗ ( y 2 ≻ µ ) = 1 / 4. Plugging this into equation (9) with Ψ = I we get that π ∗ ( y 1 ) = exp(0 . 75 τ -1 ) exp(0 . 75 τ -1 )+exp(0 . 25 τ -1 ) = σ (0 . 5 τ -1 ), and π ∗ ( y 2 ) = σ ( -0 . 5 τ -1 ), where σ is the sigmoid function. Hence we see that if we have large regularisation as τ → + ∞ , then π ∗ converges to the uniform policy π ref , and on the flip side as τ → +0, then π ∗ ( y 1 ) → 1 and π ∗ ( y 2 ) → 0, which is the deterministic optimal policy.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.3.1 Asymptotic Setting",
        "chunkIndex": 39,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-40",
      "content": "large regularisation as τ → + ∞ , then π ∗ converges to the uniform policy π ref , and on the flip side as τ → +0, then π ∗ ( y 1 ) → 1 and π ∗ ( y 2 ) → 0, which is the deterministic optimal policy. The regularisation parameter τ can now actually be used to control how close to π ref we are.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.3.1 Asymptotic Setting",
        "chunkIndex": 40,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-41",
      "content": "So far we relied on the closed-form optimal policy from Eq. (9) to study DPO and IPO 's stability, but this equation is not applicable to more complex settings where we only have access to sampled preference instead of p ⋆ . We can still however find accurate approximations of the optimal policy by choosing a parametrisation π θ and optimize θ with an empirical loss over a dataset and iterative gradient-based updates. We will use this approach to show two non-asymptotic examples where DPO over-fits the dataset of preferences and ignore π ref : when one action y wins against all others DPO pushes π θ ( y ) to 1 regardless of τ , and conversely when one action y never wins against the others DPO pushes π θ ( y ) to 0 again regardless of τ . In the same scenarios, IPO does not converge to these degenerate solutions but instead remains close to π ref based on the strength of the regularisation τ .",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 41,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-42",
      "content": "hes π θ ( y ) to 0 again regardless of τ . In the same scenarios, IPO does not converge to these degenerate solutions but instead remains close to π ref based on the strength of the regularisation τ .\n\nFor both scenarios we consider a discrete space Y = { y a , y b , y c } with 3 actions, and select a dataset of pairs D = { ( y w,i , y l,j ) } . Given D , we leverage the empirical losses from Eq. 4 and Eq. 13 to find DPO 's and IPO 's optimal policy. We encode policies as π θ ( y i ) = softmax( θ ) i using a vector θ ∈ R 3 , and optimize them for 18000 steps using Adam (Kingma and Ba, 2014) with learning rate 0 . 01 and mini-batch size 9. Minibatches are constructed using uniform sampling with replacement from D . Both policies and losses are implemented using the flax python framework (Bradbury et al., 2018; Heek et al., 2023), and the Adam implementation is from optax (Babuschkin et al., 2020).",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 42,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-43",
      "content": "ement from D . Both policies and losses are implemented using the flax python framework (Bradbury et al., 2018; Heek et al., 2023), and the Adam implementation is from optax (Babuschkin et al., 2020).\n\nFigure 1: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for D 1\n\n<!-- image -->\n\nFor each set of hyper-parameters we repeat the experiment 10 times with different seeds, and report mean and 95% confidence intervals. All experiments are executed on a modern cloud virtual machine with 4 cores and 32GB of ram.\n\nIPO Avoids Greedy Policies For the first example we sample each unique action pair once to collect a dataset D containing 3 observed preferences. Due to symmetries of pairwise preferences sampling only 3 preferences can results in only two outcomes (up to permutations of the actions):\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 43,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-44",
      "content": "aining 3 observed preferences. Due to symmetries of pairwise preferences sampling only 3 preferences can results in only two outcomes (up to permutations of the actions):\n\n<!-- formula-not-decoded -->\n\nwhere we focus on D 1 , which represent a total ordering, rather than D 2 , which represent a cycle. The outcome of the experiment is reported in Fig. 1 in which, we report the learning curves for varying values of τ . We observe that DPO always converges to the deterministic policy for all values of τ . In other word DPO completely ignores the reference policy, no matter how strong is the regularisation term, and converges to the action which is preferred in the dataset. On the other hand, IPO prevent the policy from becoming greedy when the regularisation is strong.\n\nIPO Does not Exclude Actions In the first example DPO converges to a deterministic policy because one action strictly dominates all others and the loss continues to push up its likelihood until it saturates.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 44,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-45",
      "content": "s not Exclude Actions In the first example DPO converges to a deterministic policy because one action strictly dominates all others and the loss continues to push up its likelihood until it saturates. The opposite effect happens for the logical opposite\n\nFigure 2: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for D 3\n\n<!-- image -->\n\ncondition, i.e., when one action does not have at least a victory in the dataset DPO will sets its probability to 0 regardless of τ . While this is less disruptive than the first example (a single probability is perturbed whereas previously the whole policy was warped by an over-achieving action) it is also much more common in real-world data. In particular, whenever the action space is large but the dataset small, some actions will necessarily be sampled rarely or only once, making it likely to never observe a victory.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 45,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-46",
      "content": "in real-world data. In particular, whenever the action space is large but the dataset small, some actions will necessarily be sampled rarely or only once, making it likely to never observe a victory. Especially because we do not have data on their performance π should stick close to π ref for safety, but DPO 's objective does not promote this.\n\nIn the final example the dataset consists of two observed preferences D 3 = { ( y a , y b ) , ( y b , y a ) } and leave the pair ( y a , y c ) completely unobserved. We compute solutions using Adam once again, and report the results in Fig. 2 for varying values of τ . We observe again here that DPO ignores the prior π ref completely, no matter how strong we regularize the objective, whereas IPO gradually decreases the probability of unobserved action with τ .",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "5.4 Sampled Preferences",
        "chunkIndex": 46,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-47",
      "content": "We presented a unified objective, called Ψ PO , for learning from preferences. It unifies RLHF and DPO methods. In addition, we introduced a particular case of Ψ PO , called IPO , that allows to learn directly from preferences without a reward modelling stage and without relying on the Bradley-Terry modelisation assumption\n\nthat assumes that pairwise preferences can be substituted with pointwise rewards. This is important because it allows to avoid the overfitting problem. This theoretical contribution is only useful in practice if an empirical sampled loss function can be derived. This is what we have done in Sec 5 where we show that IPO can be formulated as a root-finding problem from which an empirical sampled loss function can be derived. The IPO loss function is simple, easy to implement and theoretically justified. Finally, in Sec. 5.3 and Sec.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "6 Conclusion and Future Work",
        "chunkIndex": 47,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-48",
      "content": "ed as a root-finding problem from which an empirical sampled loss function can be derived. The IPO loss function is simple, easy to implement and theoretically justified. Finally, in Sec. 5.3 and Sec. 5.4, we provide illustrative examples where we highlight the instabilities of DPO when the preferences are fully-known as well as when they are sampled. Those minimal experiments are sufficient to prove that IPO is better suited to learn from sampled preferences than DPO . Future works should scale those experiments to more complex settings such as training language models on human preferences data.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "6 Conclusion and Future Work",
        "chunkIndex": 48,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-49",
      "content": "- Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, et al. The DeepMind JAX ecosystem, 2020, 2020. URL http://github.com/deepmind .\n- Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the Elo: Realworld games are transitive, not additive. In Proceedings of the International Conference on Artificial Intelligence and Statistics , 2023.\n- Stephen P. Boyd and Lieven Vandenberghe. Convex optimization . Cambridge University Press, 2004.\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:// github.com/google/jax .\n- Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 49,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-50",
      "content": "formations of Python+NumPy programs, 2018. URL http:// github.com/google/jax .\n- Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika , 39(3/4):324-345, 1952.\n- R´ obert Busa-Fekete, Bal´ azs Sz¨ or´ enyi, Paul Weng, Weiwei Cheng, and Eyke H¨ ullermeier. Preference-based reinforcement learning: Evolutionary direct policy search using a preference-based racing algorithm. Machine Learning , (3):327-351, 2014.\n- R´ obert Busa-Fekete, Bal´ azs Sz¨ orenyi, Paul Weng, Weiwei Cheng, and Eyke H¨ ullermeier. Preference-based evolutionary direct policy search. In Autonomous Learning Workshop @ ICRA , 2013.\n- Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In Proceedings of the International Conference on Machine Learning , 2022.\n- Paul F.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 50,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-51",
      "content": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In Proceedings of the International Conference on Machine Learning , 2022.\n- Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems , 2017.\n- Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision processes. In Proceedings of the International Conference on Machine Learning , 2019.\n- Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network li-\n- brary and ecosystem for JAX, 2023. URL http: //github.com/google/flax .\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 51,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-52",
      "content": "er, and Marc van Zee. Flax: A neural network li-\n- brary and ecosystem for JAX, 2023. URL http: //github.com/google/flax .\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations , 2014.\n- Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. In Proceedings of the Annual Meeting of Association for Computational Linguistics , 2020.\n- Tyler Lu, D´ avid P´ al, and Martin P´ al. Contextual multi-armed bandits. In Proceedings of the International Conference on Artificial Intelligence and Statistics , 2010.\n- Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 52,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-53",
      "content": "Conference on Artificial Intelligence and Statistics , 2010.\n- Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In Proceedings of the International Conference on Machine Learning , 2020.\n- Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Proceedings of the Conference on Uncertainty in Artificial Intelligence , 2020.\n\nOpenAI. Gpt-4 technical report, 2023.\n\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller amd Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 53,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-54",
      "content": "cob Hilton, Fraser Kelton, Luke Miller amd Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems , 2022.\n- Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling RL: Reinforcement learning with trajectory preferences. arXiv , 2023.\n- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv , 2023.\n- Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence to sequence learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processings , 2016.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 54,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-55",
      "content": "it Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence to sequence learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processings , 2016.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv , 2017.\n- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 2020.\n- Yuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? arXiv , 2023.\n- Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 55,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-56",
      "content": "standard RL? arXiv , 2023.\n- Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proceedings of the International Conference on Learning Representations , 2022.\n- Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv , 2023.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "References",
        "chunkIndex": 56,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-57",
      "content": "For completeness, we briefly recall the proof of existence and uniqueness of the argmaximum of the following regularized criterion that can also be found in the work of Rafailov et al. (2023):\n\n<!-- formula-not-decoded -->\n\nwhere S is a finite set, f ∈ R S a function mapping elements of S to real numbers, τ ∈ R ∗ + a strictly positive real number, δ ∈ ∆ S and η ∈ ∆ S are discrete probability distributions over S . In particular, we recall that a discrete probability distribution δ ∈ ∆ S can be identified as a positive real function δ ∈ R S + verifying:\n\n<!-- formula-not-decoded -->\n\nNow, if we define the softmax probability δ ∗ ∈ ∆ S as:\n\n<!-- formula-not-decoded -->\n\nthen, under the previous definitions, we have the following result:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "A.1 Existence and uniqueness of the regularized argmaximum",
        "chunkIndex": 57,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-58",
      "content": "By definition of the KL, we now that δ ∗ = arg max δ ∈ ∆ S [ -KL( δ || δ ∗ ) ] and as:\n\n<!-- formula-not-decoded -->\n\nwhere log ( ∑ s ′ ∈S η ( s ′ ) exp( τ -1 f ( s ′ )) ) is a constant (does not depend on δ ) and τ a positive multiplicative term, then -KL( δ || δ ∗ ) and L τ ( δ ) share the same argmaximum. This concludes the proof.\n\n̸",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "APPENDICES",
        "chunkIndex": 58,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-59",
      "content": "Notice that if we search for a solution where the support of π is strictly larger than that of µ then there could be multiple solutions. Let us illustrate this case with a simple example. Consider a single state x and 3 actions y 1 , y 2 , y 3 . The reference policy π ref is uniform over { y 1 , y 2 , y 3 } and the policy µ assigns a probability 1 / 2 to both y 1 and y 2 and 0 probability to y 3 .\n\nThus the loss is L ( π ) = 2 ( τ -1 ( p ∗ ( y 1 ≻ µ ) -p ∗ ( y 2 ≻ µ ) ) -log π ( y 1 ) π ( y 2 ) ) 2 . We deduce that any policy π = ( p, q, 1 -p -q ) such that p q = e τ -1 ( p ∗ ( y 1 ≻ µ ) -p ∗ ( y 2 ≻ µ )) is a global minimum of L ( π ).\n\nIn particular there are an infinity of solutions different from the optimal solution π ∗ . The problem comes from the fact that when the support of µ does not cover the whole action space there are not enough constraints to uniquely characterize π ∗ .",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "A.2 Non-uniqueness when Supp ( π ( · )) = Supp ( µ ) :",
        "chunkIndex": 59,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-60",
      "content": "fferent from the optimal solution π ∗ . The problem comes from the fact that when the support of µ does not cover the whole action space there are not enough constraints to uniquely characterize π ∗ . Assuming that the supports of π ref and µ coincide enables us to recover uniqueness of the solution, as proven in Theorem 2.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "A.2 Non-uniqueness when Supp ( π ( · )) = Supp ( µ ) :",
        "chunkIndex": 60,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-61",
      "content": "In this section, we show the equivalence of DPO and RLHF , regardless of whether the preference model p ∗ corresponds to a Bradley-Terry model. Note that the assumption of the existence of a minimizer is to exclude cases where the loss is minimized by taking the rewards of certain actions to + / -∞ .\n\nProposition 4. Consider a preference model p ∗ such that there exists a minimizer to the Bradley-Terry loss\n\n<!-- formula-not-decoded -->\n\nThen, the optimal policy for the DPO objective in Equation (4) and for the RLHF objective in Equation (3) with reward model given as the minimizer to the Bradley-Terry loss above are identical, regardless of whether or not p ∗ corresponds to a Bradley-Terry preference model.\n\nProof. Recall that the optimal policy π ∗ r for a given reward function r for the objective in Equation (3) is given by π ∗ r ( y | x ) ∝ π ref ( y | x ) exp( τ -1 r ( x, y )). It therefore follows that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "B Additional results",
        "chunkIndex": 61,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-62",
      "content": "function r for the objective in Equation (3) is given by π ∗ r ( y | x ) ∝ π ref ( y | x ) exp( τ -1 r ( x, y )). It therefore follows that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn words, the value of the Bradley-Terry reward objective for r is the value of the DPO objective for π ∗ r . We recall also that the map r ↦→ π ∗ r is surjective.\n\nNow, suppose r is optimal for the Bradley-Terry reward objective, meaning that π ∗ r is optimal for the RLHF objective. If π ∗ r is not optimal for the DPO objective, then there exists another policy π ′ that obtains a strictly lower value for the DPO loss. But then there exists a reward function r ′ such that π ′ = π ∗ r ′ , such as r ′ ( x, y ) = τ log( π ′ ( y | x ) /π ref ( y | x )), and this r ′ therefore obtains a lower Bradley-Terry loss than r , a contradiction.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "B Additional results",
        "chunkIndex": 62,
        "totalChunks": 64
      }
    },
    {
      "id": "2310.12036v2-chunk-63",
      "content": "xists a reward function r ′ such that π ′ = π ∗ r ′ , such as r ′ ( x, y ) = τ log( π ′ ( y | x ) /π ref ( y | x )), and this r ′ therefore obtains a lower Bradley-Terry loss than r , a contradiction.\n\nSimilarly, if π ∗ is optimal for the DPO objective, the corresponding reward function r ( x, y ) = τ log( π ∗ ( y | x ) /π ref ( y | x )) must be optimal for the Bradley-Terry reward loss. The corresponding optimizer for the RLHF objective is then given by π ( y | x ) ∝ π ref ( y | x ) exp( τ -1 τ log( π ∗ ( y | x ) /π ref ( y | x ))) = π ∗ ( y | x ), as required.",
      "metadata": {
        "source": "arxiv:2310.12036v2",
        "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences",
        "authors": [
          "Mohammad Gheshlaghi Azar",
          "Mark Rowland",
          "Bilal Piot",
          "Daniel Guo",
          "Daniele Calandriello",
          "Michal Valko",
          "Rémi Munos"
        ],
        "section": "B Additional results",
        "chunkIndex": 63,
        "totalChunks": 64
      }
    }
  ],
  "fullText": "## A General Theoretical Paradigm to Understand Learning from Human Preferences\n\nMohammad Gheshlaghi Azar\n\nDaniel Guo\n\nMark Rowland\n\nDaniele Calandriello\n\nMichal Valko\n\nGoogle DeepMind\n\n## Abstract\n\nThe prevalent deployment of learning from human preferences through reinforcement learning ( RLHF ) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation ( DPO ) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.\n\nIn this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called Ψ PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of Ψ PO ) and to identify their potential pitfalls. We then consider another special case for Ψ PO by setting Ψ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.\n\nUnder review.\n\n## 1 Introduction\n\nLearning from human preferences (Christiano et al., 2017) is a paradigm adopted in the natural language processing literature to better align pretrained (Radford et al., 2018; Ramachandran et al., 2016) and instruction-tuned (Wei et al., 2022) generative language models to human desiderata. It consists in first collecting large amounts of data where each datum is composed of a context, pairs of continuations of the context, also called generations, and a pairwise human preference that indicates which generation is the best. Then, a policy generating good generations given a context is learnt from the collected data. We frame the problem of learning from human preferences as an offline contextual bandit problem (Lu et al., 2010). The goal of this bandit problem is that given a context to choose an action (playing the role of the generation) which is most preferred by a human rater under the constraint that the resulting bandit policy should be close to some known reference policy. The constraint of staying close to a known reference policy can be satisfied e.g., by using KL regularisation (Geist et al., 2019) and its role is to avoid model drift (Lazaridou et al., 2020; Lu et al., 2020).\n\nA prominent approach to tackle the problem of learning from human preferences is through reinforcement learning from human feedback ( RLHF , Ouyang et al., 2022; Stiennon et al., 2020) in which first a reward model is trained in the form of a classifier of preferred and dispreferred actions. Then the bandit policy is trained through RL to maximize this learned reward model while minimizing the distance with the reference policy. Recently RLHF has been used successfully in solving the problem of aligning generative language models with human preferences (Ouyang et al., 2022). Furthermore recent works such as direct preference optimisation ( DPO , Rafailov et al., 2023) and ( SLiC-HF , Zhao et al., 2023) have shown that it is possible to optimize the bandit policy directly from human preferences without learning a reward model. They also have shown that on a selection of standard language\n\nBilal Piot R´ emi Munos\n\ntasks they are competitive with the state of the art RLHF while they are simpler to implement and require less resources.\n\nDespite this practical success, little is known regarding theoretical foundations of these practical methods. Notable exceptions, that consider specific special cases, are (Wang et al., 2023; Chen et al., 2022) and prior work on preference-based (Busa-Fekete et al., 2014, 2013) and dueling bandits and RL (Novoseller et al., 2020; Pacchiano et al., 2023). However, these theoretical works focus on providing theoretical guarantees in terms of regret bounds in the standard bandit setting and they do not deal with the practical setting of RLHF , DPO and SLiC-HF .\n\nIn this work, our focus is on bridging the gap between theory and practice by introducing a simple and general theoretical representation of the practical algorithms for learning from human preferences. In particular, we show that it is possible to characterise the objective functions of RLHF and DPO as special cases of a more general objective exclusively expressed in terms of pairwise preferences. We call this objective Ψ-preference optimisation (Ψ PO ) objective, where Ψ is an arbitrary non-deceasing mapping. We then analyze this objective function in the special cases of RLHF and DPO and investigate its potential pitfalls. Our theoretical investigation of RLHF and DPO reveals that in principle they can be both vulnerable to overfitting. This is due to the fact that those methods rely on the strong assumption that pairwise preferences can be substituted with ELo-score (pointwise rewards) via a Bradley-Terry (BT) modelisation (Bradley and Terry, 1952). In particular, this assumption could be problematic when the (sampled) preferences are deterministic or nearly deterministic as it leads to over-fitting to the preference dataset at the expense of ignoring the KL-regularisation term (see Sec. 4.2). We then present a simple solution to avoid the problem of overfitting, namely by setting Ψ to identity in the Ψ PO . This method is called IdentityPO ( IPO ) and by construction bypasses the BT modelisation assumption for preferences (see Sec. 5). Finally, we propose a practical solution, via a sampled loss function (see Sec. 5.2), to optimize this simplified version of Ψ PO empirically and, we compare its performance with DPO on simple bandit examples, providing empirical support for our theoretical findings (see Sec. 5.3 and Sec. 5.4).\n\n## 2 Notations\n\nIn the remaining, we build on the notations of DPO (Rafailov et al., 2023). Given a context x ∈ X , where X is the finite space of contexts, we assume a finite action space Y . A policy π ∈ ∆ X Y associates to each context x ∈ X a discrete probability distribution π ( . | x ) ∈ ∆ Y where ∆ Y is the set of discrete distributions over Y . We denote µ ∈ ∆ X Y the behavior policy. From a given context x , let y, y ′ ∼ µ ( x ) be two actions generated independently by the reference policy. These are then presented to human raters who express preferences for one of the generations, denoted as y w ≻ y l where y w and y l denote the preferred and dispreferred actions amongst { y, y ′ } respectively. We then write true human preference p ∗ ( y ≻ y ′ | x ) the probability of y being preferred to y ′ knowing the context x . The probability comes from the randomness of the choice of the human we ask for their preference. So p ∗ ( y ≻ y ′ | x ) = E h [ I { h prefers y to y ′ given x } ], where the expectation is over humans h . We also introduce the expected preference of a generation y over a distribution µ knowing x , noted p ∗ ( y ≻ µ | x ), via the following equation:\n\n<!-- formula-not-decoded -->\n\nFor any two policy π, µ ∈ ∆ X Y and a context distribution ρ we denote the total preference of policy π to µ as\n\n<!-- formula-not-decoded -->\n\nIn practice, we do not observe p ∗ directly, but samples I ( y, y ′ | x ) from a Bernoulli distribution with mean p ∗ ( y ≻ y ′ | x ) (i.e., I ( y, y ′ | x ) is 1 with probability p ∗ ( y ≻ y ′ | x ) and 0 otherwise). In particular, we assume we have access to the preferences through a dataset of rated generations D = ( x i , y i , y ′ i ) N i =1 = ( x i , y w,i ≻ y l,i ) N i = i , where N is the dataset size. In addition, for a general finite set S , a discrete probability distribution η ∈ ∆ S and a real function f ∈ R S , we note the expectation of f under η as E s ∼ η [ f ( s )] = ∑ s ∈S f ( s ) η ( s ). For a finite dataset D = ( s i ) N i =1 , with s i ∈ S for each i , and a real function f ∈ R S , we denote the empirical expectation of f under D as E s ∼ D [ f ( s )] = 1 N ∑ N i =1 f ( s i ).\n\n## 3 Background\n\n## 3.1 Reinforcement Learning from Human Feedback ( RLHF )\n\nThe standard RLHF paradigm (Christiano et al., 2017; Stiennon et al., 2020) consists of two main stages: (i) learning the reward model; (ii) policy optimisation using the learned reward. Here we provide a recap of these stages.\n\n## 3.1.1 Learning the Reward Model\n\nLearning a reward model consists in training a binary classifier to discriminate between the preferred and dispreferred actions using a logistic regression loss. For the classifier, a popular choice is Bradley-Terry model: for a given context x and action y , we denote the pointwise reward, which can also be interpreted as an Elo score, of y given x by r ( x, y ). The Bradley-Terry model represents the preference function p ( y ≻ y ′ | x ) (classifier) as a sigmoid of the difference of rewards:\n\n<!-- formula-not-decoded -->\n\nwhere σ ( · ) denotes the sigmoid function and plays the role of normalisation. Given the dataset D = ( x i , y w,i ≻ y l,i ) N i =1 one can learn the reward function by optimizing the following logistic regression loss\n\n<!-- formula-not-decoded -->\n\nAssuming that p ∗ ( y ≻ y ′ | x ) conforms to the BradleyTerry model, one can show that as the size of the dataset D grows, p ( y ≻ y ′ | x ) becomes a more and more accurate estimate of true p ∗ ( y ≻ y ′ | x ) and in the limit converges to p ∗ ( y ≻ y ′ | x ).\n\n## 3.1.2 Policy Optimisation with the Learned Reward\n\nUsing the reward (Elo-score) r ( x, y ) the RLHF objective is simply to optimize for the policy π ∈ ∆ X Y that maximizes the expected reward while minimizing the distance between π and some reference policy π ref ∈ ∆ X Y through the following KL-regularized objective function:\n\n<!-- formula-not-decoded -->\n\nin which the context x is drawn from ρ and the action y is drawn from π ( . | x ). The divergence D KL ( π || π ref ) is defined as follows:\n\n<!-- formula-not-decoded -->\n\nwhere:\n\n<!-- formula-not-decoded -->\n\nThe objective in Equation (3) is essentially optimized by PPO (Schulman et al., 2017) or similar approaches.\n\nThe combination of RLHF +PPO has been used with great success in practice (e.g., InsturctGPT and GPT4 Ouyang et al., 2022; OpenAI, 2023).\n\n## 3.2 Direct Preference Optimisation\n\nAn alternative approach to the RL paradigm described above is direct preference optimisation ( DPO ; Rafailov et al., 2023), which avoids the training of a reward model altogether. The loss that DPO optimises, given an empirical dataset D , as a function of π , is given by\n\n<!-- formula-not-decoded -->\n\nIn its population form, the loss takes on the form\n\n<!-- formula-not-decoded -->\n\nRafailov et al. (2023) show that when (i) the BradleyTerry model in Equation (1) perfectly fits the preference data and (ii) the optimal reward function r is obtained from the loss in Equation (2), then the global optimisers of the RLHF objective in Equation (3) and the DPO objective in Equation (5) perfectly coincide. In fact, this correspondence is true more generally; see Proposition 4 in Appendix B.\n\n## 4 A General Objective for Preference Optimisation\n\nA central conceptual contribution of the paper is to propose a general objective for RLHF , based on maximizing a non-linear function of preferences. To this end, we consider a general non-decreasing function Ψ : [0 , 1] → R , a reference policy π ref ∈ ∆ X Y , and a real positive regularisation parameter τ ∈ R ∗ + , and define the Ψ -preference optimisation objective (Ψ PO ) as\n\n<!-- formula-not-decoded -->\n\nThis objective balances the maximisation of a potentially non-linear function of preference probabilities with the KL regularisation term which encourages policies to be close to the reference π ref . This is motivated by the form of Equation (3), and we will see in the next subsection that it strictly generalises both RLHF and DPO , when the BT model holds.\n\n## 4.1 A Deeper Analysis of DPO and RLHF\n\nIn the remaining, we omit the dependency on x for the ease of notations. This is without losing generality and all the following results are true for all x ∈ Supp ( ρ ).\n\nWe first connect DPO and RLHF with the Ψ-preference objective in Equation (6), under the special choice of Ψ( q ) = log( q/ (1 -q )). More precisely, the following proposition establishes this connection.\n\nProposition 1. Suppose Ψ( q ) = log( q/ (1 -q )). When the Bradley-Terry model holds for p ∗ , that is, there exists r : Y → R such that\n\n<!-- formula-not-decoded -->\n\nthen the optimal policy for Equation (6), for the RLHF objective in Equation (3), and for the standard DPO objective in Equation (5) are identical.\n\nProof. Note that under the assumption that the Bradley-Terry model holds, we have\n\n<!-- formula-not-decoded -->\n\nThis is equal to the reward in Equation (3), up to an additive constant, and so it therefore follows that the optimal policy for Equation (6) and for optimizing the objective in Equation (3) are identical. Further, as shown by Rafailov et al. (2023), the optimal policy for the DPO objective in Equation (5) and the objective in Equation (3) are identical, which gives the statement of the proposition.\n\nApplying this proposition to the objective function of Equation (6), for which there exists an analytical solution, reveals that under the BT assumption the closedform solution to DPO and RLHF can be written as\n\n<!-- formula-not-decoded -->\n\nThe derivations leading to Equation 7 is a well known result and is provided in App. A.1 for completeness.\n\n## 4.2 Weak Regularisation and Overfitting\n\nIt is worth taking a step back, and asking what kinds of policies the above objective leads us to discover. This highly non-linear transformation of the preference probabilities means that small increases in preference probabilities already close to 1 are just as incentivized as larger increases in preference probabilities around 50%, which may be undesirable. The maximisation of logit-preferences, or Elo score in game-theoretic terminology, can also have counter-intuitive effects, even in transitive settings (Bertrand et al., 2023).\n\nConsider the simple example where we have two actions y and y ′ such that p ∗ ( y ≻ y ′ ) = 1, i.e., y is always preferred to y ′ . Then the Bradley-Terry model would require that ( r ( y ) -r ( y ′ )) → + ∞ to satisfy (1). If we plug this into the optimal policy (7) then we would get that π ∗ ( y ′ ) π ∗ ( y ) = 0 (i.e., π ∗ ( y ′ ) = 0) irrespective of what constant τ is used for the KL-regularisation. Thus the strength of the KL-regularisation becomes weaker and weaker the more deterministic the preferences.\n\nThe weakness of the KL-regularisation becomes even more pronounced in the finite data regime, where we only have access to a sample estimate of the preference ˆ p ( y ≻ y ′ ). Even if the true preference is, e.g., p ∗ ( y ≻ y ′ ) = 0 . 8, empirically it can be very possible when we only have a few data points to estimate ˆ p ( y ≻ y ′ ) = 1, in which case the empirical optimal policy would make π ( y ′ ) = 0 for any τ . This means that overfitting can be a substantial empirical issue, especially when the context and action spaces are extremely large as it is for large language models.\n\nWhy may standard RLHF be more robust to this problem in practice? While a purported advantage of DPO is that it avoids the need to fit a reward function, we observe that in practice when empirical preference probabilities are in the set { 0 , 1 } , the reward function ends up being underfit . The optimal rewards in the presence of { 0 , 1 } preference probabilities are infinite, but these values are avoided, and indeed regularisation of the reward function has been observed to be an important aspect of RLHF training in practice (Christiano et al., 2017). This underfitting of the reward function is thus crucial in obtaining a final policy that is sufficiently regularised towards the reference policy π ref , and DPO , in avoiding the training of the reward function, loses the regularisation of the policy that the underfitted reward function affords.\n\nWhile standard empirical practices such as earlystopping can still be used as an additional form of regularisation to curtail this kind of overfitting, in the next section, we will introduce a modification of the Ψ PO objective such that the optimal empirical policy can be close to π ref even when preferences are deterministic.\n\n## 5 IPO : Ψ PO with identity mapping\n\nWe have observed in the previous section that DPO is prone to overfitting, and this stems from a combination of the unboundedness of Ψ, together with not training an explicit reward function. Not training a reward function directly is a clear advantage of DPO , but we would like to avoid the problems of overfitting as well.\n\nThis analysis of DPO motivates choices of Ψ which are bounded, ensuring that the KL regularisation in Equation 6 remains effective even in the regime of { 0 , 1 } -valued preferences, as it is often the case when working with empirical datasets. A particularly natural form of objective to consider is given by taking Ψ to be the identity mapping in Equation (6), leading to direct regularized optimisation of total preferences :\n\n<!-- formula-not-decoded -->\n\nThe standard approach to optimize the objective function of Equation (8) is through RLHF with the choice of reward r ( y ) = p ∗ ( y ≻ µ ). However both using RL and estimating the reward model r ( y ) can be costly. Inspired by DPO one would like to devise an empirical solution for the optimisation problem of Equation (8) which can directly learn from the preference dataset. Thus it would be able to avoid RL and reward modeling altogether.\n\n## 5.1 Derivations and Computationally Efficient Algorithm\n\nAs with DPO , it will be beneficial to re-express Equation (8) as an offline learning objective. To derive such an expression, we begin by following the derivation of Rafailov et al. (2023), manipulating the analytic expression for the optimal policy into a system of rootfinding problems. As in the previous section, we drop dependence on the context x from our notation, as all arguments can be applied on a per-context basis.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nFor any y, y ′ ∈ Supp ( π ref ), we therefore have\n\n<!-- formula-not-decoded -->\n\nBy letting\n\n<!-- formula-not-decoded -->\n\nand rearranging Equation (10), we obtain\n\n<!-- formula-not-decoded -->\n\nThe core idea now is to consider a policy π , define\n\n<!-- formula-not-decoded -->\n\nand aim to solve the equations:\n\n<!-- formula-not-decoded -->\n\nLoss for IPO . We now depart from the approach to the analysis employed by Rafailov et al. (2023), to obtain a novel offline formulation of Equation (6), in the specific case of Ψ as the identity function. In this case, Equation (12) reduces to\n\n<!-- formula-not-decoded -->\n\nWe begin by re-expressing these root-finding problems as a single optimisation problem L ( π ):\n\n<!-- formula-not-decoded -->\n\nOne can easily show that for the choice of π ∗ we have L ( π ∗ ) = 0. Thus π ∗ is a global minimizer of L ( π ). The following theorem establishes the uniqueness of this solution.\n\nTheorem 2 (Uniqueness of Global/Local Optima) . Assume that Supp ( µ ) = Supp ( π ref ) and define Π to be the set of policies π such that Supp ( π ) = Supp ( µ ). Then π ↦→ L ( π ) has a unique local/global minimum in Π, which is π ∗ .\n\nProof. By assumption, π ∗ ∈ Π, and by definition ∀ π ∈ Π , L ( π ) ≥ 0 as L ( π ) is an expectation of squared terms. Further, from Equation (11), it follows immediately that L ( π ∗ ) = 0, and so we deduce that π ∗ is a global optimum for L . We now show that there are no other local/global minima for L in Π.\n\nWe write J = Supp ( µ ). We parametrise the set Π via vectors of logits s ∈ R J , setting π s ( y ) = exp( s ( y )) / ∑ y ′ ∈ J exp( s ( y ′ )) for y ∈ J , and π s ( y ) = 0 otherwise. Let us write L ( s ) = L ( π s ) for the objective as a function of the logits s .\n\n<!-- formula-not-decoded -->\n\nThe objective is quadratic as a function of the logits s . Further, by expanding the quadratic above, we see that the loss can be expressed as a sum of squares\n\n<!-- formula-not-decoded -->\n\nplus linear and constant terms. This is therefore a positive-semidefinite quadratic, and hence is convex. We thus deduce that all local minimisers of the loss L ( s ) are global minimisers as well (Boyd and Vandenberghe, 2004, Chap. 4). We now notice since π s is a surjective continuous mapping from s to π one can easily show from the definition of local minimum that every local minimiser π of L corresponds to a set of local minimisers S π of L . Thus all local minimums of L are also global minimums as well.\n\nFinally, the only direction s in which the quadratic in Equation (15) does not increase away from 0 is when all bracketed terms remain 0; that is, in the direction (1 , . . . , 1) ∈ R J . Thus, L ( s ) is strictly convex, except in the direction (1 , . . . , 1). (Boyd and Vandenberghe, 2004, Chap. 3). However, modifying logits in the direction e = (1 , . . . , 1) does not modify the resulting policy π s , since, for y ∈ J ,\n\n<!-- formula-not-decoded -->\n\nThe strict convexity combined with the fact that π ∗ is a global minima proves that π ∗ is the unique global/local minima in Π (Boyd and Vandenberghe, 2004, Chap. 4).\n\n## 5.2 Sampled Loss for IPO\n\nIn order to obtain the sampled loss for IPO we need to show that we can build an unbiased estimate of the right-hand side of the equation (13). To this end, we consider the Population IPO Loss :\n\n<!-- formula-not-decoded -->\n\nwhere I ( y, y ′ ) is drawn from a Bernoulli distribution with mean p ∗ ( y ≻ y ′ ), i.e., I ( y, y ′ ) is 1 if y is preferred to y ′ (which happens with probability p ∗ ( y ≻ y ′ )), and 0 otherwise. This straightforwardly yields a sample-based loss that can be used, by sampling a pair ( y, y ′ ) from the preference dataset, and consulting the recorded preference to obtain a sample from I ( y, y ′ ). The following proposition justifies the switch from Equation (13) to Equation (16), by demonstrating their equality.\n\nProposition 3. The expressions in Equation (13) and Equation (16) are equal, up to an additive constant independent of π .\n\nProof. This equivalence is not completely trivial, since in general the conditional expectation\n\n<!-- formula-not-decoded -->\n\nis not equal to the corresponding quantity appearing in Equation (13), namely\n\n<!-- formula-not-decoded -->\n\nWe instead need to exploit some symmetry between the distributions of y and y ′ , and use the fact that h π ( y, y ′ ) decomposes as an additive function of y and y ′ . To show this equality of losses, it is enough to focus on the 'cross-terms' obtained when expanding the quadratics in Equations (13) and (16); that is, to show\n\n<!-- formula-not-decoded -->\n\nNow, starting with the right-hand side, and using the shorthand π y = log( π ( y )), π R y = log( π ref ( y )), p y = p ∗ ( y ≻ µ ), and similarly for y ′ , we have\n\n<!-- formula-not-decoded -->\n\nwhere we have used iid-ness of y and y ′ , and E y ∼ µ [ p y ] = 1 / 2. Turning to the left-hand side, we have\n\n<!-- formula-not-decoded -->\n\nwhere we use the fact that E y ′ ∼ µ I ( y, y ′ ) = p y and E y ∼ µ I ( y, y ′ ) = 1 -p y ′ . This demonstrates equality of the losses, as required.\n\nWe now discuss how to approximate the loss in Equation (16) with an empirical dataset. As in our earlier discussion, the empirical dataset D takes the form\n\n( y w,i , y l,i ) N i = i . Note that each datapoint ( y w , i , y l,i ) contributes two terms to an empirical approximation of Equation (16), with ( y, y ′ , I ( y, y ′ )) = ( y w,i , y l,i , 1), and also ( y, y ′ , I ( y, y ′ )) = ( y l,i , y w,i , 0). This symmetry is important to exploit, and leads to a reduction in the variance of the loss. The overall empirical loss is therefore given by\n\n<!-- formula-not-decoded -->\n\nwhich up to a constant equals:\n\n<!-- formula-not-decoded -->\n\nThis simplified form of the loss provides some valuable insights on the way in which IPO optimizes the policy π : IPO learns from preferences dataset simply by regressing the gap between log-likelihood ratios log( π ( y w ) /π ( y l )) and log( π ref ( y w ) /π ref ( y l )) to τ -1 2 . So the weaker the regularisation becomes, the higher would be the log-likelihood ratio of y w to y l . In other words IPO , unlike DPO , always regularizes its solution towards π ref by controlling the gap between the log-likelihood ratios log( π ( y w ) /π ( y l )) and log( π ref ( y w ) /π ref ( y l )), thus avoiding the over-fitting to the preference dataset. We summarize the sampled IPO in Algorithm 1:\n\n## Algorithm 1 Sampled IPO\n\nRequire: Dataset D of prompts, preferred and dispreferred generations x , y w and y l , respectively. A reference policy π ref\n\n- 1: Define\n\n<!-- formula-not-decoded -->\n\n- 2: Starting from π = π ref minimize\n\n<!-- formula-not-decoded -->\n\n## 5.3 Illustrative Examples\n\nTo illustrate the qualitative difference between our algorithm and DPO we will consider a few simple cases. For simplicity we assume there is no context x , i.e., we are in the bandit setting.\n\n## 5.3.1 Asymptotic Setting\n\nWe first consider the simple case where we have 2 actions only, y 1 and y 2 , and a deterministic preference between them: p ∗ ( y 1 ≻ y 2 ) = 1. Suppose we start with a uniform π ref and µ . We know from Section 4.2 that DPO will converge to the deterministic policy π ∗ ( y 1 ) = 1, π ∗ ( y 2 ) = 0 regardless of the value of τ . Thus even when the regularisation coefficient τ is very large, this is very different from the uniform π ref .\n\nNow, let us derive the optimal policy for IPO. We have p ∗ ( y 1 ≻ µ ) = 3 / 4 and p ∗ ( y 2 ≻ µ ) = 1 / 4. Plugging this into equation (9) with Ψ = I we get that π ∗ ( y 1 ) = exp(0 . 75 τ -1 ) exp(0 . 75 τ -1 )+exp(0 . 25 τ -1 ) = σ (0 . 5 τ -1 ), and π ∗ ( y 2 ) = σ ( -0 . 5 τ -1 ), where σ is the sigmoid function. Hence we see that if we have large regularisation as τ → + ∞ , then π ∗ converges to the uniform policy π ref , and on the flip side as τ → +0, then π ∗ ( y 1 ) → 1 and π ∗ ( y 2 ) → 0, which is the deterministic optimal policy. The regularisation parameter τ can now actually be used to control how close to π ref we are.\n\n## 5.4 Sampled Preferences\n\nSo far we relied on the closed-form optimal policy from Eq. (9) to study DPO and IPO 's stability, but this equation is not applicable to more complex settings where we only have access to sampled preference instead of p ⋆ . We can still however find accurate approximations of the optimal policy by choosing a parametrisation π θ and optimize θ with an empirical loss over a dataset and iterative gradient-based updates. We will use this approach to show two non-asymptotic examples where DPO over-fits the dataset of preferences and ignore π ref : when one action y wins against all others DPO pushes π θ ( y ) to 1 regardless of τ , and conversely when one action y never wins against the others DPO pushes π θ ( y ) to 0 again regardless of τ . In the same scenarios, IPO does not converge to these degenerate solutions but instead remains close to π ref based on the strength of the regularisation τ .\n\nFor both scenarios we consider a discrete space Y = { y a , y b , y c } with 3 actions, and select a dataset of pairs D = { ( y w,i , y l,j ) } . Given D , we leverage the empirical losses from Eq. 4 and Eq. 13 to find DPO 's and IPO 's optimal policy. We encode policies as π θ ( y i ) = softmax( θ ) i using a vector θ ∈ R 3 , and optimize them for 18000 steps using Adam (Kingma and Ba, 2014) with learning rate 0 . 01 and mini-batch size 9. Minibatches are constructed using uniform sampling with replacement from D . Both policies and losses are implemented using the flax python framework (Bradbury et al., 2018; Heek et al., 2023), and the Adam implementation is from optax (Babuschkin et al., 2020).\n\nFigure 1: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for D 1\n\n<!-- image -->\n\nFor each set of hyper-parameters we repeat the experiment 10 times with different seeds, and report mean and 95% confidence intervals. All experiments are executed on a modern cloud virtual machine with 4 cores and 32GB of ram.\n\nIPO Avoids Greedy Policies For the first example we sample each unique action pair once to collect a dataset D containing 3 observed preferences. Due to symmetries of pairwise preferences sampling only 3 preferences can results in only two outcomes (up to permutations of the actions):\n\n<!-- formula-not-decoded -->\n\nwhere we focus on D 1 , which represent a total ordering, rather than D 2 , which represent a cycle. The outcome of the experiment is reported in Fig. 1 in which, we report the learning curves for varying values of τ . We observe that DPO always converges to the deterministic policy for all values of τ . In other word DPO completely ignores the reference policy, no matter how strong is the regularisation term, and converges to the action which is preferred in the dataset. On the other hand, IPO prevent the policy from becoming greedy when the regularisation is strong.\n\nIPO Does not Exclude Actions In the first example DPO converges to a deterministic policy because one action strictly dominates all others and the loss continues to push up its likelihood until it saturates. The opposite effect happens for the logical opposite\n\nFigure 2: Comparison Between the Learning Curves of Action Probabilities of IPO and DPO for D 3\n\n<!-- image -->\n\ncondition, i.e., when one action does not have at least a victory in the dataset DPO will sets its probability to 0 regardless of τ . While this is less disruptive than the first example (a single probability is perturbed whereas previously the whole policy was warped by an over-achieving action) it is also much more common in real-world data. In particular, whenever the action space is large but the dataset small, some actions will necessarily be sampled rarely or only once, making it likely to never observe a victory. Especially because we do not have data on their performance π should stick close to π ref for safety, but DPO 's objective does not promote this.\n\nIn the final example the dataset consists of two observed preferences D 3 = { ( y a , y b ) , ( y b , y a ) } and leave the pair ( y a , y c ) completely unobserved. We compute solutions using Adam once again, and report the results in Fig. 2 for varying values of τ . We observe again here that DPO ignores the prior π ref completely, no matter how strong we regularize the objective, whereas IPO gradually decreases the probability of unobserved action with τ .\n\n## 6 Conclusion and Future Work\n\nWe presented a unified objective, called Ψ PO , for learning from preferences. It unifies RLHF and DPO methods. In addition, we introduced a particular case of Ψ PO , called IPO , that allows to learn directly from preferences without a reward modelling stage and without relying on the Bradley-Terry modelisation assumption\n\nthat assumes that pairwise preferences can be substituted with pointwise rewards. This is important because it allows to avoid the overfitting problem. This theoretical contribution is only useful in practice if an empirical sampled loss function can be derived. This is what we have done in Sec 5 where we show that IPO can be formulated as a root-finding problem from which an empirical sampled loss function can be derived. The IPO loss function is simple, easy to implement and theoretically justified. Finally, in Sec. 5.3 and Sec. 5.4, we provide illustrative examples where we highlight the instabilities of DPO when the preferences are fully-known as well as when they are sampled. Those minimal experiments are sufficient to prove that IPO is better suited to learn from sampled preferences than DPO . Future works should scale those experiments to more complex settings such as training language models on human preferences data.\n\n## References\n\n- Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, et al. The DeepMind JAX ecosystem, 2020, 2020. URL http://github.com/deepmind .\n- Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the Elo: Realworld games are transitive, not additive. In Proceedings of the International Conference on Artificial Intelligence and Statistics , 2023.\n- Stephen P. Boyd and Lieven Vandenberghe. Convex optimization . Cambridge University Press, 2004.\n- James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:// github.com/google/jax .\n- Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika , 39(3/4):324-345, 1952.\n- R´ obert Busa-Fekete, Bal´ azs Sz¨ or´ enyi, Paul Weng, Weiwei Cheng, and Eyke H¨ ullermeier. Preference-based reinforcement learning: Evolutionary direct policy search using a preference-based racing algorithm. Machine Learning , (3):327-351, 2014.\n- R´ obert Busa-Fekete, Bal´ azs Sz¨ orenyi, Paul Weng, Weiwei Cheng, and Eyke H¨ ullermeier. Preference-based evolutionary direct policy search. In Autonomous Learning Workshop @ ICRA , 2013.\n- Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, and Liwei Wang. Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In Proceedings of the International Conference on Machine Learning , 2022.\n- Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems , 2017.\n- Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision processes. In Proceedings of the International Conference on Machine Learning , 2019.\n- Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network li-\n- brary and ecosystem for JAX, 2023. URL http: //github.com/google/flax .\n- Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations , 2014.\n- Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets natural language: Synergies between functional and structural language learning. In Proceedings of the Annual Meeting of Association for Computational Linguistics , 2020.\n- Tyler Lu, D´ avid P´ al, and Martin P´ al. Contextual multi-armed bandits. In Proceedings of the International Conference on Artificial Intelligence and Statistics , 2010.\n- Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering language drift with seeded iterated learning. In Proceedings of the International Conference on Machine Learning , 2020.\n- Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick. Dueling posterior sampling for preference-based reinforcement learning. In Proceedings of the Conference on Uncertainty in Artificial Intelligence , 2020.\n\nOpenAI. Gpt-4 technical report, 2023.\n\n- Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller amd Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems , 2022.\n- Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling RL: Reinforcement learning with trajectory preferences. arXiv , 2023.\n- Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.\n- Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv , 2023.\n- Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. Unsupervised pretraining for sequence to sequence learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processings , 2016.\n\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv , 2017.\n- Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 2020.\n- Yuanhao Wang, Qinghua Liu, and Chi Jin. Is RLHF more difficult than standard RL? arXiv , 2023.\n- Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In Proceedings of the International Conference on Learning Representations , 2022.\n- Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv , 2023.\n\n## A Proofs\n\n## A.1 Existence and uniqueness of the regularized argmaximum\n\nFor completeness, we briefly recall the proof of existence and uniqueness of the argmaximum of the following regularized criterion that can also be found in the work of Rafailov et al. (2023):\n\n<!-- formula-not-decoded -->\n\nwhere S is a finite set, f ∈ R S a function mapping elements of S to real numbers, τ ∈ R ∗ + a strictly positive real number, δ ∈ ∆ S and η ∈ ∆ S are discrete probability distributions over S . In particular, we recall that a discrete probability distribution δ ∈ ∆ S can be identified as a positive real function δ ∈ R S + verifying:\n\n<!-- formula-not-decoded -->\n\nNow, if we define the softmax probability δ ∗ ∈ ∆ S as:\n\n<!-- formula-not-decoded -->\n\nthen, under the previous definitions, we have the following result:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof.\n\n## APPENDICES\n\nBy definition of the KL, we now that δ ∗ = arg max δ ∈ ∆ S [ -KL( δ || δ ∗ ) ] and as:\n\n<!-- formula-not-decoded -->\n\nwhere log ( ∑ s ′ ∈S η ( s ′ ) exp( τ -1 f ( s ′ )) ) is a constant (does not depend on δ ) and τ a positive multiplicative term, then -KL( δ || δ ∗ ) and L τ ( δ ) share the same argmaximum. This concludes the proof.\n\n̸\n\n## A.2 Non-uniqueness when Supp ( π ( · )) = Supp ( µ ) :\n\nNotice that if we search for a solution where the support of π is strictly larger than that of µ then there could be multiple solutions. Let us illustrate this case with a simple example. Consider a single state x and 3 actions y 1 , y 2 , y 3 . The reference policy π ref is uniform over { y 1 , y 2 , y 3 } and the policy µ assigns a probability 1 / 2 to both y 1 and y 2 and 0 probability to y 3 .\n\nThus the loss is L ( π ) = 2 ( τ -1 ( p ∗ ( y 1 ≻ µ ) -p ∗ ( y 2 ≻ µ ) ) -log π ( y 1 ) π ( y 2 ) ) 2 . We deduce that any policy π = ( p, q, 1 -p -q ) such that p q = e τ -1 ( p ∗ ( y 1 ≻ µ ) -p ∗ ( y 2 ≻ µ )) is a global minimum of L ( π ).\n\nIn particular there are an infinity of solutions different from the optimal solution π ∗ . The problem comes from the fact that when the support of µ does not cover the whole action space there are not enough constraints to uniquely characterize π ∗ . Assuming that the supports of π ref and µ coincide enables us to recover uniqueness of the solution, as proven in Theorem 2.\n\n## B Additional results\n\nIn this section, we show the equivalence of DPO and RLHF , regardless of whether the preference model p ∗ corresponds to a Bradley-Terry model. Note that the assumption of the existence of a minimizer is to exclude cases where the loss is minimized by taking the rewards of certain actions to + / -∞ .\n\nProposition 4. Consider a preference model p ∗ such that there exists a minimizer to the Bradley-Terry loss\n\n<!-- formula-not-decoded -->\n\nThen, the optimal policy for the DPO objective in Equation (4) and for the RLHF objective in Equation (3) with reward model given as the minimizer to the Bradley-Terry loss above are identical, regardless of whether or not p ∗ corresponds to a Bradley-Terry preference model.\n\nProof. Recall that the optimal policy π ∗ r for a given reward function r for the objective in Equation (3) is given by π ∗ r ( y | x ) ∝ π ref ( y | x ) exp( τ -1 r ( x, y )). It therefore follows that\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIn words, the value of the Bradley-Terry reward objective for r is the value of the DPO objective for π ∗ r . We recall also that the map r ↦→ π ∗ r is surjective.\n\nNow, suppose r is optimal for the Bradley-Terry reward objective, meaning that π ∗ r is optimal for the RLHF objective. If π ∗ r is not optimal for the DPO objective, then there exists another policy π ′ that obtains a strictly lower value for the DPO loss. But then there exists a reward function r ′ such that π ′ = π ∗ r ′ , such as r ′ ( x, y ) = τ log( π ′ ( y | x ) /π ref ( y | x )), and this r ′ therefore obtains a lower Bradley-Terry loss than r , a contradiction.\n\nSimilarly, if π ∗ is optimal for the DPO objective, the corresponding reward function r ( x, y ) = τ log( π ∗ ( y | x ) /π ref ( y | x )) must be optimal for the Bradley-Terry reward loss. The corresponding optimizer for the RLHF objective is then given by π ( y | x ) ∝ π ref ( y | x ) exp( τ -1 τ log( π ∗ ( y | x ) /π ref ( y | x ))) = π ∗ ( y | x ), as required.",
  "tables": [],
  "stats": {
    "pages": 13,
    "chunksCreated": 64,
    "totalCharacters": 42046,
    "totalWords": 7774,
    "numTables": 0,
    "processingTimeMs": 12370
  }
}